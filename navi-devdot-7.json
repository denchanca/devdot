[{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.4.x/resources/state-upgrade",
  "text": "// Other Resource methods are omitted in this example var _ resource.Resource = &ThingResource{} var _ resource.ResourceWithUpgradeState = &ThingResource{} var ThingResourceTftypesDataV0 = tftypes.Object{ AttributeTypes: map[string]tftypes.Type{ \"id\": tftypes.String, \"optional_attribute\": tftypes.Bool, \"required_attribute\": tftypes.Bool, }, } var ThingResourceTftypesDataV1 = tftypes.Object{ AttributeTypes: map[string]tftypes.Type{ \"id\": tftypes.String, \"optional_attribute\": tftypes.String, \"required_attribute\": tftypes.String, }, } type ThingResource struct{/* ... */} func (r *ThingResource) Schema(_ context.Context, _ resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"id\": schema.StringAttribute{ Computed: true, }, \"optional_attribute\": schema.StringAttribute{ // As compared to prior schema.BoolAttribute below Optional: true, }, \"required_attribute\": schema.StringAttribute{ // As compared to prior schema.BoolAttribute below Required: true, }, }, // The resource has a prior state version of 0, which had the attribute // types of types.BoolType as shown below. Version: 1, } } func (r *ThingResource) UpgradeState(ctx context.Context) map[int64]resource.StateUpgrader { return map[int64]resource.StateUpgrader{ // State upgrade implementation from 0 (prior state version) to 1 (Schema.Version) 0: { StateUpgrader: func(ctx context.Context, req resource.UpgradeStateRequest, resp *resource.UpgradeStateResponse) { // Refer also to the RawState type JSON field which can be used // with json.Unmarshal() rawStateValue, err := req.RawState.Unmarshal(ThingResourceTftypesDataV0) if err != nil { resp.Diagnostics.AddError( \"Unable to Unmarshal Prior State\", err.Error(), ) return } var rawState map[string]tftypes.Value if err := rawStateValue.As(&rawState); err != nil { resp.Diagnostics.AddError( \"Unable to Convert Prior State\", err.Error(), ) return } var optionalAttributeString *string if !rawState[\"optional_attribute\"].IsNull() { var optionalAttribute bool if err := rawState[\"optional_attribute\"].As(&optionalAttribute); err != nil { resp.Diagnostics.AddAttributeError( path.Root(\"optional_attribute\"), \"Unable to Convert Prior State\", err.Error(), ) return } v := fmt.Sprintf(\"%t\", optionalAttribute) optionalAttributeString = &v } var requiredAttribute bool if err := rawState[\"required_attribute\"].As(&requiredAttribute); err != nil { resp.Diagnostics.AddAttributeError( path.Root(\"required_attribute\"), \"Unable to Convert Prior State\", err.Error(), ) return } dynamicValue, err := tfprotov6.NewDynamicValue( ThingResourceTftypesDataV1, tftypes.NewValue(ThingResourceTftypesDataV1, map[string]tftypes.Value{ \"id\": rawState[\"id\"], \"optional_attribute\": tftypes.NewValue(tftypes.String, optionalAttributeString), \"required_attribute\": tftypes.NewValue(tftypes.String, fmt.Sprintf(\"%t\", requiredAttribute)), }), ) if err != nil { resp.Diagnostics.AddError( \"Unable to Convert Upgraded State\", err.Error(), ) return } resp.DynamicValue = &dynamicValue }, }, } }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.4.x/resources/default",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.4.x/resources/delete",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.4.x/resources/plan-modification",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.4.x/resources/import",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.4.x/resources/private-state",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.4.x/resources/timeouts",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.4.x/data-sources/timeouts",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.4.x/accessing-values",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.5.x/handling-data/terraform-concepts",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.5.x/handling-data/accessing-values",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.5.x/resources/state-upgrade",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.5.x/data-sources/configure",
  "text": "// With the datasource.DataSource implementation type ThingDataSource struct { client *http.Client } func (d *ThingDataSource) Configure(ctx context.Context, req datasource.ConfigureRequest, resp *datasource.ConfigureResponse) { // Prevent panic if the provider has not been configured. if req.ProviderData == nil { return } client, ok := req.ProviderData.(*http.Client) if !ok { resp.Diagnostics.AddError( \"Unexpected Data Source Configure Type\", fmt.Sprintf(\"Expected *http.Client, got: %T. Please report this issue to the provider developers.\", req.ProviderData), ) return } d.client = client } func (d *ThingDataSource) Read(ctx context.Context, req datasource.ReadRequest, resp *datasource.ReadResponse) { // Prevent panic if the provider has not been configured. if d.client == nil { resp.Diagnostics.AddError( \"Unconfigured HTTP Client\", \"Expected configured HTTP client. Please report this issue to the provider developers.\", ) return } httpResp, err := d.client.Get(\"https://example.com\") /* ... */ }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.5.x/resources/configure",
  "text": "// With the resource.Resource implementation type ThingResource struct { client *http.Client } func (r *ThingResource) Configure(ctx context.Context, req resource.ConfigureRequest, resp *resource.ConfigureResponse) { // Prevent panic if the provider has not been configured. if req.ProviderData == nil { return } client, ok := req.ProviderData.(*http.Client) if !ok { resp.Diagnostics.AddError( \"Unexpected Resource Configure Type\", fmt.Sprintf(\"Expected *http.Client, got: %T. Please report this issue to the provider developers.\", req.ProviderData), ) return } r.client = client } func (r *ThingResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { // Prevent panic if the provider has not been configured. if r.client == nil { resp.Diagnostics.AddError( \"Unconfigured HTTP Client\", \"Expected configured HTTP client. Please report this issue to the provider developers.\", ) return } httpResp, err := r.client.Get(\"https://example.com\") /* ... */ }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.5.x/data-sources/timeouts",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.5.x/accessing-values",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.5.x/resources/create",
  "text": "// ThingResource defines the resource implementation. // Some resource.Resource interface methods are omitted for brevity. type ThingResource struct { // client is configured via a Configure method, which is not shown in this // example for brevity. Refer to the Configure Resources documentation for // additional details for setting up resources with external clients. client *http.Client } // ThingResourceModel describes the Terraform resource data model to match the // resource schema. type ThingResourceModel struct { Name types.String `tfsdk:\"name\"` Id types.String `tfsdk:\"id\"` } // ThingResourceAPIModel describes the API data model. type ThingResourceAPIModel struct { Name string `json:\"name\"` Id string `json:\"id\"` } func (r ThingResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"name\": schema.StringAttribute{ MarkdownDescription: \"Name of the thing to be saved in the service.\", Required: true, }, \"id\": schema.StringAttribute{ Computed: true, MarkdownDescription: \"Service generated identifier for the thing.\", PlanModifiers: planmodifier.String{ stringplanmodifier.UseStateForUnknown(), }, }, }, MarkdownDescription: \"Manages a thing.\", } } func (r ThingResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data ThingResourceModel // Read Terraform plan data into the model resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // Convert from Terraform data model into API data model createReq := ThingResourceAPIModel{ Name: data.Name.ValueString(), } httpReqBody, err := json.Marshal(createReq) if err != nil { resp.Diagnostics.AddError( \"Unable to Create Resource\", \"An unexpected error occurred while creating the resource create request. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Create HTTP request httpReq := http.NewRequestWithContext( ctx, http.MethodPost, \"http://example.com/things\", bytes.NewBuffer(httpReqBody), ) // Send HTTP request httpResp, err := d.client.Do(httpReq) defer httpResp.Body.Close() if err != nil { resp.Diagnostics.AddError( \"Unable to Create Resource\", \"An unexpected error occurred while attempting to create the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Error: \"+err.Error(), ) return } // Return error if the HTTP status code is not 200 OK if httpResp.StatusCode != http.StatusOK { resp.Diagnostics.AddError( \"Unable to Create Resource\", \"An unexpected error occurred while attempting to create the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Status: \"+httpResp.Status, ) return } var createResp ThingResourceAPIModel err := json.NewDecoder(httpResp.Body).Decode(&createResp) if err != nil { resp.Diagnostics.AddError( \"Unable to Create Resource\", \"An unexpected error occurred while parsing the resource creation response. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Convert from the API data model to the Terraform data model // and set any unknown attribute values. data.Id = types.StringValue(createResp.Id) // Save data into Terraform state resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.5.x/resources/read",
  "text": "// ThingResource defines the resource implementation. // Some resource.Resource interface methods are omitted for brevity. type ThingResource struct { // client is configured via a Configure method, which is not shown in this // example for brevity. Refer to the Configure Resources documentation for // additional details for setting up resources with external clients. client *http.Client } // ThingResourceModel describes the Terraform resource data model to match the // resource schema. type ThingResourceModel struct { Name types.String `tfsdk:\"name\"` Id types.String `tfsdk:\"id\"` } // ThingResourceAPIModel describes the API data model. type ThingResourceAPIModel struct { Name string `json:\"name\"` Id string `json:\"id\"` } func (r ThingResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"name\": schema.StringAttribute{ MarkdownDescription: \"Name of the thing to be saved in the service.\", Required: true, }, \"id\": schema.StringAttribute{ Computed: true, MarkdownDescription: \"Service generated identifier for the thing.\", PlanModifiers: planmodifier.String{ stringplanmodifier.UseStateForUnknown(), }, }, }, MarkdownDescription: \"Manages a thing.\", } } func (r *ThingResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { var data ThingResourceModel // Read Terraform prior state data into the model resp.Diagnostics.Append(req.State.Get(ctx, &data)...) // Convert from Terraform data model into API data model readReq := ThingResourceAPIModel{ Id: data.Id.ValueString(), Name: data.Name.ValueString(), } httpReqBody, err := json.Marshal(readReq) if err != nil { resp.Diagnostics.AddError( \"Unable to Refresh Resource\", \"An unexpected error occurred while creating the resource read request. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Create HTTP request httpReq := http.NewRequestWithContext( ctx, http.MethodPut, \"http://example.com/things\", bytes.NewBuffer(httpReqBody), ) httpResp, err := d.client.Do(httpReq) defer httpResp.Body.Close() if err != nil { resp.Diagnostics.AddError( \"Unable to Refresh Resource\", \"An unexpected error occurred while attempting to refresh resource state. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Error: \"+err.Error(), ) return } // Treat HTTP 404 Not Found status as a signal to recreate resource // and return early if httpResp.StatusCode == http.StatusNotFound { resp.State.RemoveResource(ctx) return } var readResp ThingResourceAPIModel err := json.NewDecoder(httpResp.Body).Decode(&readResp) if err != nil { resp.Diagnostics.AddError( \"Unable to Refresh Resource\", \"An unexpected error occurred while parsing the resource read response. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Convert from the API data model to the Terraform data model // and refresh any attribute values. data.Name = types.StringValue(readResp.Name) // Save updated data into Terraform state resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.5.x/resources/update",
  "text": "// ThingResource defines the resource implementation. // Some resource.Resource interface methods are omitted for brevity. type ThingResource struct { // client is configured via a Configure method, which is not shown in this // example for brevity. Refer to the Configure Resources documentation for // additional details for setting up resources with external clients. client *http.Client } // ThingResourceModel describes the Terraform resource data model to match the // resource schema. type ThingResourceModel struct { Name types.String `tfsdk:\"name\"` Id types.String `tfsdk:\"id\"` } // ThingResourceAPIModel describes the API data model. type ThingResourceAPIModel struct { Name string `json:\"name\"` Id string `json:\"id\"` } func (r ThingResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"name\": schema.StringAttribute{ MarkdownDescription: \"Name of the thing to be saved in the service.\", Required: true, }, \"id\": schema.StringAttribute{ Computed: true, MarkdownDescription: \"Service generated identifier for the thing.\", PlanModifiers: planmodifier.String{ stringplanmodifier.UseStateForUnknown(), }, }, }, MarkdownDescription: \"Manages a thing.\", } } func (r ThingResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data ThingResourceModel // Read Terraform plan data into the model resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) // Convert from Terraform data model into API data model updateReq := ThingResourceAPIModel{ Id: data.Id.ValueString(), Name: data.Name.ValueString(), } httpReqBody, err := json.Marshal(updateReq) if err != nil { resp.Diagnostics.AddError( \"Unable to Update Resource\", \"An unexpected error occurred while creating the resource update request. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Create HTTP request httpReq := http.NewRequestWithContext( ctx, http.MethodPut, \"http://example.com/things\", bytes.NewBuffer(httpReqBody), ) // Send HTTP request httpResp, err := d.client.Do(httpReq) defer httpResp.Body.Close() if err != nil { resp.Diagnostics.AddError( \"Unable to Update Resource\", \"An unexpected error occurred while attempting to update the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Error: \"+err.Error(), ) return } // Return error if the HTTP status code is not 200 OK if httpResp.StatusCode != http.StatusOK { resp.Diagnostics.AddError( \"Unable to Update Resource\", \"An unexpected error occurred while attempting to update the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Status: \"+httpResp.Status, ) return } // Save updated data into Terraform state resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.5.x/resources/delete",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.5.x/resources/plan-modification",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.5.x/resources/default",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.5.x/resources/private-state",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.5.x/resources/import",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.5.x/resources/timeouts",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.3.x/data-sources/configure",
  "text": "// With the datasource.DataSource implementation type ThingDataSource struct { client *http.Client } func (d *ThingDataSource) Configure(ctx context.Context, req datasource.ConfigureRequest, resp *datasource.ConfigureResponse) { // Prevent panic if the provider has not been configured. if req.ProviderData == nil { return } client, ok := req.ProviderData.(*http.Client) if !ok { resp.Diagnostics.AddError( \"Unexpected Data Source Configure Type\", fmt.Sprintf(\"Expected *http.Client, got: %T. Please report this issue to the provider developers.\", req.ProviderData), ) return } d.client = client } func (d *ThingDataSource) Read(ctx context.Context, req datasource.ReadRequest, resp *datasource.ReadResponse) { // Prevent panic if the provider has not been configured. if d.client == nil { resp.Diagnostics.AddError( \"Unconfigured HTTP Client\", \"Expected configured HTTP client. Please report this issue to the provider developers.\", ) return } httpResp, err := d.client.Get(\"https://example.com\") /* ... */ }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.3.x/resources/configure",
  "text": "// With the resource.Resource implementation type ThingResource struct { client *http.Client } func (r *ThingResource) Configure(ctx context.Context, req resource.ConfigureRequest, resp *resource.ConfigureResponse) { // Prevent panic if the provider has not been configured. if req.ProviderData == nil { return } client, ok := req.ProviderData.(*http.Client) if !ok { resp.Diagnostics.AddError( \"Unexpected Resource Configure Type\", fmt.Sprintf(\"Expected *http.Client, got: %T. Please report this issue to the provider developers.\", req.ProviderData), ) return } r.client = client } func (r *ThingResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { // Prevent panic if the provider has not been configured. if r.client == nil { resp.Diagnostics.AddError( \"Unconfigured HTTP Client\", \"Expected configured HTTP client. Please report this issue to the provider developers.\", ) return } httpResp, err := r.client.Get(\"https://example.com\") /* ... */ }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.3.x/handling-data/terraform-concepts",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.3.x/resources/state-upgrade",
  "text": "// Other Resource methods are omitted in this example var _ resource.Resource = &ThingResource{} var _ resource.ResourceWithUpgradeState = &ThingResource{} var ThingResourceTftypesDataV0 = tftypes.Object{ AttributeTypes: map[string]tftypes.Type{ \"id\": tftypes.String, \"optional_attribute\": tftypes.Bool, \"required_attribute\": tftypes.Bool, }, } var ThingResourceTftypesDataV1 = tftypes.Object{ AttributeTypes: map[string]tftypes.Type{ \"id\": tftypes.String, \"optional_attribute\": tftypes.String, \"required_attribute\": tftypes.String, }, } type ThingResource struct{/* ... */} func (r *ThingResource) Schema(_ context.Context, _ resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"id\": schema.StringAttribute{ Computed: true, }, \"optional_attribute\": schema.StringAttribute{ // As compared to prior schema.BoolAttribute below Optional: true, }, \"required_attribute\": schema.StringAttribute{ // As compared to prior schema.BoolAttribute below Required: true, }, }, // The resource has a prior state version of 0, which had the attribute // types of types.BoolType as shown below. Version: 1, } } func (r *ThingResource) UpgradeState(ctx context.Context) map[int64]resource.StateUpgrader { return map[int64]resource.StateUpgrader{ // State upgrade implementation from 0 (prior state version) to 1 (Schema.Version) 0: { StateUpgrader: func(ctx context.Context, req resource.UpgradeStateRequest, resp *resource.UpgradeStateResponse) { // Refer also to the RawState type JSON field which can be used // with json.Unmarshal() rawStateValue, err := req.RawState.Unmarshal(ThingResourceTftypesDataV0) if err != nil { resp.Diagnostics.AddError( \"Unable to Unmarshal Prior State\", err.Error(), ) return } var rawState map[string]tftypes.Value if err := rawStateValue.As(&rawState); err != nil { resp.Diagnostics.AddError( \"Unable to Convert Prior State\", err.Error(), ) return } var optionalAttributeString *string if !rawState[\"optional_attribute\"].IsNull() { var optionalAttribute bool if err := rawState[\"optional_attribute\"].As(&optionalAttribute); err != nil { resp.Diagnostics.AddAttributeError( path.Root(\"optional_attribute\"), \"Unable to Convert Prior State\", err.Error(), ) return } v := fmt.Sprintf(\"%t\", optionalAttribute) optionalAttributeString = &v } var requiredAttribute bool if err := rawState[\"required_attribute\"].As(&requiredAttribute); err != nil { resp.Diagnostics.AddAttributeError( path.Root(\"required_attribute\"), \"Unable to Convert Prior State\", err.Error(), ) return } dynamicValue, err := tfprotov6.NewDynamicValue( ThingResourceTftypesDataV1, tftypes.NewValue(ThingResourceTftypesDataV1, map[string]tftypes.Value{ \"id\": rawState[\"id\"], \"optional_attribute\": tftypes.NewValue(tftypes.String, optionalAttributeString), \"required_attribute\": tftypes.NewValue(tftypes.String, fmt.Sprintf(\"%t\", requiredAttribute)), }), ) if err != nil { resp.Diagnostics.AddError( \"Unable to Convert Upgraded State\", err.Error(), ) return } resp.DynamicValue = &dynamicValue }, }, } }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.3.x/handling-data/accessing-values",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.3.x/resources/create",
  "text": "// ThingResource defines the resource implementation. // Some resource.Resource interface methods are omitted for brevity. type ThingResource struct { // client is configured via a Configure method, which is not shown in this // example for brevity. Refer to the Configure Resources documentation for // additional details for setting up resources with external clients. client *http.Client } // ThingResourceModel describes the Terraform resource data model to match the // resource schema. type ThingResourceModel struct { Name types.String `tfsdk:\"name\"` Id types.String `tfsdk:\"id\"` } // ThingResourceAPIModel describes the API data model. type ThingResourceAPIModel struct { Name string `json:\"name\"` Id string `json:\"id\"` } func (r ThingResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"name\": schema.StringAttribute{ MarkdownDescription: \"Name of the thing to be saved in the service.\", Required: true, }, \"id\": schema.StringAttribute{ Computed: true, MarkdownDescription: \"Service generated identifier for the thing.\", PlanModifiers: planmodifier.String{ stringplanmodifier.UseStateForUnknown(), }, }, }, MarkdownDescription: \"Manages a thing.\", } } func (r ThingResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data *ThingResourceModel // Read Terraform plan data into the model resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // Convert from Terraform data model into API data model createReq := ThingResourceAPIModel{ Name: data.Name.ValueString(), } httpReqBody, err := json.Marshal(createReq) if err != nil { resp.Diagnostics.AddError( \"Unable to Create Resource\", \"An unexpected error occurred while creating the resource create request. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Create HTTP request httpReq := http.NewRequestWithContext( ctx, http.MethodPost, \"http://example.com/things\", bytes.NewBuffer(httpReqBody), ) // Send HTTP request httpResp, err := d.client.Do(httpReq) defer httpResp.Body.Close() if err != nil { resp.Diagnostics.AddError( \"Unable to Create Resource\", \"An unexpected error occurred while attempting to create the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Error: \"+err.Error(), ) return } // Return error if the HTTP status code is not 200 OK if httpResp.StatusCode != http.StatusOK { resp.Diagnostics.AddError( \"Unable to Create Resource\", \"An unexpected error occurred while attempting to create the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Status: \"+httpResp.Status, ) return } var createResp ThingResourceAPIModel err := json.NewDecoder(httpResp.Body).Decode(&createResp) if err != nil { resp.Diagnostics.AddError( \"Unable to Create Resource\", \"An unexpected error occurred while parsing the resource creation response. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Convert from the API data model to the Terraform data model // and set any unknown attribute values. data.Id = types.StringValue(createResp.Id) // Save data into Terraform state resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.3.x/resources/read",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.3.x/resources/update",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.3.x/resources/delete",
  "text": "// ThingResource defines the resource implementation. // Some resource.Resource interface methods are omitted for brevity. type ThingResource struct { // client is configured via a Configure method, which is not shown in this // example for brevity. Refer to the Configure Resources documentation for // additional details for setting up resources with external clients. client *http.Client } // ThingResourceModel describes the Terraform resource data model to match the // resource schema. type ThingResourceModel struct { Name types.String `tfsdk:\"name\"` Id types.String `tfsdk:\"id\"` } // ThingResourceAPIModel describes the API data model. type ThingResourceAPIModel struct { Name string `json:\"name\"` Id string `json:\"id\"` } func (r ThingResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"name\": schema.StringAttribute{ MarkdownDescription: \"Name of the thing to be saved in the service.\", Required: true, }, \"id\": schema.StringAttribute{ Computed: true, MarkdownDescription: \"Service generated identifier for the thing.\", PlanModifiers: planmodifier.String{ stringplanmodifier.UseStateForUnknown(), }, }, }, MarkdownDescription: \"Manages a thing.\", } } func (r ThingResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { var data ThingResourceModel // Read Terraform prior state data into the model resp.Diagnostics.Append(req.State.Get(ctx, &data)...) // Convert from Terraform data model into API data model readReq := ThingResourceAPIModel{ Id: data.Id.ValueString(), Name: data.Name.ValueString(), } httpReqBody, err := json.Marshal(readReq) if err != nil { resp.Diagnostics.AddError( \"Unable to Delete Resource\", \"An unexpected error occurred while creating the resource d request. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Create HTTP request httpReq := http.NewRequestWithContext( ctx, http.MethodDelete, \"http://example.com/things\", bytes.NewBuffer(httpReqBody), ) // Send HTTP request httpResp, err := d.client.Do(httpReq) defer httpResp.Body.Close() if err != nil { resp.Diagnostics.AddError( \"Unable to Delete Resource\", \"An unexpected error occurred while attempting to delete the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Error: \"+err.Error(), ) return } // Return error if the HTTP status code is not 200 OK or 404 Not Found if httpResp.StatusCode != http.StatusNotFound && httpResp.StatusCode != http.StatusOK { resp.Diagnostics.AddError( \"Unable to Delete Resource\", \"An unexpected error occurred while attempting to delete the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Status: \"+httpResp.Status, ) return } // If the logic reaches here, it implicitly succeeded and will remove // the resource from state if there are no other errors. } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.3.x/resources/default",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.3.x/resources/plan-modification",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.3.x/resources/import",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.3.x/resources/private-state",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.3.x/resources/timeouts",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.3.x/data-sources/timeouts",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.3.x/accessing-values",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.2.x/data-sources/configure",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.2.x/resources/configure",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.2.x/handling-data/terraform-concepts",
  "text": "Schemas specify the fields that a provider, resource or data source configuration can have.\nThe fields defined within the schema are either attributes or blocks.\nAttributes and blocks within Terraform configuration have different syntax, either using, or omitting an equals sign in their definition, respectively.\nProviders, resources and data sources have their own type-specific schema, which use type-specific attributes and blocks.\nThe following examples use a resource but the same schema attribute and block types are also defined with the schema for providers and data sources. The examples focus on type-related information, the Optional field is included just to provide working examples but refer to Schema for information about the other fields that can be defined within attributes and blocks.\nAttributes are used to set values.\nAttribute TypeDescription\nBoolAttribute\tBoolean values (i.e., true/false)\t\nFloat64Attribute\t64 bit floating point number values\t\nInt64Attribute\t64 bit integer number values\t\nNumberAttribute\tGeneric number with up to 512 bits of floating point or integer precision\t\nStringAttribute\tString values\t\nListAttribute\tList with a single element type (e.g., types.StringType)\t\nMapAttribute\tMap with a single element type (e.g., types.Int64Type)\t\nSetAttribute\tSet with a single element type (e.g., types.BoolType)\t\nObjectAttribute\tObject with only type information for underlying attributes\t\nListNestedAttribute\tList containing nested objects where the object attributes can be fully defined\t\nMapNestedAttribute\tMap containing nested objects where the object attributes can be fully defined\t\nSetNestedAttribute\tSet containing nested objects where the object attributes can be fully defined\t\nSingleNestedAttribute\tSingle object where the object attributes can be fully defined\t\nRefer to Attributes - Terraform Configuration and Schema for examples of Terraform configuration and schema that illustrate the usage of the various types of schema attributes.\nThe Terraform language uses a block as a container for other attributes and blocks. Terraform implements many top level blocks, such as provider and resource, while providers can implement nested blocks in their schema to enable practitioners to configure data.\nThe available nesting modes are:\nList: Ordered collection of objects\nSet: Unordered collection of objects\nSingle: One object\nRefer to Blocks - Terraform Configuration for examples of Terraform configuration and schema that illustrate the usage of nested blocks."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.2.x/handling-data/accessing-values",
  "text": "The configuration, plan, and state data is represented as an object, and accessed like an object. Refer to the conversion rules for an explanation on how objects can be converted into Go types.\nRefer to the conversion rules for more information about supported Go types."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.2.x/handling-data/conversion-rules",
  "text": "Plugin Development - Framework: Conversion Rules | Terraform\nWarning: It can be tempting to use Go types instead of attr.Value implementations when the provider doesn't care about the distinction between an empty value, unknown, and null. But if Terraform has a null or unknown value and the provider asks the framework to store it in a type that can't hold it, Get will return an error. Make sure the types you are using can hold the values they might contain!\nString\nStrings can be automatically converted to Go's string type (or any aliases of it, like type MyString string) as long as the string value is not null or unknown.\nNumber\nNumbers can be automatically converted to the following numeric types (or any aliases of them, like type MyNumber int) as long as the number value is not null or unknown:\nint, int8, int16, int32, int64\nuint, uint8, uint16, uint32, uint64\nfloat32, float64\n*big.Int, *big.Float\nAn error will be returned if the value of the number cannot be stored in the numeric type supplied because of an overflow or other loss of precision.\nBoolean\nBooleans can be automatically converted to Go's bool type (or any aliases of it, like type MyBoolean bool) as long as the boolean value is not null or unknown.\nList\nLists can be automatically converted to any Go slice type (or alias of a Go slice type, like type MyList []string), with the elements either being attr.Value implementations or being converted according to these rules. Go slice types are considered capable of handling null values; the slice will be set to nil. The Get method will still return an error for unknown list values.\nMap\nMaps can be automatically converted to any Go map type with string keys (or any alias of a Go map type with string keys, like type MyMap map[string]int), with the elements either being attr.Value implementations or being converted according to these rules. Go map types are considered capable of handling null values; the map will be set to nil. The Get method will still return an error for unknown map values.\nObject\nObjects can be automatically converted to any Go struct type with that follows these constraints:\nEvery property on the struct must have a tfsdk struct tag.\nThe tfsdk struct tag must name an attribute in the object that it is being mapped to or be set to - to explicitly declare it does not map to an attribute in the object.\nEvery attribute in the object must have a corresponding struct tag.\nThese rules help prevent typos and human error from unwittingly discarding information by failing as early, consistently, and loudly as possible.\nProperties can either be attr.Value implementations or will be converted according to these rules.\nUnknown and null objects cannot be represented as structs and will return an error. Their attributes may contain unknown or null values if the attribute's type can hold them.\nPointers\nPointers behave exactly like the type they are referencing, except they can hold null values. A pointer will be set to nil when representing a null value; otherwise, the conversion rules for that type will apply.\nDetected Interfaces\nGet detects and utilizes the following interfaces, if the target implements them.\nValueConverter\nIf a value is being set on a Go type that implements the tftypes.ValueConverter interface, that interface will be delegated to to handle the conversion.\nUnknownable\nIf the value is being set on a Go type that fills the Unknownable interface:\ntype Unknownable interface { SetUnknown(context.Context, bool) error SetValue(context.Context, interface{}) error GetUnknown(context.Context) bool GetValue(context.Context) interface{} } \nIt will be considered capable of handling unknown values, and those methods will be used to populate it and retrieve its value. The interface{} being passed and retrieved will be of a type that can be passed to tftypes.NewValue.\nNullable\nIf the value is being set on a Go type that fills the Nullable interface:\ntype Nullable interface { SetNull(context.Context, bool) error SetValue(context.Context, interface{}) error GetNull(context.Context) bool GetValue(context.Context) interface{} } \nIt will be considered capable of handling null values, and those methods will be used to populate it and retrieve its value. The interface{} being passed and retrieved will be of a type that can be passed to tftypes.NewValue.\nThe following is a list of schema types and the Go types they know how to accept in Set and SetAttribute.\nString\nStrings can be automatically created from Go's string type (or any aliases of it, like type MyString string).\nNumber\nNumbers can be automatically created from the following numeric types (or any aliases of them, like type MyNumber int):\nint, int8, int16, int32, int64\nuint, uint8, uint16, uint32, uint64\nfloat32, float64\n*big.Int, *big.Float\nBoolean\nBooleans can be automatically created from Go's bool type (or any aliases of it, like type MyBoolean bool).\nList\nLists can be automatically created from any Go slice type (or alias of a Go slice type, like type MyList []string), with the elements either being attr.Value implementations or being converted according to these rules.\nMap\nMaps can be automatically created from any Go map type with string keys (or any alias of a Go map type with string keys, like type MyMap map[string]int), with the elements either being attr.Value implementations or being converted according to these rules.\nObject\nObjects can be automatically created from any Go struct type with that follows these constraints:\nEvery property on the struct must have a tfsdk struct tag.\nThe tfsdk struct tag must name an attribute in the object that it is being mapped to or be set to - to explicitly declare it does not map to an attribute in the object.\nEvery attribute in the object must have a corresponding struct tag.\nThese rules help prevent typos and human error from unwittingly discarding information by failing as early, consistently, and loudly as possible.\nProperties can either be attr.Value implementations or will be converted according to these rules.\nPointers\nA nil pointer will be treated as a null value. Otherwise, the rules for the type the pointer is referencing apply.\nDetected Interfaces\nSet detects and utilizes the following interfaces, if the target implements them.\nValueCreator\nIf a value is set on a Go type that implements the tftypes.ValueCreator interface, that interface will be delegated to to handle the conversion.\nUnknownable\nIf a value is set on a Go type that fills the Unknownable interface:\ntype Unknownable interface { SetUnknown(context.Context, bool) error SetValue(context.Context, interface{}) error GetUnknown(context.Context) bool GetValue(context.Context) interface{} } \nIt will be used to convert the value. The interface{} being passed and retrieved will be of a type that can be passed to tftypes.NewValue.\nNullable\nIf a value is set on a Go type that fills the Nullable interface:\ntype Nullable interface { SetNull(context.Context, bool) error SetValue(context.Context, interface{}) error GetNull(context.Context) bool GetValue(context.Context) interface{} } \nIt will be used to convert the value. The interface{} being passed and retrieved will be of a type that can be passed to tftypes.NewValue."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.2.x/handling-data/custom-types",
  "text": "Plugin Development - Framework: Handling Data - Custom Types | Terraform\nYou can use custom types for both attributes and blocks.\nImportant: Specifying plan customization for attribute types is not yet supported, limiting their utility. Support is expected in the near future.\nattr.Type Interface\nUse the attr.Type interface to implement an attribute type. It tells Terraform about its constraints and tells the framework how to create new attribute values from the information Terraform supplies. attr.Type has the following methods.\nMethodDescription\nTerraformType\tReturns the tftypes.Type value that describes its type constraints. This is how Terraform will know what type of values it can accept.\t\nValueFromTerraform\tReturns an attribute value from the tftypes.Value that Terraform supplies, or to return an error if it cannot. This error should not be used for validation purposes, and is expected to indicate programmer error, not practitioner error.\t\nEqual\tReturns true if the attribute type is considered equal to the passed attribute type.\t\nAttributePathStepper Interface\nAll attribute types must implement the tftypes.AttributePathStepper interface, so the framework can access element or attribute types using attribute paths.\nxattr.TypeWithValidation Interface\nIf validation for type values is desired, use the xattr.TypeWithValidation interface to include validation logic for type values. The framework will call this functionality when validating all values based on the schema.\nMethodDescription\nValidate\tReturns any warning or error diagnostics for the given value.\t\nType-Specific Interfaces\nCaseInterfaceDescription\nElements of the same type\tTypeWithElementType\tAttribute types that contain elements of the same type, like maps and lists, are required to implement attr.TypeWithElementType, which adds WithElementType and ElementType methods to the attr.Type interface. WithElementType must return a copy of the attribute type, but with its element type set to the passed type. ElementType must return the attribute type's element type.\t\nElements of different types\tTypeWithElementTypes\tAttribute types that contain elements of differing types, like tuples, are required to implement the attr.TypeWithElementTypes, which adds WithElementTypes and ElementTypes methods to the attr.Type interface. WithElementTypes must return a copy of the attribute type, but with its element types set to the passed element types. ElementTypes must return the attribute type's element types.\t\nContain attributes\tTypeWithAttributeTypes\tAttribute types that contain attributes, like objects, are required to implement the attr.TypeWithAttributeTypes interface, which adds WithAttributeTypes and AttributeTypes methods to the attr.Type interface. WithAttributeTypes must return a copy of the attribute type, but with its attribute types set to the passed attribute types. AttributeTypes must return the attribute type's attribute types.\t\nattr.Value Interface\nUse the attr.Value interface to implement an attribute value. It tells the framework how to express that attribute value in a way that Terraform will understand. attr.Value has the following methods.\nMethodDescription\nToTerraformValue\tReturns a Go type that is valid input for tftypes.NewValue for the tftypes.Type specified by the attr.Type that creates the attr.Value.\t\nEqual\tReturns true if the passed attribute value should be considered to the attribute value the method is being called on. The passed attribute value is not guaranteed to be of the same Go type.\t\nA minimal implementation of a custom type for ListType and List that leverages embedding looks as follows:\ntype CustomListType struct { types.ListType } func (c CustomListType) ValueFromTerraform(ctx context.Context, in tftypes.Value) (attr.Value, error) { val, err := c.ListType.ValueFromTerraform(ctx, in) return CustomListValue{ // unchecked type assertion val.(types.List), }, err } type CustomListValue struct { types.List } func (c CustomListValue) DoSomething(ctx context.Context) { tflog.Info(ctx, \"called DoSomething on CustomListValue\") } \nUsing the custom type does not require any changes to the Terraform configuration.\nresource \"example_resource\" \"example\" { list_attribute = [\"list-element\", \"list-element\"] list_nested_attribute = [ { int64_attribute = 9223372036854775807 list_attribute = [\"list-element\", \"list-element\"] }, { int64_attribute = 9223372036854775807 list_attribute = [\"list-element\", \"list-element\"] } ] list_nested_block { bool_attribute = true float64_attribute = 1234.5 int64_attribute = 9223372036854775807 list_attribute = [\"list-element\", \"list-element\"] list_nested_nested_block { bool_attribute = true } list_nested_nested_block { bool_attribute = false } } list_nested_block { bool_attribute = true float64_attribute = 1234.5 int64_attribute = 9223372036854775807 list_attribute = [\"list-element\", \"list-element\"] list_nested_nested_block { bool_attribute = true } list_nested_nested_block { bool_attribute = false } } } \nUse the custom type in the schema as follows:\nfunc (e *exampleResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"list_attribute\": schema.ListAttribute{ Optional: true, ElementType: types.StringType, CustomType: CustomListType{ types.ListType{ ElemType: types.StringType, }, }, }, \"list_nested_attribute\": schema.ListNestedAttribute{ Optional: true, CustomType: CustomListType{ types.ListType{ ElemType: types.ObjectType{ AttrTypes: map[string]attr.Type{ \"int64_attribute\": types.Int64Type, \"list_attribute\": types.ListType{ ElemType: types.StringType, }, }, }, }, }, NestedObject: schema.NestedAttributeObject{ Attributes: map[string]schema.Attribute{ \"int64_attribute\": schema.Int64Attribute{ Optional: true, }, \"list_attribute\": schema.ListAttribute{ Optional: true, ElementType: types.StringType, }, }, }, }, }, Blocks: map[string]schema.Block{ \"list_nested_block\": schema.ListNestedBlock{ CustomType: CustomListType{ types.ListType{ ElemType: types.ObjectType{ AttrTypes: map[string]attr.Type{ \"bool_attribute\": types.BoolType, \"float64_attribute\": types.Float64Type, \"int64_attribute\": types.Int64Type, \"list_attribute\": types.ListType{ ElemType: types.StringType, }, \"list_nested_nested_block\": types.ListType{ ElemType: types.ObjectType{ AttrTypes: map[string]attr.Type{ \"bool_attribute\": types.BoolType, }, }, }, }, }, }, }, NestedObject: schema.NestedBlockObject{ Attributes: map[string]schema.Attribute{ \"bool_attribute\": schema.BoolAttribute{ Optional: true, }, \"float64_attribute\": schema.Float64Attribute{ Optional: true, }, \"int64_attribute\": schema.Int64Attribute{ Optional: true, }, \"list_attribute\": schema.ListAttribute{ Optional: true, ElementType: types.StringType, }, }, Blocks: map[string]schema.Block{ \"list_nested_nested_block\": schema.ListNestedBlock{ NestedObject: schema.NestedBlockObject{ Attributes: map[string]schema.Attribute{ \"bool_attribute\": schema.BoolAttribute{ Optional: true, }, }, }, }, }, }, }, }, } } \nThe custom type value is then used within the model.\nWhere previously the model would have looked as follows:\ntype exampleResourceData struct { ListAttribute types.List `tfsdk:\"list_attribute\"` ListNestedAttribute types.List `tfsdk:\"list_nested_attribute\"` ListNestedBlock types.List `tfsdk:\"list_nested_block\"` } \nThe custom type value is used by updating the model to:\ntype exampleResourceData struct { ListAttribute CustomListValue `tfsdk:\"list_attribute\"` ListNestedAttribute CustomListValue `tfsdk:\"list_nested_attribute\"` ListNestedBlock CustomListValue `tfsdk:\"list_nested_block\"` } \nThe functions on CustomListValue are then available.\nfunc (e *exampleResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleResourceData diags := req.Config.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } data.ListAttribute.DoSomething(ctx) data.ListNestedAttribute.DoSomething(ctx) data.ListNestedBlock.DoSomething(ctx) /*...*/ }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.2.x/resources/state-upgrade",
  "text": "// Other Resource methods are omitted in this example var _ resource.Resource = &ThingResource{} var _ resource.ResourceWithUpgradeState = &ThingResource{} var ThingResourceTftypesDataV0 = tftypes.Object{ AttributeTypes: map[string]tftypes.Type{ \"id\": tftypes.String, \"optional_attribute\": tftypes.Bool, \"required_attribute\": tftypes.Bool, }, } var ThingResourceTftypesDataV1 = tftypes.Object{ AttributeTypes: map[string]tftypes.Type{ \"id\": tftypes.String, \"optional_attribute\": tftypes.String, \"required_attribute\": tftypes.String, }, } type ThingResource struct{/* ... */} func (r *ThingResource) Schema(_ context.Context, _ resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"id\": schema.StringAttribute{ Computed: true, }, \"optional_attribute\": schema.StringAttribute{ // As compared to prior schema.BoolAttribute below Optional: true, }, \"required_attribute\": schema.StringAttribute{ // As compared to prior schema.BoolAttribute below Required: true, }, }, // The resource has a prior state version of 0, which had the attribute // types of types.BoolType as shown below. Version: 1, } } func (r *ThingResource) UpgradeState(ctx context.Context) map[int64]resource.StateUpgrader { return map[int64]resource.StateUpgrader{ // State upgrade implementation from 0 (prior state version) to 1 (Schema.Version) 0: { StateUpgrader: func(ctx context.Context, req resource.UpgradeStateRequest, resp *resource.UpgradeStateResponse) { // Refer also to the RawState type JSON field which can be used // with json.Unmarshal() rawStateValue, err := req.RawState.Unmarshal(ThingResourceTftypesDataV0) if err != nil { resp.Diagnostics.AddError( \"Unable to Unmarshal Prior State\", err.Error(), ) return } var rawState map[string]tftypes.Value if err := rawStateValue.As(&rawState); err != nil { resp.Diagnostics.AddError( \"Unable to Convert Prior State\", err.Error(), ) return } var optionalAttributeString *string if !rawState[\"optional_attribute\"].IsNull() { var optionalAttribute bool if err := rawState[\"optional_attribute\"].As(&optionalAttribute); err != nil { resp.Diagnostics.AddAttributeError( path.Root(\"optional_attribute\"), \"Unable to Convert Prior State\", err.Error(), ) return } v := fmt.Sprintf(\"%t\", optionalAttribute) optionalAttributeString = &v } var requiredAttribute bool if err := rawState[\"required_attribute\"].As(&requiredAttribute); err != nil { resp.Diagnostics.AddAttributeError( path.Root(\"required_attribute\"), \"Unable to Convert Prior State\", err.Error(), ) return } dynamicValue, err := tfprotov6.NewDynamicValue( ThingResourceTftypesDataV1, tftypes.NewValue(ThingResourceTftypesDataV1, map[string]tftypes.Value{ \"id\": rawState[\"id\"], \"optional_attribute\": tftypes.NewValue(tftypes.String, optionalAttributeString), \"required_attribute\": tftypes.NewValue(tftypes.String, fmt.Sprintf(\"%t\", requiredAttribute)), }), ) if err != nil { resp.Diagnostics.AddError( \"Unable to Convert Upgraded State\", err.Error(), ) return } resp.DynamicValue = &dynamicValue }, }, } }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.2.x/resources/create",
  "text": "// ThingResource defines the resource implementation. // Some resource.Resource interface methods are omitted for brevity. type ThingResource struct { // client is configured via a Configure method, which is not shown in this // example for brevity. Refer to the Configure Resources documentation for // additional details for setting up resources with external clients. client *http.Client } // ThingResourceModel describes the Terraform resource data model to match the // resource schema. type ThingResourceModel struct { Name types.String `tfsdk:\"name\"` Id types.String `tfsdk:\"id\"` } // ThingResourceAPIModel describes the API data model. type ThingResourceAPIModel struct { Name string `json:\"name\"` Id string `json:\"id\"` } func (r ThingResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"name\": schema.StringAttribute{ MarkdownDescription: \"Name of the thing to be saved in the service.\", Required: true, }, \"id\": schema.StringAttribute{ Computed: true, MarkdownDescription: \"Service generated identifier for the thing.\", PlanModifiers: planmodifier.String{ stringplanmodifier.UseStateForUnknown(), }, }, }, MarkdownDescription: \"Manages a thing.\", } } func (r ThingResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data *ThingResourceModel // Read Terraform plan data into the model resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // Convert from Terraform data model into API data model createReq := ThingResourceAPIModel{ Name: data.Name.ValueString(), } httpReqBody, err := json.Marshal(createReq) if err != nil { resp.Diagnostics.AddError( \"Unable to Create Resource\", \"An unexpected error occurred while creating the resource create request. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Create HTTP request httpReq := http.NewRequestWithContext( ctx, http.MethodPost, \"http://example.com/things\", bytes.NewBuffer(httpReqBody), ) // Send HTTP request httpResp, err := d.client.Do(httpReq) defer httpResp.Body.Close() if err != nil { resp.Diagnostics.AddError( \"Unable to Create Resource\", \"An unexpected error occurred while attempting to create the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Error: \"+err.Error(), ) return } // Return error if the HTTP status code is not 200 OK if httpResp.StatusCode != http.StatusOK { resp.Diagnostics.AddError( \"Unable to Create Resource\", \"An unexpected error occurred while attempting to create the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Status: \"+httpResp.Status, ) return } var createResp ThingResourceAPIModel err := json.NewDecoder(httpResp.Body).Decode(&createResp) if err != nil { resp.Diagnostics.AddError( \"Unable to Create Resource\", \"An unexpected error occurred while parsing the resource creation response. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Convert from the API data model to the Terraform data model // and set any unknown attribute values. data.Id = types.StringValue(createResp.Id) // Save data into Terraform state resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.2.x/resources/update",
  "text": "// ThingResource defines the resource implementation. // Some resource.Resource interface methods are omitted for brevity. type ThingResource struct { // client is configured via a Configure method, which is not shown in this // example for brevity. Refer to the Configure Resources documentation for // additional details for setting up resources with external clients. client *http.Client } // ThingResourceModel describes the Terraform resource data model to match the // resource schema. type ThingResourceModel struct { Name types.String `tfsdk:\"name\"` Id types.String `tfsdk:\"id\"` } // ThingResourceAPIModel describes the API data model. type ThingResourceAPIModel struct { Name string `json:\"name\"` Id string `json:\"id\"` } func (r ThingResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"name\": schema.StringAttribute{ MarkdownDescription: \"Name of the thing to be saved in the service.\", Required: true, }, \"id\": schema.StringAttribute{ Computed: true, MarkdownDescription: \"Service generated identifier for the thing.\", PlanModifiers: planmodifier.String{ stringplanmodifier.UseStateForUnknown(), }, }, }, MarkdownDescription: \"Manages a thing.\", } } func (r ThingResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data ThingResourceModel // Read Terraform plan data into the model resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) // Convert from Terraform data model into API data model updateReq := ThingResourceAPIModel{ Id: data.Id.StringValue(), Name: data.Name.StringValue(), } httpReqBody, err := json.Marshal(updateReq) if err != nil { resp.Diagnostics.AddError( \"Unable to Update Resource\", \"An unexpected error occurred while creating the resource update request. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Create HTTP request httpReq := http.NewRequestWithContext( ctx, http.MethodPut, \"http://example.com/things\", bytes.NewBuffer(httpReqBody), ) // Send HTTP request httpResp, err := d.client.Do(httpReq) defer httpResp.Body.Close() if err != nil { resp.Diagnostics.AddError( \"Unable to Update Resource\", \"An unexpected error occurred while attempting to update the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Error: \"+err.Error(), ) return } // Return error if the HTTP status code is not 200 OK if httpResp.StatusCode != http.StatusOK { resp.Diagnostics.AddError( \"Unable to Update Resource\", \"An unexpected error occurred while attempting to update the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Status: \"+httpResp.Status, ) return } // Save updated data into Terraform state resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.2.x/resources/delete",
  "text": "// ThingResource defines the resource implementation. // Some resource.Resource interface methods are omitted for brevity. type ThingResource struct { // client is configured via a Configure method, which is not shown in this // example for brevity. Refer to the Configure Resources documentation for // additional details for setting up resources with external clients. client *http.Client } // ThingResourceModel describes the Terraform resource data model to match the // resource schema. type ThingResourceModel struct { Name types.String `tfsdk:\"name\"` Id types.String `tfsdk:\"id\"` } // ThingResourceAPIModel describes the API data model. type ThingResourceAPIModel struct { Name string `json:\"name\"` Id string `json:\"id\"` } func (r ThingResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"name\": schema.StringAttribute{ MarkdownDescription: \"Name of the thing to be saved in the service.\", Required: true, }, \"id\": schema.StringAttribute{ Computed: true, MarkdownDescription: \"Service generated identifier for the thing.\", PlanModifiers: planmodifier.String{ stringplanmodifier.UseStateForUnknown(), }, }, }, MarkdownDescription: \"Manages a thing.\", } } func (r ThingResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { var data ThingResourceModel // Read Terraform prior state data into the model resp.Diagnostics.Append(req.State.Get(ctx, &data)...) // Convert from Terraform data model into API data model readReq := ThingResourceAPIModel{ Id: data.Id.ValueString(), Name: data.Name.ValueString(), } httpReqBody, err := json.Marshal(readReq) if err != nil { resp.Diagnostics.AddError( \"Unable to Delete Resource\", \"An unexpected error occurred while creating the resource d request. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Create HTTP request httpReq := http.NewRequestWithContext( ctx, http.MethodDelete, \"http://example.com/things\", bytes.NewBuffer(httpReqBody), ) // Send HTTP request httpResp, err := d.client.Do(httpReq) defer httpResp.Body.Close() if err != nil { resp.Diagnostics.AddError( \"Unable to Delete Resource\", \"An unexpected error occurred while attempting to delete the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Error: \"+err.Error(), ) return } // Return error if the HTTP status code is not 200 OK or 404 Not Found if httpResp.StatusCode != http.StatusNotFound && httpResp.StatusCode != http.StatusOK { resp.Diagnostics.AddError( \"Unable to Delete Resource\", \"An unexpected error occurred while attempting to delete the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Status: \"+httpResp.Status, ) return } // If the logic reaches here, it implicitly succeeded and will remove // the resource from state if there are no other errors. } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.2.x/resources/default",
  "text": "// Typically within the schema.Schema returned by Schema() for a resource. schema.StringAttribute{ // ... other Attribute configuration ... Default: stringdefault.StaticValue(\"str\"), } schema.SetAttribute{ // ... other Attribute configuration ... Default: setdefault.StaticValue( types.SetValueMust( types.StringType, []attr.Value{ types.StringValue(\"str\"), }, ), ), }, \n| Schema Type | Built-In Default Functions | | schema.BoolAttribute | resource/schema/booldefault package | | schema.Float64Attribute | resource/schema/float64default package | | schema.Int64Attribute | resource/schema/int64default package | | schema.ListAttribute / | schema.ListNestedAttribute | resource/schema/listdefault package | | schema.MapAttribute / | schema.MapNestedAttribute | resource/schema/mapdefault package | | schema.NumberAttribute | resource/schema/numberdefault package | | schema.ObjectAttribute / | schema.SingleNestedAttribute | resource/schema/objectdefault package | | schema.SetAttribute / | schema.SetNestedAttribute | resource/schema/setdefault package | | schema.StringAttribute | resource/schema/stringdefault package |"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.2.x/resources/read",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.2.x/resources/plan-modification",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.2.x/resources/import",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.2.x/resources/private-state",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.2.x/resources/timeouts",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.2.x/data-sources/timeouts",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.2.x/accessing-values",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.1.x/data-sources/configure",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.1.x/resources/create",
  "text": "// ThingResource defines the resource implementation. // Some resource.Resource interface methods are omitted for brevity. type ThingResource struct { // client is configured via a Configure method, which is not shown in this // example for brevity. Refer to the Configure Resources documentation for // additional details for setting up resources with external clients. client *http.Client } // ThingResourceModel describes the Terraform resource data model to match the // resource schema. type ThingResourceModel struct { Name types.String `tfsdk:\"name\"` Id types.String `tfsdk:\"id\"` } // ThingResourceAPIModel describes the API data model. type ThingResourceAPIModel struct { Name string `json:\"name\"` Id string `json:\"id\"` } func (r ThingResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"name\": schema.StringAttribute{ MarkdownDescription: \"Name of the thing to be saved in the service.\", Required: true, }, \"id\": schema.StringAttribute{ Computed: true, MarkdownDescription: \"Service generated identifier for the thing.\", PlanModifiers: planmodifier.String{ stringplanmodifier.UseStateForUnknown(), }, }, }, MarkdownDescription: \"Manages a thing.\", } } func (r ThingResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data *ThingResourceModel // Read Terraform plan data into the model resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // Convert from Terraform data model into API data model createReq := ThingResourceAPIModel{ Name: data.Name.ValueString(), } httpReqBody, err := json.Marshal(createReq) if err != nil { resp.Diagnostics.AddError( \"Unable to Create Resource\", \"An unexpected error occurred while creating the resource create request. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Create HTTP request httpReq := http.NewRequestWithContext( ctx, http.MethodPost, \"http://example.com/things\", bytes.NewBuffer(httpReqBody), ) // Send HTTP request httpResp, err := d.client.Do(httpReq) defer httpResp.Body.Close() if err != nil { resp.Diagnostics.AddError( \"Unable to Create Resource\", \"An unexpected error occurred while attempting to create the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Error: \"+err.Error(), ) return } // Return error if the HTTP status code is not 200 OK if httpResp.StatusCode != http.StatusOK { resp.Diagnostics.AddError( \"Unable to Create Resource\", \"An unexpected error occurred while attempting to create the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Status: \"+httpResp.Status, ) return } var createResp ThingResourceAPIModel err := json.NewDecoder(httpResp.Body).Decode(&createResp) if err != nil { resp.Diagnostics.AddError( \"Unable to Create Resource\", \"An unexpected error occurred while parsing the resource creation response. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Convert from the API data model to the Terraform data model // and set any unknown attribute values. data.Id = types.StringValue(createResp.Id) // Save data into Terraform state resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.1.x/resources/configure",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.1.x/resources/delete",
  "text": "// ThingResource defines the resource implementation. // Some resource.Resource interface methods are omitted for brevity. type ThingResource struct { // client is configured via a Configure method, which is not shown in this // example for brevity. Refer to the Configure Resources documentation for // additional details for setting up resources with external clients. client *http.Client } // ThingResourceModel describes the Terraform resource data model to match the // resource schema. type ThingResourceModel struct { Name types.String `tfsdk:\"name\"` Id types.String `tfsdk:\"id\"` } // ThingResourceAPIModel describes the API data model. type ThingResourceAPIModel struct { Name string `json:\"name\"` Id string `json:\"id\"` } func (r ThingResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"name\": schema.StringAttribute{ MarkdownDescription: \"Name of the thing to be saved in the service.\", Required: true, }, \"id\": schema.StringAttribute{ Computed: true, MarkdownDescription: \"Service generated identifier for the thing.\", PlanModifiers: planmodifier.String{ stringplanmodifier.UseStateForUnknown(), }, }, }, MarkdownDescription: \"Manages a thing.\", } } func (r ThingResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { var data ThingResourceModel // Read Terraform prior state data into the model resp.Diagnostics.Append(req.State.Get(ctx, &data)...) // Convert from Terraform data model into API data model readReq := ThingResourceAPIModel{ Id: data.Id.ValueString(), Name: data.Name.ValueString(), } httpReqBody, err := json.Marshal(readReq) if err != nil { resp.Diagnostics.AddError( \"Unable to Delete Resource\", \"An unexpected error occurred while creating the resource d request. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Create HTTP request httpReq := http.NewRequestWithContext( ctx, http.MethodDelete, \"http://example.com/things\", bytes.NewBuffer(httpReqBody), ) // Send HTTP request httpResp, err := d.client.Do(httpReq) defer httpResp.Body.Close() if err != nil { resp.Diagnostics.AddError( \"Unable to Delete Resource\", \"An unexpected error occurred while attempting to delete the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Error: \"+err.Error(), ) return } // Return error if the HTTP status code is not 200 OK or 404 Not Found if httpResp.StatusCode != http.StatusNotFound && httpResp.StatusCode != http.StatusOK { resp.Diagnostics.AddError( \"Unable to Delete Resource\", \"An unexpected error occurred while attempting to delete the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Status: \"+httpResp.Status, ) return } // If the logic reaches here, it implicitly succeeded and will remove // the resource from state if there are no other errors. } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.1.x/resources/update",
  "text": "// ThingResource defines the resource implementation. // Some resource.Resource interface methods are omitted for brevity. type ThingResource struct { // client is configured via a Configure method, which is not shown in this // example for brevity. Refer to the Configure Resources documentation for // additional details for setting up resources with external clients. client *http.Client } // ThingResourceModel describes the Terraform resource data model to match the // resource schema. type ThingResourceModel struct { Name types.String `tfsdk:\"name\"` Id types.String `tfsdk:\"id\"` } // ThingResourceAPIModel describes the API data model. type ThingResourceAPIModel struct { Name string `json:\"name\"` Id string `json:\"id\"` } func (r ThingResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"name\": schema.StringAttribute{ MarkdownDescription: \"Name of the thing to be saved in the service.\", Required: true, }, \"id\": schema.StringAttribute{ Computed: true, MarkdownDescription: \"Service generated identifier for the thing.\", PlanModifiers: planmodifier.String{ stringplanmodifier.UseStateForUnknown(), }, }, }, MarkdownDescription: \"Manages a thing.\", } } func (r ThingResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data ThingResourceModel // Read Terraform plan data into the model resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) // Convert from Terraform data model into API data model updateReq := ThingResourceAPIModel{ Id: data.Id.StringValue(), Name: data.Name.StringValue(), } httpReqBody, err := json.Marshal(updateReq) if err != nil { resp.Diagnostics.AddError( \"Unable to Update Resource\", \"An unexpected error occurred while creating the resource update request. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Create HTTP request httpReq := http.NewRequestWithContext( ctx, http.MethodPut, \"http://example.com/things\", bytes.NewBuffer(httpReqBody), ) // Send HTTP request httpResp, err := d.client.Do(httpReq) defer httpResp.Body.Close() if err != nil { resp.Diagnostics.AddError( \"Unable to Update Resource\", \"An unexpected error occurred while attempting to update the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Error: \"+err.Error(), ) return } // Return error if the HTTP status code is not 200 OK if httpResp.StatusCode != http.StatusOK { resp.Diagnostics.AddError( \"Unable to Update Resource\", \"An unexpected error occurred while attempting to update the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Status: \"+httpResp.Status, ) return } // Save updated data into Terraform state resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.1.x/resources/plan-modification",
  "text": "// stringDefaultModifier is a plan modifier that sets a default value for a // types.StringType attribute when it is not configured. The attribute must be // marked as Optional and Computed. When setting the state during the resource // Create, Read, or Update methods, this default value must also be included or // the Terraform CLI will generate an error. type stringDefaultModifier struct { Default string } // Description returns a plain text description of the validator's behavior, suitable for a practitioner to understand its impact. func (m stringDefaultModifier) Description(ctx context.Context) string { return fmt.Sprintf(\"If value is not configured, defaults to %s\", m.Default) } // MarkdownDescription returns a markdown formatted description of the validator's behavior, suitable for a practitioner to understand its impact. func (m stringDefaultModifier) MarkdownDescription(ctx context.Context) string { return fmt.Sprintf(\"If value is not configured, defaults to `%s`\", m.Default) } // PlanModifyString runs the logic of the plan modifier. // Access to the configuration, plan, and state is available in `req`, while // `resp` contains fields for updating the planned value, triggering resource // replacement, and returning diagnostics. func (m stringDefaultModifier) PlanModifyString(ctx context.Context, req planmodifier.StringRequest, resp *planmodifier.StringResponse) { // If the value is unknown or known, do not set default value. if !req.PlanValue.IsNull() { return } resp.PlanValue = types.StringValue(m.Default) } \nfunc stringDefault(defaultValue string) planmodifier.String { return stringDefaultModifier{ Default: defaultValue, } } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.1.x/resources/read",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.1.x/resources/state-upgrade",
  "text": "// Other Resource methods are omitted in this example var _ resource.Resource = &ThingResource{} var _ resource.ResourceWithUpgradeState = &ThingResource{} var ThingResourceTftypesDataV0 = tftypes.Object{ AttributeTypes: map[string]tftypes.Type{ \"id\": tftypes.String, \"optional_attribute\": tftypes.Bool, \"required_attribute\": tftypes.Bool, }, } var ThingResourceTftypesDataV1 = tftypes.Object{ AttributeTypes: map[string]tftypes.Type{ \"id\": tftypes.String, \"optional_attribute\": tftypes.String, \"required_attribute\": tftypes.String, }, } type ThingResource struct{/* ... */} func (r *ThingResource) Schema(_ context.Context, _ resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"id\": schema.StringAttribute{ Computed: true, }, \"optional_attribute\": schema.StringAttribute{ // As compared to prior schema.BoolAttribute below Optional: true, }, \"required_attribute\": schema.StringAttribute{ // As compared to prior schema.BoolAttribute below Required: true, }, }, // The resource has a prior state version of 0, which had the attribute // types of types.BoolType as shown below. Version: 1, } } func (r *ThingResource) UpgradeState(ctx context.Context) map[int64]resource.StateUpgrader { return map[int64]resource.StateUpgrader{ // State upgrade implementation from 0 (prior state version) to 1 (Schema.Version) 0: { StateUpgrader: func(ctx context.Context, req resource.UpgradeStateRequest, resp *resource.UpgradeStateResponse) { // Refer also to the RawState type JSON field which can be used // with json.Unmarshal() rawStateValue, err := req.RawState.Unmarshal(ThingResourceTftypesDataV0) if err != nil { resp.Diagnostics.AddError( \"Unable to Unmarshal Prior State\", err.Error(), ) return } var rawState map[string]tftypes.Value if err := rawStateValue.As(&rawState); err != nil { resp.Diagnostics.AddError( \"Unable to Convert Prior State\", err.Error(), ) return } var optionalAttributeString *string if !rawState[\"optional_attribute\"].IsNull() { var optionalAttribute bool if err := rawState[\"optional_attribute\"].As(&optionalAttribute); err != nil { resp.Diagnostics.AddAttributeError( path.Root(\"optional_attribute\"), \"Unable to Convert Prior State\", err.Error(), ) return } v := fmt.Sprintf(\"%t\", optionalAttribute) optionalAttributeString = &v } var requiredAttribute bool if err := rawState[\"required_attribute\"].As(&requiredAttribute); err != nil { resp.Diagnostics.AddAttributeError( path.Root(\"required_attribute\"), \"Unable to Convert Prior State\", err.Error(), ) return } dynamicValue, err := tfprotov6.NewDynamicValue( ThingResourceTftypesDataV1, tftypes.NewValue(ThingResourceTftypesDataV1, map[string]tftypes.Value{ \"id\": rawState[\"id\"], \"optional_attribute\": tftypes.NewValue(tftypes.String, optionalAttributeString), \"required_attribute\": tftypes.NewValue(tftypes.String, fmt.Sprintf(\"%t\", requiredAttribute)), }), ) if err != nil { resp.Diagnostics.AddError( \"Unable to Convert Upgraded State\", err.Error(), ) return } resp.DynamicValue = &dynamicValue }, }, } }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.1.x/resources/import",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.1.x/resources/timeouts",
  "text": "func (t *exampleResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { return schema.Schema{ Attributes: map[string]schema.Attribute{ /* ... */ \"timeouts\": timeouts.Attributes(ctx, timeouts.Opts{ Create: true, }), }, \nThe exampleResourceData model needs to be modified to include a field for timeouts, which is types.Object.\ntype exampleResourceData struct { /* ... */ Timeouts types.Object `tfsdk:\"timeouts\"` \nfunc (e *exampleResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleResourceData diags := req.Plan.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } defaultCreateTimeout := 20 * time.Minute createTimeout := timeouts.Create(ctx, data.Timeouts, defaultCreateTimeout) ctx, cancel := context.WithTimeout(ctx, createTimeout) defer cancel() /* ... */ }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.7.x/schemas",
  "text": "Plugin Development - Framework: Schemas | Terraform\nSchemas specify the constraints of Terraform configuration blocks. They define what fields a provider, resource, or data source configuration block has, and give Terraform metadata about those fields. You can think of the schema as the \"type information\" or the \"shape\" of a resource, data source, or provider.\nThe tfsdk.Schema struct defines schemas that are returned from methods on the provider, resources, and data sources.\nEvery schema has a version, which is an integer that allows you to track changes to your schemas. It is generally only used when upgrading resource state, to help massage resources created with earlier schemas into the shape defined by the current schema. It will never be used for provider or data source schemas and can be omitted.\nNot every resource, data source, or provider will be supported forever. Sometimes designs change or APIs are deprecated. Schemas that have their DeprecationMessage property set will display that message as a warning when that provider, data source, or resource is used. A good message will tell practitioners that the provider, resource, or data source is deprecated, and will indicate a migration strategy.\nVarious tooling like terraform-plugin-docs and the language server can use metadata in the schema to generate documentation or offer a better editor experience for practitioners. Use the Description property to add a description of a resource, data source, or provider that these tools can leverage.\nSimilar to the Description property, the MarkdownDescription is used to provide a markdown-formatted version of the description to tooling like terraform-plugin-docs. It is a best practice to only alter the formatting, not the content, between the Description and MarkdownDescription.\nAt the moment, if the MarkdownDescription property is set it will always be used instead of the Description property. It is possible that a different strategy may be employed in the future to surface descriptions to other tooling in a different format, so we recommend specifying both fields.\nAttributes are the main point of a schema. They are used to describe the fields of a provider, resource, or data source. Attributes are defined as a map of tfsdk.Attributes, with string keys.\nThe keys are the names of the fields, and should only contain lowercase letters, numbers, and underscores. Practitioners will enter these strings in the configuration block of the provider, resource, or data source to set a value for that field.\nThe values are descriptions of the constraints on that field and the metadata about it, expressed as a tfsdk.Attribute struct value.\nType\nThe Type property of a tfsdk.Attribute is used to specify the attribute type of the attribute. If the practitioner enters data of the wrong type, Terraform will automatically return a validation error to the practitioner, and this type determines what kind of attribute value is used when accessing state, config, and plan values.\nYou must specify one of the Type and Attributes properties of tfsdk.Attribute. You cannot specify both.\nThe Attributes property of a tfsdk.Attribute is used to specify the attribute's nested attributes. Nested attributes are attributes that are grouped beneath another attribute:\nresource \"example_foo\" \"bar\" { nested_attribute = { hello = \"world\" demo = true } } \nIn this example, hello and demo are nested attributes of the nested_attribute attribute. Nested attributes have their own computed, optional, required, and sensitive flags, and their own properties to hold descriptions, markdown-formatted descriptions, and deprecation messages.\nIf an attribute is required and its nested attribute is optional, the attribute must be specified, and the nested attribute can be omitted inside the curly braces. If an attribute is optional and its nested attribute is required, the attribute can be omitted, but if it is specified, the nested attribute must be specified as well.\nIf an attribute is computed, all its nested attributes are considered computed, as well. Likewise, if an attribute is considered sensitive or deprecated, all its nested attributes are considered sensitive or deprecated.\nYou must specify one of the Type and Attributes properties of tfsdk.Attribute. You cannot specify both.\nNote: We recommend using an attribute with nested attributes when any of the inner fields should have their own flags or metadata (required, optional, computed, deprecated, sensitive, descriptions, etc.). If the attribute is an atomic unit, we recommend using an object or list of objects instead.\nSingleNestedAttributes\nWhen the Attributes property of a tfsdk.Attribute is set to the schema.SingleNestedAttributes type, the nested attributes behave like an object. There can only be one group of the nested attributes.\nresource \"example_foo\" \"bar\" { nested_attribute = { hello = \"world\" demo = true } } \nListNestedAttributes\nWhen the Attributes property of a tfsdk.Attribute is set to the schema.ListNestedAttributes type, the nested attributes behave like a list of objects. The practitioner can specify any number of groups of these attributes.\nresource \"example_foo\" \"bar\" { nested_attribute = [ { hello = \"world\" demo = true }, { hello = \"moon\" demo = false }, ] } \nIn addition to a map of the attributes, ListNestedAttributes accepts a schema.ListNestedAttributesOptions struct that can be used to specify a minimum or maximum number of instances of that group of fields. If the user enters fewer than or more than those numbers of groups, Terraform will automatically return a validation error.\nMapNestedAttributes\nWhen the Attributes property of a tfsdk.Attribute is set to the schema.MapNestedAttributes type, the nested attributes behave like a map of objects. The practitioner can specify any number of groups of these attributes, with string keys associated with each group.\nresource \"example_foo\" \"bar\" { nested_attribute = { \"red\" = { hello = \"world\" demo = true }, \"blue\" = { hello = \"moon\" demo = false }, } } \nIn addition to a map of the attributes, MapNestedAttributes accepts a schema.MapNestedAttributesOptions struct that can be used to specify a minimum or maximum number of instances of that group of fields. If the user enters fewer than or more than those numbers of groups, Terraform will automatically return a validation error.\nRequired\nSetting the Required property to true indicates that the attribute is required. Practitioners will be automatically given a validation error if they omit the attribute in their configuration.\nAttributes that set Required to true cannot set Optional or Computed to true.\nOptional\nSetting the Optional property to true indicates that the attribute is optional. Practitioners are free to specify that attribute or omit it, and Terraform will not automatically generate a validation error based on the presence or absence of that attribute.\nAttributes that set Optional to true cannot set Required to true, but may set Computed to true, in which case the behavior of Computed applies, too.\nComputed\nSetting the Computed property to true indicates that the attribute can be set by the provider. Any attribute that does not have Computed set to true cannot be influenced by the provider; it can only be changed via the config or via state refreshing.\nIf Computed is set to true and Optional is not set to true, the attribute will be considered read-only and Terraform will automatically generate a validation error if the practitioner attempts to enter a configuration value for that attribute.\nIf Computed is set to true and Optional is set to true, Terraform will not do any validation on the presence or absence of the value.\nIf Computed is set to true, Required must not be set to true.\nBecause providers don't set provider configuration values in state, provider schemas should never set Computed to true.\nSensitive\nSetting the Sensitive property to true indicates to Terraform that the value should always be considered sensitive. This does not at this time add any encryption or otherwise hide the value from Terraform; see the documentation for SDKv2 for more information on sensitive state and Terraform. It does, however, hide the value in Terraform's outputs and in Terraform Cloud.\nMuch like resources, data sources, and providers can have a description, so too can individual attributes.\nMarkdownDescription\nMuch like resources, data sources, and providers can have a markdown-formatted description, so too can individual attributes.\nDeprecationMessage\nMuch like resources, data sources, and providers can be deprecated, so too can individual attributes."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.1.x/data-sources/timeouts",
  "text": "func (d *ThingDataSource) Schema(ctx context.Context, req datasource.SchemaRequest, resp *datasource.SchemaResponse) { resp.Schema = schema.Schema{ /* ... */ Blocks: map[string]schema.Block{ \"timeouts\": timeouts.Block(ctx, timeouts.Opts{ Read: true, }), }, \nfunc (d *ThingDataSource) Schema(ctx context.Context, req datasource.SchemaRequest, resp *datasource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ /* ... */ \"timeouts\": timeouts.Attributes(ctx, timeouts.Opts{ Read: true, }), }, \nModify the exampleDataSourceData model to include a field for timeouts using a types.Object type.\ntype exampleDataSourceData struct { /* ... */ Timeouts types.Object `tfsdk:\"timeouts\"` \nfunc (e *exampleDataSource) Read(ctx context.Context, req datasource.ReadRequest, resp *datasource.ReadResponse) { var data exampleDataSourceData diags := req.Config.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } defaultReadTimeout := 20 * time.Minute readTimeout := timeouts.Read(ctx, data.Timeouts, defaultReadTimeout) ctx, cancel := context.WithTimeout(ctx, readTimeout) defer cancel() /* ... */ }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.1.x/accessing-values",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.1.x/resources/private-state",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.17.x/resources/state-upgrade",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.7.x/types",
  "text": "Plugin Development - Framework: Attribute Types | Terraform\nAttributes are the fields in a resource, data source, or provider. They hold the values that end up in state. Every attribute has an attribute type, which describes the constraints on the data the attribute can hold. When you access an attribute from the configuration, state, or plan, you are accessing attribute values, which are the actual data that was found in the configuration, state, or plan.\nYou can either use the built-in attribute type and value implementations or implement your own.\nThere are two values every attribute in Terraform can hold, regardless of their type: null and unknown.\nNull\nUnknown\nA collection of attribute type and attribute value implementations is available in the types package.\nStringType and String\nStrings are a UTF-8 encoded collection of bytes.\nThey are used by specifying the types.StringType constant in your tfsdk.Attribute's Type property, and are represented by a types.String struct in config, state, and plan. The types.String struct has the following properties:\nValue contains the string's value as a Go string type.\nNull is set to true when the string's value is null.\nUnknown is set to true when the string's value is unknown.\nInt64Type and Int64\nInt64 are 64-bit integer values, such as 1234.\nThey are used by specifying the types.Int64Type constant in your tfsdk.Attribute's Type property, and are represented by a types.Int64 struct in config, state, and plan. The types.Int64 struct has the following properties:\nValue contains the number's value as a Go int64 type.\nNull is set to true when the number's value is null.\nUnknown is set to true when the number's value is unknown.\nFor 64-bit floating point numbers, see Float64Type and Float64. For generic number handling, see NumberType and Number64.\nFloat64Type and Float64\nFloat64 are 64-bit floating point values, such as 1234.5.\nThey are used by specifying the types.Float64Type constant in your tfsdk.Attribute's Type property, and are represented by a types.Float64 struct in config, state, and plan. The types.Float64 struct has the following properties:\nValue contains the number's value as a Go float64 type.\nNull is set to true when the number's value is null.\nUnknown is set to true when the number's value is unknown.\nFor 64-bit integer numbers, see Int64Type and Int64. For generic number handling, see NumberType and Number64.\nNumberType and Number\nNumbers are numeric values, both whole values like 12 or fractional values like 3.14.\nThey are used by specifying the types.NumberType constant in your tfsdk.Attribute's Type property, and are represented by a types.Number struct in config, state, and plan. The types.Number struct has the following properties:\nValue contains the number's value as a Go *big.Float type.\nNull is set to true when the number's value is null.\nUnknown is set to true when the number's value is unknown.\nFor 64-bit integer numbers, see Int64Type and Int64. For 64-bit floating point numbers, see Float64Type and Float64.\nBoolType and Bool\nBools are boolean values that can either be true or false.\nThey are used by specifying the types.BoolType constant in your tfsdk.Attribute's Type property, and are represented by a types.Bool struct in config, state, and plan. The types.Bool struct has the following properties:\nValue contains the boolean's value as a Go bool type.\nNull is set to true when the boolean's value is null.\nUnknown is set to true when the boolean's value is unknown.\nListType and List\nLists are ordered collections of other types. Their elements, the values inside the list, must all be of the same type.\nhello = [\"red\", \"blue\", \"green\"] \nThey are used by specifying a types.ListType value in your tfsdk.Attribute's Type property. You must specify an ElemType property for your list, indicating what type the elements should be. Lists are represented by a types.List struct in config, state, and plan. The types.List struct has the following properties:\nElemType will always contain the same type as the ElemType property of the types.ListType that created the types.List.\nElem contains a list of values, one for each element in the list. The values will all be of the value type produced by the ElemType for the list.\nNull is set to true when the entire list's value is null. Individual elements may still be null even if the list's Null property is false.\nUnknown is set to true when the entire list's value is unknown. Individual elements may still be unknown even if the list's Unknown property is false.\nElements of a types.List with a non-null, non-unknown value can be accessed without using type assertions by using the types.List's ElementsAs method, which uses the same conversion rules as the Get methods described in Access State, Config, and Plan.\nFor an unordered collection with uniqueness constraints, see SetType and Set.\nMapType and Map\nMaps are unordered collections of other types with unique string indexes. Their elements, the values inside the map, must all be of the same type. The keys used to index the elements must be strings, but there are (theoretically) no limitations on what keys are acceptable or how many there can be.\nhello = { pi = 3.14 random = 4 \"meaning of life\" = 42 } \nThey are used by specifying a types.MapType value in your tfsdk.Attribute's Type property. You must specify an ElemType property for your map, indicating what type the elements should be. Maps are represented by a types.Map struct in config, state, and plan. The types.Map struct has the following properties:\nElemType will always contain the same type as the ElemType property of the types.MapType that created the types.Map.\nElem contains a map of values, one for each element in the map. The keys will be the keys defined in the config, state, or plan, and the values will all be of the value type produced by the ElemType for the map.\nNull is set to true when the entire map's value is null. Individual elements may still be null even if the map's Null property is false.\nUnknown is set to true when the entire map's value is unknown. Individual elements may still be unknown even if the map's Unknown property is false.\nElements of a types.Map with a non-null, non-unknown value can be accessed without using type assertions by using the types.Map's ElementsAs method, which uses the same conversion rules as the Get methods described in Access State, Config, and Plan.\nObjectType and Object\nObjects are unordered collections of other types with unique, pre-specified attributes. The attributes have names represented by strings, and each attribute can specify its own type, similar to a Go struct type. The attributes and their types are considered part of the object's type; two objects are not the same type unless they have the same attributes, and those attributes have the same types.\nhello = { pi = 3.14 demo = true color = \"red\" } \nThey are used by specifying a types.ObjectType value in your tfsdk.Attribute's Type property. You must specify an AttrTypes property for your object, indicating a map of the attribute names and the types of those attributes. Objects are represented by a types.Object struct in config, state, and plan. The types.Object struct has the following properties:\nAttrTypes will always contain the same attribute names and associated types as the AttrTypes property of the types.ObjectType that created the types.Object.\nAttrs contains a map of attribute names to values. Each attribute is guaranteed to always be present in the map. The values will always be of the value type for that attribute in the AttrTypes of the object.\nNull is set to true when the entire object's value is null. Individual attributes may still be null even if the object's Null property is false.\nUnknown is set to true when the entire object's value is unknown. Individual attributes may still be unknown even if the object's Unknown property is false.\nA non-null, non-unknown types.Object value can be converted to a Go struct without using type assertions by using the types.Object's As method, which uses the same conversion rules as the Get methods described in Access State, Config, and Plan.\nSetType and Set\nSets are unordered collections of other types. Their elements, the values inside the set, must all be of the same type and must be unique.\nhello = [\"red\", \"blue\", \"green\"] \nThey are used by specifying a types.SetType value in your tfsdk.Attribute's Type property. You must specify an ElemType property for your set, indicating what type the elements should be. Sets are represented by a types.Set struct in config, state, and plan. The types.Set struct has the following properties:\nElemType will always contain the same type as the ElemType property of the types.SetType that created the types.Set.\nElem contains a list of values, one for each element in the set. The values will all be of the value type produced by the ElemType for the list. Each element must be unique.\nNull is set to true when the entire set's value is null. Individual elements may still be null even if the set's Null property is false.\nUnknown is set to true when the entire set's value is unknown. Individual elements may still be unknown even if the set's Unknown property is false.\nElements of a types.Set with a non-null, non-unknown value can be accessed without using type assertions by using the types.Set's ElementsAs method, which uses the same conversion rules as the Get methods described in Access State, Config, and Plan.\nFor an ordered collection without uniqueness constraints, see ListType and List.\nYou may want to build your own attribute value and type implementations to allow your provider to combine validation, description, and plan customization behaviors into a reusable bundle. This helps avoid duplication or reimplementation and ensures consistency.\nImportant: Specifying plan customization for attribute types is not yet supported, limiting their utility. Support is expected in the near future.\nattr.Type Interface\nUse the attr.Type interface to implement an attribute type. It tells Terraform about its constraints and tells the framework how to create new attribute values from the information Terraform supplies. attr.Type has the following methods.\nMethodDescription\nTerraformType\tReturns the tftypes.Type value that describes its type constraints. This is how Terraform will know what type of values it can accept.\t\nValueFromTerraform\tReturns an attribute value from the tftypes.Value that Terraform supplies, or to return an error if it cannot. This error should not be used for validation purposes, and is expected to indicate programmer error, not practitioner error.\t\nEqual\tReturns true if the attribute type is considered equal to the passed attribute type.\t\nAttributePathStepper Interface\nAll attribute types must implement the tftypes.AttributePathStepper interface, so the framework can access element or attribute types using attribute paths.\nType-Specific Interfaces\nCaseInterfaceDescription\nElements of the same type\tTypeWithElementType\tAttribute types that contain elements of the same type, like maps and lists, are required to implement attr.TypeWithElementType, which adds WithElementType and ElementType methods to the attr.Type interface. WithElementType must return a copy of the attribute type, but with its element type set to the passed type. ElementType must return the attribute type's element type.\t\nElements of different types\tTypeWithElementTypes\tAttribute types that contain elements of differing types, like tuples, are required to implement the attr.TypeWithElementTypes, which adds WithElementTypes and ElementTypes methods to the attr.Type interface. WithElementTypes must return a copy of the attribute type, but with its element types set to the passed element types. ElementTypes must return the attribute type's element types.\t\nContain attributes\tTypeWithAttributeTypes\tAttribute types that contain attributes, like objects, are required to implement the attr.TypeWithAttributeTypes interface, which adds WithAttributeTypes and AttributeTypes methods to the attr.Type interface. WithAttributeTypes must return a copy of the attribute type, but with its attribute types set to the passed attribute types. AttributeTypes must return the attribute type's attribute types.\t\nattr.Value Interface\nUse the attr.Value interface to implement an attribute value. It tells the framework how to express that attribute value in a way that Terraform will understand. attr.Value has the following methods.\nToTerraformValue\tReturns a Go type that is valid input for tftypes.NewValue for the tftypes.Type specified by the attr.Type that creates the attr.Value.\t\nEqual\tReturns true if the passed attribute value should be considered to the attribute value the method is being called on. The passed attribute value is not guaranteed to be of the same Go type."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.7.x/accessing-values",
  "text": "func (m myResource) Create(ctx context.Context, req tfsdk.CreateResourceRequest, resp *tfsdk.CreateResourceResponse) \ntype resourceData struct { Name types.String `tfsdk:\"name\"` Age types.Number `tfsdk:\"age\"` Registered types.Bool `tfsdk:\"registered\"` Pets types.List `tfsdk:\"pets\"` Tags types.Map `tfsdk:\"tags\"` Address types.Object `tfsdk:\"address\"` } func (m myResource) Create(ctx context.Context, req tfsdk.CreateResourceRequest, resp *tfsdk.CreateResourceResponse) { var plan resourceData diags := req.Plan.Get(ctx, &plan) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } // values can now be accessed like plan.Name.Value // check if things are null with plan.Name.Null // check if things are unknown with plan.Name.Unknown } \nThe configuration, plan, and state data is represented as an object, and accessed like an object. See the conversion rules for an explanation on how objects can be converted into Go types.\nHowever, using the attr.Value implementations can surface unnecessary complexity. For example, in a create function, non-computed values are guaranteed to be defined. Likewise, a required value will never be null.\nTo aid in this, Get can do some conversion to Go types that can hold the data:\ntype resourceData struct { Name string `tfsdk:\"name\"` Age int64 `tfsdk:\"age\"` Registered bool `tfsdk:\"registered\"` Pets []string `tfsdk:\"pets\"` Tags map[string]string `tfsdk:\"tags\"` Address struct{ Street string `tfsdk:\"street\"` City string `tfsdk:\"city\"` State string `tfsdk:\"state\"` Zip int64 `tfsdk:\"zip\"` } `tfsdk:\"address\"` } \nSee below for the rules about conversion.\nAnother way to interact with configuration, plan, and state values is to retrieve a single value from the configuration, plan, or state and convert it into a Go type. This does not require defining a type (except for objects), but means each attribute access steps outside of what the compiler can check, and may return an error at runtime. It also requires a type assertion, though the type will always be the type produced by that attribute's attr.Type.\nfunc (m myResource) Create(ctx context.Context, req tfsdk.CreateResourceRequest, resp *tfsdk.CreateResourceResponse) { attr, diags := req.Config.GetAttribute(ctx, tftypes.NewAttributePath().WithAttributeName(\"age\")) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } age := attr.(types.Number) } \nNote: The call to tftypes.NewAttributePath is creating an attribute path pointing to the specific attribute. A less-verbose way to specify attribute paths is coming soon.\nNote: Helpers to access attr.Values using the same reflection rules Get has are planned, to avoid the need to type assert. We hope to release them soon.\nWarning: It can be tempting to use Go types instead of attr.Value implementations when the provider doesn't care about the distinction between an empty value, unknown, and null. But if Terraform has a null or unknown value and the provider asks the framework to store it in a type that can't hold it, Get will return an error. Make sure the types you are using can hold the values they might contain! An opt-in conversion of null or unknown values to the empty value is coming in the future.\nString\nStrings can be automatically converted to Go's string type (or any aliases of it, like type MyString string) as long as the string value is not null or unknown.\nNumber\nNumbers can be automatically converted to the following numeric types (or any aliases of them, like type MyNumber int) as long as the number value is not null or unknown:\nint, int8, int16, int32, int64\nuint, uint8, uint16, uint32, uint64\nfloat32, float64\n*big.Int, *big.Float\nAn error will be returned if the value of the number cannot be stored in the numeric type supplied because of an overflow or other loss of precision.\nBoolean\nBooleans can be automatically converted to Go's bool type (or any aliases of it, like type MyBoolean bool) as long as the boolean value is not null or unknown.\nList\nLists can be automatically converted to any Go slice type (or alias of a Go slice type, like type MyList []string), with the elements either being attr.Value implementations or being converted according to these rules. Go slice types are considered capable of handling null values; the slice will be set to nil. The Get method will still return an error for unknown list values.\nMap\nMaps can be automatically converted to any Go map type with string keys (or any alias of a Go map type with string keys, like type MyMap map[string]int), with the elements either being attr.Value implementations or being converted according to these rules. Go map types are considered capable of handling null values; the map will be set to nil. The Get method will still return an error for unknown map values.\nObject\nObjects can be automatically converted to any Go struct type with that follows these constraints:\nEvery property on the struct must have a tfsdk struct tag.\nThe tfsdk struct tag must name an attribute in the object that it is being mapped to or be set to - to explicitly declare it does not map to an attribute in the object.\nEvery attribute in the object must have a corresponding struct tag.\nThese rules help prevent typos and human error from unwittingly discarding information by failing as early, consistently, and loudly as possible.\nProperties can either be attr.Value implementations or will be converted according to these rules.\nUnknown and null objects cannot be represented as structs and will return an error. Their attributes may contain unknown or null values if the attribute's type can hold them.\nPointers\nPointers behave exactly like the type they are referencing, except they can hold null values. A pointer will be set to nil when representing a null value; otherwise, the conversion rules for that type will apply.\nDetected Interfaces\nGet detects and utilizes the following interfaces, if the target implements them.\nValueConverter\nIf a value is being set on a Go type that implements the tftypes.ValueConverter interface, that interface will be delegated to to handle the conversion.\nUnknownable\nIf the value is being set on a Go type that fills the Unknownable interface:\ntype Unknownable interface { SetUnknown(context.Context, bool) error SetValue(context.Context, interface{}) error GetUnknown(context.Context) bool GetValue(context.Context) interface{} } \nIt will be considered capable of handling unknown values, and those methods will be used to populate it and retrieve its value. The interface{} being passed and retrieved will be of a type that can be passed to tftypes.NewValue.\nNullable\nIf the value is being set on a Go type that fills the Nullable interface:\ntype Nullable interface { SetNull(context.Context, bool) error SetValue(context.Context, interface{}) error GetNull(context.Context) bool GetValue(context.Context) interface{} } \nIt will be considered capable of handling null values, and those methods will be used to populate it and retrieve its value. The interface{} being passed and retrieved will be of a type that can be passed to tftypes.NewValue."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.7.x/writing-state",
  "text": "One of the primary jobs of a Terraform provider is to manage the provider's resources and data sources in the Terraform statefile. Writing values to state is something that provider developers will do frequently.\nfunc (m myResource) Create(ctx context.Context, req tfsdk.CreateResourceRequest, resp *tfsdk.CreateResourceResponse) \ntype resourceData struct { Name types.Strings `tfsdk:\"name\"` Age types.Number `tfsdk:\"age\"` Registered types.Bool `tfsdk:\"registered\"` Pets types.List `tfsdk:\"pets\"` Tags types.Map `tfsdk:\"tags\"` Address types.Object `tfsdk:\"address\"` } func (m myResource) Create(ctx context.Context, req tfsdk.CreateResourceRequest, resp *tfsdk.CreateResourceResponse) { var newState resourceData // update newState by modifying each property as usual for Go values newState.Name.Value = \"J. Doe\" // persist the values to state diags := resp.State.Set(ctx, &newState) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } } \nThe state information is represented as an object, and gets persisted like an object. See the conversion rules for an explanation on how objects get persisted and what Go types are valid for persisting as an object.\nUsing the attr.Value implementations can surface complexity that is unnecessary, however. For example, you can never set an unknown value in state, so there's no need to be able to express unknown values when setting state.\nTo make things a little easier, and to ensure that any type that Get can convert into can also be used as a value for Set, the framework can do some conversion on values passed to Set:\ntype resourceData struct { Name string `tfsdk:\"name\"` Age int64 `tfsdk:\"age\"` Registered bool `tfsdk:\"registered\"` Pets []string `tfsdk:\"pets\"` Tags map[string]string `tfsdk:\"tags\"` Address struct{ Street string `tfsdk:\"street\"` City string `tfsdk:\"city\"` State string `tfsdk:\"state\"` Zip int64 `tfsdk:\"zip\"` } `tfsdk:\"address\"` } \nSee below for the rules about conversion.\nAnother way to set values in the state is to write each new value separately. This doesn't require defining a type (except for objects), but means each value update steps outside of what the compiler can check, and may return an error at runtime.\nfunc (m myResource) Create(ctx context.Context, req tfsdk.CreateResourceRequest, resp *tfsdk.CreateResourceResponse) { age := types.Number{Value: big.NewFloat(7)} diags := resp.State.SetAttribute(ctx, tftypes.NewAttributePath().WithAttributeName(\"age\"), &age) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } } \nLike Set, SetAttribute can also do some conversion to standard Go types:\nfunc (m myResource) Create(ctx context.Context, req tfsdk.CreateResourceRequest, resp *tfsdk.CreateResourceResponse) { var age int64 age = 7 diags := resp.State.SetAttribute(ctx, tftypes.NewAttributePath().WithAttributeName(\"age\"), &age) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } } \nSee below for the rules about conversion.\nNote: The call to tftypes.NewAttributePath is creating an attribute path pointing to the specific attribute being updated. A less-verbose way to specify attribute paths is coming soon.\nThe following is a list of schema types and the Go types they know how to accept in Set and SetAttribute.\nString\nStrings can be automatically created from Go's string type (or any aliases of it, like type MyString string).\nNumber\nNumbers can be automatically created from the following numeric types (or any aliases of them, like type MyNumber int):\nint, int8, int16, int32, int64\nuint, uint8, uint16, uint32, uint64\nfloat32, float64\n*big.Int, *big.Float\nBoolean\nBooleans can be automatically created from Go's bool type (or any aliases of it, like type MyBoolean bool).\nList\nLists can be automatically created from any Go slice type (or alias of a Go slice type, like type MyList []string), with the elements either being attr.Value implementations or being converted according to these rules.\nMap\nMaps can be automatically created from any Go map type with string keys (or any alias of a Go map type with string keys, like type MyMap map[string]int), with the elements either being attr.Value implementations or being converted according to these rules.\nObject\nObjects can be automatically created from any Go struct type with that follows these constraints:\nEvery property on the struct must have a tfsdk struct tag.\nThe tfsdk struct tag must name an attribute in the object that it is being mapped to or be set to - to explicitly declare it does not map to an attribute in the object.\nEvery attribute in the object must have a corresponding struct tag.\nThese rules help prevent typos and human error from unwittingly discarding information by failing as early, consistently, and loudly as possible.\nProperties can either be attr.Value implementations or will be converted according to these rules.\nPointers\nA nil pointer will be treated as a null value. Otherwise, the rules for the type the pointer is referencing apply.\nDetected Interfaces\nSet detects and utilizes the following interfaces, if the target implements them.\nValueCreator\nIf a value is set on a Go type that implements the tftypes.ValueCreator interface, that interface will be delegated to to handle the conversion.\nUnknownable\nIf a value is set on a Go type that fills the Unknownable interface:\ntype Unknownable interface { SetUnknown(context.Context, bool) error SetValue(context.Context, interface{}) error GetUnknown(context.Context) bool GetValue(context.Context) interface{} } \nIt will be used to convert the value. The interface{} being passed and retrieved will be of a type that can be passed to tftypes.NewValue.\nNullable\nIf a value is set on a Go type that fills the Nullable interface:\ntype Nullable interface { SetNull(context.Context, bool) error SetValue(context.Context, interface{}) error GetNull(context.Context) bool GetValue(context.Context) interface{} } \nIt will be used to convert the value. The interface{} being passed and retrieved will be of a type that can be passed to tftypes.NewValue."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.17.x/data-sources/configure",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.17.x/resources/create",
  "text": "// ThingResource defines the resource implementation. // Some resource.Resource interface methods are omitted for brevity. type ThingResource struct { // client is configured via a Configure method, which is not shown in this // example for brevity. Refer to the Configure Resources documentation for // additional details for setting up resources with external clients. client *http.Client } // ThingResourceModel describes the Terraform resource data model to match the // resource schema. type ThingResourceModel struct { Name types.String `tfsdk:\"name\"` Id types.String `tfsdk:\"id\"` } // ThingResourceAPIModel describes the API data model. type ThingResourceAPIModel struct { Name string `json:\"name\"` Id string `json:\"id\"` } func (r ThingResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaReponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"name\": schema.StringAttribute{ MarkdownDescription: \"Name of the thing to be saved in the service.\", Required: true, }, \"id\": schema.StringAttribute{ Computed: true, MarkdownDescription: \"Service generated identifier for the thing.\", PlanModifiers: planmodifier.String{ stringplanmodifier.UseStateForUnknown(), }, }, }, MarkdownDescription: \"Manages a thing.\", } } func (r ThingResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data *ThingResourceModel // Read Terraform plan data into the model resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // Convert from Terraform data model into API data model createReq := ThingResourceAPIModel{ Name: data.Name.ValueString(), } httpReqBody, err := json.Marshal(createReq) if err != nil { resp.Diagnostics.AddError( \"Unable to Create Resource\", \"An unexpected error occurred while creating the resource create request. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Create HTTP request httpReq := http.NewRequestWithContext( ctx, http.MethodPost, \"http://example.com/things\", bytes.NewBuffer(httpReqBody), ) // Send HTTP request httpResp, err := d.client.Do(httpReq) defer httpResp.Body.Close() if err != nil { resp.Diagnostics.AddError( \"Unable to Create Resource\", \"An unexpected error occurred while attempting to create the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Error: \"+err.Error(), ) return } // Return error if the HTTP status code is not 200 OK if httpResp.StatusCode != http.StatusOK { resp.Diagnostics.AddError( \"Unable to Create Resource\", \"An unexpected error occurred while attempting to create the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Status: \"+httpResp.Status, ) return } var createResp ThingResourceAPIModel err := json.NewDecoder(httpResp.Body).Decode(&createResp) if err != nil { resp.Diagnostics.AddError( \"Unable to Create Resource\", \"An unexpected error occurred while parsing the resource creation response. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Convert from the API data model to the Terraform data model // and set any unknown attribute values. data.Id = types.StringValue(createResp.Id) // Save data into Terraform state resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.17.x/resources/configure",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.17.x/resources/read",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.17.x/resources/update",
  "text": "// ThingResource defines the resource implementation. // Some resource.Resource interface methods are omitted for brevity. type ThingResource struct { // client is configured via a Configure method, which is not shown in this // example for brevity. Refer to the Configure Resources documentation for // additional details for setting up resources with external clients. client *http.Client } // ThingResourceModel describes the Terraform resource data model to match the // resource schema. type ThingResourceModel struct { Name types.String `tfsdk:\"name\"` Id types.String `tfsdk:\"id\"` } // ThingResourceAPIModel describes the API data model. type ThingResourceAPIModel struct { Name string `json:\"name\"` Id string `json:\"id\"` } func (r ThingResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"name\": schema.StringAttribute{ MarkdownDescription: \"Name of the thing to be saved in the service.\", Required: true, }, \"id\": schema.StringAttribute{ Computed: true, MarkdownDescription: \"Service generated identifier for the thing.\", PlanModifiers: planmodifier.String{ stringplanmodifier.UseStateForUnknown(), }, }, }, MarkdownDescription: \"Manages a thing.\", } } func (r ThingResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data ThingResourceModel // Read Terraform plan data into the model resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) // Convert from Terraform data model into API data model updateReq := ThingResourceAPIModel{ Id: data.Id.StringValue(), Name: data.Name.StringValue(), } httpReqBody, err := json.Marshal(updateReq) if err != nil { resp.Diagnostics.AddError( \"Unable to Update Resource\", \"An unexpected error occurred while creating the resource update request. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Create HTTP request httpReq := http.NewRequestWithContext( ctx, http.MethodPut, \"http://example.com/things\", bytes.NewBuffer(httpReqBody), ) // Send HTTP request httpResp, err := d.client.Do(httpReq) defer httpResp.Body.Close() if err != nil { resp.Diagnostics.AddError( \"Unable to Update Resource\", \"An unexpected error occurred while attempting to update the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Error: \"+err.Error(), ) return } // Return error if the HTTP status code is not 200 OK if httpResp.StatusCode != http.StatusOK { resp.Diagnostics.AddError( \"Unable to Update Resource\", \"An unexpected error occurred while attempting to update the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Status: \"+httpResp.Status, ) return } // Save updated data into Terraform state resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.17.x/resources/delete",
  "text": "// ThingResource defines the resource implementation. // Some resource.Resource interface methods are omitted for brevity. type ThingResource struct { // client is configured via a Configure method, which is not shown in this // example for brevity. Refer to the Configure Resources documentation for // additional details for setting up resources with external clients. client *http.Client } // ThingResourceModel describes the Terraform resource data model to match the // resource schema. type ThingResourceModel struct { Name types.String `tfsdk:\"name\"` Id types.String `tfsdk:\"id\"` } // ThingResourceAPIModel describes the API data model. type ThingResourceAPIModel struct { Name string `json:\"name\"` Id string `json:\"id\"` } func (r ThingResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"name\": schema.StringAttribute{ MarkdownDescription: \"Name of the thing to be saved in the service.\", Required: true, }, \"id\": schema.StringAttribute{ Computed: true, MarkdownDescription: \"Service generated identifier for the thing.\", PlanModifiers: planmodifier.String{ stringplanmodifier.UseStateForUnknown(), }, }, }, MarkdownDescription: \"Manages a thing.\", } } func (r ThingResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { var data ThingResourceModel // Read Terraform prior state data into the model resp.Diagnostics.Append(req.State.Get(ctx, &data)...) // Convert from Terraform data model into API data model readReq := ThingResourceAPIModel{ Id: data.Id.ValueString(), Name: data.Name.ValueString(), } httpReqBody, err := json.Marshal(readReq) if err != nil { resp.Diagnostics.AddError( \"Unable to Delete Resource\", \"An unexpected error occurred while creating the resource d request. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Create HTTP request httpReq := http.NewRequestWithContext( ctx, http.MethodDelete, \"http://example.com/things\", bytes.NewBuffer(httpReqBody), ) // Send HTTP request httpResp, err := d.client.Do(httpReq) defer httpResp.Body.Close() if err != nil { resp.Diagnostics.AddError( \"Unable to Delete Resource\", \"An unexpected error occurred while attempting to delete the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Error: \"+err.Error(), ) return } // Return error if the HTTP status code is not 200 OK or 404 Not Found if httpResp.StatusCode != http.StatusNotFound && httpResp.StatusCode != http.StatusOK { resp.Diagnostics.AddError( \"Unable to Delete Resource\", \"An unexpected error occurred while attempting to delete the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Status: \"+httpResp.Status, ) return } // If the logic reaches here, it implicitly succeeded and will remove // the resource from state if there are no other errors. } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.17.x/resources/plan-modification",
  "text": "After validation and before applying configuration changes, Terraform generates a plan that describes the expected values and behaviors of those changes. Providers can then tailor the plan to match the expected end state. For example, they may replace unknown values with expected known values or mark a resource that must be replaced. Users can perform this plan modification for an attribute or an entire resource.\nTerraform and the framework support three types of plan modification on resources:\nAdjusting attribute values, such as providing a known remote default value when a configuration is not present.\nReturning warning or error diagnostics on planned resource destruction with Terraform 1.3 and later.\nApply attribute plan modifiers.\nApply resource plan modifiers.\nNOTE: Providers and data sources do not use the same planning mechanism as resources within Terraform. Neither support the concept of plan modification. Data sources should set any planned values in the Read method.\n// stringDefaultModifier is a plan modifier that sets a default value for a // types.StringType attribute when it is not configured. The attribute must be // marked as Optional and Computed. When setting the state during the resource // Create, Read, or Update methods, this default value must also be included or // the Terraform CLI will generate an error. type stringDefaultModifier struct { Default string } // Description returns a plain text description of the validator's behavior, suitable for a practitioner to understand its impact. func (m stringDefaultModifier) Description(ctx context.Context) string { return fmt.Sprintf(\"If value is not configured, defaults to %s\", m.Default) } // MarkdownDescription returns a markdown formatted description of the validator's behavior, suitable for a practitioner to understand its impact. func (m stringDefaultModifier) MarkdownDescription(ctx context.Context) string { return fmt.Sprintf(\"If value is not configured, defaults to `%s`\", m.Default) } // Modify runs the logic of the plan modifier. // Access to the configuration, plan, and state is available in `req`, while // `resp` contains fields for updating the planned value, triggering resource // replacement, and returning diagnostics. func (m stringDefaultModifier) ModifyString(ctx context.Context, req planmodifier.StringRequest, resp *planmodifier.StringResponse) { // If the value is unknown or known, do not set default value. if !req.PlanValue.IsNull() { return } resp.AttributePlan = types.StringValue(m.Default) } \nfunc stringDefault(defaultValue string) planmodifier.String { return stringDefaultModifier{ Default: defaultValue, } } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.17.x/resources/timeouts",
  "text": "func (t *exampleResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { return schema.Schema{ Attributes: map[string]schema.Attribute{ /* ... */ \"timeouts\": timeouts.Attributes(ctx, timeouts.Opts{ Create: true, }), }, \nThe exampleResourceData model needs to be modified to include a field for timeouts, which is types.Object.\ntype exampleResourceData struct { /* ... */ Timeouts types.Object `tfsdk:\"timeouts\"` \nfunc (e *exampleResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleResourceData diags := req.Plan.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } defaultCreateTimeout := 20 * time.Minute createTimeout := timeouts.Create(ctx, data.Timeouts, defaultCreateTimeout) ctx, cancel := context.WithTimeout(ctx, createTimeout) defer cancel() /* ... */ }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.17.x/resources/private-state",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.17.x/resources/import",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.17.x/data-sources/timeouts",
  "text": "func (d *ThingDataSource) Schema(ctx context.Context, req datasource.SchemaRequest, resp *datasource.SchemaResponse) { resp.Schema = schema.Schema{ /* ... */ Blocks: map[string]schema.Block{ \"timeouts\": timeouts.Block(ctx, timeouts.Opts{ Read: true, }), }, \nfunc (d *ThingDataSource) Schema(ctx context.Context, req datasource.SchemaRequest, resp *datasource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ /* ... */ \"timeouts\": timeouts.Attributes(ctx, timeouts.Opts{ Read: true, }), }, \nModify the exampleDataSourceData model to include a field for timeouts using a types.Object type.\ntype exampleDataSourceData struct { /* ... */ Timeouts types.Object `tfsdk:\"timeouts\"` \nfunc (e *exampleDataSource) Read(ctx context.Context, req datasource.ReadRequest, resp *datasource.ReadResponse) { var data exampleDataSourceData diags := req.Config.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } defaultReadTimeout := 20 * time.Minute readTimeout := timeouts.Read(ctx, data.Timeouts, defaultReadTimeout) ctx, cancel := context.WithTimeout(ctx, readTimeout) defer cancel() /* ... */ }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.3.x/deprecations",
  "text": "Plugin Development - Deprecations, Removals, and Renames Best Practices | Terraform\nTerraform is trusted for managing many facets of infrastructure across many organizations. Part of that trust is due to consistent versioning guidelines and setting expectations for various levels of upgrades. Ensuring backwards compatibility for all patch and minor releases, potentially in concert with any upcoming major changes, is recommended and supported by the Terraform development framework. This allows operators to iteratively update their Terraform configurations rather than require massive refactoring.\nThis guide is designed to walk through various scenarios where existing Terraform functionality requires future removal, while maintaining backwards compatibility. Further information about the versioning terminology (e.g. MAJOR.MINOR.PATCH) in this guide can be found in the versioning guidelines documentation.\nNOTE: Removals should only ever occur in MAJOR version upgrades.\nNOTE: This documentation references usage of the DeprecationMessage field, please see the schema documentation for more detailed guidance on how to structure warning messages and when those warnings will be raised to practitioners.\nTable of Contents\nProvider Attribute Removal\nProvider Attribute Rename\nRenaming a Required Attribute\nRenaming an Optional Attribute\nRenaming a Computed Attribute\nProvider Data Source or Resource Removal\nProvider Data Source or Resource Rename\nThe recommended process for removing an attribute from a data source or resource in a provider is as follows:\nAdd a DeprecationMessage in the attribute schema definition. Set this field to a practitioner actionable message such as \"Remove this attribute's configuration as it's no longer in use and the attribute will be removed in the next major version of the provider.\"\nEnsure the changelog has an entry noting the deprecation.\nRelease a MINOR version with the deprecation.\nIn the next MAJOR version, remove all code associated with the attribute including the schema definition.\nEnsure the changelog has an entry noting the removal.\nRelease the MAJOR version.\nWhen renaming an attribute from one name to another, it is important to keep backwards compatibility with both existing Terraform configurations and the Terraform state while operators migrate. To accomplish this, there will be some duplicated logic to support both attributes until the next MAJOR release. Once both attributes are appropriately handled, the process for deprecating and removing the old attribute is the same as noted in the Provider Attribute Removal section.\nThe procedure for renaming an attribute depends on what type of attribute it is:\nRenaming a Required Attribute\nRenaming an Optional Attribute\nRenaming a Computed Attribute\nRenaming a Required Attribute\nNOTE: If the schema definition does not contain Optional or Required, see the Renaming a Computed Attribute section instead. If the schema definition contains Optional instead of Required, see the Renaming an Optional Attribute section.\nRequired attributes are also referred to as required \"arguments\" throughout the Terraform documentation.\nIn general, the procedure here does two things:\nPrevents the operator from needing to define two attributes with the same value.\nAllows the operator to migrate the configuration to the new attribute at the same time requiring that any other references only work with the new attribute. This is to prevent a situation with Terraform showing a difference when the existing attribute is configured, but the new attribute is saved into the Terraform state. For example, in terraform plan output format:\nexisting_attribute: \"\" => \"value\" new_attribute: \"value\" => \"\" \nThe recommended process is as follows:\nReplace Required: true with Optional: true in the existing attribute schema definition.\nReplace Required with Optional in the existing attribute documentation.\nDuplicate the schema definition of the existing attribute, renaming one of them with the new attribute name.\nDuplicate the documentation of the existing attribute, renaming one of them with the new attribute name.\nAdd a DeprecationMessage to the schema definition of the existing (now the \"old\") attribute, noting to use the new attribute in the message.\nAdd **Deprecated** to the documentation of the existing (now the \"old\") attribute, noting to use the new attribute.\nAdd a note to the documentation that either the existing (now the \"old\") attribute or new attribute must be configured.\nAdd the type-specific validator {type}validator.ExactlyOneOf to the schema definition of the new attribute, with a path expression matching the old attribute. This will ensure at least one of the attributes is configured, but present an error to the operator if both are configured at the same time. For example, an attribute of type string would use the stringvalidator.ExactlyOneOf validator.\nAdd conditional logic in the Create and Update functions of the data source or resource to handle both attributes. Generally, this involves using {type}.IsNull().\nFollow the rest of the procedures in the Provider Attribute Removal section. When the old attribute is removed, update the schema definition and documentation of the new attribute back to Required, and remove the {type}validator.ExactlyOneOf validator.\nExample Renaming of a Required Attribute\nGiven this sample resource:\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"existing_attribute\": schema.StringAttribute{ Required: true, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... ExistingAttribute types.String `tfsdk:\"existing_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add attribute to provider create API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { // ... other logic ... } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add attribute to provider update API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \nIn order to support renaming existing_attribute to new_attribute, this sample can be written as the following to support both attributes simultaneously until the existing_attribute is removed:\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework-validators/stringvalidator\" \"github.com/hashicorp/terraform-plugin-framework/path\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/schema/validator\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"existing_attribute\": schema.StringAttribute{ Optional: true, DeprecationMessage: \"use new_attribute instead\", }, \"new_attribute\": schema.StringAttribute{ Optional: true, Validators: []validator.String{ stringvalidator.ExactlyOneOf(path.Expressions{ path.MatchRoot(\"existing_attribute\"), }...), }, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... ExistingAttribute types.String `tfsdk:\"existing_attribute\"` NewAttribute types.String `tfsdk:\"new_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } if !data.NewAttribute.IsNull() { // add NewAttribute to provider create API call } else { // add ExistingAttribute to provider create API call } // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { // ... other logic ... } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } if !data.NewAttribute.IsNull() { // add NewAttribute to provider create API call } else { // add ExistingAttribute to provider create API call } // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \nWhen the existing_attribute is ready for removal, then this can be written as:\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"new_attribute\": schema.StringAttribute{ Required: true, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... NewAttribute types.String `tfsdk:\"new_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add NewAttribute to provider create API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { // ... other logic ... } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add NewAttribute to provider create API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \nRenaming an Optional Attribute\nNOTE: If the schema definition does not contain Optional or Required, see the Renaming a Computed Attribute section instead. If the schema definition contains Required instead of Optional, see the Renaming a Required Attribute section.\nOptional attributes are also referred to as optional \"arguments\" throughout the Terraform documentation.\nIn general, the procedure here allows the operator to migrate the configuration to the new attribute at the same time requiring that any other references only work with the new attribute. This is to prevent a situation with Terraform showing a difference when the existing attribute is configured, but the new attribute is saved into the Terraform state. For example, in terraform plan output format:\nexisting_attribute: \"\" => \"value\" new_attribute: \"value\" => \"\" \nThe recommended process is as follows:\nDuplicate the schema definition of the existing attribute, renaming one of them with the new attribute name.\nDuplicate the documentation of the existing attribute, renaming one of them with the new attribute name.\nAdd a DeprecationMessage to the schema definition of the existing (now the \"old\") attribute, noting to use the new attribute in the message.\nAdd **Deprecated** to the documentation of the existing (now the \"old\") attribute, noting to use the new attribute.\nAdd the type-specific validator {type}validator.ExactlyOneOf to the schema definition of the new attribute, with a path expression matching the old attribute. This will ensure at least one of the attributes is configured, but present an error to the operator if both are configured at the same time. For example, an attribute of type string would use the stringvalidator.ExactlyOneOf validator.\nAdd conditional logic in the Create and Update functions of the data source or resource to handle both attributes. Generally, this involves using {type}.IsNull().\nFollow the rest of the procedures in the Provider Attribute Removal section. When the old attribute is removed, remove the {type}validator.ExactlyOneOf validator.\nExample Renaming of an Optional Attribute\nGiven this sample resource:\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"existing_attribute\": schema.StringAttribute{ Optional: true, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... ExistingAttribute types.String `tfsdk:\"existing_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add attribute to provider create API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { // ... other logic ... } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add attribute to provider update API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \nIn order to support renaming existing_attribute to new_attribute, this sample can be written as the following to support both attributes simultaneously until the existing_attribute is removed:\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework-validators/stringvalidator\" \"github.com/hashicorp/terraform-plugin-framework/path\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/schema/validator\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"existing_attribute\": schema.StringAttribute{ Optional: true, DeprecationMessage: \"use new_attribute instead\", }, \"new_attribute\": schema.StringAttribute{ Optional: true, Validators: []validator.String{ stringvalidator.ExactlyOneOf(path.Expressions{ path.MatchRoot(\"existing_attribute\"), }...), }, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... ExistingAttribute types.String `tfsdk:\"existing_attribute\"` NewAttribute types.String `tfsdk:\"new_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } if !data.NewAttribute.IsNull() { // add NewAttribute to provider create API call } else { // add ExistingAttribute to provider create API call } // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { // ... other logic ... } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } if !data.NewAttribute.IsNull() { // add NewAttribute to provider create API call } else { // add ExistingAttribute to provider create API call } // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \nWhen the existing_attribute is ready for removal, then this can be written as:\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"new_attribute\": schema.StringAttribute{ Optional: true, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... NewAttribute types.String `tfsdk:\"new_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add NewAttribute to provider create API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { // ... other logic ... } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add NewAttribute to provider create API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \nRenaming a Computed Attribute\nNOTE: If the schema definition contains Optional see the Renaming an Optional Attribute section instead. If the schema definition contains Required see the Renaming a Required Attribute section instead.\nThe recommended process is as follows:\nDuplicate the schema definition of the existing attribute, renaming one of them with the new attribute name.\nDuplicate the documentation of the existing attribute, renaming one of them with the new attribute name.\nAdd a DeprecationMessage to the schema definition of the existing (now the \"old\") attribute, noting to use the new attribute in the message.\nAdd **Deprecated** to the documentation of the existing (now the \"old\") attribute, noting to use the new attribute.\nSet both attributes in the Terraform state in the Create, Update, and Read functions of the resource (Read only for data source).\nFollow the rest of the procedures in the Provider Attribute Removal section.\nExample Renaming of a Computed Attribute\nGiven this sample resource:\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"existing_attribute\": schema.StringAttribute{ Computed: true, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... ExistingAttribute types.String `tfsdk:\"existing_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.ExistingAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.State.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.ExistingAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.ExistingAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \nIn order to support renaming existing_attribute to new_attribute, this sample can be written as the following to support both attributes simultaneously until the existing_attribute is removed:\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"existing_attribute\": schema.StringAttribute{ Computed: true, DeprecationMessage: \"use new_attribute instead\", }, \"new_attribute\": schema.StringAttribute{ Computed: true, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... ExistingAttribute types.String `tfsdk:\"existing_attribute\"` NewAttribute types.String `tfsdk:\"new_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.ExistingAttribute = // set to computed value data.NewAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.State.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.ExistingAttribute = // set to computed value data.NewAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.ExistingAttribute = // set to computed value data.NewAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \nWhen the existing_attribute is ready for removal, then this can be written as:\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"new_attribute\": schema.StringAttribute{ Computed: true, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... NewAttribute types.String `tfsdk:\"new_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.NewAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.State.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.NewAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.NewAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \nThe recommended process for removing a data source or resource from a provider is as follows:\nAdd a DeprecationMessage in the data source or resource schema definition. After an operator upgrades to this version, they will be shown a warning with the message provided when using the deprecated data source or resource, but the Terraform run will still complete.\nEnsure the changelog has an entry noting the deprecation.\nRelease a MINOR version with the deprecation.\nIn the next MAJOR version, remove all code associated with the deprecated data source or resource except for the schema and replace the Create, Read, Update, and Delete functions to always return an error diagnostic. Remove the documentation sidebar link and update the resource or data source documentation page to include information about the removal and any potential migration information. After an operator upgrades to this version, they will be shown an error about the missing data source or resource.\nEnsure the changelog has an entry noting the removal.\nRelease the MAJOR version.\nIn the next MAJOR version, remove all code associated with the removed data source or resource. Remove the resource or data source documentation page.\nRelease the MAJOR version.\nExample Resource Removal\nGiven this sample provider and resource:\n// ... provider implementation ... func (p *exampleProvider) Resources(ctx context.Context) []func() resource.Resource { return []func() resource.Resource{ //... other resources ... NewWidgetResource, } } \nfunc NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... }, } } // ... resource implementation ... \nIn order to deprecate example_widget, this sample can be written as:\n// ... provider implementation ... func (p *exampleProvider) Resources(ctx context.Context) []func() resource.Resource { return []func() resource.Resource{ //... other resources ... NewWidgetResource, } } \nfunc NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... }, DeprecationMessage: \"use example_thing resource instead\", } } // ... resource implementation ... \nTo soft remove example_widget with a friendly error message, this sample can be written as:\n// ... provider implementation ... func (p *exampleProvider) Resources(ctx context.Context) []func() resource.Resource { return []func() resource.Resource{ //... other resources ... NewWidgetResource, } } \nfunc NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... }, DeprecationMessage: \"use example_thing resource instead\", } } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_widget resource deprecated\", \"use example_thing resource instead\"), ) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_widget resource deprecated\", \"use example_thing resource instead\"), ) } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_widget resource deprecated\", \"use example_thing resource instead\"), ) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_widget resource deprecated\", \"use example_thing resource instead\"), ) } \nTo remove example_widget:\n// ... provider implementation ... func (p *exampleProvider) Resources(ctx context.Context) []func() resource.Resource { return []func() resource.Resource{ //... other resources ... } } \nWhen renaming a resource from one name to another, it is important to keep backwards compatibility with both existing Terraform configurations and the Terraform state while operators migrate. To accomplish this, there will be some duplicated logic to support both resources until the next MAJOR release. Once both resources are appropriately handled, the process for deprecating and removing the old resource is the same as noted in the Provider Data Source or Resource Removal section.\nThe recommended process is as follows:\nDuplicate the code of the existing resource, renaming (and potentially modifying) functions as necessary.\nDuplicate the documentation of the existing resource, renaming (and potentially modifying) as necessary.\nAdd a DeprecationMessage to the schema definition of the existing (now the \"old\") resource, noting to use the new resource in the message.\nAdd ~> This resource is deprecated and will be removed in the next major version to the documentation of the existing (now the \"old\") resource, noting to use the new resource.\nAdd the new resource to the provider Resources function\nFollow the rest of the procedures in the Provider Data Source or Resource Removal section.\nExample Resource Renaming\nGiven this sample provider and resource:\n// ... provider implementation ... func (p *exampleProvider) Resources(ctx context.Context) []func() resource.Resource { return []func() resource.Resource{ //... other resources ... NewExistingWidgetResource, } } \nfunc NewExistingWidgetResource() resource.Resource { return &exampleExistingWidgetResource{} } func (e *exampleExistingWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... }, } } // ... resource implementation ... \nIn order to support renaming example_existing_widget to example_new_widget, this sample can be written as the following to support both resources simultaneously until the example_existing_widget resource is removed:\n// ... provider implementation ... func (p *exampleProvider) Resources(ctx context.Context) []func() resource.Resource { return []func() resource.Resource{ //... other resources ... NewExistingWidgetResource, NewWidgetResource, } } \nfunc NewExistingWidgetResource() resource.Resource { return &exampleExistingWidgetResource{} } func (e *exampleExistingWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... }, DeprecationMessage: \"use example_new_widget resource instead\", } } // ... resource implementation ... \nfunc NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... }, } } // ... resource implementation ... \nTo soft remove example_existing_widget with a friendly error message:\n// ... provider implementation ... func (p *exampleProvider) Resources(ctx context.Context) []func() resource.Resource { return []func() resource.Resource{ //... other resources ... NewExistingWidgetResource, NewWidgetResource, } } \nfunc NewExistingWidgetResource() resource.Resource { return &exampleExistingWidgetResource{} } func (e *exampleExistingWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... }, DeprecationMessage: \"use example_new_widget resource instead\", } } func (e *exampleExistingWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_existing_widget resource deprecated\", \"use example_new_widget resource instead\"), ) } func (e *exampleExistingWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_existing_widget resource deprecated\", \"use example_new_widget resource instead\"), ) } func (e *exampleExistingWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_existing_widget resource deprecated\", \"use example_new_widget resource instead\"), ) } func (e *exampleExistingWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_existing_widget resource deprecated\", \"use example_new_widget resource instead\"), ) } // ... resource implementation ... \nfunc NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... }, } } // ... resource implementation ... \nTo remove example_existing_widget:\n// ... provider implementation ... func (p *exampleProvider) Resources(ctx context.Context) []func() resource.Resource { return []func() resource.Resource{ //... other resources ... NewWidgetResource, } } \nfunc NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... }, } } // ... resource implementation ..."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.0.x/data-sources/configure",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.0.x/resources/configure",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.7.x/resources",
  "text": "Plugin Development - Framework: Resources | Terraform\nResources are an abstraction that allow Terraform to manage infrastructure objects, such as a compute instance, an access policy, or disk. Terraform assumes that every resource:\noperates as a pure key/value store, with values getting returned exactly as they were written.\nneeds only one API call to update or return its state.\ncan be be created, read, updated, and deleted.\nThis page describes the initial implementation details required for supporting a resource within the provider. Resource lifecycle management functionality is also required:\nFurther documentation is available for deeper resource concepts:\nConfigure resources with provider-level data types or clients.\nDefault for specifying a default value for an attribute that is null within the configuration.\nImport state so practitioners can bring existing resources under Terraform lifecycle management.\nManage private state to store additional data in resource state that is not shown in plans.\nModify plans to enrich the output for expected resource behaviors during changes, or marking a resource for replacement if an in-place update cannot occur.\nUpgrade state to transparently update state data outside plans.\nValidate practitioner configuration against acceptable values.\nTimeouts in practitioner configuration for use in resource create, read, update and delete functions.\nImplement the resource.Resource interface. Ensure the Add Resource To Provider documentation is followed so the resource becomes part of the provider implementation, and therefore available to practitioners.\nThe resource.Resource interface Metadata method defines the resource name as it would appear in Terraform configurations. This name should include the provider type prefix, an underscore, then the resource specific name. For example, a provider named examplecloud and a resource that reads \"thing\" resources would be named examplecloud_thing.\nIn this example, the resource name in an examplecloud provider that reads \"thing\" resources is hardcoded to examplecloud_thing:\n// With the resource.Resource implementation func (r *ThingResource) Metadata(ctx context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = \"examplecloud_thing\" } \nTo simplify resource implementations, the provider.MetadataResponse.TypeName field from the provider.Provider interface Metadata method can set the provider name so it is available in the resource.MetadataRequest.ProviderTypeName field.\nIn this example, the provider defines the examplecloud name for itself, and the data source is named examplecloud_thing:\n// With the provider.Provider implementation func (p *ExampleCloudProvider) Metadata(ctx context.Context, req provider.MetadataRequest, resp *provider.MetadataResponse) { resp.TypeName = \"examplecloud\" } // With the resource.Resource implementation func (d *ThingDataSource) Metadata(ctx context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_thing\" } \nSchema Method\nThe resource.Resource interface Schema method defines a schema describing what data is available in the resource's configuration, plan, and state.\nResources become available to practitioners when they are included in the provider implementation via the provider.Provider interface Resources method.\nIn this example, the ThingResource type, which implements the resource.Resource interface, is added to the provider implementation:\n// With the provider.Provider implementation func (p *ExampleCloudProvider) Resources(_ context.Context) []func() resource.Resource { return []func() resource.Resource{ func() resource.Resource { return &ThingResource{}, }, } } \nTo simplify provider implementations, a named function can be created with the resource implementation.\nIn this example, the ThingResource code includes an additional NewThingResource function, which simplifies the provider implementation:\n// With the provider.Provider implementation func (p *ExampleCloudProvider) Resources(_ context.Context) []func() resource.Resource { return []func() resource.Resource{ NewThingResource, } } // With the resource.Resource implementation func NewThingResource() resource.Resource { return &ThingResource{} }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.7.x/functions",
  "text": "Plugin Development - Framework: Functions | Terraform\nProvider-defined function support is in technical preview and offered without compatibility promises until Terraform 1.8 is generally available.\nFunctions are an abstraction that allow providers to expose computational logic beyond Terraform's built-in functions and simplify practitioner configurations. Provider-defined functions are supported in Terraform 1.8 and later.\nNote\nIt is possible to add functions to an existing provider. If you do not have an existing provider, you will need to create your own provider to contain the functions. Please see Getting Started - Code Walkthrough to learn how to create your first provider.\nLearn about Terraform's concepts for provider-defined functions, such as intended purpose, example use cases, and terminology. The framework's implementation details, such as naming, are based on these concepts.\nLearn about how to implement code for a provider-defined function in the framework.\nLearn about how to ensure a provider-defined function implementation works as expected via unit testing and acceptance testing.\nLearn about how to document a provider-defined function implementation so practitioners can discover and use the function."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.7.x/handling-data/schemas",
  "text": "Plugin Development - Framework: Schemas | Terraform\nSchemas specify the constraints of Terraform configuration blocks. They define what fields a provider, resource, or data source configuration block has, and give Terraform metadata about those fields. You can think of the schema as the \"type information\" or the \"shape\" of a resource, data source, or provider.\nEach concept has its own schema package and Schema type, which defines functionality available to that concept:\nProviders: provider/schema.Schema\nResources: resource/schema.Schema\nData Sources: datasource/schema.Schema\nDuring execution of the terraform validate, terraform plan and terraform apply commands, Terraform calls the provider GetProviderSchema RPC, in which the framework calls the provider.Provider interface Schema method, and the resource.Resource interface Schema method and datasource.DataSource interface Schema method on each of the resources and data sources, respectively.\nVersion is only valid for resources.\nEvery schema has a version, which is an integer that allows you to track changes to your schemas. It is generally only used when upgrading resource state, to help massage resources created with earlier schemas into the shape defined by the current schema. It will never be used for provider or data source schemas and can be omitted.\nNot every resource, data source, or provider will be supported forever. Sometimes designs change or APIs are deprecated. Schemas that have their DeprecationMessage property set will display that message as a warning when that provider, data source, or resource is used. A good message will tell practitioners that the provider, resource, or data source is deprecated, and will indicate a migration strategy.\nVarious tooling like terraform-plugin-docs and the language server can use metadata in the schema to generate documentation or offer a better editor experience for practitioners. Use the Description property to add a description of a resource, data source, or provider that these tools can leverage.\nSimilar to the Description property, the MarkdownDescription is used to provide a markdown-formatted version of the description to tooling like terraform-plugin-docs. It is a best practice to only alter the formatting, not the content, between the Description and MarkdownDescription.\nAt the moment, if the MarkdownDescription property is set it will always be used instead of the Description property. It is possible that a different strategy may be employed in the future to surface descriptions to other tooling in a different format, so we recommend specifying both fields.\nSchemas can be unit tested via each of the schema.Schema type ValidateImplementation() methods. This unit testing raises schema implementation issues more quickly in comparison to acceptance tests, but does not replace the purpose of acceptance testing.\nIn this example, a thing_resource_test.go file is created alongside the thing_resource.go implementation file:\nimport ( \"context\" \"testing\" // The fwresource import alias is so there is no collistion // with the more typical acceptance testing import: // \"github.com/hashicorp/terraform-plugin-testing/helper/resource\" fwresource \"github.com/hashicorp/terraform-plugin-framework/resource\" ) func TestThingResourceSchema(t *testing.T) { t.Parallel() ctx := context.Background() schemaRequest := fwresource.SchemaRequest{} schemaResponse := &fwresource.SchemaResponse{} // Instantiate the resource.Resource and call its Schema method NewThingResource().Schema(ctx, schemaRequest, schemaResponse) if schemaResponse.Diagnostics.HasError() { t.Fatalf(\"Schema method diagnostics: %+v\", schemaResponse.Diagnostics) } // Validate the schema diagnostics := schemaResponse.Schema.ValidateImplementation(ctx) if diagnostics.HasError() { t.Fatalf(\"Schema validation diagnostics: %+v\", diagnostics) } }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.7.x-benefits",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.7.x/data-sources",
  "text": "Plugin Development - Framework: Data Sources | Terraform\nData sources are an abstraction that allow Terraform to reference external data. Unlike managed resources, Terraform does not manage the lifecycle of the resource or data. Data sources are intended to have no side-effects.\nThis page describes the basic implementation details required for supporting a data source within the provider. Further documentation is available for deeper data source concepts:\nConfigure data sources with provider-level data types or clients.\nValidate practitioner configuration against acceptable values.\nTimeouts in practitioner configuration for use in a data source read function.\nImplement the datasource.DataSource interface. Each of the methods is described in more detail below.\nIn this example, a data source named examplecloud_thing with hardcoded behavior is defined:\n// Ensure the implementation satisfies the desired interfaces. var _ datasource.DataSource = &ThingDataSource{} type ThingDataSource struct {} type ThingDataSourceModel struct { ExampleAttribute types.String `tfsdk:\"example_attribute\"` ID types.String `tfsdk:\"id\"` } func (d *ThingDataSource) Metadata(ctx context.Context, req datasource.MetadataRequest, resp *datasource.MetadataResponse) { resp.TypeName = \"examplecloud_thing\" } func (d *ThingDataSource) Schema(ctx context.Context, req datasource.SchemaRequest, resp *datasource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"example_attribute\": schema.StringAttribute{ Required: true, }, \"id\": schema.StringAttribute{ Computed: true, }, }, } } func (d *ThingDataSource) Read(ctx context.Context, req datasource.ReadRequest, resp *datasource.ReadResponse) { var data ThingDataSourceModel // Read Terraform configuration data into the model resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) // Typically data sources will make external calls, however this example // hardcodes setting the id attribute to a specific value for brevity. data.ID = types.StringValue(\"example-id\") // Save data into Terraform state resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } \nThe datasource.DataSource interface Metadata method defines the data source name as it would appear in Terraform configurations. This name should include the provider type prefix, an underscore, then the data source specific name. For example, a provider named examplecloud and a data source that reads \"thing\" resources would be named examplecloud_thing. Ensure the Add Data Source To Provider documentation is followed so the data source becomes part of the provider implementation, and therefore available to practitioners.\nIn this example, the data source name in an examplecloud provider that reads \"thing\" resources is hardcoded to examplecloud_thing:\n// With the datasource.DataSource implementation func (d *ThingDataSource) Metadata(ctx context.Context, req datasource.MetadataRequest, resp *datasource.MetadataResponse) { resp.TypeName = \"examplecloud_thing\" } \nTo simplify data source implementations, the provider.MetadataResponse.TypeName field from the provider.Provider interface Metadata method can set the provider name so it is available in the datasource.MetadataRequest.ProviderTypeName field.\nIn this example, the provider defines the examplecloud name for itself, and the data source is named examplecloud_thing:\n// With the provider.Provider implementation func (p *ExampleCloudProvider) Metadata(ctx context.Context, req provider.MetadataRequest, resp *provider.MetadataResponse) { resp.TypeName = \"examplecloud\" } // With the datasource.DataSource implementation func (d *ThingDataSource) Metadata(ctx context.Context, req datasource.MetadataRequest, resp *datasource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_thing\" } \nSchema Method\nThe datasource.DataSource interface Schema method defines a schema describing what data is available in the data source's configuration and state.\nRead Method\nThe datasource.DataSource interface Read method defines how the data source updates Terraform's state to reflect the retrieved data. There is no plan or prior state to work with in Read requests, only configuration.\nDuring the terraform apply, terraform plan, and terraform refresh commands, Terraform calls the provider ReadDataSource RPC, in which the framework calls the datasource.DataSource interface Read method.\nAccessing configuration data from the datasource.ReadRequest.Config field.\nRetriving any additional data, such as remote system information.\nWriting state data into the datasource.ReadResponse.State field.\nIf the logic needs to return warning or error diagnostics, they can added into the datasource.ReadResponse.Diagnostics field.\nData sources become available to practitioners when they are included in the provider implementation via the provider.ProviderWithDataSources interface DataSources method.\nIn this example, the ThingDataSource type, which implements the datasource.DataSource interface, is added to the provider implementation:\n// With the provider.Provider implementation func (p *ExampleCloudProvider) DataSources(_ context.Context) []func() datasource.DataSource { return []func() datasource.DataSource{ func() datasource.DataSource { return &ThingDataSource{}, }, } } \nTo simplify provider implementations, a named function can be created with the data source implementation.\nIn this example, the ThingDataSource code includes an additional NewThingDataSource function, which simplifies the provider implementation:\n// With the provider.Provider implementation func (p *ExampleCloudProvider) DataSources(_ context.Context) []func() datasource.DataSource { return []func() datasource.DataSource{ NewThingDataSource, } } // With the datasource.DataSource implementation func NewThingDataSource() datasource.DataSource { return &ThingDataSource{} }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.2.x/deprecations",
  "text": "Plugin Development - Deprecations, Removals, and Renames Best Practices | Terraform\nTerraform is trusted for managing many facets of infrastructure across many organizations. Part of that trust is due to consistent versioning guidelines and setting expectations for various levels of upgrades. Ensuring backwards compatibility for all patch and minor releases, potentially in concert with any upcoming major changes, is recommended and supported by the Terraform development framework. This allows operators to iteratively update their Terraform configurations rather than require massive refactoring.\nThis guide is designed to walk through various scenarios where existing Terraform functionality requires future removal, while maintaining backwards compatibility. Further information about the versioning terminology (e.g. MAJOR.MINOR.PATCH) in this guide can be found in the versioning guidelines documentation.\nNOTE: Removals should only ever occur in MAJOR version upgrades.\nNOTE: This documentation references usage of the DeprecationMessage field, please see the schema documentation for more detailed guidance on how to structure warning messages and when those warnings will be raised to practitioners.\nTable of Contents\nProvider Attribute Removal\nProvider Attribute Rename\nRenaming a Required Attribute\nRenaming an Optional Attribute\nRenaming a Computed Attribute\nProvider Data Source or Resource Removal\nProvider Data Source or Resource Rename\nThe recommended process for removing an attribute from a data source or resource in a provider is as follows:\nAdd a DeprecationMessage in the attribute schema definition. Set this field to a practitioner actionable message such as \"Remove this attribute's configuration as it's no longer in use and the attribute will be removed in the next major version of the provider.\"\nEnsure the changelog has an entry noting the deprecation.\nRelease a MINOR version with the deprecation.\nIn the next MAJOR version, remove all code associated with the attribute including the schema definition.\nEnsure the changelog has an entry noting the removal.\nRelease the MAJOR version.\nWhen renaming an attribute from one name to another, it is important to keep backwards compatibility with both existing Terraform configurations and the Terraform state while operators migrate. To accomplish this, there will be some duplicated logic to support both attributes until the next MAJOR release. Once both attributes are appropriately handled, the process for deprecating and removing the old attribute is the same as noted in the Provider Attribute Removal section.\nThe procedure for renaming an attribute depends on what type of attribute it is:\nNOTE: If the schema definition does not contain Optional or Required, see the Renaming a Computed Attribute section instead. If the schema definition contains Optional instead of Required, see the Renaming an Optional Attribute section.\nRequired attributes are also referred to as required \"arguments\" throughout the Terraform documentation.\nIn general, the procedure here does two things:\nPrevents the operator from needing to define two attributes with the same value.\nAllows the operator to migrate the configuration to the new attribute at the same time requiring that any other references only work with the new attribute. This is to prevent a situation with Terraform showing a difference when the existing attribute is configured, but the new attribute is saved into the Terraform state. For example, in terraform plan output format:\nexisting_attribute: \"\" => \"value\" new_attribute: \"value\" => \"\" \nReplace Required: true with Optional: true in the existing attribute schema definition.\nReplace Required with Optional in the existing attribute documentation.\nDuplicate the schema definition of the existing attribute, renaming one of them with the new attribute name.\nDuplicate the documentation of the existing attribute, renaming one of them with the new attribute name.\nAdd a DeprecationMessage to the schema definition of the existing (now the \"old\") attribute, noting to use the new attribute in the message.\nAdd **Deprecated** to the documentation of the existing (now the \"old\") attribute, noting to use the new attribute.\nAdd a note to the documentation that either the existing (now the \"old\") attribute or new attribute must be configured.\nAdd the type-specific validator {type}validator.ExactlyOneOf to the schema definition of the new attribute, with a path expression matching the old attribute. This will ensure at least one of the attributes is configured, but present an error to the operator if both are configured at the same time. For example, an attribute of type string would use the stringvalidator.ExactlyOneOf validator.\nAdd conditional logic in the Create and Update functions of the data source or resource to handle both attributes. Generally, this involves using {type}.IsNull().\nFollow the rest of the procedures in the Provider Attribute Removal section. When the old attribute is removed, update the schema definition and documentation of the new attribute back to Required, and remove the {type}validator.ExactlyOneOf validator.\nExample Renaming of a Required Attribute\nGiven this sample resource:\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"existing_attribute\": schema.StringAttribute{ Required: true, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... ExistingAttribute types.String `tfsdk:\"existing_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add attribute to provider create API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { // ... other logic ... } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add attribute to provider update API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \nIn order to support renaming existing_attribute to new_attribute, this sample can be written as the following to support both attributes simultaneously until the existing_attribute is removed:\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework-validators/stringvalidator\" \"github.com/hashicorp/terraform-plugin-framework/path\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/schema/validator\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"existing_attribute\": schema.StringAttribute{ Optional: true, DeprecationMessage: \"use new_attribute instead\", }, \"new_attribute\": schema.StringAttribute{ Optional: true, Validators: []validator.String{ stringvalidator.ExactlyOneOf(path.Expressions{ path.MatchRoot(\"existing_attribute\"), }...), }, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... ExistingAttribute types.String `tfsdk:\"existing_attribute\"` NewAttribute types.String `tfsdk:\"new_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } if !data.NewAttribute.IsNull() { // add NewAttribute to provider create API call } else { // add ExistingAttribute to provider create API call } // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { // ... other logic ... } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } if !data.NewAttribute.IsNull() { // add NewAttribute to provider create API call } else { // add ExistingAttribute to provider create API call } // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \nWhen the existing_attribute is ready for removal, then this can be written as:\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"new_attribute\": schema.StringAttribute{ Required: true, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... NewAttribute types.String `tfsdk:\"new_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add NewAttribute to provider create API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { // ... other logic ... } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add NewAttribute to provider create API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \nNOTE: If the schema definition does not contain Optional or Required, see the Renaming a Computed Attribute section instead. If the schema definition contains Required instead of Optional, see the Renaming a Required Attribute section.\nOptional attributes are also referred to as optional \"arguments\" throughout the Terraform documentation.\nIn general, the procedure here allows the operator to migrate the configuration to the new attribute at the same time requiring that any other references only work with the new attribute. This is to prevent a situation with Terraform showing a difference when the existing attribute is configured, but the new attribute is saved into the Terraform state. For example, in terraform plan output format:\nexisting_attribute: \"\" => \"value\" new_attribute: \"value\" => \"\" \nAdd the type-specific validator {type}validator.ExactlyOneOf to the schema definition of the new attribute, with a path expression matching the old attribute. This will ensure at least one of the attributes is configured, but present an error to the operator if both are configured at the same time. For example, an attribute of type string would use the stringvalidator.ExactlyOneOf validator.\nAdd conditional logic in the Create and Update functions of the data source or resource to handle both attributes. Generally, this involves using {type}.IsNull().\nFollow the rest of the procedures in the Provider Attribute Removal section. When the old attribute is removed, remove the {type}validator.ExactlyOneOf validator.\nExample Renaming of an Optional Attribute\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"existing_attribute\": schema.StringAttribute{ Optional: true, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... ExistingAttribute types.String `tfsdk:\"existing_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add attribute to provider create API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { // ... other logic ... } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add attribute to provider update API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework-validators/stringvalidator\" \"github.com/hashicorp/terraform-plugin-framework/path\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/schema/validator\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"existing_attribute\": schema.StringAttribute{ Optional: true, DeprecationMessage: \"use new_attribute instead\", }, \"new_attribute\": schema.StringAttribute{ Optional: true, Validators: []validator.String{ stringvalidator.ExactlyOneOf(path.Expressions{ path.MatchRoot(\"existing_attribute\"), }...), }, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... ExistingAttribute types.String `tfsdk:\"existing_attribute\"` NewAttribute types.String `tfsdk:\"new_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } if !data.NewAttribute.IsNull() { // add NewAttribute to provider create API call } else { // add ExistingAttribute to provider create API call } // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { // ... other logic ... } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } if !data.NewAttribute.IsNull() { // add NewAttribute to provider create API call } else { // add ExistingAttribute to provider create API call } // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"new_attribute\": schema.StringAttribute{ Optional: true, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... NewAttribute types.String `tfsdk:\"new_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add NewAttribute to provider create API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { // ... other logic ... } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add NewAttribute to provider create API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \nNOTE: If the schema definition contains Optional see the Renaming an Optional Attribute section instead. If the schema definition contains Required see the Renaming a Required Attribute section instead.\nSet both attributes in the Terraform state in the Create, Update, and Read functions of the resource (Read only for data source).\nFollow the rest of the procedures in the Provider Attribute Removal section.\nExample Renaming of a Computed Attribute\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"existing_attribute\": schema.StringAttribute{ Computed: true, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... ExistingAttribute types.String `tfsdk:\"existing_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.ExistingAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.State.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.ExistingAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.ExistingAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"existing_attribute\": schema.StringAttribute{ Computed: true, DeprecationMessage: \"use new_attribute instead\", }, \"new_attribute\": schema.StringAttribute{ Computed: true, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... ExistingAttribute types.String `tfsdk:\"existing_attribute\"` NewAttribute types.String `tfsdk:\"new_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.ExistingAttribute = // set to computed value data.NewAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.State.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.ExistingAttribute = // set to computed value data.NewAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.ExistingAttribute = // set to computed value data.NewAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"new_attribute\": schema.StringAttribute{ Computed: true, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... NewAttribute types.String `tfsdk:\"new_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.NewAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.State.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.NewAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.NewAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \nThe recommended process for removing a data source or resource from a provider is as follows:\nAdd a DeprecationMessage in the data source or resource schema definition. After an operator upgrades to this version, they will be shown a warning with the message provided when using the deprecated data source or resource, but the Terraform run will still complete.\nEnsure the changelog has an entry noting the deprecation.\nRelease a MINOR version with the deprecation.\nIn the next MAJOR version, remove all code associated with the deprecated data source or resource except for the schema and replace the Create, Read, Update, and Delete functions to always return an error diagnostic. Remove the documentation sidebar link and update the resource or data source documentation page to include information about the removal and any potential migration information. After an operator upgrades to this version, they will be shown an error about the missing data source or resource.\nEnsure the changelog has an entry noting the removal.\nIn the next MAJOR version, remove all code associated with the removed data source or resource. Remove the resource or data source documentation page.\nExample Resource Removal\nGiven this sample provider and resource:\nfunc NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... }, } } // ... resource implementation ... \nIn order to deprecate example_widget, this sample can be written as:\nfunc NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... }, DeprecationMessage: \"use example_thing resource instead\", } } // ... resource implementation ... \nTo soft remove example_widget with a friendly error message, this sample can be written as:\nfunc NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... }, DeprecationMessage: \"use example_thing resource instead\", } } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_widget resource deprecated\", \"use example_thing resource instead\"), ) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_widget resource deprecated\", \"use example_thing resource instead\"), ) } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_widget resource deprecated\", \"use example_thing resource instead\"), ) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_widget resource deprecated\", \"use example_thing resource instead\"), ) } \nTo remove example_widget:\n// ... provider implementation ... func (p *exampleProvider) Resources(ctx context.Context) []func() resource.Resource { return []func() resource.Resource{ //... other resources ... } } \nWhen renaming a resource from one name to another, it is important to keep backwards compatibility with both existing Terraform configurations and the Terraform state while operators migrate. To accomplish this, there will be some duplicated logic to support both resources until the next MAJOR release. Once both resources are appropriately handled, the process for deprecating and removing the old resource is the same as noted in the Provider Data Source or Resource Removal section.\nDuplicate the code of the existing resource, renaming (and potentially modifying) functions as necessary.\nDuplicate the documentation of the existing resource, renaming (and potentially modifying) as necessary.\nAdd a DeprecationMessage to the schema definition of the existing (now the \"old\") resource, noting to use the new resource in the message.\nAdd ~> This resource is deprecated and will be removed in the next major version to the documentation of the existing (now the \"old\") resource, noting to use the new resource.\nAdd the new resource to the provider Resources function\nFollow the rest of the procedures in the Provider Data Source or Resource Removal section.\nExample Resource Renaming\nGiven this sample provider and resource:\n// ... provider implementation ... func (p *exampleProvider) Resources(ctx context.Context) []func() resource.Resource { return []func() resource.Resource{ //... other resources ... NewExistingWidgetResource, } } \nfunc NewExistingWidgetResource() resource.Resource { return &exampleExistingWidgetResource{} } func (e *exampleExistingWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... }, } } // ... resource implementation ... \nIn order to support renaming example_existing_widget to example_new_widget, this sample can be written as the following to support both resources simultaneously until the example_existing_widget resource is removed:\n// ... provider implementation ... func (p *exampleProvider) Resources(ctx context.Context) []func() resource.Resource { return []func() resource.Resource{ //... other resources ... NewExistingWidgetResource, NewWidgetResource, } } \nfunc NewExistingWidgetResource() resource.Resource { return &exampleExistingWidgetResource{} } func (e *exampleExistingWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... }, DeprecationMessage: \"use example_new_widget resource instead\", } } // ... resource implementation ... \nTo soft remove example_existing_widget with a friendly error message:\n// ... provider implementation ... func (p *exampleProvider) Resources(ctx context.Context) []func() resource.Resource { return []func() resource.Resource{ //... other resources ... NewExistingWidgetResource, NewWidgetResource, } } \nfunc NewExistingWidgetResource() resource.Resource { return &exampleExistingWidgetResource{} } func (e *exampleExistingWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... }, DeprecationMessage: \"use example_new_widget resource instead\", } } func (e *exampleExistingWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_existing_widget resource deprecated\", \"use example_new_widget resource instead\"), ) } func (e *exampleExistingWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_existing_widget resource deprecated\", \"use example_new_widget resource instead\"), ) } func (e *exampleExistingWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_existing_widget resource deprecated\", \"use example_new_widget resource instead\"), ) } func (e *exampleExistingWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_existing_widget resource deprecated\", \"use example_new_widget resource instead\"), ) } // ... resource implementation ... \nTo remove example_existing_widget:\nfunc NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... }, } } // ... resource implementation ..."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.7.x/providers",
  "text": "Plugin Development - Framework: Providers | Terraform\nProviders are Terraform plugins that define resources and data sources for practitioners to use. Providers are wrapped by a provider server for interacting with Terraform.\nThis page describes the basic implementation details required for defining a provider. Further documentation is available for deeper provider concepts:\nConfigure data sources with provider-level data types or clients.\nConfigure resources with provider-level data types or clients.\nValidate practitioner configuration against acceptable values.\nImplement the provider.Provider interface. Each of the methods described in more detail below.\nIn this example, a provider implementation is scaffolded:\n// Ensure the implementation satisfies the provider.Provider interface. var _ provider.Provider = &ExampleCloudProvider{} type ExampleCloudProvider struct{ // Version is an example field that can be set with an actual provider // version on release, \"dev\" when the provider is built and ran locally, // and \"test\" when running acceptance testing. Version string } // Metadata satisfies the provider.Provider interface for ExampleCloudProvider func (p *ExampleCloudProvider) Metadata(ctx context.Context, req provider.MetadataRequest, resp *provider.MetadataResponse) { resp.TypeName = // provider specific implementation } // Schema satisfies the provider.Provider interface for ExampleCloudProvider. func (p *ExampleCloudProvider) Schema(ctx context.Context, req provider.SchemaRequest, resp *provider.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ // Provider specific implementation. }, } } // Configure satisfies the provider.Provider interface for ExampleCloudProvider. func (p *ExampleCloudProvider) Configure(ctx context.Context, req provider.ConfigureRequest, resp *provider.ConfigureResponse) { // Provider specific implementation. } // DataSources satisfies the provider.Provider interface for ExampleCloudProvider. func (p *ExampleCloudProvider) DataSources(ctx context.Context) []func() datasource.DataSource { return []func() datasource.DataSource{ // Provider specific implementation } } // Resources satisfies the provider.Provider interface for ExampleCloudProvider. func (p *ExampleCloudProvider) Resources(ctx context.Context) []func() resource.Resource { return []func() resource.Resource{ // Provider specific implementation } } \nConventionally, many providers also create a helper function named New which can simplify provider server implementations.\nfunc New(version string) func() provider.Provider { return func() provider.Provider { return &ExampleCloudProvider{ Version: version, } } } \nThe provider.Provider interface Metadata method defines information about the provider itself, such as its type name and version. This information is used to simplify creating data sources and resources.\nIn this example, the provider type name is set to examplecloud:\nfunc (p *ExampleCloudProvider) Metadata(ctx context.Context, req provider.MetadataRequest, resp *provider.MetadataResponse) { resp.TypeName = \"examplecloud\" } \nSchema Method\nThe provider.Provider interface Schema method defines a schema describing what data is available in the provider's configuration. This configuration block is used to offer practitioners the opportunity to supply values to the provider and configure its behavior, rather than needing to include those values in every resource and data source. It is usually used to gather credentials, endpoints, and the other data used to authenticate with the API, but it is not limited to those uses.\nDuring the terraform validate, terraform plan and terraform apply commands, Terraform calls the provider GetProviderSchema RPC, in which the framework calls the provider.Provider interface Schema method.\nIn this example, a sample configuration and schema definition are provided:\n// Example Terraform configuration: // // provider \"examplecloud\" { // api_token = \"v3rYs3cr3tt0k3n\" // endpoint = \"https://example.com/\" // } func (p *ExampleCloudProvider) Schema(ctx context.Context, req provider.SchemaRequest, resp *provider.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"api_token\": schema.StringAttribute{ Optional: true, }, \"endpoint\": schema.StringAttribute{ Optional: true, }, }, } } \nIf the provider does not accept practitioner Terraform configuration, leave the method defined, but empty.\nConfigure Method\nThe provider.Provider interface Configure method handles the configuration of any provider-level data or clients. These configuration values may be from the practitioner Terraform configuration, environment variables, or other means such as reading vendor-specific configuration files.\nDuring the terraform plan and terraform apply commands, Terraform calls the provider ConfigureProvider RPC, in which the framework calls the provider.Provider interface Configure method.\nThis is the only chance the provider has to configure provider-level data or clients, so they need to be persisted if other data source or resource logic will need to reference them. Refer to the Configure Data Sources and Configure Resources pages for additional implementation details.\nIf the logic needs to return warning or error diagnostics, they can added into the provider.ConfigureResponse.Diagnostics field.\nIn this example, the provider API token and endpoint are configured via environment variable or Terraform configuration:\ntype ExampleCloudProvider struct {} type ExampleCloudProviderModel struct { ApiToken types.String `tfsdk:\"api_token\"` Endpoint types.String `tfsdk:\"endpoint\"` } func (p *ExampleCloudProvider) Schema(ctx context.Context, req provider.SchemaRequest, resp *provider.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"api_token\": schema.StringAttribute{ Optional: true, }, \"endpoint\": schema.StringAttribute{ Optional: true, }, }, } } func (p *ExampleCloudProvider) Configure(ctx context.Context, req provider.ConfigureRequest, resp *provider.ConfigureResponse) { // Check environment variables apiToken := os.Getenv(\"EXAMPLECLOUD_API_TOKEN\") endpoint := os.Getenv(\"EXAMPLECLOUD_ENDPOINT\") var data ExampleCloudProviderModel // Read configuration data into model resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) // Check configuration data, which should take precedence over // environment variable data, if found. if data.ApiToken.ValueString() != \"\" { apiToken = data.ApiToken.ValueString() } if data.Endpoint.ValueString() != \"\" { endpoint = data.Endpoint.ValueString() } if apiToken == \"\" { resp.Diagnostics.AddError( \"Missing API Token Configuration\", \"While configuring the provider, the API token was not found in \"+ \"the EXAMPLECLOUD_API_TOKEN environment variable or provider \"+ \"configuration block api_token attribute.\", ) // Not returning early allows the logic to collect all errors. } if endpoint == \"\" { resp.Diagnostics.AddError( \"Missing Endpoint Configuration\", \"While configuring the provider, the endpoint was not found in \"+ \"the EXAMPLECLOUD_ENDPOINT environment variable or provider \"+ \"configuration block endpoint attribute.\", ) // Not returning early allows the logic to collect all errors. } // Create data/clients and persist to resp.DataSourceData and // resp.ResourceData as appropriate. } \nNot all values are guaranteed to be known when Configure is called. For example, if a practitioner interpolates a resource's unknown value into the block, that value may show up as unknown depending on how the graph executes:\nresource \"random_string\" \"example\" {} provider \"examplecloud\" { api_token = random_string.example.result endpoint = \"https://example.com/\" } \nIn the example above, random_string.example.result is a read-only field on random_string.example that won't be set until after random_string.example has been applied. So the Configure method for the provider may report that the value is unknown. You can choose how your provider handles this. If some resources or data sources can be used without knowing that value, it may be worthwhile to emit a warning and check whether the value is set in resources and data sources before attempting to use it. If resources and data sources can't provide any functionality without knowing that value, it's often better to return an error, which will halt the apply.\nResources\nThe provider.Provider interface Resources method returns a slice of resources. Each element in the slice is a function to create a new resource.Resource so data is not inadvertently shared across multiple, disjointed resource instance operations unless explicitly coded. Information such as the resource type name is managed by the resource.Resource implementation.\nThe provider.Provider interface Resources method is called during execution of the terraform validate, terraform plan and terraform apply commands when the ValidateResourceConfig, ReadResource, PlanResourceChange and ApplyResourceChange RPCs are sent.\nIn this example, the provider implements a single resource:\n// With the provider.Provider implementation func (p *ExampleCloudProvider) Resources(_ context.Context) []func() resource.Resource { return []func() resource.Resource{ NewThingResource, } } // With the resource.Resource implementation func NewThingResource() resource.Resource { return &ThingResource{} } type ThingResource struct {} \nUse Go slice techniques to include large numbers of resources outside the provider Resources method code.\nIn this example, the provider codebase implements multiple \"services\" which group their own resources:\n// With the provider.Provider implementation func (p *ExampleCloudProvider) Resources(_ context.Context) []func() resource.Resource { return []func() resource.Resource{ servicex.Resources..., servicey.Resources..., } } // With the servicex implementation package servicex var Resources = []func() resource.Resource { NewThingResource, NewWidgetResource, } func NewThingResource() resource.Resource { return &ThingResource{} } type ThingResource struct {} func NewWidgetResource() resource.Resource { return &WidgetResource{} } type WidgetResource struct {} \nDataSources\nThe provider.Provider interface DataSources method returns a slice of data sources. Each element in the slice is a function to create a new datasource.DataSource so data is not inadvertently shared across multiple, disjointed datasource instance operations unless explicitly coded. Information such as the datasource type name is managed by the datasource.DataSource implementation.\nThe provider.Provider interface DataSources method is called during execution of the terraform validate, terraform plan and terraform apply commands when the ValidateDataResourceConfig and ReadDataSource RPCs are sent.\nIn this example, the provider implements a single data source:\n// With the provider.Provider implementation func (p *ExampleCloudProvider) DataSources(_ context.Context) []func() datasource.DataSource { return []func() datasource.DataSource{ NewThingDataSource, } } // With the datasource.DataSource implementation func NewThingDataSource() datasource.DataSource { return &ThingDataSource{} } type ThingDataSource struct {} \nUse Go slice techniques to include large numbers of data sources outside the provider DataSources method code.\nIn this example, the provider codebase implements multiple \"services\" which group their own datasources:\n// With the provider.Provider implementation func (p *ExampleCloudProvider) DataSources(_ context.Context) []func() datasource.DataSource { return []func() datasource.DataSource{ servicex.DataSources..., servicey.DataSources..., } } // With the servicex implementation package servicex var DataSources = []func() datasource.DataSource { NewThingDataSource, NewWidgetDataSource, } func NewThingDataSource() datasource.DataSource { return &ThingDataSource{} } type ThingDataSource struct {} func NewWidgetDataSource() datasource.DataSource { return &WidgetDataSource{} } type WidgetDataSource struct {}"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.0.x/resources/create",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.0.x/resources/read",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.0.x/resources/update",
  "text": "// ThingResource defines the resource implementation. // Some resource.Resource interface methods are omitted for brevity. type ThingResource struct { // client is configured via a Configure method, which is not shown in this // example for brevity. Refer to the Configure Resources documentation for // additional details for setting up resources with external clients. client *http.Client } // ThingResourceModel describes the Terraform resource data model to match the // resource schema. type ThingResourceModel struct { Name types.String `tfsdk:\"name\"` Id types.String `tfsdk:\"id\"` } // ThingResourceAPIModel describes the API data model. type ThingResourceAPIModel struct { Name string `json:\"name\"` Id string `json:\"id\"` } func (r ThingResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"name\": schema.StringAttribute{ MarkdownDescription: \"Name of the thing to be saved in the service.\", Required: true, }, \"id\": schema.StringAttribute{ Computed: true, MarkdownDescription: \"Service generated identifier for the thing.\", PlanModifiers: planmodifier.String{ stringplanmodifier.UseStateForUnknown(), }, }, }, MarkdownDescription: \"Manages a thing.\", } } func (r ThingResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data ThingResourceModel // Read Terraform plan data into the model resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) // Convert from Terraform data model into API data model updateReq := ThingResourceAPIModel{ Id: data.Id.StringValue(), Name: data.Name.StringValue(), } httpReqBody, err := json.Marshal(updateReq) if err != nil { resp.Diagnostics.AddError( \"Unable to Update Resource\", \"An unexpected error occurred while creating the resource update request. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Create HTTP request httpReq := http.NewRequestWithContext( ctx, http.MethodPut, \"http://example.com/things\", bytes.NewBuffer(httpReqBody), ) // Send HTTP request httpResp, err := d.client.Do(httpReq) defer httpResp.Body.Close() if err != nil { resp.Diagnostics.AddError( \"Unable to Update Resource\", \"An unexpected error occurred while attempting to update the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Error: \"+err.Error(), ) return } // Return error if the HTTP status code is not 200 OK if httpResp.StatusCode != http.StatusOK { resp.Diagnostics.AddError( \"Unable to Update Resource\", \"An unexpected error occurred while attempting to update the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Status: \"+httpResp.Status, ) return } // Save updated data into Terraform state resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.0.x/resources/delete",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.0.x/resources/plan-modification",
  "text": "// stringDefaultModifier is a plan modifier that sets a default value for a // types.StringType attribute when it is not configured. The attribute must be // marked as Optional and Computed. When setting the state during the resource // Create, Read, or Update methods, this default value must also be included or // the Terraform CLI will generate an error. type stringDefaultModifier struct { Default string } // Description returns a plain text description of the validator's behavior, suitable for a practitioner to understand its impact. func (m stringDefaultModifier) Description(ctx context.Context) string { return fmt.Sprintf(\"If value is not configured, defaults to %s\", m.Default) } // MarkdownDescription returns a markdown formatted description of the validator's behavior, suitable for a practitioner to understand its impact. func (m stringDefaultModifier) MarkdownDescription(ctx context.Context) string { return fmt.Sprintf(\"If value is not configured, defaults to `%s`\", m.Default) } // PlanModifyString runs the logic of the plan modifier. // Access to the configuration, plan, and state is available in `req`, while // `resp` contains fields for updating the planned value, triggering resource // replacement, and returning diagnostics. func (m stringDefaultModifier) PlanModifyString(ctx context.Context, req planmodifier.StringRequest, resp *planmodifier.StringResponse) { // If the value is unknown or known, do not set default value. if !req.PlanValue.IsNull() { return } resp.PlanValue = types.StringValue(m.Default) } \nfunc stringDefault(defaultValue string) planmodifier.String { return stringDefaultModifier{ Default: defaultValue, } } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.0.x/resources/import",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.0.x/resources/state-upgrade",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.0.x/resources/private-state",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.0.x/resources/timeouts",
  "text": "func (t *exampleResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { return schema.Schema{ Attributes: map[string]schema.Attribute{ /* ... */ \"timeouts\": timeouts.Attributes(ctx, timeouts.Opts{ Create: true, }), }, \nThe exampleResourceData model needs to be modified to include a field for timeouts, which is types.Object.\ntype exampleResourceData struct { /* ... */ Timeouts types.Object `tfsdk:\"timeouts\"` \nfunc (e *exampleResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleResourceData diags := req.Plan.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } defaultCreateTimeout := 20 * time.Minute createTimeout := timeouts.Create(ctx, data.Timeouts, defaultCreateTimeout) ctx, cancel := context.WithTimeout(ctx, createTimeout) defer cancel() /* ... */ }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.0.x/data-sources/timeouts",
  "text": "func (d *ThingDataSource) Schema(ctx context.Context, req datasource.SchemaRequest, resp *datasource.SchemaResponse) { resp.Schema = schema.Schema{ /* ... */ Blocks: map[string]schema.Block{ \"timeouts\": timeouts.Block(ctx, timeouts.Opts{ Read: true, }), }, \nfunc (d *ThingDataSource) Schema(ctx context.Context, req datasource.SchemaRequest, resp *datasource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ /* ... */ \"timeouts\": timeouts.Attributes(ctx, timeouts.Opts{ Read: true, }), }, \nModify the exampleDataSourceData model to include a field for timeouts using a types.Object type.\ntype exampleDataSourceData struct { /* ... */ Timeouts types.Object `tfsdk:\"timeouts\"` \nfunc (e *exampleDataSource) Read(ctx context.Context, req datasource.ReadRequest, resp *datasource.ReadResponse) { var data exampleDataSourceData diags := req.Config.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } defaultReadTimeout := 20 * time.Minute readTimeout := timeouts.Read(ctx, data.Timeouts, defaultReadTimeout) ctx, cancel := context.WithTimeout(ctx, readTimeout) defer cancel() /* ... */ }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.0.x/accessing-values",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.16.x/resources/state-upgrade",
  "text": "Ensure the tfsdk.Schema type Version field for the resource.Resource is greater than 0, then implement the resource.ResourceWithStateUpgrade interface for the resource.Resource. Conventionally the version is incremented by 1 for each state upgrade.\n// Other Resource methods are omitted in this example var _ resource.Resource = &ThingResource{} var _ resource.ResourceWithUpgradeState = &ThingResource{} type ThingResource struct{/* ... */} func (r *ThingResource) GetSchema(_ context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ // ... other fields ... // This example conventionally declares that the resource has prior // state versions of 0 and 1, while the current version is 2. Version: 2, } } func (r *ThingResource) UpgradeState(ctx context.Context) map[int64]resource.StateUpgrader { return map[int64]resource.StateUpgrader{ // State upgrade implementation from 0 (prior state version) to 2 (Schema.Version) 0: { // Optionally, the PriorSchema field can be defined. StateUpgrader: func(ctx context.Context, req resource.UpgradeStateRequest, resp *resource.UpgradeStateResponse) { /* ... */ }, }, // State upgrade implementation from 1 (prior state version) to 2 (Schema.Version) 1: { // Optionally, the PriorSchema field can be defined. StateUpgrader: func(ctx context.Context, req resource.UpgradeStateRequest, resp *resource.UpgradeStateResponse) { /* ... */ }, }, } } \n// Other Resource methods are omitted in this example var _ resource.Resource = &ThingResource{} var _ resource.ResourceWithUpgradeState = &ThingResource{} type ThingResource struct{/* ... */} type ThingResourceModelV0 struct { Id string `tfsdk:\"id\"` OptionalAttribute *bool `tfsdk:\"optional_attribute\"` RequiredAttribute bool `tfsdk:\"required_attribute\"` } type ThingResourceModelV1 struct { Id string `tfsdk:\"id\"` OptionalAttribute *string `tfsdk:\"optional_attribute\"` RequiredAttribute string `tfsdk:\"required_attribute\"` } func (r *ThingResource) GetSchema(_ context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ Attributes: map[string]Attribute{ \"id\": { Type: types.StringType, Computed: true, }, \"optional_attribute\": { Type: types.StringType, // As compared to prior types.BoolType below Optional: true, }, \"required_attribute\": { Type: types.StringType, // As compared to prior types.BoolType below Required: true, }, }, // The resource has a prior state version of 0, which had the attribute // types of types.BoolType as shown below. Version: 1, } } func (r *ThingResource) UpgradeState(ctx context.Context) map[int64]resource.StateUpgrader { return map[int64]resource.StateUpgrader{ // State upgrade implementation from 0 (prior state version) to 1 (Schema.Version) 0: { PriorSchema: &tfsdk.Schema{ Attributes: map[string]Attribute{ \"id\": { Type: types.StringType, Computed: true, }, \"optional_attribute\": { Type: types.BoolType, // As compared to current types.StringType above Optional: true, }, \"required_attribute\": { Type: types.BoolType, // As compared to current types.StringType above Required: true, }, }, }, StateUpgrader: func(ctx context.Context, req resource.UpgradeStateRequest, resp *resource.UpgradeStateResponse) { var priorStateData ThingResourceModelV0 resp.Diagnostics.Append(req.State.Get(ctx, &priorStateData)...) if resp.Diagnostics.HasError() { return } upgradedStateData := ThingResourceModelV1{ Id: priorStateData.Id, RequiredAttribute: fmt.Sprintf(\"%t\", priorStateData.RequiredAttribute), } if priorStateData.OptionalAttribute != nil { v := fmt.Sprintf(\"%t\", *priorStateData.OptionalAttribute) upgradedStateData.OptionalAttribute = &v } resp.Diagnostics.Append(resp.State.Set(ctx, upgradedStateData)...) }, }, } } \n// Other Resource methods are omitted in this example var _ resource.Resource = &ThingResource{} var _ resource.ResourceWithUpgradeState = &ThingResource{} var ThingResourceTftypesDataV0 = tftypes.Object{ AttributeTypes: map[string]tftypes.Type{ \"id\": tftypes.String, \"optional_attribute\": tftypes.Bool, \"required_attribute\": tftypes.Bool, }, } var ThingResourceTftypesDataV1 = tftypes.Object{ AttributeTypes: map[string]tftypes.Type{ \"id\": tftypes.String, \"optional_attribute\": tftypes.String, \"required_attribute\": tftypes.String, }, } type ThingResource struct{/* ... */} func (r *ThingResource) GetSchema(_ context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ Attributes: map[string]Attribute{ \"id\": { Type: types.StringType, Computed: true, }, \"optional_attribute\": { Type: types.StringType, // As compared to prior types.BoolType below Optional: true, }, \"required_attribute\": { Type: types.StringType, // As compared to prior types.BoolType below Required: true, }, }, // The resource has a prior state version of 0, which had the attribute // types of types.BoolType as shown below. Version: 1, } } func (r *ThingResource) UpgradeState(ctx context.Context) map[int64]resource.StateUpgrader { return map[int64]resource.StateUpgrader{ // State upgrade implementation from 0 (prior state version) to 1 (Schema.Version) 0: { StateUpgrader: func(ctx context.Context, req resource.UpgradeStateRequest, resp *resource.UpgradeStateResponse) { // Refer also to the RawState type JSON field which can be used // with json.Unmarshal() rawStateValue, err := req.RawState.Unmarshal(ThingResourceTftypesDataV0) if err != nil { resp.Diagnostics.AddError( \"Unable to Unmarshal Prior State\", err.Error(), ) return } var rawState map[string]tftypes.Value if err := rawStateValue.As(&rawState); err != nil { resp.Diagnostics.AddError( \"Unable to Convert Prior State\", err.Error(), ) return } var optionalAttributeString *string if !rawState[\"optional_attribute\"].IsNull() { var optionalAttribute bool if err := rawState[\"optional_attribute\"].As(&optionalAttribute); err != nil { resp.Diagnostics.AddAttributeError( path.Root(\"optional_attribute\"), \"Unable to Convert Prior State\", err.Error(), ) return } v := fmt.Sprintf(\"%t\", optionalAttribute) optionalAttributeString = &v } var requiredAttribute bool if err := rawState[\"required_attribute\"].As(&requiredAttribute); err != nil { resp.Diagnostics.AddAttributeError( path.Root(\"required_attribute\"), \"Unable to Convert Prior State\", err.Error(), ) return } dynamicValue, err := tfprotov6.NewDynamicValue( ThingResourceTftypesDataV1, tftypes.NewValue(ThingResourceTftypesDataV1, map[string]tftypes.Value{ \"id\": rawState[\"id\"], \"optional_attribute\": tftypes.NewValue(tftypes.String, optionalAttributeString), \"required_attribute\": tftypes.NewValue(tftypes.String, fmt.Sprintf(\"%t\", requiredAttribute)), }), ) if err != nil { resp.Diagnostics.AddError( \"Unable to Convert Upgraded State\", err.Error(), ) return } resp.DynamicValue = &dynamicValue }, }, } }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.7.x/deprecations",
  "text": "Plugin Development - Deprecations, Removals, and Renames Best Practices | Terraform\nTerraform is trusted for managing many facets of infrastructure across many organizations. Part of that trust is due to consistent versioning guidelines and setting expectations for various levels of upgrades. Ensuring backwards compatibility for all patch and minor releases, potentially in concert with any upcoming major changes, is recommended and supported by the Terraform development framework. This allows operators to iteratively update their Terraform configurations rather than require massive refactoring.\nThis guide is designed to walk through various scenarios where existing Terraform functionality requires future removal, while maintaining backwards compatibility. Further information about the versioning terminology (e.g. MAJOR.MINOR.PATCH) in this guide can be found in the versioning guidelines documentation.\nNOTE: Removals should only ever occur in MAJOR version upgrades.\nNOTE: This documentation references usage of the DeprecationMessage field, please see the schema documentation for more detailed guidance on how to structure warning messages and when those warnings will be raised to practitioners.\nTable of Contents\nProvider Attribute Removal\nProvider Attribute Rename\nProvider Data Source or Resource Removal\nProvider Data Source or Resource Rename\nThe recommended process for removing an attribute from a data source or resource in a provider is as follows:\nAdd a DeprecationMessage in the attribute schema definition. Set this field to a practitioner actionable message such as \"Remove this attribute's configuration as it's no longer in use and the attribute will be removed in the next major version of the provider.\"\nIn the next MAJOR version, remove all code associated with the attribute including the schema definition.\nWhen renaming an attribute from one name to another, it is important to keep backwards compatibility with both existing Terraform configurations and the Terraform state while operators migrate. To accomplish this, there will be some duplicated logic to support both attributes until the next MAJOR release. Once both attributes are appropriately handled, the process for deprecating and removing the old attribute is the same as noted in the Provider Attribute Removal section.\nThe procedure for renaming an attribute depends on what type of attribute it is:\nNOTE: If the schema definition does not contain Optional or Required, see the Renaming a Computed Attribute section instead. If the schema definition contains Optional instead of Required, see the Renaming an Optional Attribute section.\nRequired attributes are also referred to as required \"arguments\" throughout the Terraform documentation.\nIn general, the procedure here does two things:\nPrevents the operator from needing to define two attributes with the same value.\nAllows the operator to migrate the configuration to the new attribute at the same time requiring that any other references only work with the new attribute. This is to prevent a situation with Terraform showing a difference when the existing attribute is configured, but the new attribute is saved into the Terraform state. For example, in terraform plan output format:\nReplace Required: true with Optional: true in the existing attribute schema definition.\nReplace Required with Optional in the existing attribute documentation.\nAdd a note to the documentation that either the existing (now the \"old\") attribute or new attribute must be configured.\nFollow the rest of the procedures in the Provider Attribute Removal section. When the old attribute is removed, update the schema definition and documentation of the new attribute back to Required, and remove the {type}validator.ExactlyOneOf validator.\nExample Renaming of a Required Attribute\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"existing_attribute\": schema.StringAttribute{ Required: true, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... ExistingAttribute types.String `tfsdk:\"existing_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add attribute to provider create API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { // ... other logic ... } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add attribute to provider update API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"new_attribute\": schema.StringAttribute{ Required: true, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... NewAttribute types.String `tfsdk:\"new_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add NewAttribute to provider create API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { // ... other logic ... } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add NewAttribute to provider create API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \nNOTE: If the schema definition does not contain Optional or Required, see the Renaming a Computed Attribute section instead. If the schema definition contains Required instead of Optional, see the Renaming a Required Attribute section.\nOptional attributes are also referred to as optional \"arguments\" throughout the Terraform documentation.\nIn general, the procedure here allows the operator to migrate the configuration to the new attribute at the same time requiring that any other references only work with the new attribute. This is to prevent a situation with Terraform showing a difference when the existing attribute is configured, but the new attribute is saved into the Terraform state. For example, in terraform plan output format:\nFollow the rest of the procedures in the Provider Attribute Removal section. When the old attribute is removed, remove the {type}validator.ExactlyOneOf validator.\nExample Renaming of an Optional Attribute\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"existing_attribute\": schema.StringAttribute{ Optional: true, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... ExistingAttribute types.String `tfsdk:\"existing_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add attribute to provider create API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { // ... other logic ... } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add attribute to provider update API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"new_attribute\": schema.StringAttribute{ Optional: true, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... NewAttribute types.String `tfsdk:\"new_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add NewAttribute to provider create API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { // ... other logic ... } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add NewAttribute to provider create API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \nNOTE: If the schema definition contains Optional see the Renaming an Optional Attribute section instead. If the schema definition contains Required see the Renaming a Required Attribute section instead.\nSet both attributes in the Terraform state in the Create, Update, and Read functions of the resource (Read only for data source).\nFollow the rest of the procedures in the Provider Attribute Removal section.\nExample Renaming of a Computed Attribute\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"existing_attribute\": schema.StringAttribute{ Computed: true, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... ExistingAttribute types.String `tfsdk:\"existing_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.ExistingAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.State.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.ExistingAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.ExistingAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"existing_attribute\": schema.StringAttribute{ Computed: true, DeprecationMessage: \"use new_attribute instead\", }, \"new_attribute\": schema.StringAttribute{ Computed: true, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... ExistingAttribute types.String `tfsdk:\"existing_attribute\"` NewAttribute types.String `tfsdk:\"new_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.ExistingAttribute = // set to computed value data.NewAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.State.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.ExistingAttribute = // set to computed value data.NewAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.ExistingAttribute = // set to computed value data.NewAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"new_attribute\": schema.StringAttribute{ Computed: true, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... NewAttribute types.String `tfsdk:\"new_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.NewAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.State.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.NewAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.NewAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \nThe recommended process for removing a data source or resource from a provider is as follows:\nAdd a DeprecationMessage in the data source or resource schema definition. After an operator upgrades to this version, they will be shown a warning with the message provided when using the deprecated data source or resource, but the Terraform run will still complete.\nIn the next MAJOR version, remove all code associated with the deprecated data source or resource except for the schema and replace the Create, Read, Update, and Delete functions to always return an error diagnostic. Remove the documentation sidebar link and update the resource or data source documentation page to include information about the removal and any potential migration information. After an operator upgrades to this version, they will be shown an error about the missing data source or resource.\nIn the next MAJOR version, remove all code associated with the removed data source or resource. Remove the resource or data source documentation page.\nExample Resource Removal\nIn order to deprecate example_widget, this sample can be written as:\nfunc NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... }, DeprecationMessage: \"use example_thing resource instead\", } } // ... resource implementation ... \nTo soft remove example_widget with a friendly error message, this sample can be written as:\nfunc NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... }, DeprecationMessage: \"use example_thing resource instead\", } } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_widget resource deprecated\", \"use example_thing resource instead\"), ) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_widget resource deprecated\", \"use example_thing resource instead\"), ) } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_widget resource deprecated\", \"use example_thing resource instead\"), ) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_widget resource deprecated\", \"use example_thing resource instead\"), ) } \nTo remove example_widget:\n// ... provider implementation ... func (p *exampleProvider) Resources(ctx context.Context) []func() resource.Resource { return []func() resource.Resource{ //... other resources ... } } \nWhen renaming a resource from one name to another, it is important to keep backwards compatibility with both existing Terraform configurations and the Terraform state while operators migrate. To accomplish this, there will be some duplicated logic to support both resources until the next MAJOR release. Once both resources are appropriately handled, the process for deprecating and removing the old resource is the same as noted in the Provider Data Source or Resource Removal section.\nDuplicate the code of the existing resource, renaming (and potentially modifying) functions as necessary.\nDuplicate the documentation of the existing resource, renaming (and potentially modifying) as necessary.\nAdd a DeprecationMessage to the schema definition of the existing (now the \"old\") resource, noting to use the new resource in the message.\nAdd ~> This resource is deprecated and will be removed in the next major version to the documentation of the existing (now the \"old\") resource, noting to use the new resource.\nAdd the new resource to the provider Resources function\nFollow the rest of the procedures in the Provider Data Source or Resource Removal section.\nExample Resource Renaming\n// ... provider implementation ... func (p *exampleProvider) Resources(ctx context.Context) []func() resource.Resource { return []func() resource.Resource{ //... other resources ... NewExistingWidgetResource, } } \nfunc NewExistingWidgetResource() resource.Resource { return &exampleExistingWidgetResource{} } func (e *exampleExistingWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... }, } } // ... resource implementation ... \nIn order to support renaming example_existing_widget to example_new_widget, this sample can be written as the following to support both resources simultaneously until the example_existing_widget resource is removed:\nfunc NewExistingWidgetResource() resource.Resource { return &exampleExistingWidgetResource{} } func (e *exampleExistingWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... }, DeprecationMessage: \"use example_new_widget resource instead\", } } // ... resource implementation ... \nTo soft remove example_existing_widget with a friendly error message:\nfunc NewExistingWidgetResource() resource.Resource { return &exampleExistingWidgetResource{} } func (e *exampleExistingWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... }, DeprecationMessage: \"use example_new_widget resource instead\", } } func (e *exampleExistingWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_existing_widget resource deprecated\", \"use example_new_widget resource instead\"), ) } func (e *exampleExistingWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_existing_widget resource deprecated\", \"use example_new_widget resource instead\"), ) } func (e *exampleExistingWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_existing_widget resource deprecated\", \"use example_new_widget resource instead\"), ) } func (e *exampleExistingWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_existing_widget resource deprecated\", \"use example_new_widget resource instead\"), ) } // ... resource implementation ... \nTo remove example_existing_widget:\nfunc NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... }, } } // ... resource implementation ..."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.16.x/data-sources/configure",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.16.x/resources/configure",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.5.x/deprecations",
  "text": "Plugin Development - Deprecations, Removals, and Renames Best Practices | Terraform\nTerraform is trusted for managing many facets of infrastructure across many organizations. Part of that trust is due to consistent versioning guidelines and setting expectations for various levels of upgrades. Ensuring backwards compatibility for all patch and minor releases, potentially in concert with any upcoming major changes, is recommended and supported by the Terraform development framework. This allows operators to iteratively update their Terraform configurations rather than require massive refactoring.\nThis guide is designed to walk through various scenarios where existing Terraform functionality requires future removal, while maintaining backwards compatibility. Further information about the versioning terminology (e.g. MAJOR.MINOR.PATCH) in this guide can be found in the versioning guidelines documentation.\nNOTE: Removals should only ever occur in MAJOR version upgrades.\nNOTE: This documentation references usage of the DeprecationMessage field, please see the schema documentation for more detailed guidance on how to structure warning messages and when those warnings will be raised to practitioners.\nTable of Contents\nProvider Attribute Removal\nProvider Attribute Rename\nProvider Data Source or Resource Removal\nProvider Data Source or Resource Rename\nThe recommended process for removing an attribute from a data source or resource in a provider is as follows:\nAdd a DeprecationMessage in the attribute schema definition. Set this field to a practitioner actionable message such as \"Remove this attribute's configuration as it's no longer in use and the attribute will be removed in the next major version of the provider.\"\nIn the next MAJOR version, remove all code associated with the attribute including the schema definition.\nWhen renaming an attribute from one name to another, it is important to keep backwards compatibility with both existing Terraform configurations and the Terraform state while operators migrate. To accomplish this, there will be some duplicated logic to support both attributes until the next MAJOR release. Once both attributes are appropriately handled, the process for deprecating and removing the old attribute is the same as noted in the Provider Attribute Removal section.\nThe procedure for renaming an attribute depends on what type of attribute it is:\nNOTE: If the schema definition does not contain Optional or Required, see the Renaming a Computed Attribute section instead. If the schema definition contains Optional instead of Required, see the Renaming an Optional Attribute section.\nRequired attributes are also referred to as required \"arguments\" throughout the Terraform documentation.\nIn general, the procedure here does two things:\nPrevents the operator from needing to define two attributes with the same value.\nAllows the operator to migrate the configuration to the new attribute at the same time requiring that any other references only work with the new attribute. This is to prevent a situation with Terraform showing a difference when the existing attribute is configured, but the new attribute is saved into the Terraform state. For example, in terraform plan output format:\nReplace Required: true with Optional: true in the existing attribute schema definition.\nReplace Required with Optional in the existing attribute documentation.\nAdd a note to the documentation that either the existing (now the \"old\") attribute or new attribute must be configured.\nFollow the rest of the procedures in the Provider Attribute Removal section. When the old attribute is removed, update the schema definition and documentation of the new attribute back to Required, and remove the {type}validator.ExactlyOneOf validator.\nExample Renaming of a Required Attribute\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"existing_attribute\": schema.StringAttribute{ Required: true, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... ExistingAttribute types.String `tfsdk:\"existing_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add attribute to provider create API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { // ... other logic ... } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add attribute to provider update API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"new_attribute\": schema.StringAttribute{ Required: true, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... NewAttribute types.String `tfsdk:\"new_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add NewAttribute to provider create API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { // ... other logic ... } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add NewAttribute to provider create API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \nNOTE: If the schema definition does not contain Optional or Required, see the Renaming a Computed Attribute section instead. If the schema definition contains Required instead of Optional, see the Renaming a Required Attribute section.\nOptional attributes are also referred to as optional \"arguments\" throughout the Terraform documentation.\nIn general, the procedure here allows the operator to migrate the configuration to the new attribute at the same time requiring that any other references only work with the new attribute. This is to prevent a situation with Terraform showing a difference when the existing attribute is configured, but the new attribute is saved into the Terraform state. For example, in terraform plan output format:\nFollow the rest of the procedures in the Provider Attribute Removal section. When the old attribute is removed, remove the {type}validator.ExactlyOneOf validator.\nExample Renaming of an Optional Attribute\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"existing_attribute\": schema.StringAttribute{ Optional: true, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... ExistingAttribute types.String `tfsdk:\"existing_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add attribute to provider create API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { // ... other logic ... } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add attribute to provider update API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"new_attribute\": schema.StringAttribute{ Optional: true, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... NewAttribute types.String `tfsdk:\"new_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add NewAttribute to provider create API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { // ... other logic ... } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // add NewAttribute to provider create API call // ... other logic ... resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \nNOTE: If the schema definition contains Optional see the Renaming an Optional Attribute section instead. If the schema definition contains Required see the Renaming a Required Attribute section instead.\nSet both attributes in the Terraform state in the Create, Update, and Read functions of the resource (Read only for data source).\nFollow the rest of the procedures in the Provider Attribute Removal section.\nExample Renaming of a Computed Attribute\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"existing_attribute\": schema.StringAttribute{ Computed: true, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... ExistingAttribute types.String `tfsdk:\"existing_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.ExistingAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.State.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.ExistingAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.ExistingAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"existing_attribute\": schema.StringAttribute{ Computed: true, DeprecationMessage: \"use new_attribute instead\", }, \"new_attribute\": schema.StringAttribute{ Computed: true, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... ExistingAttribute types.String `tfsdk:\"existing_attribute\"` NewAttribute types.String `tfsdk:\"new_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.ExistingAttribute = // set to computed value data.NewAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.State.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.ExistingAttribute = // set to computed value data.NewAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.ExistingAttribute = // set to computed value data.NewAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ resource.Resource = (*exampleWidgetResource)(nil) type exampleWidgetResource struct{} func NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_widget\" } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... \"new_attribute\": schema.StringAttribute{ Computed: true, }, }, } } type exampleWidgetResourceData struct { // ... other attributes ... NewAttribute types.String `tfsdk:\"new_attribute\"` } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.NewAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.State.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.NewAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleWidgetResourceData resp.Diagnostics.Append(req.Config.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // ... other logic ... data.NewAttribute = // set to computed value resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { // ... other logic ... } \nThe recommended process for removing a data source or resource from a provider is as follows:\nAdd a DeprecationMessage in the data source or resource schema definition. After an operator upgrades to this version, they will be shown a warning with the message provided when using the deprecated data source or resource, but the Terraform run will still complete.\nIn the next MAJOR version, remove all code associated with the deprecated data source or resource except for the schema and replace the Create, Read, Update, and Delete functions to always return an error diagnostic. Remove the documentation sidebar link and update the resource or data source documentation page to include information about the removal and any potential migration information. After an operator upgrades to this version, they will be shown an error about the missing data source or resource.\nIn the next MAJOR version, remove all code associated with the removed data source or resource. Remove the resource or data source documentation page.\nExample Resource Removal\nIn order to deprecate example_widget, this sample can be written as:\nfunc NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... }, DeprecationMessage: \"use example_thing resource instead\", } } // ... resource implementation ... \nTo soft remove example_widget with a friendly error message, this sample can be written as:\nfunc NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... }, DeprecationMessage: \"use example_thing resource instead\", } } func (e *exampleWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_widget resource deprecated\", \"use example_thing resource instead\"), ) } func (e *exampleWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_widget resource deprecated\", \"use example_thing resource instead\"), ) } func (e *exampleWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_widget resource deprecated\", \"use example_thing resource instead\"), ) } func (e *exampleWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_widget resource deprecated\", \"use example_thing resource instead\"), ) } \nTo remove example_widget:\n// ... provider implementation ... func (p *exampleProvider) Resources(ctx context.Context) []func() resource.Resource { return []func() resource.Resource{ //... other resources ... } } \nWhen renaming a resource from one name to another, it is important to keep backwards compatibility with both existing Terraform configurations and the Terraform state while operators migrate. To accomplish this, there will be some duplicated logic to support both resources until the next MAJOR release. Once both resources are appropriately handled, the process for deprecating and removing the old resource is the same as noted in the Provider Data Source or Resource Removal section.\nDuplicate the code of the existing resource, renaming (and potentially modifying) functions as necessary.\nDuplicate the documentation of the existing resource, renaming (and potentially modifying) as necessary.\nAdd a DeprecationMessage to the schema definition of the existing (now the \"old\") resource, noting to use the new resource in the message.\nAdd ~> This resource is deprecated and will be removed in the next major version to the documentation of the existing (now the \"old\") resource, noting to use the new resource.\nAdd the new resource to the provider Resources function\nFollow the rest of the procedures in the Provider Data Source or Resource Removal section.\nExample Resource Renaming\n// ... provider implementation ... func (p *exampleProvider) Resources(ctx context.Context) []func() resource.Resource { return []func() resource.Resource{ //... other resources ... NewExistingWidgetResource, } } \nfunc NewExistingWidgetResource() resource.Resource { return &exampleExistingWidgetResource{} } func (e *exampleExistingWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... }, } } // ... resource implementation ... \nIn order to support renaming example_existing_widget to example_new_widget, this sample can be written as the following to support both resources simultaneously until the example_existing_widget resource is removed:\nfunc NewExistingWidgetResource() resource.Resource { return &exampleExistingWidgetResource{} } func (e *exampleExistingWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... }, DeprecationMessage: \"use example_new_widget resource instead\", } } // ... resource implementation ... \nTo soft remove example_existing_widget with a friendly error message:\nfunc NewExistingWidgetResource() resource.Resource { return &exampleExistingWidgetResource{} } func (e *exampleExistingWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... }, DeprecationMessage: \"use example_new_widget resource instead\", } } func (e *exampleExistingWidgetResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_existing_widget resource deprecated\", \"use example_new_widget resource instead\"), ) } func (e *exampleExistingWidgetResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_existing_widget resource deprecated\", \"use example_new_widget resource instead\"), ) } func (e *exampleExistingWidgetResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_existing_widget resource deprecated\", \"use example_new_widget resource instead\"), ) } func (e *exampleExistingWidgetResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { resp.Diagnostics.Append( diag.NewErrorDiagnostic(\"example_existing_widget resource deprecated\", \"use example_new_widget resource instead\"), ) } // ... resource implementation ... \nTo remove example_existing_widget:\nfunc NewWidgetResource() resource.Resource { return &exampleWidgetResource{} } func (e *exampleWidgetResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other configuration ... Attributes: map[string]schema.Attribute{ // ... other attributes ... }, } } // ... resource implementation ..."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.16.x/resources/create",
  "text": "// ThingResource defines the resource implementation. // Some resource.Resource interface methods are omitted for brevity. type ThingResource struct { // client is configured via a Configure method, which is not shown in this // example for brevity. Refer to the Configure Resources documentation for // additional details for setting up resources with external clients. client *http.Client } // ThingResourceModel describes the Terraform resource data model to match the // resource schema. type ThingResourceModel struct { Name types.String `tfsdk:\"name\"` Id types.String `tfsdk:\"id\"` } // ThingResourceAPIModel describes the API data model. type ThingResourceAPIModel struct { Name string `json:\"name\"` Id string `json:\"id\"` } func (r ThingResource) GetSchema(ctx context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ Attributes: map[string]tfsdk.Attribute{ \"name\": { MarkdownDescription: \"Name of the thing to be saved in the service.\", Required: true, Type: types.StringType, }, \"id\": { Computed: true, MarkdownDescription: \"Service generated identifier for the thing.\", PlanModifiers: tfsdk.AttributePlanModifiers{ resource.UseStateForUnknown(), }, Type: types.StringType, }, }, MarkdownDescription: \"Manages a thing.\", }, nil } func (r ThingResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data *ThingResourceModel // Read Terraform plan data into the model resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // Convert from Terraform data model into API data model createReq := ThingResourceAPIModel{ Name: data.Name.ValueString(), } httpReqBody, err := json.Marshal(createReq) if err != nil { resp.Diagnostics.AddError( \"Unable to Create Resource\", \"An unexpected error occurred while creating the resource create request. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Create HTTP request httpReq := http.NewRequestWithContext( ctx, http.MethodPost, \"http://example.com/things\", bytes.NewBuffer(httpReqBody), ) // Send HTTP request httpResp, err := d.client.Do(httpReq) defer httpResp.Body.Close() if err != nil { resp.Diagnostics.AddError( \"Unable to Create Resource\", \"An unexpected error occurred while attempting to create the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Error: \"+err.Error(), ) return } // Return error if the HTTP status code is not 200 OK if httpResp.StatusCode != http.StatusOK { resp.Diagnostics.AddError( \"Unable to Create Resource\", \"An unexpected error occurred while attempting to create the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Status: \"+httpResp.Status, ) return } var createResp ThingResourceAPIModel err := json.NewDecoder(httpResp.Body).Decode(&createResp) if err != nil { resp.Diagnostics.AddError( \"Unable to Create Resource\", \"An unexpected error occurred while parsing the resource creation response. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Convert from the API data model to the Terraform data model // and set any unknown attribute values. data.Id = types.StringValue(createResp.Id) // Save data into Terraform state resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.16.x/resources/read",
  "text": "// ThingResource defines the resource implementation. // Some resource.Resource interface methods are omitted for brevity. type ThingResource struct { // client is configured via a Configure method, which is not shown in this // example for brevity. Refer to the Configure Resources documentation for // additional details for setting up resources with external clients. client *http.Client } // ThingResourceModel describes the Terraform resource data model to match the // resource schema. type ThingResourceModel struct { Name types.String `tfsdk:\"name\"` Id types.String `tfsdk:\"id\"` } // ThingResourceAPIModel describes the API data model. type ThingResourceAPIModel struct { Name string `json:\"name\"` Id string `json:\"id\"` } func (r ThingResource) GetSchema(ctx context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ Attributes: map[string]tfsdk.Attribute{ \"name\": { MarkdownDescription: \"Name of the thing to be saved in the service.\", Required: true, Type: types.StringType, }, \"id\": { Computed: true, MarkdownDescription: \"Service generated identifier for the thing.\", PlanModifiers: tfsdk.AttributePlanModifiers{ resource.UseStateForUnknown(), }, Type: types.StringType, }, }, MarkdownDescription: \"Manages a thing.\", }, nil } func (r *ThingResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { var data ThingResourceModel // Read Terraform prior state data into the model resp.Diagnostics.Append(req.State.Get(ctx, &data)...) // Convert from Terraform data model into API data model readReq := ThingResourceAPIModel{ Id: data.Id.ValueString(), Name: data.Name.ValueString(), } httpReqBody, err := json.Marshal(readReq) if err != nil { resp.Diagnostics.AddError( \"Unable to Refresh Resource\", \"An unexpected error occurred while creating the resource read request. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Create HTTP request httpReq := http.NewRequestWithContext( ctx, http.MethodPut, \"http://example.com/things\", bytes.NewBuffer(httpReqBody), ) httpResp, err := d.client.Do(httpReq) defer httpResp.Body.Close() if err != nil { resp.Diagnostics.AddError( \"Unable to Refresh Resource\", \"An unexpected error occurred while attempting to refresh resource state. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Error: \"+err.Error(), ) return } // Treat HTTP 404 Not Found status as a signal to recreate resource // and return early if httpResp.StatusCode == http.StatusNotFound { resp.State.RemoveResource(ctx) return } var readResp ThingResourceAPIModel err := json.NewDecoder(httpResp.Body).Decode(&readResp) if err != nil { resp.Diagnostics.AddError( \"Unable to Refresh Resource\", \"An unexpected error occurred while parsing the resource read response. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Convert from the API data model to the Terraform data model // and refresh any attribute values. data.Name = types.StringValue(readResp.Name) // Save updated data into Terraform state resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.16.x/resources/update",
  "text": "// ThingResource defines the resource implementation. // Some resource.Resource interface methods are omitted for brevity. type ThingResource struct { // client is configured via a Configure method, which is not shown in this // example for brevity. Refer to the Configure Resources documentation for // additional details for setting up resources with external clients. client *http.Client } // ThingResourceModel describes the Terraform resource data model to match the // resource schema. type ThingResourceModel struct { Name types.String `tfsdk:\"name\"` Id types.String `tfsdk:\"id\"` } // ThingResourceAPIModel describes the API data model. type ThingResourceAPIModel struct { Name string `json:\"name\"` Id string `json:\"id\"` } func (r ThingResource) GetSchema(ctx context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ Attributes: map[string]tfsdk.Attribute{ \"name\": { MarkdownDescription: \"Name of the thing to be saved in the service.\", Required: true, Type: types.StringType, }, \"id\": { Computed: true, MarkdownDescription: \"Service generated identifier for the thing.\", PlanModifiers: tfsdk.AttributePlanModifiers{ resource.UseStateForUnknown(), }, Type: types.StringType, }, }, MarkdownDescription: \"Manages a thing.\", }, nil } func (r ThingResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data ThingResourceModel // Read Terraform plan data into the model resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) // Convert from Terraform data model into API data model updateReq := ThingResourceAPIModel{ Id: data.Id.StringValue(), Name: data.Name.StringValue(), } httpReqBody, err := json.Marshal(updateReq) if err != nil { resp.Diagnostics.AddError( \"Unable to Update Resource\", \"An unexpected error occurred while creating the resource update request. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Create HTTP request httpReq := http.NewRequestWithContext( ctx, http.MethodPut, \"http://example.com/things\", bytes.NewBuffer(httpReqBody), ) // Send HTTP request httpResp, err := d.client.Do(httpReq) defer httpResp.Body.Close() if err != nil { resp.Diagnostics.AddError( \"Unable to Update Resource\", \"An unexpected error occurred while attempting to update the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Error: \"+err.Error(), ) return } // Return error if the HTTP status code is not 200 OK if httpResp.StatusCode != http.StatusOK { resp.Diagnostics.AddError( \"Unable to Update Resource\", \"An unexpected error occurred while attempting to update the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Status: \"+httpResp.Status, ) return } // Save updated data into Terraform state resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.16.x/resources/delete",
  "text": "// ThingResource defines the resource implementation. // Some resource.Resource interface methods are omitted for brevity. type ThingResource struct { // client is configured via a Configure method, which is not shown in this // example for brevity. Refer to the Configure Resources documentation for // additional details for setting up resources with external clients. client *http.Client } // ThingResourceModel describes the Terraform resource data model to match the // resource schema. type ThingResourceModel struct { Name types.String `tfsdk:\"name\"` Id types.String `tfsdk:\"id\"` } // ThingResourceAPIModel describes the API data model. type ThingResourceAPIModel struct { Name string `json:\"name\"` Id string `json:\"id\"` } func (r ThingResource) GetSchema(ctx context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ Attributes: map[string]tfsdk.Attribute{ \"name\": { MarkdownDescription: \"Name of the thing to be saved in the service.\", Required: true, Type: types.StringType, }, \"id\": { Computed: true, MarkdownDescription: \"Service generated identifier for the thing.\", PlanModifiers: tfsdk.AttributePlanModifiers{ resource.UseStateForUnknown(), }, Type: types.StringType, }, }, MarkdownDescription: \"Manages a thing.\", }, nil } func (r ThingResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { var data ThingResourceModel // Read Terraform prior state data into the model resp.Diagnostics.Append(req.State.Get(ctx, &data)...) // Convert from Terraform data model into API data model readReq := ThingResourceAPIModel{ Id: data.Id.ValueString(), Name: data.Name.ValueString(), } httpReqBody, err := json.Marshal(readReq) if err != nil { resp.Diagnostics.AddError( \"Unable to Delete Resource\", \"An unexpected error occurred while creating the resource d request. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Create HTTP request httpReq := http.NewRequestWithContext( ctx, http.MethodDelete, \"http://example.com/things\", bytes.NewBuffer(httpReqBody), ) // Send HTTP request httpResp, err := d.client.Do(httpReq) defer httpResp.Body.Close() if err != nil { resp.Diagnostics.AddError( \"Unable to Delete Resource\", \"An unexpected error occurred while attempting to delete the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Error: \"+err.Error(), ) return } // Return error if the HTTP status code is not 200 OK or 404 Not Found if httpResp.StatusCode != http.StatusNotFound && httpResp.StatusCode != http.StatusOK { resp.Diagnostics.AddError( \"Unable to Delete Resource\", \"An unexpected error occurred while attempting to delete the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Status: \"+httpResp.Status, ) return } // If the logic reaches here, it implicitly succeeded and will remove // the resource from state if there are no other errors. } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.16.x/resources/plan-modification",
  "text": "After validation and before applying configuration changes, Terraform generates a plan that describes the expected values and behaviors of those changes. Providers can then tailor the plan to match the expected end state. For example, they may replace unknown values with expected known values or mark a resource that must be replaced. Users can perform this plan modification for an attribute or an entire resource.\nTerraform and the framework support three types of plan modification on resources:\nAdjusting attribute values, such as providing a known remote default value when a configuration is not present.\nReturning warning or error diagnostics on planned resource destruction with Terraform 1.3 and later.\nApply attribute plan modifiers.\nApply resource plan modifiers.\nNOTE: Providers and data sources do not use the same planning mechanism as resources within Terraform. Neither support the concept of plan modification. Data sources should set any planned values in the Read method.\nYou can supply the tfsdk.Attribute type PlanModifiers field with a list of plan modifiers for that attribute. For example:\n// Typically within the tfsdk.Schema returned by GetSchema() for a resource. tfsdk.Attribute{ // ... other Attribute configuration ... PlanModifiers: []AttributePlanModifiers{ resource.RequiresReplace(), }, } \nThe framework implements some common use case modifiers:\nresource.RequiresReplace(): If the value of the attribute changes, in-place update is not possible and instead the resource should be replaced for the change to occur. Refer to the Go documentation for full details on its behavior.\nresource.RequiresReplaceIf(): Similar to resource.RequiresReplace(), however it also accepts provider-defined conditional logic. Refer to the Go documentation for full details on its behavior.\nresource.UseStateForUnknown(): Copies the prior state value, if not null. This is useful for reducing (known after apply) plan outputs for computed attributes which are known to not change over time.\nTo create an attribute plan modifier, you must implement the tfsdk.AttributePlanModifier interface. For example:\n// stringDefaultModifier is a plan modifier that sets a default value for a // types.StringType attribute when it is not configured. The attribute must be // marked as Optional and Computed. When setting the state during the resource // Create, Read, or Update methods, this default value must also be included or // the Terraform CLI will generate an error. type stringDefaultModifier struct { Default string } // Description returns a plain text description of the validator's behavior, suitable for a practitioner to understand its impact. func (m stringDefaultModifier) Description(ctx context.Context) string { return fmt.Sprintf(\"If value is not configured, defaults to %s\", m.Default) } // MarkdownDescription returns a markdown formatted description of the validator's behavior, suitable for a practitioner to understand its impact. func (m stringDefaultModifier) MarkdownDescription(ctx context.Context) string { return fmt.Sprintf(\"If value is not configured, defaults to `%s`\", m.Default) } // Modify runs the logic of the plan modifier. // Access to the configuration, plan, and state is available in `req`, while // `resp` contains fields for updating the planned value, triggering resource // replacement, and returning diagnostics. func (m stringDefaultModifier) Modify(ctx context.Context, req tfsdk.ModifyAttributePlanRequest, resp *tfsdk.ModifyAttributePlanResponse) { // If the value is unknown or known, do not set default value. if !req.AttributePlan.IsNull() { return } // types.String must be the attr.Value produced by the attr.Type in the schema for this attribute // for generic plan modifiers, use // https://pkg.go.dev/github.com/hashicorp/terraform-plugin-framework/tfsdk#ConvertValue // to convert into a known type. var str types.String diags := tfsdk.ValueAs(ctx, req.AttributePlan, &str) resp.Diagnostics.Append(diags...) if diags.HasError() { return } resp.AttributePlan = types.StringValue(m.Default) } \nfunc stringDefault(defaultValue string) stringDefaultModifier { return stringDefaultModifier{ Default: defaultValue, } } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.16.x/resources/import",
  "text": "func (r *ThingResource) GetSchema(ctx context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ Attributes: map[string]tfsdk.Attribute{ \"attr_one\": { Type: types.StringType, Required: true, }, \"attr_two\": { Type: types.StringType, Required: true, }, /* ... */ }, }, nil } func (r *ThingResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { var attrOne, attrTwo string resp.Diagnostics.Append(req.State.GetAttribute(ctx, path.Root(\"attr_one\"), &attrOne)...) resp.Diagnostics.Append(req.State.GetAttribute(ctx, path.Root(\"attr_two\"), &attrTwo)...) if resp.Diagnostics.HasError() { return } // API call using attrOne and attrTwo } func (r *ThingResource) ImportState(ctx context.Context, req resource.ImportStateRequest, resp *resource.ImportStateResponse) { idParts := strings.Split(req.ID, \",\") if len(idParts) != 2 || idParts[0] == \"\" || idParts[1] == \"\" { resp.Diagnostics.AddError( \"Unexpected Import Identifier\", fmt.Sprintf(\"Expected import identifier with format: attr_one,attr_two. Got: %q\", req.ID), ) return } resp.Diagnostics.Append(resp.State.SetAttribute(ctx, path.Root(\"attr_one\"), idParts[0])...) resp.Diagnostics.Append(resp.State.SetAttribute(ctx, path.Root(\"attr_two\"), idParts[1])...) } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.16.x/resources/private-state",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.16.x/resources/timeouts",
  "text": "You can use this module to mutate the tfsdk.Schema as follows:\nfunc (t *exampleResource) GetSchema(ctx context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ /* ... */ Blocks: map[string]tfsdk.Block{ \"timeouts\": timeouts.Block(ctx, timeouts.Opts{ Create: true, }), }, \nYou can use this module to mutate the tfsdk.Schema as follows:\nfunc (t *exampleResource) GetSchema(ctx context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ Attributes: map[string]tfsdk.Attribute{ /* ... */ \"timeouts\": timeouts.Attributes(ctx, timeouts.Opts{ Create: true, }), }, \nThe exampleResourceData model needs to be modified to include a field for timeouts, which is types.Object.\ntype exampleResourceData struct { /* ... */ Timeouts types.Object `tfsdk:\"timeouts\"` \nfunc (e *exampleResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleResourceData diags := req.Plan.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } defaultCreateTimeout := 20 * time.Minute createTimeout := timeouts.Create(ctx, data.Timeouts, defaultCreateTimeout) ctx, cancel := context.WithTimeout(ctx, createTimeout) defer cancel() /* ... */ }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.13.x/resources/import",
  "text": "Plugin Development - Framework: Resource Import | Terraform\nPractitioners can use the terraform import command to let Terraform begin managing existing infrastructure resources. Resources can implement the ImportState method, which must either specify enough Terraform state for the Read method to refresh resource.Resource or return an error.\nThe resource.ResourceWithImportState interface on the resource.Resource interface implementation will enable practitioner support for importing an existing resource.\nImplement the ImportState method by:\nAccessing the import identifier from the resource.ImportStateRequest.ID field\nWriting state data into the resource.ImportStateResponse.State field.\nIn this example, the resource state has the id attribute set to the value passed into the terraform import command using the resource.ImportStatePassthroughID function:\nfunc (r *ThingResource) ImportState(ctx context.Context, req resource.ImportStateRequest, resp *resource.ImportStateResponse) { resource.ImportStatePassthroughID(ctx, path.Root(\"id\"), req, resp) } \nMultiple Attributes\nWhen the Read method requires multiple attributes to refresh, you must write custom logic in the ImportState method.\nImplement the ImportState method by:\nAccessing the import identifier from the resource.ImportStateRequest.ID field\nPerforming the custom logic.\nWriting state data into the resource.ImportStateResponse.State field.\nThe terraform import command will need to accept the multiple attribute values as a single import identifier string. A typical convention is to use a separator character, such as a comma (,), between the values. The ImportState method will then need to parse the import identifier string into the multiple separate values and save them appropriately into the Terraform state.\nIn this example, the resource requires two attributes to refresh state and accepts them as an import identifier of attr_one,attr_two:\nfunc (r *ThingResource) GetSchema(ctx context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ Attributes: map[string]tfsdk.Attribute{ \"attr_one\": { Type: types.StringType, Required: true, }, \"attr_two\": { Type: types.StringType, Required: true, }, /* ... */ }, }, nil } func (r *ThingResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { var attrOne, attrTwo string resp.Diagnostics.Append(req.State.GetAttribute(ctx, path.Root(\"attr_one\"), &attrOne)...) resp.Diagnostics.Append(req.State.GetAttribute(ctx, path.Root(\"attr_two\"), &attrTwo)...) if resp.Diagnostics.HasError() { return } // API call using attrOne and attrTwo } func (r *ThingResource) ImportState(ctx context.Context, req resource.ImportStateRequest, resp *resource.ImportStateResponse) { idParts := strings.Split(req.ID, \",\") if len(idParts) != 2 || idParts[0] == \"\" || idParts[1] == \"\" { resp.Diagnostics.AddError( \"Unexpected Import Identifier\", fmt.Sprintf(\"Expected import identifier with format: attr_one,attr_two. Got: %q\", req.ID), ) return } resp.Diagnostics.Append(resp.State.SetAttribute(ctx, path.Root(\"attr_one\"), idParts[0])...) resp.Diagnostics.Append(resp.State.SetAttribute(ctx, path.Root(\"attr_two\"), idParts[1])...) } \nIf the resource does not support terraform import, skip the ImportState method implementation.\nWhen a practitioner runs terraform import, Terraform CLI will return:\n$ terraform import example_resource.example some-identifier example_resource.example: Importing from ID \"some-identifier\"...   Error: Resource Import Not Implemented   This resource does not support import. Please contact the provider developer for additional information. "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.13.x/resources/plan-modification",
  "text": "Plugin Development - Framework: Plan Modification | Terraform\nAfter validation and before applying configuration changes, Terraform generates a plan that describes the expected values and behaviors of those changes. Providers can then tailor the plan to match the expected end state. For example, they may replace unknown values with expected known values or mark a resource that must be replaced. Users can perform this plan modification for an attribute or an entire resource.\nTerraform and the framework support three types of plan modification on resources:\nAdjusting attribute values, such as providing a known remote default value when a configuration is not present.\nMarking resources that should be replaced, such as when an in-place update is not supported for a change.\nReturning warning or error diagnostics on planned resource destruction with Terraform 1.3 and later.\nWhen the provider receives a request to generate the plan for a resource change via the framework, the following occurs:\nIf the plan differs from the current resource state, the framework marks computed attributes that are null in the configuration as unknown in the plan. This is intended to prevent unexpected Terraform errors. Providers can later enter any values that may be known.\nApply attribute plan modifiers.\nApply resource plan modifiers.\nWhen the Resource interface Update method runs to apply a change, all attribute state values must match their associated planned values or Terraform will generate a Provider produced inconsistent result error. You can mark values as unknown in the plan if the full expected value is not known.\nRefer to the Resource Instance Change Lifecycle document for more details about the concepts and processes relevant to the plan and apply workflows.\nNOTE: Providers and data sources do not use the same planning mechanism as resources within Terraform. Neither support the concept of plan modification. Data sources should set any planned values in the Read method.\nYou can supply the tfsdk.Attribute type PlanModifiers field with a list of plan modifiers for that attribute. For example:\n// Typically within the tfsdk.Schema returned by GetSchema() for a resource. tfsdk.Attribute{ // ... other Attribute configuration ... PlanModifiers: []AttributePlanModifiers{ resource.RequiresReplace(), }, } \nIf defined, plan modifiers are applied to the current attribute. If any nested attributes define plan modifiers, then those are applied afterwards. Any plan modifiers that return an error will prevent Terraform from applying further modifiers of that attribute as well as any nested attribute plan modifiers.\nCommon Use Case Attribute Plan Modifiers\nThe framework implements some common use case modifiers:\nresource.RequiresReplace(): If the value of the attribute changes, in-place update is not possible and instead the resource should be replaced for the change to occur. Refer to the Go documentation for full details on its behavior.\nresource.RequiresReplaceIf(): Similar to resource.RequiresReplace(), however it also accepts provider-defined conditional logic. Refer to the Go documentation for full details on its behavior.\nresource.UseStateForUnknown(): Copies the prior state value, if not null. This is useful for reducing (known after apply) plan outputs for computed attributes which are known to not change over time.\nCreating Attribute Plan Modifiers\nTo create an attribute plan modifier, you must implement the tfsdk.AttributePlanModifier interface. For example:\n// stringDefaultModifier is a plan modifier that sets a default value for a // types.StringType attribute when it is not configured. The attribute must be // marked as Optional and Computed. When setting the state during the resource // Create, Read, or Update methods, this default value must also be included or // the Terraform CLI will generate an error. type stringDefaultModifier struct { Default string } // Description returns a plain text description of the validator's behavior, suitable for a practitioner to understand its impact. func (m stringDefaultModifier) Description(ctx context.Context) string { return fmt.Sprintf(\"If value is not configured, defaults to %s\", m.Default) } // MarkdownDescription returns a markdown formatted description of the validator's behavior, suitable for a practitioner to understand its impact. func (m stringDefaultModifier) MarkdownDescription(ctx context.Context) string { return fmt.Sprintf(\"If value is not configured, defaults to `%s`\", m.Default) } // Modify runs the logic of the plan modifier. // Access to the configuration, plan, and state is available in `req`, while // `resp` contains fields for updating the planned value, triggering resource // replacement, and returning diagnostics. func (m stringDefaultModifier) Modify(ctx context.Context, req tfsdk.ModifyAttributePlanRequest, resp *tfsdk.ModifyAttributePlanResponse) { // types.String must be the attr.Value produced by the attr.Type in the schema for this attribute // for generic plan modifiers, use // https://pkg.go.dev/github.com/hashicorp/terraform-plugin-framework/tfsdk#ConvertValue // to convert into a known type. var str types.String diags := tfsdk.ValueAs(ctx, req.AttributePlan, &str) resp.Diagnostics.Append(diags...) if diags.HasError() { return } if !str.Null { return } resp.AttributePlan = types.String{Value: m.Default} } \nOptionally, you may also want to create a helper function to instantiate the plan modifier. For example:\nfunc stringDefault(defaultValue string) stringDefaultModifier { return stringDefaultModifier{ Default: defaultValue, } } \nResources also support plan modification across all attributes. This is helpful when working with logic that applies to the resource as a whole, or in Terraform 1.3 and later, to return diagnostics during resource destruction. Implement the resource.ResourceWithModifyPlan interface to support resource-level plan modification. For example:\n// Ensure the Resource satisfies the resource.ResourceWithModifyPlan interface. // Other methods to implement the resource.Resource interface are omitted for brevity var _ resource.ResourceWithModifyPlan = exampleResource{} type exampleResource struct {} func (r exampleResource) ModifyPlan(ctx context.Context, req tfsdk.ModifyResourcePlanRequest, resp *tfsdk.ModifyResourcePlanResponse) { // Fill in logic. } \nResource Destroy Plan Diagnostics\nSupport for handling resource destruction during planning is available in Terraform 1.3 and later.\nImplement the ModifyPlan method by checking if the tfsdk.ModifyResourcePlanRequest type Plan field is a null value:\nfunc (r exampleResource) ModifyPlan(ctx context.Context, req tfsdk.ModifyResourcePlanRequest, resp *tfsdk.ModifyResourcePlanResponse) { // If the entire plan is null, the resource is planned for destruction. if req.Plan.Raw.IsNull() { // Return an example warning diagnostic to practitioners. resp.Diagnostics.AddWarning( \"Resource Destruction Considerations\", \"Applying this resource destruction will only remove the resource from the Terraform state \"+ \"and will not call the deletion API due to API limitations. Manually use the web \"+ \"interface to fully destroy this resource.\", ) } } \nEnsure the response plan remains entirely null when the request plan is entirely null."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.13.x/resources/private-state",
  "text": "Plugin Development - Framework: Private State Management | Terraform\nResource private state is provider maintained data that is stored in Terraform state alongside the schema-defined data. Private state is never accessed or exposed by Terraform plans, however providers can use this data storage for advanced use cases.\nExample uses in the framework include:\nStoring and retrieving values that are not important to show to practitioners, but are required for API calls, such as ETags.\nResource timeout functionality.\nPrivate state data is byte data stored in the Terraform state and is intended for provider usage only (i.e., it is only used by the Framework and provider code). Providers have the ability to save this data during create, import, planning, read, and update operations and the ability to read this data during delete, planning, read, and update operations.\nPrivate state data can be read from a privatestate.ProviderData type in the Private field present in the request that is passed into:\nPrivate state data can be saved to a privatestate.ProviderData type in the Private field present in the response that is returned from:\nReading Private State Data\nPrivate state data can be read using the GetKey function. For example:\nfunc (r *resourceExample) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { value, diags := req.Private.GetKey(ctx, \"key\") resp.Diagnostics.Append(diags...) if value != nil { // value will be []byte. ... } } \nIf the key supplied is reserved for framework usage, an error diagnostic will be returned.\nIf the key is valid but no private state data is found, nil is returned.\nSaving Private State Data\nPrivate state data can be saved using the SetKey function. For example:\nfunc (r *resourceExample) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { value := []byte(`{\"valid\": \"json\", \"utf8\": \"safe\"}`) diags := resp.Private.SetKey(ctx, \"key\", value) resp.Diagnostics.Append(diags...) } \nIf the key supplied is reserved for framework usage, an error diagnostic will be returned.\nIf the value is not valid JSON and UTF-8 safe, an error diagnostic will be returned.\nReserved Keys\nKeys supplied to GetKey and SetKey are validated using ValidateProviderDataKey.\nKeys using a period ('.') as a prefix cannot be used for provider private state data as they are reserved for framework usage."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.12.x/resources/import",
  "text": "Plugin Development - Framework: Resource Import | Terraform\nPractitioners can use the terraform import command to let Terraform begin managing existing infrastructure resources. Resources can implement the ImportState method, which must either specify enough Terraform state for the Read method to refresh resource.Resource or return an error.\nThe resource.ResourceWithImportState interface on the resource.Resource interface implementation will enable practitioner support for importing an existing resource.\nImplement the ImportState method by:\nAccessing the import identifier from the resource.ImportStateRequest.ID field\nWriting state data into the resource.ImportStateResponse.State field.\nIn this example, the resource state has the id attribute set to the value passed into the terraform import command using the resource.ImportStatePassthroughID function:\nfunc (r *ThingResource) ImportState(ctx context.Context, req resource.ImportStateRequest, resp *resource.ImportStateResponse) { resource.ImportStatePassthroughID(ctx, path.Root(\"id\"), req, resp) } \nMultiple Attributes\nWhen the Read method requires multiple attributes to refresh, you must write custom logic in the ImportState method.\nImplement the ImportState method by:\nAccessing the import identifier from the resource.ImportStateRequest.ID field\nPerforming the custom logic.\nWriting state data into the resource.ImportStateResponse.State field.\nThe terraform import command will need to accept the multiple attribute values as a single import identifier string. A typical convention is to use a separator character, such as a comma (,), between the values. The ImportState method will then need to parse the import identifier string into the multiple separate values and save them appropriately into the Terraform state.\nIn this example, the resource requires two attributes to refresh state and accepts them as an import identifier of attr_one,attr_two:\nfunc (r *ThingResource) GetSchema(ctx context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ Attributes: map[string]tfsdk.Attribute{ \"attr_one\": { Type: types.StringType, Required: true, }, \"attr_two\": { Type: types.StringType, Required: true, }, /* ... */ }, }, nil } func (r *ThingResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { var attrOne, attrTwo string resp.Diagnostics.Append(req.State.GetAttribute(ctx, path.Root(\"attr_one\"), &attrOne)...) resp.Diagnostics.Append(req.State.GetAttribute(ctx, path.Root(\"attr_two\"), &attrTwo)...) if resp.Diagnostics.HasError() { return } // API call using attrOne and attrTwo } func (r *ThingResource) ImportState(ctx context.Context, req resource.ImportStateRequest, resp *resource.ImportStateResponse) { idParts := strings.Split(req.ID, \",\") if len(idParts) != 2 || idParts[0] == \"\" || idParts[1] == \"\" { resp.Diagnostics.AddError( \"Unexpected Import Identifier\", fmt.Sprintf(\"Expected import identifier with format: attr_one,attr_two. Got: %q\", req.ID), ) return } resp.Diagnostics.Append(resp.State.SetAttribute(ctx, path.Root(\"attr_one\"), idParts[0])...) resp.Diagnostics.Append(resp.State.SetAttribute(ctx, path.Root(\"attr_two\"), idParts[1])...) } \nIf the resource does not support terraform import, skip the ImportState method implementation.\nWhen a practitioner runs terraform import, Terraform CLI will return:\n$ terraform import example_resource.example some-identifier example_resource.example: Importing from ID \"some-identifier\"...   Error: Resource Import Not Implemented   This resource does not support import. Please contact the provider developer for additional information. "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.12.x/resources/private-state",
  "text": "Plugin Development - Framework: Private State Management | Terraform\nResource private state is provider maintained data that is stored in Terraform state alongside the schema-defined data. Private state is never accessed or exposed by Terraform plans, however providers can use this data storage for advanced use cases.\nExample uses in the framework include:\nStoring and retrieving values that are not important to show to practitioners, but are required for API calls, such as ETags.\nResource timeout functionality.\nPrivate state data is byte data stored in the Terraform state and is intended for provider usage only (i.e., it is only used by the Framework and provider code). Providers have the ability to save this data during create, import, planning, read, and update operations and the ability to read this data during delete, planning, read, and update operations.\nPrivate state data can be read from a privatestate.ProviderData type in the Private field present in the request that is passed into:\nPrivate state data can be saved to a privatestate.ProviderData type in the Private field present in the response that is returned from:\nReading Private State Data\nPrivate state data can be read using the GetKey function. For example:\nfunc (r *resourceExample) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { value, diags := req.Private.GetKey(ctx, \"key\") resp.Diagnostics.Append(diags...) if value != nil { // value will be []byte. ... } } \nIf the key supplied is reserved for framework usage, an error diagnostic will be returned.\nIf the key is valid but no private state data is found, nil is returned.\nSaving Private State Data\nPrivate state data can be saved using the SetKey function. For example:\nfunc (r *resourceExample) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { value := []byte(`{\"valid\": \"json\", \"utf8\": \"safe\"}`) diags := resp.Private.SetKey(ctx, \"key\", value) resp.Diagnostics.Append(diags...) } \nIf the key supplied is reserved for framework usage, an error diagnostic will be returned.\nIf the value is not valid JSON and UTF-8 safe, an error diagnostic will be returned.\nReserved Keys\nKeys supplied to GetKey and SetKey are validated using ValidateProviderDataKey.\nKeys using a period ('.') as a prefix cannot be used for provider private state data as they are reserved for framework usage."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.12.x/resources/plan-modification",
  "text": "Plugin Development - Framework: Plan Modification | Terraform\nAfter validation and before applying configuration changes, Terraform generates a plan that describes the expected values and behaviors of those changes. Providers can then tailor the plan to match the expected end state. For example, they may replace unknown values with expected known values or mark a resource that must be replaced. Users can perform this plan modification for an attribute or an entire resource.\nTerraform and the framework support three types of plan modification on resources:\nAdjusting attribute values, such as providing a known remote default value when a configuration is not present.\nMarking resources that should be replaced, such as when an in-place update is not supported for a change.\nReturning warning or error diagnostics on planned resource destruction with Terraform 1.3 and later.\nWhen the provider receives a request to generate the plan for a resource change via the framework, the following occurs:\nIf the plan differs from the current resource state, the framework marks computed attributes that are null in the configuration as unknown in the plan. This is intended to prevent unexpected Terraform errors. Providers can later enter any values that may be known.\nApply attribute plan modifiers.\nApply resource plan modifiers.\nWhen the Resource interface Update method runs to apply a change, all attribute state values must match their associated planned values or Terraform will generate a Provider produced inconsistent result error. You can mark values as unknown in the plan if the full expected value is not known.\nRefer to the Resource Instance Change Lifecycle document for more details about the concepts and processes relevant to the plan and apply workflows.\nNOTE: Providers and data sources do not use the same planning mechanism as resources within Terraform. Neither support the concept of plan modification. Data sources should set any planned values in the Read method.\nYou can supply the tfsdk.Attribute type PlanModifiers field with a list of plan modifiers for that attribute. For example:\n// Typically within the tfsdk.Schema returned by GetSchema() for a resource. tfsdk.Attribute{ // ... other Attribute configuration ... PlanModifiers: []AttributePlanModifiers{ resource.RequiresReplace(), }, } \nIf defined, plan modifiers are applied to the current attribute. If any nested attributes define plan modifiers, then those are applied afterwards. Any plan modifiers that return an error will prevent Terraform from applying further modifiers of that attribute as well as any nested attribute plan modifiers.\nCommon Use Case Attribute Plan Modifiers\nThe framework implements some common use case modifiers:\nresource.RequiresReplace(): If the value of the attribute changes, in-place update is not possible and instead the resource should be replaced for the change to occur. Refer to the Go documentation for full details on its behavior.\nresource.RequiresReplaceIf(): Similar to resource.RequiresReplace(), however it also accepts provider-defined conditional logic. Refer to the Go documentation for full details on its behavior.\nresource.UseStateForUnknown(): Copies the prior state value, if not null. This is useful for reducing (known after apply) plan outputs for computed attributes which are known to not change over time.\nCreating Attribute Plan Modifiers\nTo create an attribute plan modifier, you must implement the tfsdk.AttributePlanModifier interface. For example:\n// stringDefaultModifier is a plan modifier that sets a default value for a // types.StringType attribute when it is not configured. The attribute must be // marked as Optional and Computed. When setting the state during the resource // Create, Read, or Update methods, this default value must also be included or // the Terraform CLI will generate an error. type stringDefaultModifier struct { Default string } // Description returns a plain text description of the validator's behavior, suitable for a practitioner to understand its impact. func (m stringDefaultModifier) Description(ctx context.Context) string { return fmt.Sprintf(\"If value is not configured, defaults to %s\", m.Default) } // MarkdownDescription returns a markdown formatted description of the validator's behavior, suitable for a practitioner to understand its impact. func (m stringDefaultModifier) MarkdownDescription(ctx context.Context) string { return fmt.Sprintf(\"If value is not configured, defaults to `%s`\", m.Default) } // Modify runs the logic of the plan modifier. // Access to the configuration, plan, and state is available in `req`, while // `resp` contains fields for updating the planned value, triggering resource // replacement, and returning diagnostics. func (m stringDefaultModifier) Modify(ctx context.Context, req tfsdk.ModifyAttributePlanRequest, resp *tfsdk.ModifyAttributePlanResponse) { // types.String must be the attr.Value produced by the attr.Type in the schema for this attribute // for generic plan modifiers, use // https://pkg.go.dev/github.com/hashicorp/terraform-plugin-framework/tfsdk#ConvertValue // to convert into a known type. var str types.String diags := tfsdk.ValueAs(ctx, req.AttributePlan, &str) resp.Diagnostics.Append(diags...) if diags.HasError() { return } if !str.Null { return } resp.AttributePlan = types.String{Value: m.Default} } \nOptionally, you may also want to create a helper function to instantiate the plan modifier. For example:\nfunc stringDefault(defaultValue string) stringDefaultModifier { return stringDefaultModifier{ Default: defaultValue, } } \nResources also support plan modification across all attributes. This is helpful when working with logic that applies to the resource as a whole, or in Terraform 1.3 and later, to return diagnostics during resource destruction. Implement the resource.ResourceWithModifyPlan interface to support resource-level plan modification. For example:\n// Ensure the Resource satisfies the resource.ResourceWithModifyPlan interface. // Other methods to implement the resource.Resource interface are omitted for brevity var _ resource.ResourceWithModifyPlan = exampleResource{} type exampleResource struct {} func (r exampleResource) ModifyPlan(ctx context.Context, req tfsdk.ModifyResourcePlanRequest, resp *tfsdk.ModifyResourcePlanResponse) { // Fill in logic. } \nResource Destroy Plan Diagnostics\nSupport for handling resource destruction during planning is available in Terraform 1.3 and later.\nImplement the ModifyPlan method by checking if the tfsdk.ModifyResourcePlanRequest type Plan field is a null value:\nfunc (r exampleResource) ModifyPlan(ctx context.Context, req tfsdk.ModifyResourcePlanRequest, resp *tfsdk.ModifyResourcePlanResponse) { // If the entire plan is null, the resource is planned for destruction. if req.Plan.Raw.IsNull() { // Return an example warning diagnostic to practitioners. resp.Diagnostics.AddWarning( \"Resource Destruction Considerations\", \"Applying this resource destruction will only remove the resource from the Terraform state \"+ \"and will not call the deletion API due to API limitations. Manually use the web \"+ \"interface to fully destroy this resource.\", ) } } \nEnsure the response plan remains entirely null when the request plan is entirely null."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.11.x/resources/state-upgrade",
  "text": "Plugin Development - Framework: State Upgrade | Terraform\nA resource schema captures the structure and types of the resource state. Any state data that does not conform to the resource schema will generate errors or may not be persisted properly. Over time, it may be necessary for resources to make breaking changes to their schemas, such as changing an attribute type. Terraform supports versioning of these resource schemas and the current version is saved into the Terraform state. When the provider advertises a newer schema version, Terraform will call back to the provider to attempt to upgrade from the saved schema version to the one advertised. This operation is performed prior to planning, but with a configured provider.\nSome versions of Terraform CLI will also request state upgrades even when the current schema version matches the state version. The framework will automatically handle this request.\nWhen generating a plan, Terraform CLI will request the current resource schema, which contains a version.\nIf Terraform CLI detects that an existing state with its saved version does not match the current version, Terraform CLI will request a state upgrade from the provider with the prior state version and expecting the state to match the current version.\nThe framework will check the resource to see if it defines state upgrade support:\nIf no state upgrade support is defined, an error diagnostic is returned.\nIf state upgrade support is defined, but not for the requested prior state version, an error diagnostic is returned.\nIf state upgrade support is defined and has an implementation for the requested prior state version, the provider defined implementation is executed.\nEnsure the tfsdk.Schema type Version field for the provider.ResourceType is greater than 0, then implement the resource.ResourceWithStateUpgrade interface for the resource.Resource. Conventionally the version is incremented by 1 for each state upgrade.\nThis example shows a ResourceType which incremented the Schema type Version field:\n// Other ResourceType methods are omitted in this example var _ provider.ResourceType = exampleResourceType{} type exampleResourceType struct{/* ... */} func (t exampleResourceType) GetSchema(_ context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ // ... other fields ... // This example conventionally declares that the resource has prior // state versions of 0 and 1, while the current version is 2. Version: 2, } } \nThis example shows a Resource with the necessary StateUpgrade method to implement the ResourceWithStateUpgrade interface:\n// Other Resource methods are omitted in this example var _ resource.Resource = exampleResource{} var _ resource.ResourceWithUpgradeState = exampleResource{} type exampleResource struct{/* ... */} func (r exampleResource) UpgradeState(ctx context.Context) map[int64]resource.StateUpgrader { return map[int64]resource.StateUpgrader{ // State upgrade implementation from 0 (prior state version) to 2 (Schema.Version) 0: { // Optionally, the PriorSchema field can be defined. StateUpgrader: func(ctx context.Context, req resource.UpgradeStateRequest, resp *resource.UpgradeStateResponse) { /* ... */ }, }, // State upgrade implementation from 1 (prior state version) to 2 (Schema.Version) 1: { // Optionally, the PriorSchema field can be defined. StateUpgrader: func(ctx context.Context, req resource.UpgradeStateRequest, resp *resource.UpgradeStateResponse) { /* ... */ }, }, } } \nEach resource.StateUpgrader implementation is expected to wholly upgrade the resource state from the prior version to the current version. The framework does not iterate through intermediate version implementations as incrementing versions by 1 is only conventional and not required.\nAll state data must be populated in the resource.UpgradeStateResponse. The framework does not copy any prior state data from the resource.UpgradeStateRequest.\nThere are two approaches to implementing the provider logic for state upgrades in StateUpgrader. The recommended approach is defining the prior schema matching the resource state, which allows for prior state access similar to other parts of the framework. The second, more advanced, approach is accessing the prior resource state using lower level data handlers.\nStateUpgrader With PriorSchema\nImplement the StateUpgrader type PriorSchema field to enable the framework to populate the resource.UpgradeStateRequest type State field for the provider defined state upgrade logic. Access the request State using methods such as Get() or GetAttribute(). Write the resource.UpgradeStateResponse type State field using methods such as Set() or SetAttribute().\nThis example shows a resource that changes the type for two attributes, using the PriorSchema approach:\n// Other ResourceType methods are omitted in this example var _ provider.ResourceType = exampleResourceType{} // Other Resource methods are omitted in this example var _ resource.Resource = exampleResource{} var _ resource.ResourceWithUpgradeState = exampleResource{} type exampleResourceType struct{/* ... */} type exampleResource struct{/* ... */} type exampleResourceDataV0 struct { Id string `tfsdk:\"id\"` OptionalAttribute *bool `tfsdk:\"optional_attribute\"` RequiredAttribute bool `tfsdk:\"required_attribute\"` } type exampleResourceDataV1 struct { Id string `tfsdk:\"id\"` OptionalAttribute *string `tfsdk:\"optional_attribute\"` RequiredAttribute string `tfsdk:\"required_attribute\"` } func (t exampleResourceType) GetSchema(_ context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ Attributes: map[string]Attribute{ \"id\": { Type: types.StringType, Computed: true, }, \"optional_attribute\": { Type: types.StringType, // As compared to prior types.BoolType below Optional: true, }, \"required_attribute\": { Type: types.StringType, // As compared to prior types.BoolType below Required: true, }, }, // The resource has a prior state version of 0, which had the attribute // types of types.BoolType as shown below. Version: 1, } } func (r exampleResource) UpgradeState(ctx context.Context) map[int64]resource.StateUpgrader { return map[int64]resource.StateUpgrader{ // State upgrade implementation from 0 (prior state version) to 1 (Schema.Version) 0: { PriorSchema: &tfsdk.Schema{ Attributes: map[string]Attribute{ \"id\": { Type: types.StringType, Computed: true, }, \"optional_attribute\": { Type: types.BoolType, // As compared to current types.StringType above Optional: true, }, \"required_attribute\": { Type: types.BoolType, // As compared to current types.StringType above Required: true, }, }, }, StateUpgrader: func(ctx context.Context, req resource.UpgradeStateRequest, resp *resource.UpgradeStateResponse) { var priorStateData exampleResourceDataV0 resp.Diagnostics.Append(req.State.Get(ctx, &priorStateData)...) if resp.Diagnostics.HasError() { return } upgradedStateData := exampleResourceDataV1{ Id: priorStateData.Id, RequiredAttribute: fmt.Sprintf(\"%t\", priorStateData.RequiredAttribute), } if priorStateData.OptionalAttribute != nil { v := fmt.Sprintf(\"%t\", *priorStateData.OptionalAttribute) upgradedStateData.OptionalAttribute = &v } resp.Diagnostics.Append(resp.State.Set(ctx, upgradedStateData)...) }, }, } } \nStateUpgrader Without PriorSchema\nRead prior state data from the resource.UpgradeStateRequest type RawState field. Write the resource.UpgradeStateResponse type State field using methods such as Set() or SetAttribute(), or for more advanced use cases, write the resource.UpgradeStateResponse type DynamicValue field.\nThis example shows a resource that changes the type for two attributes, using the RawState approach for the request and DynamicValue approach for the response:\n// Other ResourceType methods are omitted in this example var _ provider.ResourceType = exampleResourceType{} // Other Resource methods are omitted in this example var _ resource.Resource = exampleResource{} var _ resource.ResourceWithUpgradeState = exampleResource{} var exampleResourceTftypesDataV0 = tftypes.Object{ AttributeTypes: map[string]tftypes.Type{ \"id\": tftypes.String, \"optional_attribute\": tftypes.Bool, \"required_attribute\": tftypes.Bool, }, } var exampleResourceTftypesDataV1 = tftypes.Object{ AttributeTypes: map[string]tftypes.Type{ \"id\": tftypes.String, \"optional_attribute\": tftypes.String, \"required_attribute\": tftypes.String, }, } type exampleResourceType struct{/* ... */} type exampleResource struct{/* ... */} func (t exampleResourceType) GetSchema(_ context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ Attributes: map[string]Attribute{ \"id\": { Type: types.StringType, Computed: true, }, \"optional_attribute\": { Type: types.StringType, // As compared to prior types.BoolType below Optional: true, }, \"required_attribute\": { Type: types.StringType, // As compared to prior types.BoolType below Required: true, }, }, // The resource has a prior state version of 0, which had the attribute // types of types.BoolType as shown below. Version: 1, } } func (r exampleResource) UpgradeState(ctx context.Context) map[int64]resource.StateUpgrader { return map[int64]resource.StateUpgrader{ // State upgrade implementation from 0 (prior state version) to 1 (Schema.Version) 0: { StateUpgrader: func(ctx context.Context, req resource.UpgradeStateRequest, resp *resource.UpgradeStateResponse) { // Refer also to the RawState type JSON field which can be used // with json.Unmarshal() rawStateValue, err := req.RawState.Unmarshal(exampleResourceTftypesDataV0) if err != nil { resp.Diagnostics.AddError( \"Unable to Unmarshal Prior State\", err.Error(), ) return } var rawState map[string]tftypes.Value if err := rawStateValue.As(&rawState); err != nil { resp.Diagnostics.AddError( \"Unable to Convert Prior State\", err.Error(), ) return } var optionalAttributeString *string if !rawState[\"optional_attribute\"].IsNull() { var optionalAttribute bool if err := rawState[\"optional_attribute\"].As(&optionalAttribute); err != nil { resp.Diagnostics.AddAttributeError( path.Root(\"optional_attribute\"), \"Unable to Convert Prior State\", err.Error(), ) return } v := fmt.Sprintf(\"%t\", optionalAttribute) optionalAttributeString = &v } var requiredAttribute bool if err := rawState[\"required_attribute\"].As(&requiredAttribute); err != nil { resp.Diagnostics.AddAttributeError( path.Root(\"required_attribute\"), \"Unable to Convert Prior State\", err.Error(), ) return } dynamicValue, err := tfprotov6.NewDynamicValue( exampleResourceTftypesDataV1, tftypes.NewValue(exampleResourceTftypesDataV1, map[string]tftypes.Value{ \"id\": rawState[\"id\"], \"optional_attribute\": tftypes.NewValue(tftypes.String, optionalAttributeString), \"required_attribute\": tftypes.NewValue(tftypes.String, fmt.Sprintf(\"%t\", requiredAttribute)), }), ) if err != nil { resp.Diagnostics.AddError( \"Unable to Convert Upgraded State\", err.Error(), ) return } resp.DynamicValue = &dynamicValue }, }, } }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.11.x/resources/import",
  "text": "Plugin Development - Framework: Resource Import | Terraform\nPractitioners can use the terraform import command to let Terraform begin managing existing infrastructure resources. Resources can implement the ImportState method, which must either specify enough Terraform state for the Read method to refresh resource.Resource or return an error.\nWhen the Read method requires a single attribute to refresh, use the resource.ImportStatePassthroughID function to write the import identifier argument for terraform import.\nIn the following example, the terraform import command passes the import identifier to the id attribute in Terraform state.\nfunc (r exampleResource) ImportState(ctx context.Context, req resource.ImportStateRequest, resp *resource.ImportStateResponse) { resource.ImportStatePassthroughID(ctx, path.Root(\"id\"), req, resp) } \nWhen the Read method requires multiple attributes to refresh, you must write custom logic in the ImportState method. Specifically, the implementation must:\nUse the import identifier from the resource.ImportStateRequest. \nPerform the custom logic.\nSet state data in the resource.ImportStateResponse.\nFor example, if the provider.ResourceType implementation has the following GetSchema method:\nfunc (t exampleResourceType) GetSchema(ctx context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ Attributes: map[string]tfsdk.Attribute{ \"attr_one\": { Type: types.StringType, Required: true, }, \"attr_two\": { Type: types.StringType, Required: true, }, // ... potentially other Attributes ... }, }, nil } \nAlong with a resource.Resource implementation with the following Read method:\nfunc (r exampleResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { var attrOne, attrTwo string resp.Diagnostics.Append(req.State.GetAttribute(ctx, path.Root(\"attr_one\"), &attrOne)...) resp.Diagnostics.Append(req.State.GetAttribute(ctx, path.Root(\"attr_two\"), &attrTwo)...) if resp.Diagnostics.HasError() { return } // API call using attrOne and attrTwo } \nThe terraform import command will need to accept both attribute values as a single import identifier string. A typical convention is to use a separator character, such as a comma (,), between the values. The ImportState method will then need to parse the import identifier string into the two separate values and save them appropriately into the Terraform state.\nYou could define the ImportState method using a comma-separated value as follows:\nfunc (r exampleResource) ImportState(ctx context.Context, req resource.ImportStateRequest, resp *resource.ImportStateResponse) { idParts := strings.Split(req.ID, \",\") if len(idParts) != 2 || idParts[0] == \"\" || idParts[1] == \"\" { resp.Diagnostics.AddError( \"Unexpected Import Identifier\", fmt.Sprintf(\"Expected import identifier with format: attr_one,attr_two. Got: %q\", req.ID), ) return } resp.Diagnostics.Append(resp.State.SetAttribute(ctx, path.Root(\"attr_one\"), idParts[0])...) resp.Diagnostics.Append(resp.State.SetAttribute(ctx, path.Root(\"attr_two\"), idParts[1])...) } \nIf the resource does not support terraform import, skip the ImportState method implementation.\nWhen a practitioner runs terraform import, Terraform CLI will return:\n$ terraform import example_resource.example some-identifier example_resource.example: Importing from ID \"some-identifier\"...   Error: Resource Import Not Implemented   This resource does not support import. Please contact the provider developer for additional information. "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.11.x/resources/plan-modification",
  "text": "Plugin Development - Framework: Plan Modification | Terraform\nAfter validation and before applying configuration changes, Terraform generates a plan that describes the expected values and behaviors of those changes. Providers can then tailor the plan to match the expected end state. For example, they may replace unknown values with expected known values or mark a resource that must be replaced. Users can perform this plan modification for an attribute or an entire resource.\nTerraform and the framework support three types of plan modification on resources:\nAdjusting attribute values, such as providing a known remote default value when a configuration is not present.\nMarking resources that should be replaced, such as when an in-place update is not supported for a change.\nReturning warning or error diagnostics on planned resource destruction with Terraform 1.3 and later.\nWhen the provider receives a request to generate the plan for a resource change via the framework, the following occurs:\nIf the plan differs from the current resource state, the framework marks computed attributes that are null in the configuration as unknown in the plan. This is intended to prevent unexpected Terraform errors. Providers can later enter any values that may be known.\nApply attribute plan modifiers.\nApply resource plan modifiers.\nWhen the Resource interface Update method runs to apply a change, all attribute state values must match their associated planned values or Terraform will generate a Provider produced inconsistent result error. You can mark values as unknown in the plan if the full expected value is not known.\nRefer to the Resource Instance Change Lifecycle document for more details about the concepts and processes relevant to the plan and apply workflows.\nNOTE: Providers and data sources do not use the same planning mechanism as resources within Terraform. Neither support the concept of plan modification. Data sources should set any planned values in the Read method.\nYou can supply the tfsdk.Attribute type PlanModifiers field with a list of plan modifiers for that attribute. For example:\n// Typically within the tfsdk.Schema returned by GetSchema() for a resource. tfsdk.Attribute{ // ... other Attribute configuration ... PlanModifiers: []AttributePlanModifiers{ resource.RequiresReplace(), }, } \nIf defined, plan modifiers are applied to the current attribute. If any nested attributes define plan modifiers, then those are applied afterwards. Any plan modifiers that return an error will prevent Terraform from applying further modifiers of that attribute as well as any nested attribute plan modifiers.\nCommon Use Case Attribute Plan Modifiers\nThe framework implements some common use case modifiers:\nresource.RequiresReplace(): If the value of the attribute changes, in-place update is not possible and instead the resource should be replaced for the change to occur. Refer to the Go documentation for full details on its behavior.\nresource.RequiresReplaceIf(): Similar to resource.RequiresReplace(), however it also accepts provider-defined conditional logic. Refer to the Go documentation for full details on its behavior.\nresource.UseStateForUnknown(): Copies the prior state value, if not null. This is useful for reducing (known after apply) plan outputs for computed attributes which are known to not change over time.\nCreating Attribute Plan Modifiers\nTo create an attribute plan modifier, you must implement the tfsdk.AttributePlanModifier interface. For example:\n// stringDefaultModifier is a plan modifier that sets a default value for a // types.StringType attribute when it is not configured. The attribute must be // marked as Optional and Computed. When setting the state during the resource // Create, Read, or Update methods, this default value must also be included or // the Terraform CLI will generate an error. type stringDefaultModifier struct { Default string } // Description returns a plain text description of the validator's behavior, suitable for a practitioner to understand its impact. func (m stringDefaultModifier) Description(ctx context.Context) string { return fmt.Sprintf(\"If value is not configured, defaults to %s\", m.Default) } // MarkdownDescription returns a markdown formatted description of the validator's behavior, suitable for a practitioner to understand its impact. func (m stringDefaultModifier) MarkdownDescription(ctx context.Context) string { return fmt.Sprintf(\"If value is not configured, defaults to `%s`\", m.Default) } // Modify runs the logic of the plan modifier. // Access to the configuration, plan, and state is available in `req`, while // `resp` contains fields for updating the planned value, triggering resource // replacement, and returning diagnostics. func (m stringDefaultModifier) Modify(ctx context.Context, req tfsdk.ModifyAttributePlanRequest, resp *tfsdk.ModifyAttributePlanResponse) { // types.String must be the attr.Value produced by the attr.Type in the schema for this attribute // for generic plan modifiers, use // https://pkg.go.dev/github.com/hashicorp/terraform-plugin-framework/tfsdk#ConvertValue // to convert into a known type. var str types.String diags := tfsdk.ValueAs(ctx, req.AttributePlan, &str) resp.Diagnostics.Append(diags...) if diags.HasError() { return } if !str.Null { return } resp.AttributePlan = types.String{Value: m.Default} } \nOptionally, you may also want to create a helper function to instantiate the plan modifier. For example:\nfunc stringDefault(defaultValue string) stringDefaultModifier { return stringDefaultModifier{ Default: defaultValue, } } \nResources also support plan modification across all attributes. This is helpful when working with logic that applies to the resource as a whole, or in Terraform 1.3 and later, to return diagnostics during resource destruction. Implement the resource.ResourceWithModifyPlan interface to support resource-level plan modification. For example:\n// Ensure the Resource satisfies the resource.ResourceWithModifyPlan interface. // Other methods to implement the resource.Resource interface are omitted for brevity var _ resource.ResourceWithModifyPlan = exampleResource{} type exampleResource struct {} func (r exampleResource) ModifyPlan(ctx context.Context, req tfsdk.ModifyResourcePlanRequest, resp *tfsdk.ModifyResourcePlanResponse) { // Fill in logic. } \nResource Destroy Plan Diagnostics\nSupport for handling resource destruction during planning is available in Terraform 1.3 and later.\nImplement the ModifyPlan method by checking if the tfsdk.ModifyResourcePlanRequest type Plan field is a null value:\nfunc (r exampleResource) ModifyPlan(ctx context.Context, req tfsdk.ModifyResourcePlanRequest, resp *tfsdk.ModifyResourcePlanResponse) { // If the entire plan is null, the resource is planned for destruction. if req.Plan.Raw.IsNull() { // Return an example warning diagnostic to practitioners. resp.Diagnostics.AddWarning( \"Resource Destruction Considerations\", \"Applying this resource destruction will only remove the resource from the Terraform state \"+ \"and will not call the deletion API due to API limitations. Manually use the web \"+ \"interface to fully destroy this resource.\", ) } } \nEnsure the response plan remains entirely null when the request plan is entirely null."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.10.x/resources/state-upgrade",
  "text": "Plugin Development - Framework: State Upgrade | Terraform\nA resource schema captures the structure and types of the resource state. Any state data that does not conform to the resource schema will generate errors or may not be persisted properly. Over time, it may be necessary for resources to make breaking changes to their schemas, such as changing an attribute type. Terraform supports versioning of these resource schemas and the current version is saved into the Terraform state. When the provider advertises a newer schema version, Terraform will call back to the provider to attempt to upgrade from the saved schema version to the one advertised. This operation is performed prior to planning, but with a configured provider.\nSome versions of Terraform CLI will also request state upgrades even when the current schema version matches the state version. The framework will automatically handle this request.\nWhen generating a plan, Terraform CLI will request the current resource schema, which contains a version.\nIf Terraform CLI detects that an existing state with its saved version does not match the current version, Terraform CLI will request a state upgrade from the provider with the prior state version and expecting the state to match the current version.\nThe framework will check the resource to see if it defines state upgrade support:\nIf no state upgrade support is defined, an error diagnostic is returned.\nIf state upgrade support is defined, but not for the requested prior state version, an error diagnostic is returned.\nIf state upgrade support is defined and has an implementation for the requested prior state version, the provider defined implementation is executed.\nEnsure the tfsdk.Schema type Version field for the tfsdk.ResourceType is greater than 0, then implement the tfsdk.ResourceWithStateUpgrade interface for the tfsdk.Resource. Conventionally the version is incremented by 1 for each state upgrade.\nThis example shows a ResourceType which incremented the Schema type Version field:\n// Other ResourceType methods are omitted in this example var _ tfsdk.ResourceType = exampleResourceType{} type exampleResourceType struct{/* ... */} func (t exampleResourceType) GetSchema(_ context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ // ... other fields ... // This example conventionally declares that the resource has prior // state versions of 0 and 1, while the current version is 2. Version: 2, } } \nThis example shows a Resource with the necessary StateUpgrade method to implement the ResourceWithStateUpgrade interface:\n// Other Resource methods are omitted in this example var _ tfsdk.Resource = exampleResource{} var _ tfsdk.ResourceWithUpgradeState = exampleResource{} type exampleResource struct{/* ... */} func (r exampleResource) UpgradeState(ctx context.Context) map[int64]tfsdk.ResourceStateUpgrader { return map[int64]tfsdk.ResourceStateUpgrader{ // State upgrade implementation from 0 (prior state version) to 2 (Schema.Version) 0: { // Optionally, the PriorSchema field can be defined. StateUpgrader: func(ctx context.Context, req tfsdk.UpgradeResourceStateRequest, resp *tfsdk.UpgradeResourceStateResponse) { /* ... */ }, }, // State upgrade implementation from 1 (prior state version) to 2 (Schema.Version) 1: { // Optionally, the PriorSchema field can be defined. StateUpgrader: func(ctx context.Context, req tfsdk.UpgradeResourceStateRequest, resp *tfsdk.UpgradeResourceStateResponse) { /* ... */ }, }, } } \nEach tfsdk.ResourceStateUpgrader implementation is expected to wholly upgrade the resource state from the prior version to the current version. The framework does not iterate through intermediate version implementations as incrementing versions by 1 is only conventional and not required.\nAll state data must be populated in the tfsdk.UpgradeResourceStateResponse. The framework does not copy any prior state data from the tfsdk.UpgradeResourceStateRequest.\nThere are two approaches to implementing the provider logic for state upgrades in ResourceStateUpgrader. The recommended approach is defining the prior schema matching the resource state, which allows for prior state access similar to other parts of the framework. The second, more advanced, approach is accessing the prior resource state using lower level data handlers.\nResourceStateUpgrader With PriorSchema\nImplement the ResourceStateUpgrader type PriorSchema field to enable the framework to populate the tfsdk.UpgradeResourceStateRequest type State field for the provider defined state upgrade logic. Access the request State using methods such as Get() or GetAttribute(). Write the tfsdk.UpgradeResourceStateResponse type State field using methods such as Set() or SetAttribute().\nThis example shows a resource that changes the type for two attributes, using the PriorSchema approach:\n// Other ResourceType methods are omitted in this example var _ tfsdk.ResourceType = exampleResourceType{} // Other Resource methods are omitted in this example var _ tfsdk.Resource = exampleResource{} var _ tfsdk.ResourceWithUpgradeState = exampleResource{} type exampleResourceType struct{/* ... */} type exampleResource struct{/* ... */} type exampleResourceDataV0 struct { Id string `tfsdk:\"id\"` OptionalAttribute *bool `tfsdk:\"optional_attribute\"` RequiredAttribute bool `tfsdk:\"required_attribute\"` } type exampleResourceDataV1 struct { Id string `tfsdk:\"id\"` OptionalAttribute *string `tfsdk:\"optional_attribute\"` RequiredAttribute string `tfsdk:\"required_attribute\"` } func (t exampleResourceType) GetSchema(_ context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ Attributes: map[string]Attribute{ \"id\": { Type: types.StringType, Computed: true, }, \"optional_attribute\": { Type: types.StringType, // As compared to prior types.BoolType below Optional: true, }, \"required_attribute\": { Type: types.StringType, // As compared to prior types.BoolType below Required: true, }, }, // The resource has a prior state version of 0, which had the attribute // types of types.BoolType as shown below. Version: 1, } } func (r exampleResource) UpgradeState(ctx context.Context) map[int64]tfsdk.ResourceStateUpgrader { return map[int64]tfsdk.ResourceStateUpgrader{ // State upgrade implementation from 0 (prior state version) to 1 (Schema.Version) 0: { PriorSchema: &tfsdk.Schema{ Attributes: map[string]Attribute{ \"id\": { Type: types.StringType, Computed: true, }, \"optional_attribute\": { Type: types.BoolType, // As compared to current types.StringType above Optional: true, }, \"required_attribute\": { Type: types.BoolType, // As compared to current types.StringType above Required: true, }, }, }, StateUpgrader: func(ctx context.Context, req tfsdk.UpgradeResourceStateRequest, resp *tfsdk.UpgradeResourceStateResponse) { var priorStateData exampleResourceDataV0 resp.Diagnostics.Append(req.State.Get(ctx, &priorStateData)...) if resp.Diagnostics.HasError() { return } upgradedStateData := exampleResourceDataV1{ Id: priorStateData.Id, RequiredAttribute: fmt.Sprintf(\"%t\", priorStateData.RequiredAttribute), } if priorStateData.OptionalAttribute != nil { v := fmt.Sprintf(\"%t\", *priorStateData.OptionalAttribute) upgradedStateData.OptionalAttribute = &v } resp.Diagnostics.Append(resp.State.Set(ctx, upgradedStateData)...) }, }, } } \nResourceStateUpgrader Without PriorSchema\nRead prior state data from the tfsdk.UpgradeResourceStateRequest type RawState field. Write the tfsdk.UpgradeResourceStateResponse type State field using methods such as Set() or SetAttribute(), or for more advanced use cases, write the tfsdk.UpgradeResourceStateResponse type DynamicValue field.\nThis example shows a resource that changes the type for two attributes, using the RawState approach for the request and DynamicValue approach for the response:\n// Other ResourceType methods are omitted in this example var _ tfsdk.ResourceType = exampleResourceType{} // Other Resource methods are omitted in this example var _ tfsdk.Resource = exampleResource{} var _ tfsdk.ResourceWithUpgradeState = exampleResource{} var exampleResourceTftypesDataV0 = tftypes.Object{ AttributeTypes: map[string]tftypes.Type{ \"id\": tftypes.String, \"optional_attribute\": tftypes.Bool, \"required_attribute\": tftypes.Bool, }, } var exampleResourceTftypesDataV1 = tftypes.Object{ AttributeTypes: map[string]tftypes.Type{ \"id\": tftypes.String, \"optional_attribute\": tftypes.String, \"required_attribute\": tftypes.String, }, } type exampleResourceType struct{/* ... */} type exampleResource struct{/* ... */} func (t exampleResourceType) GetSchema(_ context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ Attributes: map[string]Attribute{ \"id\": { Type: types.StringType, Computed: true, }, \"optional_attribute\": { Type: types.StringType, // As compared to prior types.BoolType below Optional: true, }, \"required_attribute\": { Type: types.StringType, // As compared to prior types.BoolType below Required: true, }, }, // The resource has a prior state version of 0, which had the attribute // types of types.BoolType as shown below. Version: 1, } } func (r exampleResource) UpgradeState(ctx context.Context) map[int64]tfsdk.ResourceStateUpgrader { return map[int64]tfsdk.ResourceStateUpgrader{ // State upgrade implementation from 0 (prior state version) to 1 (Schema.Version) 0: { StateUpgrader: func(ctx context.Context, req tfsdk.UpgradeResourceStateRequest, resp *tfsdk.UpgradeResourceStateResponse) { // Refer also to the RawState type JSON field which can be used // with json.Unmarshal() rawStateValue, err := req.RawState.Unmarshal(exampleResourceTftypesDataV0) if err != nil { resp.Diagnostics.AddError( \"Unable to Unmarshal Prior State\", err.Error(), ) return } var rawState map[string]tftypes.Value if err := rawStateValue.As(&rawState); err != nil { resp.Diagnostics.AddError( \"Unable to Convert Prior State\", err.Error(), ) return } var optionalAttributeString *string if !rawState[\"optional_attribute\"].IsNull() { var optionalAttribute bool if err := rawState[\"optional_attribute\"].As(&optionalAttribute); err != nil { resp.Diagnostics.AddAttributeError( tftypes.NewAttributePath().WithAttributeName(\"optional_attribute\"), \"Unable to Convert Prior State\", err.Error(), ) return } v := fmt.Sprintf(\"%t\", optionalAttribute) optionalAttributeString = &v } var requiredAttribute bool if err := rawState[\"required_attribute\"].As(&requiredAttribute); err != nil { resp.Diagnostics.AddAttributeError( tftypes.NewAttributePath().WithAttributeName(\"required_attribute\"), \"Unable to Convert Prior State\", err.Error(), ) return } dynamicValue, err := tfprotov6.NewDynamicValue( exampleResourceTftypesDataV1, tftypes.NewValue(exampleResourceTftypesDataV1, map[string]tftypes.Value{ \"id\": rawState[\"id\"], \"optional_attribute\": tftypes.NewValue(tftypes.String, optionalAttributeString), \"required_attribute\": tftypes.NewValue(tftypes.String, fmt.Sprintf(\"%t\", requiredAttribute)), }), ) if err != nil { resp.Diagnostics.AddError( \"Unable to Convert Upgraded State\", err.Error(), ) return } resp.DynamicValue = &dynamicValue }, }, } }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.10.x/resources/import",
  "text": "Plugin Development - Framework: Resource Import | Terraform\nPractitioners can use the terraform import command to let Terraform begin managing existing infrastructure resources. Resources can implement the ImportState method, which must either specify enough Terraform state for the Read method to refresh tfsdk.Resource or return an error.\nWhen the Read method requires a single attribute to refresh, use the tfsdk.ResourceImportStatePassthroughID function to write the import identifier argument for terraform import.\nIn the following example, the terraform import command passes the import identifier to the id attribute in Terraform state.\nfunc (r exampleResource) ImportState(ctx context.Context, req tfsdk.ImportResourceStateRequest, resp *tfsdk.ImportResourceStateResponse) { tfsdk.ResourceImportStatePassthroughID(ctx, tftypes.NewAttributePath().WithAttributeName(\"id\"), req, resp) } \nWhen the Read method requires multiple attributes to refresh, you must write custom logic in the ImportState method. Specifically, the implementation must:\nUse the import identifier from the tfsdk.ImportResourceStateRequest. \nPerform the custom logic.\nSet state data in the tfsdk.ImportResourceStateResponse.\nFor example, if the tfsdk.ResourceType implementation has the following GetSchema method:\nfunc (t exampleResourceType) GetSchema(ctx context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ Attributes: map[string]tfsdk.Attribute{ \"attr_one\": { Type: types.StringType, Required: true, }, \"attr_two\": { Type: types.StringType, Required: true, }, // ... potentially other Attributes ... }, }, nil } \nAlong with a tfsdk.Resource implementation with the following Read method:\nfunc (r exampleResource) Read(ctx context.Context, req tfsdk.ReadResourceRequest, resp *tfsdk.ReadResourceResponse) { var attrOne, attrTwo string resp.Diagnostics.Append(req.State.GetAttribute(ctx, tftypes.NewAttributePath().WithAttributeName(\"attr_one\"), &attrOne)...) resp.Diagnostics.Append(req.State.GetAttribute(ctx, tftypes.NewAttributePath().WithAttributeName(\"attr_two\"), &attrTwo)...) if resp.Diagnostics.HasError() { return } // API call using attrOne and attrTwo } \nThe terraform import command will need to accept both attribute values as a single import identifier string. A typical convention is to use a separator character, such as a comma (,), between the values. The ImportState method will then need to parse the import identifier string into the two separate values and save them appropriately into the Terraform state.\nYou could define the ImportState method using a comma-separated value as follows:\nfunc (r exampleResource) ImportState(ctx context.Context, req tfsdk.ImportResourceStateRequest, resp *tfsdk.ImportResourceStateResponse) { idParts := strings.Split(req.ID, \",\") if len(idParts) != 2 || idParts[0] == \"\" || idParts[1] == \"\" { resp.Diagnostics.AddError( \"Unexpected Import Identifier\", fmt.Sprintf(\"Expected import identifier with format: attr_one,attr_two. Got: %q\", req.ID), ) return } resp.Diagnostics.Append(resp.State.SetAttribute(ctx, tftypes.NewAttributePath().WithAttributeName(\"attr_one\"), idParts[0])...) resp.Diagnostics.Append(resp.State.SetAttribute(ctx, tftypes.NewAttributePath().WithAttributeName(\"attr_two\"), idParts[1])...) } \nIf the resource does not support terraform import, skip the ImportState method implementation.\nWhen a practitioner runs terraform import, Terraform CLI will return:\n$ terraform import example_resource.example some-identifier example_resource.example: Importing from ID \"some-identifier\"...   Error: Resource Import Not Implemented   This resource does not support import. Please contact the provider developer for additional information. "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.10.x/resources/plan-modification",
  "text": "Plugin Development - Framework: Plan Modification | Terraform\nAfter validation and before applying configuration changes, Terraform generates a plan that describes the expected values and behaviors of those changes. Providers can then tailor the plan to match the expected end state. For example, they may replace unknown values with expected known values or mark a resource that must be replaced. Users can perform this plan modification for an attribute or an entire resource.\nTerraform and the framework support three types of plan modification on resources:\nAdjusting attribute values, such as providing a known remote default value when a configuration is not present.\nMarking resources that should be replaced, such as when an in-place update is not supported for a change.\nReturning warning or error diagnostics on planned resource destruction with Terraform 1.3 and later.\nWhen the provider receives a request to generate the plan for a resource change via the framework, the following occurs:\nIf the plan differs from the current resource state, the framework marks computed attributes that are null in the configuration as unknown in the plan. This is intended to prevent unexpected Terraform errors. Providers can later enter any values that may be known.\nApply attribute plan modifiers.\nApply resource plan modifiers.\nWhen the Resource interface Update method runs to apply a change, all attribute state values must match their associated planned values or Terraform will generate a Provider produced inconsistent result error. You can mark values as unknown in the plan if the full expected value is not known.\nRefer to the Resource Instance Change Lifecycle document for more details about the concepts and processes relevant to the plan and apply workflows.\nNOTE: Providers and data sources do not use the same planning mechanism as resources within Terraform. Neither support the concept of plan modification. Data sources should set any planned values in the Read method.\nYou can supply the tfsdk.Attribute type PlanModifiers field with a list of plan modifiers for that attribute. For example:\n// Typically within the tfsdk.Schema returned by GetSchema() for a resource. tfsdk.Attribute{ // ... other Attribute configuration ... PlanModifiers: []AttributePlanModifiers{ tfsdk.RequiresReplace(), }, } \nIf defined, plan modifiers are applied to the current attribute. If any nested attributes define plan modifiers, then those are applied afterwards. Any plan modifiers that return an error will prevent Terraform from applying further modifiers of that attribute as well as any nested attribute plan modifiers.\nCommon Use Case Attribute Plan Modifiers\nThe framework implements some common use case modifiers:\ntfsdk.RequiresReplace(): If the value of the attribute changes, in-place update is not possible and instead the resource should be replaced for the change to occur. Refer to the Go documentation for full details on its behavior.\ntfsdk.RequiresReplaceIf(): Similar to tfsdk.RequiresReplace(), however it also accepts provider-defined conditional logic. Refer to the Go documentation for full details on its behavior.\ntfsdk.UseStateForUnknown(): Copies the prior state value, if not null. This is useful for reducing (known after apply) plan outputs for computed attributes which are known to not change over time.\nCreating Attribute Plan Modifiers\nTo create an attribute plan modifier, you must implement the tfsdk.AttributePlanModifier interface. For example:\n// stringDefaultModifier is a plan modifier that sets a default value for a // types.StringType attribute when it is not configured. The attribute must be // marked as Optional and Computed. When setting the state during the resource // Create, Read, or Update methods, this default value must also be included or // the Terraform CLI will generate an error. type stringDefaultModifier struct { Default string } // Description returns a plain text description of the validator's behavior, suitable for a practitioner to understand its impact. func (m stringDefaultModifier) Description(ctx context.Context) string { return fmt.Sprintf(\"If value is not configured, defaults to %s\", m.Default) } // MarkdownDescription returns a markdown formatted description of the validator's behavior, suitable for a practitioner to understand its impact. func (m stringDefaultModifier) MarkdownDescription(ctx context.Context) string { return fmt.Sprintf(\"If value is not configured, defaults to `%s`\", m.Default) } // Modify runs the logic of the plan modifier. // Access to the configuration, plan, and state is available in `req`, while // `resp` contains fields for updating the planned value, triggering resource // replacement, and returning diagnostics. func (m stringDefaultModifier) Modify(ctx context.Context, req tfsdk.ModifyAttributePlanRequest, resp *tfsdk.ModifyAttributePlanResponse) { // types.String must be the attr.Value produced by the attr.Type in the schema for this attribute // for generic plan modifiers, use // https://pkg.go.dev/github.com/hashicorp/terraform-plugin-framework/tfsdk#ConvertValue // to convert into a known type. var str types.String diags := tfsdk.ValueAs(ctx, req.AttributePlan, &str) resp.Diagnostics.Append(diags...) if diags.HasError() { return } if !str.Null { return } resp.AttributePlan = types.String{Value: m.Default} } \nOptionally, you may also want to create a helper function to instantiate the plan modifier. For example:\nfunc stringDefault(defaultValue string) stringDefaultModifier { return stringDefaultModifier{ Default: defaultValue, } } \nResources also support plan modification across all attributes. This is helpful when working with logic that applies to the resource as a whole, or in Terraform 1.3 and later, to return diagnostics during resource destruction. Implement the tfsdk.ResourceWithModifyPlan interface to support resource-level plan modification. For example:\n// Other methods to implement the tfsdk.Resource interface are omitted for brevity type exampleResource struct {} func (r exampleResource) ModifyPlan(ctx context.Context, req ModifyResourcePlanRequest, resp *ModifyResourcePlanResponse) { // Fill in logic. } \nEnsure the response plan remains entirely null when the request plan is entirely null due to resource destruction."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.8.x/resources/state-upgrade",
  "text": "Plugin Development - Framework: State Upgrade | Terraform\nA resource schema captures the structure and types of the resource state. Any state data that does not conform to the resource schema will generate errors or may not be persisted properly. Over time, it may be necessary for resources to make breaking changes to their schemas, such as changing an attribute type. Terraform supports versioning of these resource schemas and the current version is saved into the Terraform state. When the provider advertises a newer schema version, Terraform will call back to the provider to attempt to upgrade from the saved schema version to the one advertised. This operation is performed prior to planning, but with a configured provider.\nSome versions of Terraform CLI will also request state upgrades even when the current schema version matches the state version. The framework will automatically handle this request.\nWhen generating a plan, Terraform CLI will request the current resource schema, which contains a version.\nIf Terraform CLI detects that an existing state with its saved version does not match the current version, Terraform CLI will request a state upgrade from the provider with the prior state version and expecting the state to match the current version.\nThe framework will check the resource to see if it defines state upgrade support:\nIf no state upgrade support is defined, an error diagnostic is returned.\nIf state upgrade support is defined, but not for the requested prior state version, an error diagnostic is returned.\nIf state upgrade support is defined and has an implementation for the requested prior state version, the provider defined implementation is executed.\nEnsure the tfsdk.Schema type Version field for the tfsdk.ResourceType is greater than 0, then implement the tfsdk.ResourceWithStateUpgrade interface for the tfsdk.Resource. Conventionally the version is incremented by 1 for each state upgrade.\nThis example shows a ResourceType which incremented the Schema type Version field:\n// Other ResourceType methods are omitted in this example var _ tfsdk.ResourceType = exampleResourceType{} type exampleResourceType struct{/* ... */} func (t exampleResourceType) GetSchema(_ context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ // ... other fields ... // This example conventionally declares that the resource has prior // state versions of 0 and 1, while the current version is 2. Version: 2, } } \nThis example shows a Resource with the necessary StateUpgrade method to implement the ResourceWithStateUpgrade interface:\n// Other Resource methods are omitted in this example var _ tfsdk.Resource = exampleResource{} var _ tfsdk.ResourceWithUpgradeState = exampleResource{} type exampleResource struct{/* ... */} func (r exampleResource) UpgradeState(ctx context.Context) map[int64]tfsdk.ResourceStateUpgrader { return map[int64]tfsdk.ResourceStateUpgrader{ // State upgrade implementation from 0 (prior state version) to 2 (Schema.Version) 0: { // Optionally, the PriorSchema field can be defined. StateUpgrader: func(ctx context.Context, req tfsdk.UpgradeResourceStateRequest, resp *tfsdk.UpgradeResourceStateResponse) { /* ... */ }, }, // State upgrade implementation from 1 (prior state version) to 2 (Schema.Version) 1: { // Optionally, the PriorSchema field can be defined. StateUpgrader: func(ctx context.Context, req tfsdk.UpgradeResourceStateRequest, resp *tfsdk.UpgradeResourceStateResponse) { /* ... */ }, }, } } \nEach tfsdk.ResourceStateUpgrader implementation is expected to wholly upgrade the resource state from the prior version to the current version. The framework does not iterate through intermediate version implementations as incrementing versions by 1 is only conventional and not required.\nAll state data must be populated in the tfsdk.UpgradeResourceStateResponse. The framework does not copy any prior state data from the tfsdk.UpgradeResourceStateRequest.\nThere are two approaches to implementing the provider logic for state upgrades in ResourceStateUpgrader. The recommended approach is defining the prior schema matching the resource state, which allows for prior state access similar to other parts of the framework. The second, more advanced, approach is accessing the prior resource state using lower level data handlers.\nResourceStateUpgrader With PriorSchema\nImplement the ResourceStateUpgrader type PriorSchema field to enable the framework to populate the tfsdk.UpgradeResourceStateRequest type State field for the provider defined state upgrade logic. Access the request State using methods such as Get() or GetAttribute(). Write the tfsdk.UpgradeResourceStateResponse type State field using methods such as Set() or SetAttribute().\nThis example shows a resource that changes the type for two attributes, using the PriorSchema approach:\n// Other ResourceType methods are omitted in this example var _ tfsdk.ResourceType = exampleResourceType{} // Other Resource methods are omitted in this example var _ tfsdk.Resource = exampleResource{} var _ tfsdk.ResourceWithUpgradeState = exampleResource{} type exampleResourceType struct{/* ... */} type exampleResource struct{/* ... */} type exampleResourceDataV0 struct { Id string `tfsdk:\"id\"` OptionalAttribute *bool `tfsdk:\"optional_attribute\"` RequiredAttribute bool `tfsdk:\"required_attribute\"` } type exampleResourceDataV1 struct { Id string `tfsdk:\"id\"` OptionalAttribute *string `tfsdk:\"optional_attribute\"` RequiredAttribute string `tfsdk:\"required_attribute\"` } func (t exampleResourceType) GetSchema(_ context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ Attributes: map[string]Attribute{ \"id\": { Type: types.StringType, Computed: true, }, \"optional_attribute\": { Type: types.StringType, // As compared to prior types.BoolType below Optional: true, }, \"required_attribute\": { Type: types.StringType, // As compared to prior types.BoolType below Required: true, }, }, // The resource has a prior state version of 0, which had the attribute // types of types.BoolType as shown below. Version: 1, } } func (r exampleResource) UpgradeState(ctx context.Context) map[int64]tfsdk.ResourceStateUpgrader { return map[int64]tfsdk.ResourceStateUpgrader{ // State upgrade implementation from 0 (prior state version) to 1 (Schema.Version) 0: { PriorSchema: &tfsdk.Schema{ Attributes: map[string]Attribute{ \"id\": { Type: types.StringType, Computed: true, }, \"optional_attribute\": { Type: types.BoolType, // As compared to current types.StringType above Optional: true, }, \"required_attribute\": { Type: types.BoolType, // As compared to current types.StringType above Required: true, }, }, }, StateUpgrader: func(ctx context.Context, req tfsdk.UpgradeResourceStateRequest, resp *tfsdk.UpgradeResourceStateResponse) { var priorStateData exampleResourceDataV0 resp.Diagnostics.Append(req.State.Get(ctx, &priorStateData)...) if resp.Diagnostics.HasError() { return } upgradedStateData := exampleResourceDataV1{ Id: priorStateData.Id, RequiredAttribute: fmt.Sprintf(\"%t\", priorStateData.RequiredAttribute), } if priorStateData.OptionalAttribute != nil { v := fmt.Sprintf(\"%t\", *priorStateData.OptionalAttribute) upgradedStateData.OptionalAttribute = &v } resp.Diagnostics.Append(resp.State.Set(ctx, upgradedStateData)...) }, }, } } \nResourceStateUpgrader Without PriorSchema\nRead prior state data from the tfsdk.UpgradeResourceStateRequest type RawState field. Write the tfsdk.UpgradeResourceStateResponse type State field using methods such as Set() or SetAttribute(), or for more advanced use cases, write the tfsdk.UpgradeResourceStateResponse type DynamicValue field.\nThis example shows a resource that changes the type for two attributes, using the RawState approach for the request and DynamicValue approach for the response:\n// Other ResourceType methods are omitted in this example var _ tfsdk.ResourceType = exampleResourceType{} // Other Resource methods are omitted in this example var _ tfsdk.Resource = exampleResource{} var _ tfsdk.ResourceWithUpgradeState = exampleResource{} var exampleResourceTftypesDataV0 = tftypes.Object{ AttributeTypes: map[string]tftypes.Type{ \"id\": tftypes.String, \"optional_attribute\": tftypes.Bool, \"required_attribute\": tftypes.Bool, }, } var exampleResourceTftypesDataV1 = tftypes.Object{ AttributeTypes: map[string]tftypes.Type{ \"id\": tftypes.String, \"optional_attribute\": tftypes.String, \"required_attribute\": tftypes.String, }, } type exampleResourceType struct{/* ... */} type exampleResource struct{/* ... */} func (t exampleResourceType) GetSchema(_ context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ Attributes: map[string]Attribute{ \"id\": { Type: types.StringType, Computed: true, }, \"optional_attribute\": { Type: types.StringType, // As compared to prior types.BoolType below Optional: true, }, \"required_attribute\": { Type: types.StringType, // As compared to prior types.BoolType below Required: true, }, }, // The resource has a prior state version of 0, which had the attribute // types of types.BoolType as shown below. Version: 1, } } func (r exampleResource) UpgradeState(ctx context.Context) map[int64]tfsdk.ResourceStateUpgrader { return map[int64]tfsdk.ResourceStateUpgrader{ // State upgrade implementation from 0 (prior state version) to 1 (Schema.Version) 0: { StateUpgrader: func(ctx context.Context, req tfsdk.UpgradeResourceStateRequest, resp *tfsdk.UpgradeResourceStateResponse) { // Refer also to the RawState type JSON field which can be used // with json.Unmarshal() rawStateValue, err := req.RawState.Unmarshal(exampleResourceTftypesDataV0) if err != nil { resp.Diagnostics.AddError( \"Unable to Unmarshal Prior State\", err.Error(), ) return } var rawState map[string]tftypes.Value if err := rawStateValue.As(&rawState); err != nil { resp.Diagnostics.AddError( \"Unable to Convert Prior State\", err.Error(), ) return } var optionalAttributeString *string if !rawState[\"optional_attribute\"].IsNull() { var optionalAttribute bool if err := rawState[\"optional_attribute\"].As(&optionalAttribute); err != nil { resp.Diagnostics.AddAttributeError( tftypes.NewAttributePath().WithAttributeName(\"optional_attribute\"), \"Unable to Convert Prior State\", err.Error(), ) return } v := fmt.Sprintf(\"%t\", optionalAttribute) optionalAttributeString = &v } var requiredAttribute bool if err := rawState[\"required_attribute\"].As(&requiredAttribute); err != nil { resp.Diagnostics.AddAttributeError( tftypes.NewAttributePath().WithAttributeName(\"required_attribute\"), \"Unable to Convert Prior State\", err.Error(), ) return } dynamicValue, err := tfprotov6.NewDynamicValue( exampleResourceTftypesDataV1, tftypes.NewValue(exampleResourceTftypesDataV1, map[string]tftypes.Value{ \"id\": rawState[\"id\"], \"optional_attribute\": tftypes.NewValue(tftypes.String, optionalAttributeString), \"required_attribute\": tftypes.NewValue(tftypes.String, fmt.Sprintf(\"%t\", requiredAttribute)), }), ) if err != nil { resp.Diagnostics.AddError( \"Unable to Convert Upgraded State\", err.Error(), ) return } resp.DynamicValue = &dynamicValue }, }, } }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.8.x/resources/plan-modification",
  "text": "Terraform and the framework support two types of plan modification on resources:\n// Typically within the tfsdk.Schema returned by GetSchema() for a resource. tfsdk.Attribute{ // ... other Attribute configuration ... PlanModifiers: []AttributePlanModifiers{ tfsdk.RequiresReplace(), }, } \ntfsdk.RequiresReplace(): If the value of the attribute changes, in-place update is not possible and instead the resource should be replaced for the change to occur. Refer to the Go documentation for full details on its behavior.\ntfsdk.RequiresReplaceIf(): Similar to tfsdk.RequiresReplace(), however it also accepts provider-defined conditional logic. Refer to the Go documentation for full details on its behavior.\ntfsdk.UseStateForUnknown(): Copies the prior state value, if not null. This is useful for reducing (known after apply) plan outputs for computed attributes which are known to not change over time.\nResource schemas also support plan modification across all attributes. This is helpful when working with logic that applies to the resource as a whole. To create a resource schema plan modification, you must implement the tfsdk.ResourceWithModifyPlan interface. For example:\n// Other methods to implement the tfsdk.Resource interface are omitted for brevity type exampleResource struct {} func (r exampleResource) ModifyPlan(ctx context.Context, req ModifyResourcePlanRequest, resp *ModifyResourcePlanResponse) { // Fill in logic. }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.8.x/resources/import",
  "text": "Practitioners can use the terraform import command to let Terraform begin managing existing infrastructure resources. Resources can implement the ImportState method, which must either specify enough Terraform state for the Read method to refresh tfsdk.Resource or return an error.\nWhen the Read method requires a single attribute to refresh, use the tfsdk.ResourceImportStatePassthroughID function to write the import identifier argument for terraform import.\nIn the following example, the terraform import command passes the import identifier to the id attribute in Terraform state.\nfunc (r exampleResource) ImportState(ctx context.Context, req tfsdk.ImportResourceStateRequest, resp *tfsdk.ImportResourceStateResponse) { tfsdk.ResourceImportStatePassthroughID(ctx, tftypes.NewAttributePath().WithAttributeName(\"id\"), req, resp) } \nWhen the Read method requires multiple attributes to refresh, you must write custom logic in the ImportState method. Specifically, the implementation must:\nUse the import identifier from the tfsdk.ImportResourceStateRequest. \nPerform the custom logic.\nSet state data in the tfsdk.ImportResourceStateResponse.\nFor example, if the tfsdk.ResourceType implementation has the following GetSchema method:\nfunc (t exampleResourceType) GetSchema(ctx context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ Attributes: map[string]tfsdk.Attribute{ \"attr_one\": { Type: types.StringType, Required: true, }, \"attr_two\": { Type: types.StringType, Required: true, }, // ... potentially other Attributes ... }, }, nil } \nAlong with a tfsdk.Resource implementation with the following Read method:\nfunc (r exampleResource) Read(ctx context.Context, req tfsdk.ReadResourceRequest, resp *tfsdk.ReadResourceResponse) { var attrOne, attrTwo string resp.Diagnostics.Append(req.State.GetAttribute(ctx, tftypes.NewAttributePath().WithAttributeName(\"attr_one\"), &attrOne)...) resp.Diagnostics.Append(req.State.GetAttribute(ctx, tftypes.NewAttributePath().WithAttributeName(\"attr_two\"), &attrTwo)...) if resp.Diagnostics.HasError() { return } // API call using attrOne and attrTwo } \nThe terraform import command will need to accept both attribute values as a single import identifier string. A typical convention is to use a separator character, such as a comma (,), between the values. The ImportState method will then need to parse the import identifier string into the two separate values and save them appropriately into the Terraform state.\nYou could define the ImportState method using a comma-separated value as follows:\nfunc (r exampleResource) ImportState(ctx context.Context, req tfsdk.ImportResourceStateRequest, resp *tfsdk.ImportResourceStateResponse) { idParts := strings.Split(req.ID, \",\") if len(idParts) != 2 || idParts[0] == \"\" || idParts[1] == \"\" { resp.Diagnostics.AddError( \"Unexpected Import Identifier\", fmt.Sprintf(\"Expected import identifier with format: attr_one,attr_two. Got: %q\", req.ID), ) return } resp.Diagnostics.Append(resp.State.SetAttribute(ctx, tftypes.NewAttributePath().WithAttributeName(\"attr_one\"), idParts[0])...) resp.Diagnostics.Append(resp.State.SetAttribute(ctx, tftypes.NewAttributePath().WithAttributeName(\"attr_two\"), idParts[1])...) } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.1.x/handling-data/accessing-values",
  "text": "Plugin Development - Framework: Access State, Config, and Plan | Terraform\nAccessing State, Config, and Plan\nThere are various points at which the provider needs access to the data from the practitioner's configuration, Terraform's state, or generated plan. The same patterns are used for accessing this data, regardless of its source.\nThe data is usually stored in a request object:\nfunc (r ThingResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) \nIn this example, req holds the configuration and plan, and there is no state value because the resource does not yet exist in state.\nOne way to interact with configuration, plan, and state values is to convert the entire configuration, plan, or state into a Go type, then treat them as regular Go values. This has the benefit of letting the compiler check all your code that accesses values, but requires defining a type to contain the values.\nUse the Get method to retrieve the first level of configuration, plan, and state data.\ntype ThingResourceModel struct { Address types.Object `tfsdk:\"address\"` Age types.Int64 `tfsdk:\"age\"` Name types.String `tfsdk:\"name\"` Pets types.List `tfsdk:\"pets\"` Registered types.Bool `tfsdk:\"registered\"` Tags types.Map `tfsdk:\"tags\"` } func (r ThingResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var plan ThingResourceModel diags := req.Plan.Get(ctx, &plan) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } // values can now be accessed like plan.Name.ValueString() // check if things are null with plan.Name.IsNull() // check if things are unknown with plan.Name.IsUnknown() } \nThe configuration, plan, and state data is represented as an object, and accessed like an object. Refer to the conversion rules for an explanation on how objects can be converted into Go types.\nTo descend into deeper nested data structures, the types.List, types.Map, and types.Set types each have an ElementsAs() method. The types.Object type has an As() method.\nUse the GetAttribute method to retrieve a top level attribute or block value from the configuration, plan, and state.\nfunc (r ThingResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { var name types.String diags := req.State.GetAttribute(ctx, path.Root(\"name\"), &name) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } // ... } \nA lot of conversion rules say an error will be returned if a value is unknown or null. It is safe to assume:\nRequired attributes will never be null or unknown in Create, Read, Update, or Delete methods.\nOptional attributes that are not computed will never be unknown in Create, Read, Update, or Delete methods.\nComputed attributes, whether optional or not, will never be null in the plan for Create, Read, Update, or Delete methods.\nComputed attributes that are read-only (Optional is not true) will always be unknown in the plan for Create, Read, Update, or Delete methods. They will always be null in the configuration for Create, Read, Update, and Delete methods.\nRequired attributes will never be null in a provider's Configure method. They may be unknown.\nThe state never contains unknown values.\nThe configuration for Create, Read, Update, and Delete methods never contains unknown values.\nIn any other circumstances, the provider is responsible for handling the possibility that an unknown or null value may be presented to it.\nRefer to the conversion rules for more information about supported Go types."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.7.x/handling-data/accessing-values",
  "text": "Plugin Development - Framework: Access State, Config, and Plan | Terraform\nAccessing State, Config, and Plan\nThere are various points at which the provider needs access to the data from the practitioner's configuration, Terraform's state, or generated plan. The same patterns are used for accessing this data, regardless of its source.\nThe data is usually stored in a request object:\nfunc (r ThingResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) \nIn this example, req holds the configuration and plan, and there is no state value because the resource does not yet exist in state.\nOne way to interact with configuration, plan, and state values is to convert the entire configuration, plan, or state into a Go type, then treat them as regular Go values. This has the benefit of letting the compiler check all your code that accesses values, but requires defining a type to contain the values.\nUse the Get method to retrieve the first level of configuration, plan, and state data.\ntype ThingResourceModel struct { Address types.Object `tfsdk:\"address\"` Age types.Int64 `tfsdk:\"age\"` Name types.String `tfsdk:\"name\"` Pets types.List `tfsdk:\"pets\"` Registered types.Bool `tfsdk:\"registered\"` Tags types.Map `tfsdk:\"tags\"` } func (r ThingResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var plan ThingResourceModel diags := req.Plan.Get(ctx, &plan) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } // values can now be accessed like plan.Name.ValueString() // check if things are null with plan.Name.IsNull() // check if things are unknown with plan.Name.IsUnknown() } \nThe configuration, plan, and state data is represented as an object, and accessed like an object. Refer to the object type documentation for an explanation on how objects can be converted into Go types.\nTo descend into deeper nested data structures, the types.List, types.Map, and types.Set types each have an ElementsAs() method. The types.Object type has an As() method.\nUse the GetAttribute method to retrieve a top level attribute or block value from the configuration, plan, and state.\nfunc (r ThingResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { var name types.String diags := req.State.GetAttribute(ctx, path.Root(\"name\"), &name) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } // ... } \nA lot of conversion rules say an error will be returned if a value is unknown or null. It is safe to assume:\nRequired attributes will never be null or unknown in Create, Read, Update, or Delete methods.\nOptional attributes that are not computed will never be unknown in Create, Read, Update, or Delete methods.\nComputed attributes, whether optional or not, will never be null in the plan for Create, Read, Update, or Delete methods.\nComputed attributes that are read-only (Optional is not true) will always be unknown in the plan for Create, Read, Update, or Delete methods. They will always be null in the configuration for Create, Read, Update, and Delete methods.\nRequired attributes will never be null in a provider's Configure method. They may be unknown.\nThe state never contains unknown values.\nThe configuration for Create, Read, Update, and Delete methods never contains unknown values.\nIn any other circumstances, the provider is responsible for handling the possibility that an unknown or null value may be presented to it."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.0.x/handling-data/accessing-values",
  "text": "Plugin Development - Framework: Access State, Config, and Plan | Terraform\nAccessing State, Config, and Plan\nThere are various points at which the provider needs access to the data from the practitioner's configuration, Terraform's state, or generated plan. The same patterns are used for accessing this data, regardless of its source.\nThe data is usually stored in a request object:\nfunc (r ThingResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) \nIn this example, req holds the configuration and plan, and there is no state value because the resource does not yet exist in state.\nOne way to interact with configuration, plan, and state values is to convert the entire configuration, plan, or state into a Go type, then treat them as regular Go values. This has the benefit of letting the compiler check all your code that accesses values, but requires defining a type to contain the values.\nUse the Get method to retrieve the first level of configuration, plan, and state data.\ntype ThingResourceModel struct { Address types.Object `tfsdk:\"address\"` Age types.Int64 `tfsdk:\"age\"` Name types.String `tfsdk:\"name\"` Pets types.List `tfsdk:\"pets\"` Registered types.Bool `tfsdk:\"registered\"` Tags types.Map `tfsdk:\"tags\"` } func (r ThingResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var plan ThingResourceModel diags := req.Plan.Get(ctx, &plan) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } // values can now be accessed like plan.Name.ValueString() // check if things are null with plan.Name.IsNull() // check if things are unknown with plan.Name.IsUnknown() } \nThe configuration, plan, and state data is represented as an object, and accessed like an object. Refer to the conversion rules for an explanation on how objects can be converted into Go types.\nTo descend into deeper nested data structures, the types.List, types.Map, and types.Set types each have an ElementsAs() method. The types.Object type has an As() method.\nUse the GetAttribute method to retrieve a top level attribute or block value from the configuration, plan, and state.\nfunc (r ThingResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { var name types.String diags := req.State.GetAttribute(ctx, path.Root(\"name\"), &name) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } // ... } \nA lot of conversion rules say an error will be returned if a value is unknown or null. It is safe to assume:\nRequired attributes will never be null or unknown in Create, Read, Update, or Delete methods.\nOptional attributes that are not computed will never be unknown in Create, Read, Update, or Delete methods.\nComputed attributes, whether optional or not, will never be null in the plan for Create, Read, Update, or Delete methods.\nComputed attributes that are read-only (Optional is not true) will always be unknown in the plan for Create, Read, Update, or Delete methods. They will always be null in the configuration for Create, Read, Update, and Delete methods.\nRequired attributes will never be null in a provider's Configure method. They may be unknown.\nThe state never contains unknown values.\nThe configuration for Create, Read, Update, and Delete methods never contains unknown values.\nIn any other circumstances, the provider is responsible for handling the possibility that an unknown or null value may be presented to it.\nRefer to the conversion rules for more information about supported Go types."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.7.x/handling-data/terraform-concepts",
  "text": "Plugin Development - Framework: Handling Data - Terraform Concepts | Terraform\nThis page describes Terraform concepts as they relate to handling data within framework-based provider code. The What is Terraform, Terraform language, and Plugin Development documentation covers more general concepts behind Terraform's workflow, its configuration, and how it interacts with providers.\nSchemas specify the data structure and types of a provider, resource, or data source that is exposed to Terraform. This includes the configuration written by practitioners, any planning data, and the state stored by Terraform which can be referenced in other configuration. Providers, resources, and data sources have their own concept-specific types and available functionality.\nEach part of the data within a schema is defined as either an attribute or block. In general, attributes set values and blocks are containers for other attributes and blocks. Each have differing configuration syntax and behaviors.\nDiscover more about framework implementations of this concept in the schema documentation.\nAttributes\nAttributes set values in a schema, such as a string or list. The Terraform type system documentation provides a general overview of the available kinds of data within Terraform, which is similar with type system concepts available in many programming languages. However, there are subtle differences with values discussed in the type system section.\nDiscover more about the framework implementations of this concept in the attribute documentation.\nBlocks\nTip\nUse nested attributes for new schema implementations. Block support is mainly for migrating prior SDK-based providers.\nA block is a container for other attributes and blocks. Terraform implements many top level blocks, such as provider and resource, while a schema supports nested blocks.\nDiscover more about the framework implementations of this concept in the block documentation.\nSchema Based Data\nSchemas provide the structures and types for representing data with Terraform. This section discusses the concepts behind the different types of data influenced by a schema.\nConfiguration\nAs part of the Terraform workflow, a practitioner writes configuration files, which are parsed into a graph of operations by Terraform commands. The structures and types of that configuration is the most visible aspect of a schema, since a schema has a direct effect on the expected configuration syntax.\nIn Terraform operations where the configuration data is available to providers, the framework typically represents this as a Config field in the request type.\nPlan\nTip\nOnly managed resources implement this data concept.\nAs part of the Terraform workflow, any configuration changes are planned before they are applied into state so practitioners have an opportunity to review whether those anticipated changes have their intended effect. Conceptually, this data represents the merging of configuration data and any prior state data. Terraform refers to this data as the proposed new state, while the framework more generally refers to this data as the plan.\nPlan data requires careful handling to prevent unexpected Terraform errors for practitioners. The framework implements various schema concepts and logic discussed in the resource plan modification documentation. In-depth documentation about Terraform requirements is available in the Terraform Resource Instance Change Lifecycle documentation.\nIn Terraform operations where the plan data is available to providers, the framework typically represents this as a Plan field in the request or response type.\nState\nTip\nOnly managed resources and data resources implement this data concept.\nAs part of the Terraform workflow, any data that should be stored for configuration references or future Terraform executions must be written to the state. This data must exactly match any configuration data, and if applicable, any plan data with unknown values converted to known values.\nIn Terraform operations where the plan data is available to providers, the framework typically represents this as a State field in the request or response type.\nThe Terraform type system supports deeper functionality than the standard type systems built into programming languages. While the types in the Terraform type system are similar to what is found in most languages, values have extra metadata associated with them. The framework implements its own types to prevent the loss of this additional metadata that cannot be represented by Go built-in types.\nImportant value metadata concepts when implementing a provider include:\nNull values: Absence of a value.\nUnknown values: Value that is not yet known.\nAs Terraform exposes additional value metadata to providers, the framework type system will be updated. Therefore, it is always recommended to use the framework type system to ensure all Terraform value functionality is available in provider code.\nNull Values\nNull represents the absence of a Terraform value. It is usually encountered with optional attributes that the practitioner neglected to specify a value for, but can show up on any non-required attribute. Required attributes can never be null.\nUnknown Values\nUnknown represents a Terraform value that is not yet known. Terraform uses a graph of providers, resources, and data sources to do things in the right order, and when a provider, resource, or data source relies on a value from another provider, resource, or data source that has not been resolved yet, it represents that state by using the unknown value. For example:\nresource \"example_foo\" \"bar\" { hello = \"world\" demo = true } resource \"example_baz\" \"quux\" { foo_id = example_foo.bar.id } \nIn the example above, example_baz.quux is relying on the id attribute of example_foo.bar. The id attribute of example_foo.bar isn't known until after the apply. The plan would list it as (known after apply). During the plan phase, example_baz.quux would get an unknown value as the value for foo_id.\nBecause they can result from interpolations in the practitioner's config, you have no control over what attributes may contain an unknown value. However, by the time a resource is expected to be created, read, updated, or deleted, only its computed attributes can be unknown. The rest are guaranteed to have known values (or be null).\nProvider configuration values can be unknown, and providers should handle that situation, even if that means just returning an error."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.1.x/handling-data/terraform-concepts",
  "text": "Plugin Development - Framework: Handling Data - Terraform Concepts | Terraform\nSchemas specify the fields that a provider, resource or data source configuration can have.\nThe fields defined within the schema are either attributes or blocks.\nAttributes and blocks within Terraform configuration have different syntax, either using, or omitting an equals sign in their definition, respectively.\nProviders, resources and data sources have their own type-specific schema, which use type-specific attributes and blocks.\nThe following examples use a resource but the same schema attribute and block types are also defined with the schema for providers and data sources. The examples focus on type-related information, the Optional field is included just to provide working examples but refer to Schema for information about the other fields that can be defined within attributes and blocks.\nAttributes are used to set values.\nAttribute TypeDescription\nBoolAttribute\tBoolean values (i.e., true/false)\t\nFloat64Attribute\t64 bit floating point number values\t\nInt64Attribute\t64 bit integer number values\t\nNumberAttribute\tGeneric number with up to 512 bits of floating point or integer precision\t\nStringAttribute\tString values\t\nListAttribute\tList with a single element type (e.g., types.StringType)\t\nMapAttribute\tMap with a single element type (e.g., types.Int64Type)\t\nSetAttribute\tSet with a single element type (e.g., types.BoolType)\t\nObjectAttribute\tObject with only type information for underlying attributes\t\nListNestedAttribute\tList containing nested objects where the object attributes can be fully defined\t\nMapNestedAttribute\tMap containing nested objects where the object attributes can be fully defined\t\nSetNestedAttribute\tSet containing nested objects where the object attributes can be fully defined\t\nSingleNestedAttribute\tSingle object where the object attributes can be fully defined\t\nRefer to Attributes - Terraform Configuration and Schema for examples of Terraform configuration and schema that illustrate the usage of the various types of schema attributes.\nThe Terraform language uses a block as a container for other attributes and blocks. Terraform implements many top level blocks, such as provider and resource, while providers can implement nested blocks in their schema to enable practitioners to configure data.\nUse nested attributes for new schema implementations. Block support is mainly for migrating prior SDK-based providers.\nThe available nesting modes are:\nList: Ordered collection of objects\nSet: Unordered collection of objects\nSingle: One object\nRefer to Blocks - Terraform Configuration for examples of Terraform configuration and schema that illustrate the usage of nested blocks."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.0.x/handling-data/terraform-concepts",
  "text": "Plugin Development - Framework: Handling Data - Terraform Concepts | Terraform\nSchemas specify the fields that a provider, resource or data source configuration can have.\nThe fields defined within the schema are either attributes or blocks.\nAttributes and blocks within Terraform configuration have different syntax, either using, or omitting an equals sign in their definition, respectively.\nProviders, resources and data sources have their own type-specific schema, which use type-specific attributes and blocks.\nThe following examples use a resource but the same schema attribute and block types are also defined with the schema for providers and data sources. The examples focus on type-related information, the Optional field is included just to provide working examples but refer to Schema for information about the other fields that can be defined within attributes and blocks.\nAttributes are used to set values.\nAttribute TypeDescription\nBoolAttribute\tBoolean values (i.e., true/false)\t\nFloat64Attribute\t64 bit floating point number values\t\nInt64Attribute\t64 bit integer number values\t\nNumberAttribute\tGeneric number with up to 512 bits of floating point or integer precision\t\nStringAttribute\tString values\t\nListAttribute\tList with a single element type (e.g., types.StringType)\t\nMapAttribute\tMap with a single element type (e.g., types.Int64Type)\t\nSetAttribute\tSet with a single element type (e.g., types.BoolType)\t\nObjectAttribute\tObject with only type information for underlying attributes\t\nListNestedAttribute\tList containing nested objects where the object attributes can be fully defined\t\nMapNestedAttribute\tMap containing nested objects where the object attributes can be fully defined\t\nSetNestedAttribute\tSet containing nested objects where the object attributes can be fully defined\t\nSingleNestedAttribute\tSingle object where the object attributes can be fully defined\t\nRefer to Attributes - Terraform Configuration and Schema for examples of Terraform configuration and schema that illustrate the usage of the various types of schema attributes.\nThe Terraform language uses a block as a container for other attributes and blocks. Terraform implements many top level blocks, such as provider and resource, while providers can implement nested blocks in their schema to enable practitioners to configure data.\nUse nested attributes for new schema implementations. Block support is mainly for migrating prior SDK-based providers.\nThe available nesting modes are:\nList: Ordered collection of objects\nSet: Unordered collection of objects\nSingle: One object\nRefer to Blocks - Terraform Configuration for examples of Terraform configuration and schema that illustrate the usage of nested blocks."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.7.x/handling-data/dynamic-data",
  "text": "Plugin Development - Framework: Handling Data - Dynamic Data | Terraform\nNote\nStatic types should always be preferred over dynamic types, when possible.\nDynamic data handling uses the framework dynamic type to communicate to Terraform that the value type of a specific field will be determined at runtime. This allows a provider developer to handle multiple value types of data with a single attribute, parameter, or return.\nDynamic data can be defined with:\nDynamic attribute\nA dynamic attribute type in an object attribute\nDynamic function parameter\nDynamic function return\nA dynamic attribute type in an object parameter\nA dynamic attribute type in an object return\nUsing dynamic data has a negative impact on practitioner experience when using Terraform and downstream tooling, like practitioner configuration editor integrations. Dynamics do not change how Terraform's static type system behaves and all data consistency rules are applied the same as static types. Provider developers should understand all the below considerations when creating a provider with a dynamic type.\nOnly use a dynamic type when there is not a suitable static type alternative.\nWhen dynamic data is used, Terraform will no longer have any static information about the value types expected for a given attribute, function parameter, or function return. This results in behaviors that the provider developer will need to account for with additional documentation, code, error messaging, etc.\nDownstream Tooling\nPractitioner configuration editor integrations, like the Terraform VSCode extension and language server, cannot provide any static information when using dynamic data in configurations. This can result in practitioners using dynamic data in expressions (like for) incorrectly that will only error at runtime.\nGiven this example, a resource schema defines a top level computed dynamic attribute named example_attribute:\nfunc (r ThingResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"example_attribute\": schema.DynamicAttribute{ Computed: true, // ... potentially other fields ... }, // ... potentially other attributes ... }, } } \nThe configuration below would be valid until a practitioner runs an apply. If the type of example_attribute is not iterable, then the practitioner will receive an error only when they run a command:\nresource \"examplecloud_thing\" \"example\" {} output \"dynamic_output\" { value = [for val in examplecloud_thing.example.example_attribute : val] } \nResults in the following error:\n Error: Iteration over non-iterable value   on resource.tf line 15, in output \"dynamic_output\":  15: value = [for val in examplecloud_thing.example.example_attribute : val]     examplecloud_thing.example.example_attribute is \"string value\"   A value of type string cannot be used as the collection in a 'for' expression. \nDynamic data that is meant for practitioners to utilize in configurations should document all potential output types and expected usage to avoid confusing errors.\nHandling All Possible Types\nTerraform will not automatically convert values to conform to a static type, exposing provider developers to the Terraform type system directly. Provider developers will need to deal with this lack of type conversion by writing logic that handles every possible type that Terraform supports.\nIn this example, a resource schema defines a top level required dynamic attribute named example_attribute:\nfunc (r ThingResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"example_attribute\": schema.DynamicAttribute{ Required: true, // ... potentially other fields ... }, // ... potentially other attributes ... }, } } \nAn example of handling every possible Terraform type that could be provided to a configuration would be:\n// Example data model definition // type ExampleModel struct { // ExampleAttribute types.Dynamic `tfsdk:\"example_attribute\"` // } switch value := data.ExampleAttribute.UnderlyingValue().(type) { case types.Bool: // Handle boolean value case types.Number: // Handle float64, int64, and number values case types.List: // Handle list value case types.Map: // Handle map value case types.Object: // Handle object value case types.Set: // Handle set value case types.String: // Handle string value case types.Tuple: // Handle tuple value } \nWhen writing test configurations and debugging provider issues, developers will also want to understand how Terraform represents complex type literals. For example, Terraform does not provide any way to directly represent lists, maps, or sets.\nHandling Underlying Null and Unknown Values\nWith dynamic data, in addition to typical null and unknown value handling, provider developers will need to implement additional logic to determine if an underlying value for a dynamic is null or unknown.\nUnderlying Null\nIn the configuration below, Terraform knows the underlying value type, string, but the underlying string value is null:\nresource \"examplecloud_thing\" \"example\" { example_attribute = var.null_string } variable \"null_string\" { type = string default = null } \nThis will result in a known dynamic value, with an underlying value that is a null string type. This can be detected utilizing the (types.Dynamic).IsUnderlyingValueNull() method. An equivalent framework value to this scenario would be:\ndynValWithNullString := types.DynamicValue(types.StringNull()) \nUnderlying Unknown\nIn the configuration below, Terraform knows the underlying value type of random_shuffle.result, a list(string), but the underlying list value is unknown:\nresource \"random_shuffle\" \"example\" { input = [\"one\", \"two\"] result_count = 2 } resource \"examplecloud_thing\" \"this\" { example_attribute = random_shuffle.example.result } \nThis will result in a known dynamic value, with an underlying value that is an unknown list of string types. This can be detected utilizing the (types.Dynamic).IsUnderlyingValueUnknown() method. An equivalent framework value to this scenario would be:\ndynValWithUnknownList := types.DynamicValue(types.ListUnknown(types.StringType)) \nUnderstanding Type Consistency\nFor managed resources, Terraform core implements data consistency rules between configuration, plan, and state data. With dynamic attributes, these consistency rules are also applied to the type of data.\nFor example, given a dynamic example_attribute that is computed and optional:\nfunc (r ThingResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"example_attribute\": schema.DynamicAttribute{ Computed: true, Optional: true, // ... potentially other fields ... }, // ... potentially other attributes ... }, } } \nIf a practitioner configures this resource as:\nresource \"examplecloud_thing\" \"example\" { # This literal expression is a tuple[string, string] example_attribute = [\"one\", \"two\"] } \nThen the exact type must be planned and stored in state during apply as a tuple with two string element types. If provider code attempts to store this attribute as a different type, like a list of strings, even with the same data values, Terraform will produce an error during apply:\n Error: Provider produced inconsistent result after apply   When applying changes to examplecloud_thing.example, provider \"provider[\\\"TYPE\\\"]\" produced an unexpected new value: .example_attribute: wrong final value type: tuple required.   This is a bug in the provider, which should be reported in the providers own issue tracker. \nIf a practitioner configures this same resource as:\nresource \"examplecloud_thing\" \"example\" { example_attribute = tolist([\"one\", \"two\"]) } \nThen the exact type must be planned and stored in state during apply as a list of strings."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/functions/parameters/dynamic",
  "text": "Plugin Development - Framework: Dynamic Function Parameter | Terraform\nStatic types should always be preferred over dynamic types, when possible.\nDevelopers creating a function with a dynamic parameter will need to have extensive knowledge of the Terraform type system, as no type conversion will be performed to incoming argument data.\nRefer to Dynamic Data - Considerations for more information.\nDynamic function parameters can receive any value type from a practitioner configuration. Values are accessible in function logic by the framework dynamic type.\nIn this Terraform configuration example, a dynamic parameter is set to the boolean value true:\nprovider::example::example(true) \nIn this example, the same dynamic parameter is set to a tuple (not a list) of string values one and two:\nprovider::example::example([\"one\", \"two\"]) \nIn this example, the same dynamic parameter is set to an object type with mapped values of attr1 to \"value1\" and attr2 to 123:\nprovider::example::example({ attr1 = \"value1\", attr2 = 123, }) \nUse the function.DynamicParameter type in the function definition to accept a dynamic value.\nIn this example, a function definition includes a first position dynamic parameter:\nfunc (f ExampleFunction) Definition(ctx context.Context, req function.DefinitionRequest, resp *function.DefinitionResponse) { resp.Definition = function.Definition{ // ... other Definition fields ... Parameters: []function.Parameter{ function.DynamicParameter{ Name: \"dynamic_param\", // ... potentially other DynamicParameter fields ... }, }, } } \nDynamic values are not supported as the element type of a collection type or within collection parameter types.\nIf the dynamic value should be a value type of an object parameter type, set the AttributeTypes map value according to the framework dynamic type. Refer to the object parameter type documentation for additional details.\nAllow Null Values\nA known dynamic value with an underlying value that contains nulls (such as a list with null element values) will always be sent to the function logic, regardless of the AllowNullValue setting. Data handling must always account for this situation.\nBy default, Terraform will not pass null values to the function logic. Use the AllowNullValue field to explicitly allow null values, if there is a meaningful distinction that should occur in function logic.\nAllow Unknown Values\nBy default, Terraform will not pass unknown values to the function logic. Use the AllowUnknownValues field to explicitly allow unknown values, if there is a meaningful distinction that should occur in function logic.\nCustom Types\nYou may want to build your own data value and type implementations to allow your provider to combine validation and other behaviors into a reusable bundle. This helps avoid duplication and ensures consistency. These implementations use the CustomType field in the parameter type.\nRefer to Custom Types for further details on creating provider-defined types and values.\nDocumentation\nRefer to function documentation for information about the Name, Description, and MarkdownDescription fields available.\nThe function implementation documentation covers the general methods for reading function argument data in function logic.\nWhen retrieving the argument value for this parameter:\nIf CustomType is set, use its associated value type.\nOtherwise, you must use the framework dynamic type.\nIn this example, a function defines a single dynamic parameter and accesses its argument value:\nfunc (f ExampleFunction) Definition(ctx context.Context, req function.DefinitionRequest, resp *function.DefinitionResponse) { resp.Definition = function.Definition{ // ... other Definition fields ... Parameters: []function.Parameter{ function.DynamicParameter{ Name: \"dynamic_param\", // ... potentially other DynamicParameter fields ... }, }, } } func (f ExampleFunction) Run(ctx context.Context, req function.RunRequest, resp *function.RunResponse) { var dynamicArg types.Dynamic resp.Error = function.ConcatFuncErrors(resp.Error, req.Arguments.Get(ctx, &dynamicArg)) // dynamicArg is now populated // ... other logic ... } \nFor more detail on working with dynamic values, see the framework dynamic type documentation.\nUtilizing function.DynamicParameter in the VariadicParameter field will allow zero, one, or more values of potentially different types.\nTo handle this scenario of multiple values with different types, utilize types.Tuple or []types.Dynamic when reading a dynamic variadic argument.\nfunc (f *ExampleFunction) Definition(ctx context.Context, req function.DefinitionRequest, resp *function.DefinitionResponse) { resp.Definition = function.Definition{ // ... other Definition fields ... VariadicParameter: function.DynamicParameter{ Name: \"variadic_param\", }, } } func (f *ExampleFunction) Run(ctx context.Context, req function.RunRequest, resp *function.RunResponse) { var dynValues []types.Dynamic resp.Error = function.ConcatFuncErrors(resp.Error, req.Arguments.Get(ctx, &dynValues)) if resp.Error != nil { return } for _, dynValue := range dynValues { if dynValue.IsNull() || dynValue.IsUnknown() { continue } // ... do something with argument value, i.e. dynValue.UnderlyingValue() ... } // ... other logic ... } \nIn these Terraform configuration examples, the function variadic argument will receive the following value types:\n# []types.Dynamic{} provider::example::example() # []types.Dynamic{types.String} provider::example::example(\"hello world\") # []types.Dynamic{types.Bool, types.Number} provider::example::example(true, 1) # []types.Dynamic{types.String, types.Tuple[types.String, types.String], types.List[types.String]} provider::example::example(\"hello\", [\"one\", \"two\"], tolist([\"one\", \"two\"]))"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/handling-data/types/dynamic",
  "text": "Plugin Development - Framework: Dynamic Type | Terraform\nTip\nStatic types should always be preferred over dynamic types, when possible.\nDevelopers dealing with dynamic data will need to have extensive knowledge of the Terraform type system to properly handle all potential practitioner configuration scenarios.\nRefer to Dynamic Data - Considerations for more information.\nDynamic is a container type that can have an underlying value of any type.\nBy default, dynamic values from schema (configuration, plan, and state) data are represented in the framework by types.DynamicType and its associated value storage type of types.Dynamic. These types fully support Terraform's type system concepts that cannot be represented in Go built-in types. Framework types can be extended by provider code or shared libraries to provide specific use case functionality.\nUse one of the following attribute types to directly add a dynamic value to a schema or nested attribute type:\nDynamic values are not supported as the element type of a collection type or within collection attribute types.\nIf the dynamic value should be a value type of an object attribute type, set the AttrTypes map value to types.DynamicType or the appropriate custom type.\nReview the attribute documentation to understand how schema-based data gets mapped into accessible values, such as a types.Dynamic in this case.\nAccess types.Dynamic information via the following methods:\n(types.Dynamic).IsNull() bool: Returns true if the dynamic value is null.\n(types.Dynamic).IsUnderlyingValueNull() bool: Returns true if the dynamic value is known but the underlying value is null. See the Dynamic Data section for more information about null underlying values.\n(types.Dynamic).IsUnknown() bool: Returns true if the dynamic value is unknown.\n(types.Dynamic).IsUnderlyingValueUnknown() bool: Returns true if the dynamic value is known but the underlying value is unknown. See the Dynamic Data section for more information about unknown underlying values.\n(types.Dynamic).UnderlyingValue() attr.Value: Returns the underlying value of the dynamic container, will be nil if null or unknown.\nIn this example, a dynamic value is checked for being null or unknown value first, before accessing its known value:\n// Example data model definition // type ExampleModel struct { // ExampleAttribute types.Dynamic `tfsdk:\"example_attribute\"` // } // // This would be filled in, such as calling: req.Plan.Get(ctx, &data) var data ExampleModel // optional logic for handling null value if data.ExampleAttribute.IsNull() { // ... } // optional logic for handling unknown value if data.ExampleAttribute.IsUnknown() { // ... } // myDynamicVal now contains the underlying value, determined by Terraform at runtime myDynamicVal := data.ExampleAttribute.UnderlyingValue() \nHandling the Underlying Value\nIf a dynamic value is known, a Go type switch can be used to access the type-specific methods for data handling:\nswitch value := data.ExampleAttribute.UnderlyingValue().(type) { case types.Bool: // Handle boolean value case types.Number: // Handle float64, int64, and number values case types.List: // Handle list value case types.Map: // Handle map value case types.Object: // Handle object value case types.Set: // Handle set value case types.String: // Handle string value case types.Tuple: // Handle tuple value } \nFloat64 and Int64 framework types will never appear in the underlying value as both are represented as the Terraform type number.\nThe type of the underlying value is determined at runtime by Terraform if the value is from configuration. Developers dealing with dynamic data will need to have extensive knowledge of the Terraform type system to properly handle all potential practitioner configuration scenarios.\nRefer to the Dynamic Data documentation for more information.\nCall one of the following to create a types.Dynamic value:\ntypes.DynamicNull(): A null dynamic value.\ntypes.DynamicUnknown(): An unknown dynamic value where the final static type is not known. Use types.DynamicValue() with an unknown value if the final static type is known.\ntypes.DynamicValue(attr.Value): A known dynamic value, with an underlying value determined by the attr.Value input.\nIn this example, a known dynamic value is created, where the underlying value is a known string value:\ntypes.DynamicValue(types.StringValue(\"hello world!\")) \nIn this example, a known dynamic value is created, where the underlying value is a known object value:\nelementTypes := map[string]attr.Type{ \"attr1\": types.StringType, \"attr2\": types.Int64Type, } elements := map[string]attr.Value{ \"attr1\": types.StringValue(\"value\"), \"attr2\": types.Int64Value(123), } objectValue, diags := types.ObjectValue(elementTypes, elements) // ... handle any diagnostics ... types.DynamicValue(objectValue) \nThere are no reflection rules defined for creating dynamic values, meaning they must be created using the types implementation.\nIn this example, a types.Dynamic with a known boolean value is used to set a dynamic attribute value:\ndiags := resp.State.SetAttribute(ctx, path.Root(\"example_attribute\"), types.DynamicValue(types.BoolValue(true))) \nThe framework supports extending its base type implementations with custom types. These can adjust expected provider code usage depending on their implementation."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/functions/returns/dynamic",
  "text": "Plugin Development - Framework: Dynamic Function Return | Terraform\nStatic types should always be preferred over dynamic types, when possible.\nDevelopers creating a function with a dynamic return will need to have extensive knowledge of the Terraform type system to understand how the value type returned can impact practitioner configuration.\nRefer to Dynamic Data - Considerations for more information.\nDynamic function return can be any value type from function logic. Set values in function logic with the framework dynamic type.\nUse the function.DynamicReturn type in the function definition.\nIn this example, a function definition includes a dynamic return:\nfunc (f ExampleFunction) Definition(ctx context.Context, req function.DefinitionRequest, resp *function.DefinitionResponse) { resp.Definition = function.Definition{ // ... other Definition fields ... Return: function.DynamicReturn{ // ... potentially other DynamicReturn fields ... }, } } \nCustom Types\nYou may want to build your own data value and type implementations to allow your provider to combine validation and other behaviors into a reusable bundle. This helps avoid duplication and ensures consistency. These implementations use the CustomType field in the return type.\nRefer to Custom Types for further details on creating provider-defined types and values.\nDocumentation\nReturn documentation is expected in the top-level function documentation. Refer to function documentation for information about the Summary, Description, and MarkdownDescription fields available.\nThe function implementation documentation covers the general methods for setting function return data in function logic.\nWhen setting the value for this return:\nIf CustomType is set, use its associated value type.\nOtherwise, use the framework dynamic type.\nIn this example, a function defines a dynamic return and sets its value to a string:\nfunc (f ExampleFunction) Definition(ctx context.Context, req function.DefinitionRequest, resp *function.DefinitionResponse) { resp.Definition = function.Definition{ // ... other Definition fields ... Return: function.DynamicReturn{}, } } func (f ExampleFunction) Run(ctx context.Context, req function.RunRequest, resp *function.RunResponse) { // ... other logic ... // hardcoded value for example brevity result := types.DynamicValue(types.StringValue(\"hello world!\")) resp.Error = function.ConcatFuncErrors(resp.Error, resp.Result.Set(ctx, &result)) } \nFor more detail on working with dynamic values, see the framework dynamic type documentation."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/functions/parameters/object",
  "text": "Plugin Development - Framework: Object Function Parameter | Terraform\nObject function parameters expect a single structure mapping explicit attribute names to type definitions from a practitioner configuration. Values are accessible in function logic by a Go structure type annotated with tfsdk field tags or the framework object type.\nConfigurations must include all object attributes or a configuration error is raised. Configurations explicitly setting object attribute values to null will prevent this type of configuration error while leaving that object attribute value unset. The AllowNullValue setting does not need to be enabled for object attribute null values to work in this manner.\nIn this Terraform configuration example, a object parameter is set to the mapped values of attr1 to \"value1\", attr2 to 123, and attr3 to null:\nprovider::example::example({ attr1 = \"value1\" attr2 = 123 attr3 = null }) \nUse the function.ObjectParameter type in the function definition to accept an object value.\nThe AttributeTypes field must be defined, which represents a mapping of attribute names to framework value types. An attribute type may itself contain further collection or object types, if necessary.\nIn this example, a function definition includes a first position object parameter:\nfunc (f ExampleFunction) Definition(ctx context.Context, req function.DefinitionRequest, resp *function.DefinitionResponse) { resp.Definition = function.Definition{ // ... other Definition fields ... Parameters: []function.Parameter{ function.ObjectParameter{ AttributeTypes: map[string]attr.Type{ \"attr1\": types.StringType, \"attr2\": types.Int64Type, \"attr3\": types.BoolType, }, Name: \"object_param\", // ... potentially other ObjectParameter fields ... }, }, } } \nIf the map value should be the element type of another collection parameter type, set the ElementType field according to the framework object type. Refer to the collection parameter type documentation for additional details.\nIf the map value should be a value type of an object parameter type, set the AttributeTypes map value according to the framework object type. Refer to the object parameter type documentation for additional details.\nAllow Null Values\nA known object value with null attribute values will always be sent to the function logic, regardless of the AllowNullValue setting. Data handling must always account for this situation.\nBy default, Terraform will not pass null values to the function logic. Use the AllowNullValue field to explicitly allow null values, if there is a meaningful distinction that should occur in function logic. Enabling AllowNullValue requires no changes when reading argument data.\nAllow Unknown Values\nBy default, Terraform will not pass unknown values to the function logic. Use the AllowUnknownValues field to explicitly allow unknown values, if there is a meaningful distinction that should occur in function logic. Enabling AllowUnknownValues requires using a framework object type when reading argument data.\nCustom Types\nYou may want to build your own data value and type implementations to allow your provider to combine validation and other behaviors into a reusable bundle. This helps avoid duplication and ensures consistency. These implementations use the CustomType field in the parameter type.\nRefer to Custom Types for further details on creating provider-defined types and values.\nDocumentation\nRefer to function documentation for information about the Name, Description, and MarkdownDescription fields available.\nThe function implementation documentation covers the general methods for reading function argument data in function logic.\nWhen retrieving the argument value for this parameter:\nIf CustomType is set, use its associated value type.\nIf AllowUnknownValues is enabled, you must use the framework object type.\nIf AllowNullValue is enabled, you must use a pointer to the Go structure type annotated with tfsdk field tags or the framework object type.\nOtherwise, use the Go structure type annotated with tfsdk field tags or framework object type.\nIn this example, a function defines a single object parameter and accesses its argument value:\nfunc (f ExampleFunction) Definition(ctx context.Context, req function.DefinitionRequest, resp *function.DefinitionResponse) { resp.Definition = function.Definition{ // ... other Definition fields ... Parameters: []function.Parameter{ function.ObjectParameter{ AttributeTypes: map[string]attr.Type{ \"attr1\": types.StringType, \"attr2\": types.Int64Type, \"attr3\": types.BoolType, }, Name: \"object_param\", }, }, } } func (f ExampleFunction) Run(ctx context.Context, req function.RunRequest, resp *function.RunResponse) { var objectArg struct{ Attr1 *string `tfsdk:\"attr1\"` Attr2 *int64 `tfsdk:\"attr2\"` Attr3 *bool `tfsdk:\"attr3\"` } // e.g. with AllowNullValues // var objectArg *struct{ // Attr1 *string `tfsdk:\"attr1\"` // Attr2 *int64 `tfsdk:\"attr2\"` // Attr3 *bool `tfsdk:\"attr3\"` // } // var objectArg types.Object // e.g. with AllowUnknownValues or AllowNullValues resp.Error = function.ConcatFuncErrors(resp.Error, req.Arguments.Get(ctx, &objectArg)) // objectArg is now populated // ... other logic ... }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/functions/returns/object",
  "text": "Plugin Development - Framework: Object Function Return | Terraform\nObject function return expects a single structure mapping explicit attribute names to type definitions from function logic. Set values in function logic with a Go structure type annotated with tfsdk field tags or the framework map type.\nUse the function.ObjectReturn type in the function definition.\nThe AttributeTypes field must be defined, which represents a mapping of attribute names to framework value types. An attribute type may itself contain further collection or object types, if necessary.\nIn this example, a function definition includes an object return:\nfunc (f ExampleFunction) Definition(ctx context.Context, req function.DefinitionRequest, resp *function.DefinitionResponse) { resp.Definition = function.Definition{ // ... other Definition fields ... Return: function.ObjectReturn{ AttributeTypes: map[string]attr.Type{ \"attr1\": types.StringType, \"attr2\": types.Int64Type, }, // ... potentially other ObjectReturn fields ... }, } } \nCustom Types\nYou may want to build your own data value and type implementations to allow your provider to combine validation and other behaviors into a reusable bundle. This helps avoid duplication and ensures consistency. These implementations use the CustomType field in the return type.\nRefer to Custom Types for further details on creating provider-defined types and values.\nDocumentation\nReturn documentation is expected in the top-level function documentation. Refer to function documentation for information about the Summary, Description, and MarkdownDescription fields available.\nThe function implementation documentation covers the general methods for setting function return data in function logic.\nWhen setting the value for this return:\nIf CustomType is set, use its associated value type.\nOtherwise, use a Go structure type annotated with tfsdk field tags or framework map type.\nIn this example, a function defines a map of string return and sets its value:\nfunc (f ExampleFunction) Definition(ctx context.Context, req function.DefinitionRequest, resp *function.DefinitionResponse) { resp.Definition = function.Definition{ // ... other Definition fields ... Return: function.ObjectReturn{ AttributeTypes: map[string]attr.Type{ \"attr1\": types.StringType, \"attr2\": types.Int64Type, }, }, } } func (f ExampleFunction) Run(ctx context.Context, req function.RunRequest, resp *function.RunResponse) { // ... other logic ... // hardcoded structure type and value for example brevity result := struct{ Attr1 string `tfsdk:\"attr1\"` Attr2 int64 `tfsdk:\"attr2\"` }{ Attr1: \"value1\", Attr2: 123, } resp.Error = function.ConcatFuncErrors(resp.Error, resp.Result.Set(ctx, &result)) }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/handling-data/types/list",
  "text": "Plugin Development - Framework: List Type | Terraform\nList types store an ordered collection of single element type.\nBy default, lists from schema (configuration, plan, and state) data are represented in the framework by types.ListType and its associated value storage type of types.List. These types fully support Terraform's type system concepts that cannot be represented in Go built-in types, such as a slice. Framework types can be extended by provider code or shared libraries to provide specific use case functionality.\nUse one of the following attribute types to directly add a list of a single element type to a schema or nested attribute type:\nUse one of the following attribute types to directly add a list of a nested attributes to a schema or nested attribute type:\nIf the list value should be the element type of another collection attribute type, set the ElementType field to types.ListType{ElemType: /* ... */} or the appropriate custom type.\nIf the list value should be a value type of an object attribute type, set the AttributeTypes map value to types.ListType{ElemType: /* ... */} or the appropriate custom type.\nReview the attribute documentation to understand how schema-based data gets mapped into accessible values, such as a types.List in this case.\nAccess types.List information via the following methods:\n(types.List).IsNull() bool: Returns true if the list is null.\n(types.List).IsUnknown() bool: Returns true if the list is unknown. Returns false if the number of elements is known, any of which may be unknown.\n(types.List).Elements() []attr.Value: Returns the known []attr.Value value, or nil if null or unknown.\n(types.List).ElementsAs(context.Context, any, bool) diag.Diagnostics: Converts the known values into the given Go type, if possible. It is recommended to use a slice of framework types to account for elements which may be unknown.\nIn this example, a list of strings value is checked for being null or unknown value first, before accessing its known value elements as a []types.String:\n// Example data model definition // type ExampleModel struct { // ExampleAttribute types.List `tfsdk:\"example_attribute\"` // } // // This would be filled in, such as calling: req.Plan.Get(ctx, &data) var data ExampleModel // optional logic for handling null value if data.ExampleAttribute.IsNull() { // ... } // optional logic for handling unknown value if data.ExampleAttribute.IsUnknown() { // ... } elements := make([]types.String, 0, len(data.ExampleAttribute.Elements())) diags := data.ExampleAttribute.ElementsAs(ctx, &elements, false) \nCall one of the following to create a types.List value:\ntypes.ListNull(attr.Type) types.List: A null list value with the given element type.\ntypes.ListUnknown(attr.Type) types.List: An unknown list value with the given element type.\ntypes.ListValue(attr.Type, []attr.Value) (types.List, diag.Diagnostics): A known value with the given element type and values.\ntypes.ListValueFrom(context.Context, attr.Type, any) (types.List, diag.Diagnostics): A known value with the given element type and values. This can convert the source data from standard Go types into framework types as noted in the documentation for each element type, such as giving []*string for a types.List of types.String.\ntypes.ListValueMust(attr.Type, []attr.Value) types.List: A known value with the given element type and values. Any diagnostics are converted to a runtime panic. This is recommended only for testing or exhaustively tested logic.\nIn this example, a known list value is created from framework types:\nelements := []attr.Value{types.StringValue(\"one\"), types.StringValue(\"two\")} listValue, diags := types.ListValue(types.StringType, elements) \nOtherwise, for certain framework functionality that does not require types implementations directly, such as:\n(tfsdk.State).SetAttribute()\ntypes.ListValueFrom()\ntypes.MapValueFrom()\ntypes.ObjectValueFrom()\ntypes.SetValueFrom()\nA Go built-in slice type ([]T) or type alias of a slice type such as type MyListType []T can be used instead.\nIn this example, a []string is directly used to set a list attribute value:\nelements := []string{\"one\", \"two\"} diags := resp.State.SetAttribute(ctx, path.Root(\"example_attribute\"), elements) \nIn this example, a types.List of types.String is created from a []string:\nelements := []string{\"one\", \"two\"} listValue, diags := types.ListValueFrom(ctx, types.StringType, elements) \nThe framework supports extending its base type implementations with custom types. These can adjust expected provider code usage depending on their implementation."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/handling-data/types/tuple",
  "text": "Plugin Development - Framework: Tuple Type | Terraform\nNote\nThe tuple type doesn't have associated schema attributes as it has limited real world application. Provider developers will only encounter tuples when handling provider-defined function variadic parameters or dynamic values.\nTuple types store an ordered collection of elements where each element has it's own type. Values must have exactly the same number of elements (no more and no fewer), and the value in each position must match the specified type for that position.\nThe tuple type is used to express Terraform's tuple type constraint.\nThe tuple type is not supported in schema definitions of provider, data sources, or managed resources as it has limited real world application.\nAccess types.Tuple information via the following methods:\n(types.Tuple).IsNull() bool: Returns true if the tuple is null.\n(types.Tuple).IsUnknown() bool: Returns true if the tuple is unknown. Returns false if the number of elements is known, any of which may be unknown.\n(types.Tuple).Elements() []attr.Value: Returns the known []attr.Value value, or nil if null or unknown.\nCall one of the following to create a types.Tuple value:\ntypes.TupleNull([]attr.Type) types.Tuple: A null tuple value with the given element types.\ntypes.TupleUnknown([]attr.Type) types.Tuple: An unknown tuple value with the given element types.\ntypes.TupleValue([]attr.Type, []attr.Value) (types.Tuple, diag.Diagnostics): A known value with the given element types and values.\ntypes.TupleValueMust([]attr.Type, []attr.Value) types.Tuple: A known value with the given element types and values. Any diagnostics are converted to a runtime panic. This is recommended only for testing or exhaustively tested logic.\nIn this example, a known tuple value ([\"one\", true, 123]) is created from framework types:\nelementTypes := []attr.Type{ types.StringType, types.BoolType, types.Int64Type, } elements := []attr.Value{ types.StringValue(\"one\"), types.BoolValue(true), types.Int64Value(123), } tupleValue, diags := types.TupleValue(elementTypes, elements)"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.1.x/handling-data/conversion-rules",
  "text": "Plugin Development - Framework: Conversion Rules | Terraform\nWarning: It can be tempting to use Go types instead of attr.Value implementations when the provider doesn't care about the distinction between an empty value, unknown, and null. But if Terraform has a null or unknown value and the provider asks the framework to store it in a type that can't hold it, Get will return an error. Make sure the types you are using can hold the values they might contain!\nString\nStrings can be automatically converted to Go's string type (or any aliases of it, like type MyString string) as long as the string value is not null or unknown.\nNumber\nNumbers can be automatically converted to the following numeric types (or any aliases of them, like type MyNumber int) as long as the number value is not null or unknown:\nint, int8, int16, int32, int64\nuint, uint8, uint16, uint32, uint64\nfloat32, float64\n*big.Int, *big.Float\nAn error will be returned if the value of the number cannot be stored in the numeric type supplied because of an overflow or other loss of precision.\nBoolean\nBooleans can be automatically converted to Go's bool type (or any aliases of it, like type MyBoolean bool) as long as the boolean value is not null or unknown.\nList\nLists can be automatically converted to any Go slice type (or alias of a Go slice type, like type MyList []string), with the elements either being attr.Value implementations or being converted according to these rules. Go slice types are considered capable of handling null values; the slice will be set to nil. The Get method will still return an error for unknown list values.\nMap\nMaps can be automatically converted to any Go map type with string keys (or any alias of a Go map type with string keys, like type MyMap map[string]int), with the elements either being attr.Value implementations or being converted according to these rules. Go map types are considered capable of handling null values; the map will be set to nil. The Get method will still return an error for unknown map values.\nObject\nObjects can be automatically converted to any Go struct type with that follows these constraints:\nEvery property on the struct must have a tfsdk struct tag.\nThe tfsdk struct tag must name an attribute in the object that it is being mapped to or be set to - to explicitly declare it does not map to an attribute in the object.\nEvery attribute in the object must have a corresponding struct tag.\nThese rules help prevent typos and human error from unwittingly discarding information by failing as early, consistently, and loudly as possible.\nProperties can either be attr.Value implementations or will be converted according to these rules.\nUnknown and null objects cannot be represented as structs and will return an error. Their attributes may contain unknown or null values if the attribute's type can hold them.\nPointers\nPointers behave exactly like the type they are referencing, except they can hold null values. A pointer will be set to nil when representing a null value; otherwise, the conversion rules for that type will apply.\nDetected Interfaces\nGet detects and utilizes the following interfaces, if the target implements them.\nValueConverter\nIf a value is being set on a Go type that implements the tftypes.ValueConverter interface, that interface will be delegated to to handle the conversion.\nUnknownable\nIf the value is being set on a Go type that fills the Unknownable interface:\ntype Unknownable interface { SetUnknown(context.Context, bool) error SetValue(context.Context, interface{}) error GetUnknown(context.Context) bool GetValue(context.Context) interface{} } \nIt will be considered capable of handling unknown values, and those methods will be used to populate it and retrieve its value. The interface{} being passed and retrieved will be of a type that can be passed to tftypes.NewValue.\nNullable\nIf the value is being set on a Go type that fills the Nullable interface:\ntype Nullable interface { SetNull(context.Context, bool) error SetValue(context.Context, interface{}) error GetNull(context.Context) bool GetValue(context.Context) interface{} } \nIt will be considered capable of handling null values, and those methods will be used to populate it and retrieve its value. The interface{} being passed and retrieved will be of a type that can be passed to tftypes.NewValue.\nThe following is a list of schema types and the Go types they know how to accept in Set and SetAttribute.\nString\nStrings can be automatically created from Go's string type (or any aliases of it, like type MyString string).\nNumber\nNumbers can be automatically created from the following numeric types (or any aliases of them, like type MyNumber int):\nint, int8, int16, int32, int64\nuint, uint8, uint16, uint32, uint64\nfloat32, float64\n*big.Int, *big.Float\nBoolean\nBooleans can be automatically created from Go's bool type (or any aliases of it, like type MyBoolean bool).\nList\nLists can be automatically created from any Go slice type (or alias of a Go slice type, like type MyList []string), with the elements either being attr.Value implementations or being converted according to these rules.\nMap\nMaps can be automatically created from any Go map type with string keys (or any alias of a Go map type with string keys, like type MyMap map[string]int), with the elements either being attr.Value implementations or being converted according to these rules.\nObject\nObjects can be automatically created from any Go struct type with that follows these constraints:\nEvery property on the struct must have a tfsdk struct tag.\nThe tfsdk struct tag must name an attribute in the object that it is being mapped to or be set to - to explicitly declare it does not map to an attribute in the object.\nEvery attribute in the object must have a corresponding struct tag.\nThese rules help prevent typos and human error from unwittingly discarding information by failing as early, consistently, and loudly as possible.\nProperties can either be attr.Value implementations or will be converted according to these rules.\nPointers\nA nil pointer will be treated as a null value. Otherwise, the rules for the type the pointer is referencing apply.\nDetected Interfaces\nSet detects and utilizes the following interfaces, if the target implements them.\nValueCreator\nIf a value is set on a Go type that implements the tftypes.ValueCreator interface, that interface will be delegated to to handle the conversion.\nUnknownable\nIf a value is set on a Go type that fills the Unknownable interface:\ntype Unknownable interface { SetUnknown(context.Context, bool) error SetValue(context.Context, interface{}) error GetUnknown(context.Context) bool GetValue(context.Context) interface{} } \nIt will be used to convert the value. The interface{} being passed and retrieved will be of a type that can be passed to tftypes.NewValue.\nNullable\nIf a value is set on a Go type that fills the Nullable interface:\ntype Nullable interface { SetNull(context.Context, bool) error SetValue(context.Context, interface{}) error GetNull(context.Context) bool GetValue(context.Context) interface{} } \nIt will be used to convert the value. The interface{} being passed and retrieved will be of a type that can be passed to tftypes.NewValue."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.0.x/handling-data/conversion-rules",
  "text": "Plugin Development - Framework: Conversion Rules | Terraform\nWarning: It can be tempting to use Go types instead of attr.Value implementations when the provider doesn't care about the distinction between an empty value, unknown, and null. But if Terraform has a null or unknown value and the provider asks the framework to store it in a type that can't hold it, Get will return an error. Make sure the types you are using can hold the values they might contain!\nString\nStrings can be automatically converted to Go's string type (or any aliases of it, like type MyString string) as long as the string value is not null or unknown.\nNumber\nNumbers can be automatically converted to the following numeric types (or any aliases of them, like type MyNumber int) as long as the number value is not null or unknown:\nint, int8, int16, int32, int64\nuint, uint8, uint16, uint32, uint64\nfloat32, float64\n*big.Int, *big.Float\nAn error will be returned if the value of the number cannot be stored in the numeric type supplied because of an overflow or other loss of precision.\nBoolean\nBooleans can be automatically converted to Go's bool type (or any aliases of it, like type MyBoolean bool) as long as the boolean value is not null or unknown.\nList\nLists can be automatically converted to any Go slice type (or alias of a Go slice type, like type MyList []string), with the elements either being attr.Value implementations or being converted according to these rules. Go slice types are considered capable of handling null values; the slice will be set to nil. The Get method will still return an error for unknown list values.\nMap\nMaps can be automatically converted to any Go map type with string keys (or any alias of a Go map type with string keys, like type MyMap map[string]int), with the elements either being attr.Value implementations or being converted according to these rules. Go map types are considered capable of handling null values; the map will be set to nil. The Get method will still return an error for unknown map values.\nObject\nObjects can be automatically converted to any Go struct type with that follows these constraints:\nEvery property on the struct must have a tfsdk struct tag.\nThe tfsdk struct tag must name an attribute in the object that it is being mapped to or be set to - to explicitly declare it does not map to an attribute in the object.\nEvery attribute in the object must have a corresponding struct tag.\nThese rules help prevent typos and human error from unwittingly discarding information by failing as early, consistently, and loudly as possible.\nProperties can either be attr.Value implementations or will be converted according to these rules.\nUnknown and null objects cannot be represented as structs and will return an error. Their attributes may contain unknown or null values if the attribute's type can hold them.\nPointers\nPointers behave exactly like the type they are referencing, except they can hold null values. A pointer will be set to nil when representing a null value; otherwise, the conversion rules for that type will apply.\nDetected Interfaces\nGet detects and utilizes the following interfaces, if the target implements them.\nValueConverter\nIf a value is being set on a Go type that implements the tftypes.ValueConverter interface, that interface will be delegated to to handle the conversion.\nUnknownable\nIf the value is being set on a Go type that fills the Unknownable interface:\ntype Unknownable interface { SetUnknown(context.Context, bool) error SetValue(context.Context, interface{}) error GetUnknown(context.Context) bool GetValue(context.Context) interface{} } \nIt will be considered capable of handling unknown values, and those methods will be used to populate it and retrieve its value. The interface{} being passed and retrieved will be of a type that can be passed to tftypes.NewValue.\nNullable\nIf the value is being set on a Go type that fills the Nullable interface:\ntype Nullable interface { SetNull(context.Context, bool) error SetValue(context.Context, interface{}) error GetNull(context.Context) bool GetValue(context.Context) interface{} } \nIt will be considered capable of handling null values, and those methods will be used to populate it and retrieve its value. The interface{} being passed and retrieved will be of a type that can be passed to tftypes.NewValue.\nThe following is a list of schema types and the Go types they know how to accept in Set and SetAttribute.\nString\nStrings can be automatically created from Go's string type (or any aliases of it, like type MyString string).\nNumber\nNumbers can be automatically created from the following numeric types (or any aliases of them, like type MyNumber int):\nint, int8, int16, int32, int64\nuint, uint8, uint16, uint32, uint64\nfloat32, float64\n*big.Int, *big.Float\nBoolean\nBooleans can be automatically created from Go's bool type (or any aliases of it, like type MyBoolean bool).\nList\nLists can be automatically created from any Go slice type (or alias of a Go slice type, like type MyList []string), with the elements either being attr.Value implementations or being converted according to these rules.\nMap\nMaps can be automatically created from any Go map type with string keys (or any alias of a Go map type with string keys, like type MyMap map[string]int), with the elements either being attr.Value implementations or being converted according to these rules.\nObject\nObjects can be automatically created from any Go struct type with that follows these constraints:\nEvery property on the struct must have a tfsdk struct tag.\nThe tfsdk struct tag must name an attribute in the object that it is being mapped to or be set to - to explicitly declare it does not map to an attribute in the object.\nEvery attribute in the object must have a corresponding struct tag.\nThese rules help prevent typos and human error from unwittingly discarding information by failing as early, consistently, and loudly as possible.\nProperties can either be attr.Value implementations or will be converted according to these rules.\nPointers\nA nil pointer will be treated as a null value. Otherwise, the rules for the type the pointer is referencing apply.\nDetected Interfaces\nSet detects and utilizes the following interfaces, if the target implements them.\nValueCreator\nIf a value is set on a Go type that implements the tftypes.ValueCreator interface, that interface will be delegated to to handle the conversion.\nUnknownable\nIf a value is set on a Go type that fills the Unknownable interface:\ntype Unknownable interface { SetUnknown(context.Context, bool) error SetValue(context.Context, interface{}) error GetUnknown(context.Context) bool GetValue(context.Context) interface{} } \nIt will be used to convert the value. The interface{} being passed and retrieved will be of a type that can be passed to tftypes.NewValue.\nNullable\nIf a value is set on a Go type that fills the Nullable interface:\ntype Nullable interface { SetNull(context.Context, bool) error SetValue(context.Context, interface{}) error GetNull(context.Context) bool GetValue(context.Context) interface{} } \nIt will be used to convert the value. The interface{} being passed and retrieved will be of a type that can be passed to tftypes.NewValue."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.0.x/handling-data/custom-types",
  "text": "Plugin Development - Framework: Handling Data - Custom Types | Terraform\nYou can use custom types for both attributes and blocks.\nImportant: Specifying plan customization for attribute types is not yet supported, limiting their utility. Support is expected in the near future.\nattr.Type Interface\nUse the attr.Type interface to implement an attribute type. It tells Terraform about its constraints and tells the framework how to create new attribute values from the information Terraform supplies. attr.Type has the following methods.\nMethodDescription\nTerraformType\tReturns the tftypes.Type value that describes its type constraints. This is how Terraform will know what type of values it can accept.\t\nValueFromTerraform\tReturns an attribute value from the tftypes.Value that Terraform supplies, or to return an error if it cannot. This error should not be used for validation purposes, and is expected to indicate programmer error, not practitioner error.\t\nEqual\tReturns true if the attribute type is considered equal to the passed attribute type.\t\nAttributePathStepper Interface\nAll attribute types must implement the tftypes.AttributePathStepper interface, so the framework can access element or attribute types using attribute paths.\nxattr.TypeWithValidation Interface\nIf validation for type values is desired, use the xattr.TypeWithValidation interface to include validation logic for type values. The framework will call this functionality when validating all values based on the schema.\nMethodDescription\nValidate\tReturns any warning or error diagnostics for the given value.\t\nType-Specific Interfaces\nCaseInterfaceDescription\nElements of the same type\tTypeWithElementType\tAttribute types that contain elements of the same type, like maps and lists, are required to implement attr.TypeWithElementType, which adds WithElementType and ElementType methods to the attr.Type interface. WithElementType must return a copy of the attribute type, but with its element type set to the passed type. ElementType must return the attribute type's element type.\t\nElements of different types\tTypeWithElementTypes\tAttribute types that contain elements of differing types, like tuples, are required to implement the attr.TypeWithElementTypes, which adds WithElementTypes and ElementTypes methods to the attr.Type interface. WithElementTypes must return a copy of the attribute type, but with its element types set to the passed element types. ElementTypes must return the attribute type's element types.\t\nContain attributes\tTypeWithAttributeTypes\tAttribute types that contain attributes, like objects, are required to implement the attr.TypeWithAttributeTypes interface, which adds WithAttributeTypes and AttributeTypes methods to the attr.Type interface. WithAttributeTypes must return a copy of the attribute type, but with its attribute types set to the passed attribute types. AttributeTypes must return the attribute type's attribute types.\t\nattr.Value Interface\nUse the attr.Value interface to implement an attribute value. It tells the framework how to express that attribute value in a way that Terraform will understand. attr.Value has the following methods.\nMethodDescription\nToTerraformValue\tReturns a Go type that is valid input for tftypes.NewValue for the tftypes.Type specified by the attr.Type that creates the attr.Value.\t\nEqual\tReturns true if the passed attribute value should be considered to the attribute value the method is being called on. The passed attribute value is not guaranteed to be of the same Go type.\t\nA minimal implementation of a custom type for ListType and List that leverages embedding looks as follows:\ntype CustomListType struct { types.ListType } func (c CustomListType) ValueFromTerraform(ctx context.Context, in tftypes.Value) (attr.Value, error) { val, err := c.ListType.ValueFromTerraform(ctx, in) return CustomListValue{ // unchecked type assertion val.(types.List), }, err } type CustomListValue struct { types.List } func (c CustomListValue) DoSomething(ctx context.Context) { tflog.Info(ctx, \"called DoSomething on CustomListValue\") } \nUsing the custom type does not require any changes to the Terraform configuration.\nresource \"example_resource\" \"example\" { list_attribute = [\"list-element\", \"list-element\"] list_nested_attribute = [ { int64_attribute = 9223372036854775807 list_attribute = [\"list-element\", \"list-element\"] }, { int64_attribute = 9223372036854775807 list_attribute = [\"list-element\", \"list-element\"] } ] list_nested_block { bool_attribute = true float64_attribute = 1234.5 int64_attribute = 9223372036854775807 list_attribute = [\"list-element\", \"list-element\"] list_nested_nested_block { bool_attribute = true } list_nested_nested_block { bool_attribute = false } } list_nested_block { bool_attribute = true float64_attribute = 1234.5 int64_attribute = 9223372036854775807 list_attribute = [\"list-element\", \"list-element\"] list_nested_nested_block { bool_attribute = true } list_nested_nested_block { bool_attribute = false } } } \nUse the custom type in the schema as follows:\nfunc (e *exampleResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"list_attribute\": schema.ListAttribute{ Optional: true, ElementType: types.StringType, CustomType: CustomListType{ types.ListType{ ElemType: types.StringType, }, }, }, \"list_nested_attribute\": schema.ListNestedAttribute{ Optional: true, CustomType: CustomListType{ types.ListType{ ElemType: types.ObjectType{ AttrTypes: map[string]attr.Type{ \"int64_attribute\": types.Int64Type, \"list_attribute\": types.ListType{ ElemType: types.StringType, }, }, }, }, }, NestedObject: schema.NestedAttributeObject{ Attributes: map[string]schema.Attribute{ \"int64_attribute\": schema.Int64Attribute{ Optional: true, }, \"list_attribute\": schema.ListAttribute{ Optional: true, ElementType: types.StringType, }, }, }, }, }, Blocks: map[string]schema.Block{ \"list_nested_block\": schema.ListNestedBlock{ CustomType: CustomListType{ types.ListType{ ElemType: types.ObjectType{ AttrTypes: map[string]attr.Type{ \"bool_attribute\": types.BoolType, \"float64_attribute\": types.Float64Type, \"int64_attribute\": types.Int64Type, \"list_attribute\": types.ListType{ ElemType: types.StringType, }, \"list_nested_nested_block\": types.ListType{ ElemType: types.ObjectType{ AttrTypes: map[string]attr.Type{ \"bool_attribute\": types.BoolType, }, }, }, }, }, }, }, NestedObject: schema.NestedBlockObject{ Attributes: map[string]schema.Attribute{ \"bool_attribute\": schema.BoolAttribute{ Optional: true, }, \"float64_attribute\": schema.Float64Attribute{ Optional: true, }, \"int64_attribute\": schema.Int64Attribute{ Optional: true, }, \"list_attribute\": schema.ListAttribute{ Optional: true, ElementType: types.StringType, }, }, Blocks: map[string]schema.Block{ \"list_nested_nested_block\": schema.ListNestedBlock{ NestedObject: schema.NestedBlockObject{ Attributes: map[string]schema.Attribute{ \"bool_attribute\": schema.BoolAttribute{ Optional: true, }, }, }, }, }, }, }, }, } } \nThe custom type value is then used within the model.\nWhere previously the model would have looked as follows:\ntype exampleResourceData struct { ListAttribute types.List `tfsdk:\"list_attribute\"` ListNestedAttribute types.List `tfsdk:\"list_nested_attribute\"` ListNestedBlock types.List `tfsdk:\"list_nested_block\"` } \nThe custom type value is used by updating the model to:\ntype exampleResourceData struct { ListAttribute CustomListValue `tfsdk:\"list_attribute\"` ListNestedAttribute CustomListValue `tfsdk:\"list_nested_attribute\"` ListNestedBlock CustomListValue `tfsdk:\"list_nested_block\"` } \nThe functions on CustomListValue are then available.\nfunc (e *exampleResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleResourceData diags := req.Config.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } data.ListAttribute.DoSomething(ctx) data.ListNestedAttribute.DoSomething(ctx) data.ListNestedBlock.DoSomething(ctx) /*...*/ }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.9.x/resources/state-upgrade",
  "text": "Plugin Development - Framework: State Upgrade | Terraform\nA resource schema captures the structure and types of the resource state. Any state data that does not conform to the resource schema will generate errors or may not be persisted properly. Over time, it may be necessary for resources to make breaking changes to their schemas, such as changing an attribute type. Terraform supports versioning of these resource schemas and the current version is saved into the Terraform state. When the provider advertises a newer schema version, Terraform will call back to the provider to attempt to upgrade from the saved schema version to the one advertised. This operation is performed prior to planning, but with a configured provider.\nSome versions of Terraform CLI will also request state upgrades even when the current schema version matches the state version. The framework will automatically handle this request.\nWhen generating a plan, Terraform CLI will request the current resource schema, which contains a version.\nIf Terraform CLI detects that an existing state with its saved version does not match the current version, Terraform CLI will request a state upgrade from the provider with the prior state version and expecting the state to match the current version.\nThe framework will check the resource to see if it defines state upgrade support:\nIf no state upgrade support is defined, an error diagnostic is returned.\nIf state upgrade support is defined, but not for the requested prior state version, an error diagnostic is returned.\nIf state upgrade support is defined and has an implementation for the requested prior state version, the provider defined implementation is executed.\nEnsure the tfsdk.Schema type Version field for the tfsdk.ResourceType is greater than 0, then implement the tfsdk.ResourceWithStateUpgrade interface for the tfsdk.Resource. Conventionally the version is incremented by 1 for each state upgrade.\nThis example shows a ResourceType which incremented the Schema type Version field:\n// Other ResourceType methods are omitted in this example var _ tfsdk.ResourceType = exampleResourceType{} type exampleResourceType struct{/* ... */} func (t exampleResourceType) GetSchema(_ context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ // ... other fields ... // This example conventionally declares that the resource has prior // state versions of 0 and 1, while the current version is 2. Version: 2, } } \nThis example shows a Resource with the necessary StateUpgrade method to implement the ResourceWithStateUpgrade interface:\n// Other Resource methods are omitted in this example var _ tfsdk.Resource = exampleResource{} var _ tfsdk.ResourceWithUpgradeState = exampleResource{} type exampleResource struct{/* ... */} func (r exampleResource) UpgradeState(ctx context.Context) map[int64]tfsdk.ResourceStateUpgrader { return map[int64]tfsdk.ResourceStateUpgrader{ // State upgrade implementation from 0 (prior state version) to 2 (Schema.Version) 0: { // Optionally, the PriorSchema field can be defined. StateUpgrader: func(ctx context.Context, req tfsdk.UpgradeResourceStateRequest, resp *tfsdk.UpgradeResourceStateResponse) { /* ... */ }, }, // State upgrade implementation from 1 (prior state version) to 2 (Schema.Version) 1: { // Optionally, the PriorSchema field can be defined. StateUpgrader: func(ctx context.Context, req tfsdk.UpgradeResourceStateRequest, resp *tfsdk.UpgradeResourceStateResponse) { /* ... */ }, }, } } \nEach tfsdk.ResourceStateUpgrader implementation is expected to wholly upgrade the resource state from the prior version to the current version. The framework does not iterate through intermediate version implementations as incrementing versions by 1 is only conventional and not required.\nAll state data must be populated in the tfsdk.UpgradeResourceStateResponse. The framework does not copy any prior state data from the tfsdk.UpgradeResourceStateRequest.\nThere are two approaches to implementing the provider logic for state upgrades in ResourceStateUpgrader. The recommended approach is defining the prior schema matching the resource state, which allows for prior state access similar to other parts of the framework. The second, more advanced, approach is accessing the prior resource state using lower level data handlers.\nResourceStateUpgrader With PriorSchema\nImplement the ResourceStateUpgrader type PriorSchema field to enable the framework to populate the tfsdk.UpgradeResourceStateRequest type State field for the provider defined state upgrade logic. Access the request State using methods such as Get() or GetAttribute(). Write the tfsdk.UpgradeResourceStateResponse type State field using methods such as Set() or SetAttribute().\nThis example shows a resource that changes the type for two attributes, using the PriorSchema approach:\n// Other ResourceType methods are omitted in this example var _ tfsdk.ResourceType = exampleResourceType{} // Other Resource methods are omitted in this example var _ tfsdk.Resource = exampleResource{} var _ tfsdk.ResourceWithUpgradeState = exampleResource{} type exampleResourceType struct{/* ... */} type exampleResource struct{/* ... */} type exampleResourceDataV0 struct { Id string `tfsdk:\"id\"` OptionalAttribute *bool `tfsdk:\"optional_attribute\"` RequiredAttribute bool `tfsdk:\"required_attribute\"` } type exampleResourceDataV1 struct { Id string `tfsdk:\"id\"` OptionalAttribute *string `tfsdk:\"optional_attribute\"` RequiredAttribute string `tfsdk:\"required_attribute\"` } func (t exampleResourceType) GetSchema(_ context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ Attributes: map[string]Attribute{ \"id\": { Type: types.StringType, Computed: true, }, \"optional_attribute\": { Type: types.StringType, // As compared to prior types.BoolType below Optional: true, }, \"required_attribute\": { Type: types.StringType, // As compared to prior types.BoolType below Required: true, }, }, // The resource has a prior state version of 0, which had the attribute // types of types.BoolType as shown below. Version: 1, } } func (r exampleResource) UpgradeState(ctx context.Context) map[int64]tfsdk.ResourceStateUpgrader { return map[int64]tfsdk.ResourceStateUpgrader{ // State upgrade implementation from 0 (prior state version) to 1 (Schema.Version) 0: { PriorSchema: &tfsdk.Schema{ Attributes: map[string]Attribute{ \"id\": { Type: types.StringType, Computed: true, }, \"optional_attribute\": { Type: types.BoolType, // As compared to current types.StringType above Optional: true, }, \"required_attribute\": { Type: types.BoolType, // As compared to current types.StringType above Required: true, }, }, }, StateUpgrader: func(ctx context.Context, req tfsdk.UpgradeResourceStateRequest, resp *tfsdk.UpgradeResourceStateResponse) { var priorStateData exampleResourceDataV0 resp.Diagnostics.Append(req.State.Get(ctx, &priorStateData)...) if resp.Diagnostics.HasError() { return } upgradedStateData := exampleResourceDataV1{ Id: priorStateData.Id, RequiredAttribute: fmt.Sprintf(\"%t\", priorStateData.RequiredAttribute), } if priorStateData.OptionalAttribute != nil { v := fmt.Sprintf(\"%t\", *priorStateData.OptionalAttribute) upgradedStateData.OptionalAttribute = &v } resp.Diagnostics.Append(resp.State.Set(ctx, upgradedStateData)...) }, }, } } \nResourceStateUpgrader Without PriorSchema\nRead prior state data from the tfsdk.UpgradeResourceStateRequest type RawState field. Write the tfsdk.UpgradeResourceStateResponse type State field using methods such as Set() or SetAttribute(), or for more advanced use cases, write the tfsdk.UpgradeResourceStateResponse type DynamicValue field.\nThis example shows a resource that changes the type for two attributes, using the RawState approach for the request and DynamicValue approach for the response:\n// Other ResourceType methods are omitted in this example var _ tfsdk.ResourceType = exampleResourceType{} // Other Resource methods are omitted in this example var _ tfsdk.Resource = exampleResource{} var _ tfsdk.ResourceWithUpgradeState = exampleResource{} var exampleResourceTftypesDataV0 = tftypes.Object{ AttributeTypes: map[string]tftypes.Type{ \"id\": tftypes.String, \"optional_attribute\": tftypes.Bool, \"required_attribute\": tftypes.Bool, }, } var exampleResourceTftypesDataV1 = tftypes.Object{ AttributeTypes: map[string]tftypes.Type{ \"id\": tftypes.String, \"optional_attribute\": tftypes.String, \"required_attribute\": tftypes.String, }, } type exampleResourceType struct{/* ... */} type exampleResource struct{/* ... */} func (t exampleResourceType) GetSchema(_ context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ Attributes: map[string]Attribute{ \"id\": { Type: types.StringType, Computed: true, }, \"optional_attribute\": { Type: types.StringType, // As compared to prior types.BoolType below Optional: true, }, \"required_attribute\": { Type: types.StringType, // As compared to prior types.BoolType below Required: true, }, }, // The resource has a prior state version of 0, which had the attribute // types of types.BoolType as shown below. Version: 1, } } func (r exampleResource) UpgradeState(ctx context.Context) map[int64]tfsdk.ResourceStateUpgrader { return map[int64]tfsdk.ResourceStateUpgrader{ // State upgrade implementation from 0 (prior state version) to 1 (Schema.Version) 0: { StateUpgrader: func(ctx context.Context, req tfsdk.UpgradeResourceStateRequest, resp *tfsdk.UpgradeResourceStateResponse) { // Refer also to the RawState type JSON field which can be used // with json.Unmarshal() rawStateValue, err := req.RawState.Unmarshal(exampleResourceTftypesDataV0) if err != nil { resp.Diagnostics.AddError( \"Unable to Unmarshal Prior State\", err.Error(), ) return } var rawState map[string]tftypes.Value if err := rawStateValue.As(&rawState); err != nil { resp.Diagnostics.AddError( \"Unable to Convert Prior State\", err.Error(), ) return } var optionalAttributeString *string if !rawState[\"optional_attribute\"].IsNull() { var optionalAttribute bool if err := rawState[\"optional_attribute\"].As(&optionalAttribute); err != nil { resp.Diagnostics.AddAttributeError( tftypes.NewAttributePath().WithAttributeName(\"optional_attribute\"), \"Unable to Convert Prior State\", err.Error(), ) return } v := fmt.Sprintf(\"%t\", optionalAttribute) optionalAttributeString = &v } var requiredAttribute bool if err := rawState[\"required_attribute\"].As(&requiredAttribute); err != nil { resp.Diagnostics.AddAttributeError( tftypes.NewAttributePath().WithAttributeName(\"required_attribute\"), \"Unable to Convert Prior State\", err.Error(), ) return } dynamicValue, err := tfprotov6.NewDynamicValue( exampleResourceTftypesDataV1, tftypes.NewValue(exampleResourceTftypesDataV1, map[string]tftypes.Value{ \"id\": rawState[\"id\"], \"optional_attribute\": tftypes.NewValue(tftypes.String, optionalAttributeString), \"required_attribute\": tftypes.NewValue(tftypes.String, fmt.Sprintf(\"%t\", requiredAttribute)), }), ) if err != nil { resp.Diagnostics.AddError( \"Unable to Convert Upgraded State\", err.Error(), ) return } resp.DynamicValue = &dynamicValue }, }, } }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.1.x/handling-data/custom-types",
  "text": "Plugin Development - Framework: Handling Data - Custom Types | Terraform\nYou can use custom types for both attributes and blocks.\nImportant: Specifying plan customization for attribute types is not yet supported, limiting their utility. Support is expected in the near future.\nattr.Type Interface\nUse the attr.Type interface to implement an attribute type. It tells Terraform about its constraints and tells the framework how to create new attribute values from the information Terraform supplies. attr.Type has the following methods.\nMethodDescription\nTerraformType\tReturns the tftypes.Type value that describes its type constraints. This is how Terraform will know what type of values it can accept.\t\nValueFromTerraform\tReturns an attribute value from the tftypes.Value that Terraform supplies, or to return an error if it cannot. This error should not be used for validation purposes, and is expected to indicate programmer error, not practitioner error.\t\nEqual\tReturns true if the attribute type is considered equal to the passed attribute type.\t\nAttributePathStepper Interface\nAll attribute types must implement the tftypes.AttributePathStepper interface, so the framework can access element or attribute types using attribute paths.\nxattr.TypeWithValidation Interface\nIf validation for type values is desired, use the xattr.TypeWithValidation interface to include validation logic for type values. The framework will call this functionality when validating all values based on the schema.\nValidate\tReturns any warning or error diagnostics for the given value.\t\nType-Specific Interfaces\nCaseInterfaceDescription\nElements of the same type\tTypeWithElementType\tAttribute types that contain elements of the same type, like maps and lists, are required to implement attr.TypeWithElementType, which adds WithElementType and ElementType methods to the attr.Type interface. WithElementType must return a copy of the attribute type, but with its element type set to the passed type. ElementType must return the attribute type's element type.\t\nElements of different types\tTypeWithElementTypes\tAttribute types that contain elements of differing types, like tuples, are required to implement the attr.TypeWithElementTypes, which adds WithElementTypes and ElementTypes methods to the attr.Type interface. WithElementTypes must return a copy of the attribute type, but with its element types set to the passed element types. ElementTypes must return the attribute type's element types.\t\nContain attributes\tTypeWithAttributeTypes\tAttribute types that contain attributes, like objects, are required to implement the attr.TypeWithAttributeTypes interface, which adds WithAttributeTypes and AttributeTypes methods to the attr.Type interface. WithAttributeTypes must return a copy of the attribute type, but with its attribute types set to the passed attribute types. AttributeTypes must return the attribute type's attribute types.\t\nattr.Value Interface\nUse the attr.Value interface to implement an attribute value. It tells the framework how to express that attribute value in a way that Terraform will understand. attr.Value has the following methods.\nToTerraformValue\tReturns a Go type that is valid input for tftypes.NewValue for the tftypes.Type specified by the attr.Type that creates the attr.Value.\t\nEqual\tReturns true if the passed attribute value should be considered to the attribute value the method is being called on. The passed attribute value is not guaranteed to be of the same Go type.\t\nA minimal implementation of a custom type for ListType and List that leverages embedding looks as follows:\ntype CustomListType struct { types.ListType } func (c CustomListType) ValueFromTerraform(ctx context.Context, in tftypes.Value) (attr.Value, error) { val, err := c.ListType.ValueFromTerraform(ctx, in) return CustomListValue{ // unchecked type assertion val.(types.List), }, err } type CustomListValue struct { types.List } func (c CustomListValue) DoSomething(ctx context.Context) { tflog.Info(ctx, \"called DoSomething on CustomListValue\") } \nUsing the custom type does not require any changes to the Terraform configuration.\nresource \"example_resource\" \"example\" { list_attribute = [\"list-element\", \"list-element\"] list_nested_attribute = [ { int64_attribute = 9223372036854775807 list_attribute = [\"list-element\", \"list-element\"] }, { int64_attribute = 9223372036854775807 list_attribute = [\"list-element\", \"list-element\"] } ] list_nested_block { bool_attribute = true float64_attribute = 1234.5 int64_attribute = 9223372036854775807 list_attribute = [\"list-element\", \"list-element\"] list_nested_nested_block { bool_attribute = true } list_nested_nested_block { bool_attribute = false } } list_nested_block { bool_attribute = true float64_attribute = 1234.5 int64_attribute = 9223372036854775807 list_attribute = [\"list-element\", \"list-element\"] list_nested_nested_block { bool_attribute = true } list_nested_nested_block { bool_attribute = false } } } \nUse the custom type in the schema as follows:\nfunc (e *exampleResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"list_attribute\": schema.ListAttribute{ Optional: true, ElementType: types.StringType, CustomType: CustomListType{ types.ListType{ ElemType: types.StringType, }, }, }, \"list_nested_attribute\": schema.ListNestedAttribute{ Optional: true, CustomType: CustomListType{ types.ListType{ ElemType: types.ObjectType{ AttrTypes: map[string]attr.Type{ \"int64_attribute\": types.Int64Type, \"list_attribute\": types.ListType{ ElemType: types.StringType, }, }, }, }, }, NestedObject: schema.NestedAttributeObject{ Attributes: map[string]schema.Attribute{ \"int64_attribute\": schema.Int64Attribute{ Optional: true, }, \"list_attribute\": schema.ListAttribute{ Optional: true, ElementType: types.StringType, }, }, }, }, }, Blocks: map[string]schema.Block{ \"list_nested_block\": schema.ListNestedBlock{ CustomType: CustomListType{ types.ListType{ ElemType: types.ObjectType{ AttrTypes: map[string]attr.Type{ \"bool_attribute\": types.BoolType, \"float64_attribute\": types.Float64Type, \"int64_attribute\": types.Int64Type, \"list_attribute\": types.ListType{ ElemType: types.StringType, }, \"list_nested_nested_block\": types.ListType{ ElemType: types.ObjectType{ AttrTypes: map[string]attr.Type{ \"bool_attribute\": types.BoolType, }, }, }, }, }, }, }, NestedObject: schema.NestedBlockObject{ Attributes: map[string]schema.Attribute{ \"bool_attribute\": schema.BoolAttribute{ Optional: true, }, \"float64_attribute\": schema.Float64Attribute{ Optional: true, }, \"int64_attribute\": schema.Int64Attribute{ Optional: true, }, \"list_attribute\": schema.ListAttribute{ Optional: true, ElementType: types.StringType, }, }, Blocks: map[string]schema.Block{ \"list_nested_nested_block\": schema.ListNestedBlock{ NestedObject: schema.NestedBlockObject{ Attributes: map[string]schema.Attribute{ \"bool_attribute\": schema.BoolAttribute{ Optional: true, }, }, }, }, }, }, }, }, } } \nThe custom type value is then used within the model.\nWhere previously the model would have looked as follows:\ntype exampleResourceData struct { ListAttribute types.List `tfsdk:\"list_attribute\"` ListNestedAttribute types.List `tfsdk:\"list_nested_attribute\"` ListNestedBlock types.List `tfsdk:\"list_nested_block\"` } \nThe custom type value is used by updating the model to:\ntype exampleResourceData struct { ListAttribute CustomListValue `tfsdk:\"list_attribute\"` ListNestedAttribute CustomListValue `tfsdk:\"list_nested_attribute\"` ListNestedBlock CustomListValue `tfsdk:\"list_nested_block\"` } \nThe functions on CustomListValue are then available.\nfunc (e *exampleResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleResourceData diags := req.Config.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } data.ListAttribute.DoSomething(ctx) data.ListNestedAttribute.DoSomething(ctx) data.ListNestedBlock.DoSomething(ctx) /*...*/ }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.9.x/resources/import",
  "text": "Practitioners can use the terraform import command to let Terraform begin managing existing infrastructure resources. Resources can implement the ImportState method, which must either specify enough Terraform state for the Read method to refresh tfsdk.Resource or return an error.\nWhen the Read method requires a single attribute to refresh, use the tfsdk.ResourceImportStatePassthroughID function to write the import identifier argument for terraform import.\nIn the following example, the terraform import command passes the import identifier to the id attribute in Terraform state.\nfunc (r exampleResource) ImportState(ctx context.Context, req tfsdk.ImportResourceStateRequest, resp *tfsdk.ImportResourceStateResponse) { tfsdk.ResourceImportStatePassthroughID(ctx, tftypes.NewAttributePath().WithAttributeName(\"id\"), req, resp) } \nWhen the Read method requires multiple attributes to refresh, you must write custom logic in the ImportState method. Specifically, the implementation must:\nUse the import identifier from the tfsdk.ImportResourceStateRequest. \nPerform the custom logic.\nSet state data in the tfsdk.ImportResourceStateResponse.\nFor example, if the tfsdk.ResourceType implementation has the following GetSchema method:\nfunc (t exampleResourceType) GetSchema(ctx context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ Attributes: map[string]tfsdk.Attribute{ \"attr_one\": { Type: types.StringType, Required: true, }, \"attr_two\": { Type: types.StringType, Required: true, }, // ... potentially other Attributes ... }, }, nil } \nAlong with a tfsdk.Resource implementation with the following Read method:\nfunc (r exampleResource) Read(ctx context.Context, req tfsdk.ReadResourceRequest, resp *tfsdk.ReadResourceResponse) { var attrOne, attrTwo string resp.Diagnostics.Append(req.State.GetAttribute(ctx, tftypes.NewAttributePath().WithAttributeName(\"attr_one\"), &attrOne)...) resp.Diagnostics.Append(req.State.GetAttribute(ctx, tftypes.NewAttributePath().WithAttributeName(\"attr_two\"), &attrTwo)...) if resp.Diagnostics.HasError() { return } // API call using attrOne and attrTwo } \nThe terraform import command will need to accept both attribute values as a single import identifier string. A typical convention is to use a separator character, such as a comma (,), between the values. The ImportState method will then need to parse the import identifier string into the two separate values and save them appropriately into the Terraform state.\nYou could define the ImportState method using a comma-separated value as follows:\nfunc (r exampleResource) ImportState(ctx context.Context, req tfsdk.ImportResourceStateRequest, resp *tfsdk.ImportResourceStateResponse) { idParts := strings.Split(req.ID, \",\") if len(idParts) != 2 || idParts[0] == \"\" || idParts[1] == \"\" { resp.Diagnostics.AddError( \"Unexpected Import Identifier\", fmt.Sprintf(\"Expected import identifier with format: attr_one,attr_two. Got: %q\", req.ID), ) return } resp.Diagnostics.Append(resp.State.SetAttribute(ctx, tftypes.NewAttributePath().WithAttributeName(\"attr_one\"), idParts[0])...) resp.Diagnostics.Append(resp.State.SetAttribute(ctx, tftypes.NewAttributePath().WithAttributeName(\"attr_two\"), idParts[1])...) } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.7.x/resources/state-upgrade",
  "text": "Ensure the tfsdk.Schema type Version field for the tfsdk.ResourceType is greater than 0, then implement the tfsdk.ResourceWithStateUpgrade interface for the tfsdk.Resource. Conventionally the version is incremented by 1 for each state upgrade.\n// Other ResourceType methods are omitted in this example var _ tfsdk.ResourceType = exampleResourceType{} type exampleResourceType struct{/* ... */} func (t exampleResourceType) GetSchema(_ context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ // ... other fields ... // This example conventionally declares that the resource has prior // state versions of 0 and 1, while the current version is 2. Version: 2, } } \n// Other Resource methods are omitted in this example var _ tfsdk.Resource = exampleResource{} var _ tfsdk.ResourceWithUpgradeState = exampleResource{} type exampleResource struct{/* ... */} func (r exampleResource) UpgradeState(ctx context.Context) map[int64]tfsdk.ResourceStateUpgrader { return map[int64]tfsdk.ResourceStateUpgrader{ // State upgrade implementation from 0 (prior state version) to 2 (Schema.Version) 0: { // Optionally, the PriorSchema field can be defined. StateUpgrader: func(ctx context.Context, req tfsdk.UpgradeResourceStateRequest, resp *tfsdk.UpgradeResourceStateResponse) { /* ... */ }, }, // State upgrade implementation from 1 (prior state version) to 2 (Schema.Version) 1: { // Optionally, the PriorSchema field can be defined. StateUpgrader: func(ctx context.Context, req tfsdk.UpgradeResourceStateRequest, resp *tfsdk.UpgradeResourceStateResponse) { /* ... */ }, }, } } \nEach tfsdk.ResourceStateUpgrader implementation is expected to wholly upgrade the resource state from the prior version to the current version. The framework does not iterate through intermediate version implementations as incrementing versions by 1 is only conventional and not required.\nAll state data must be populated in the tfsdk.UpgradeResourceStateResponse. The framework does not copy any prior state data from the tfsdk.UpgradeResourceStateRequest.\nThere are two approaches to implementing the provider logic for state upgrades in ResourceStateUpgrader. The recommended approach is defining the prior schema matching the resource state, which allows for prior state access similar to other parts of the framework. The second, more advanced, approach is accessing the prior resource state using lower level data handlers.\nResourceStateUpgrader With PriorSchema\nImplement the ResourceStateUpgrader type PriorSchema field to enable the framework to populate the tfsdk.UpgradeResourceStateRequest type State field for the provider defined state upgrade logic. Access the request State using methods such as Get() or GetAttribute(). Write the tfsdk.UpgradeResourceStateResponse type State field using methods such as Set() or SetAttribute().\n// Other ResourceType methods are omitted in this example var _ tfsdk.ResourceType = exampleResourceType{} // Other Resource methods are omitted in this example var _ tfsdk.Resource = exampleResource{} var _ tfsdk.ResourceWithUpgradeState = exampleResource{} type exampleResourceType struct{/* ... */} type exampleResource struct{/* ... */} type exampleResourceDataV0 struct { Id string `tfsdk:\"id\"` OptionalAttribute *bool `tfsdk:\"optional_attribute\"` RequiredAttribute bool `tfsdk:\"required_attribute\"` } type exampleResourceDataV1 struct { Id string `tfsdk:\"id\"` OptionalAttribute *string `tfsdk:\"optional_attribute\"` RequiredAttribute string `tfsdk:\"required_attribute\"` } func (t exampleResourceType) GetSchema(_ context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ Attributes: map[string]Attribute{ \"id\": { Type: types.StringType, Computed: true, }, \"optional_attribute\": { Type: types.StringType, // As compared to prior types.BoolType below Optional: true, }, \"required_attribute\": { Type: types.StringType, // As compared to prior types.BoolType below Required: true, }, }, // The resource has a prior state version of 0, which had the attribute // types of types.BoolType as shown below. Version: 1, } } func (r exampleResource) UpgradeState(ctx context.Context) map[int64]tfsdk.ResourceStateUpgrader { return map[int64]tfsdk.ResourceStateUpgrader{ // State upgrade implementation from 0 (prior state version) to 1 (Schema.Version) 0: { PriorSchema: &tfsdk.Schema{ Attributes: map[string]Attribute{ \"id\": { Type: types.StringType, Computed: true, }, \"optional_attribute\": { Type: types.BoolType, // As compared to current types.StringType above Optional: true, }, \"required_attribute\": { Type: types.BoolType, // As compared to current types.StringType above Required: true, }, }, }, StateUpgrader: func(ctx context.Context, req tfsdk.UpgradeResourceStateRequest, resp *tfsdk.UpgradeResourceStateResponse) { var priorStateData exampleResourceDataV0 resp.Diagnostics.Append(req.State.Get(ctx, &priorStateData)...) if resp.Diagnostics.HasError() { return } upgradedStateData := exampleResourceDataV1{ Id: priorStateData.Id, RequiredAttribute: fmt.Sprintf(\"%t\", priorStateData.RequiredAttribute), } if priorStateData.OptionalAttribute != nil { v := fmt.Sprintf(\"%t\", *priorStateData.OptionalAttribute) upgradedStateData.OptionalAttribute = &v } resp.Diagnostics.Append(resp.State.Set(ctx, upgradedStateData)...) }, }, } } \nResourceStateUpgrader Without PriorSchema\nRead prior state data from the tfsdk.UpgradeResourceStateRequest type RawState field. Write the tfsdk.UpgradeResourceStateResponse type State field using methods such as Set() or SetAttribute(), or for more advanced use cases, write the tfsdk.UpgradeResourceStateResponse type DynamicValue field.\n// Other ResourceType methods are omitted in this example var _ tfsdk.ResourceType = exampleResourceType{} // Other Resource methods are omitted in this example var _ tfsdk.Resource = exampleResource{} var _ tfsdk.ResourceWithUpgradeState = exampleResource{} var exampleResourceTftypesDataV0 = tftypes.Object{ AttributeTypes: map[string]tftypes.Type{ \"id\": tftypes.String, \"optional_attribute\": tftypes.Bool, \"required_attribute\": tftypes.Bool, }, } var exampleResourceTftypesDataV1 = tftypes.Object{ AttributeTypes: map[string]tftypes.Type{ \"id\": tftypes.String, \"optional_attribute\": tftypes.String, \"required_attribute\": tftypes.String, }, } type exampleResourceType struct{/* ... */} type exampleResource struct{/* ... */} func (t exampleResourceType) GetSchema(_ context.Context) (tfsdk.Schema, diag.Diagnostics) { return tfsdk.Schema{ Attributes: map[string]Attribute{ \"id\": { Type: types.StringType, Computed: true, }, \"optional_attribute\": { Type: types.StringType, // As compared to prior types.BoolType below Optional: true, }, \"required_attribute\": { Type: types.StringType, // As compared to prior types.BoolType below Required: true, }, }, // The resource has a prior state version of 0, which had the attribute // types of types.BoolType as shown below. Version: 1, } } func (r exampleResource) UpgradeState(ctx context.Context) map[int64]tfsdk.ResourceStateUpgrader { return map[int64]tfsdk.ResourceStateUpgrader{ // State upgrade implementation from 0 (prior state version) to 1 (Schema.Version) 0: { StateUpgrader: func(ctx context.Context, req tfsdk.UpgradeResourceStateRequest, resp *tfsdk.UpgradeResourceStateResponse) { // Refer also to the RawState type JSON field which can be used // with json.Unmarshal() rawStateValue, err := req.RawState.Unmarshal(exampleResourceTftypesDataV0) if err != nil { resp.Diagnostics.AddError( \"Unable to Unmarshal Prior State\", err.Error(), ) return } var rawState map[string]tftypes.Value if err := rawStateValue.As(&rawState); err != nil { resp.Diagnostics.AddError( \"Unable to Convert Prior State\", err.Error(), ) return } var optionalAttributeString *string if !rawState[\"optional_attribute\"].IsNull() { var optionalAttribute bool if err := rawState[\"optional_attribute\"].As(&optionalAttribute); err != nil { resp.Diagnostics.AddAttributeError( tftypes.NewAttributePath().WithAttributeName(\"optional_attribute\"), \"Unable to Convert Prior State\", err.Error(), ) return } v := fmt.Sprintf(\"%t\", optionalAttribute) optionalAttributeString = &v } var requiredAttribute bool if err := rawState[\"required_attribute\"].As(&requiredAttribute); err != nil { resp.Diagnostics.AddAttributeError( tftypes.NewAttributePath().WithAttributeName(\"required_attribute\"), \"Unable to Convert Prior State\", err.Error(), ) return } dynamicValue, err := tfprotov6.NewDynamicValue( exampleResourceTftypesDataV1, tftypes.NewValue(exampleResourceTftypesDataV1, map[string]tftypes.Value{ \"id\": rawState[\"id\"], \"optional_attribute\": tftypes.NewValue(tftypes.String, optionalAttributeString), \"required_attribute\": tftypes.NewValue(tftypes.String, fmt.Sprintf(\"%t\", requiredAttribute)), }), ) if err != nil { resp.Diagnostics.AddError( \"Unable to Convert Upgraded State\", err.Error(), ) return } resp.DynamicValue = &dynamicValue }, }, } }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.9.x/resources/plan-modification",
  "text": "Terraform and the framework support two types of plan modification on resources:\n// Typically within the tfsdk.Schema returned by GetSchema() for a resource. tfsdk.Attribute{ // ... other Attribute configuration ... PlanModifiers: []AttributePlanModifiers{ tfsdk.RequiresReplace(), }, } \ntfsdk.RequiresReplace(): If the value of the attribute changes, in-place update is not possible and instead the resource should be replaced for the change to occur. Refer to the Go documentation for full details on its behavior.\ntfsdk.RequiresReplaceIf(): Similar to tfsdk.RequiresReplace(), however it also accepts provider-defined conditional logic. Refer to the Go documentation for full details on its behavior.\ntfsdk.UseStateForUnknown(): Copies the prior state value, if not null. This is useful for reducing (known after apply) plan outputs for computed attributes which are known to not change over time.\nResource schemas also support plan modification across all attributes. This is helpful when working with logic that applies to the resource as a whole. To create a resource schema plan modification, you must implement the tfsdk.ResourceWithModifyPlan interface. For example:\n// Other methods to implement the tfsdk.Resource interface are omitted for brevity type exampleResource struct {} func (r exampleResource) ModifyPlan(ctx context.Context, req ModifyResourcePlanRequest, resp *ModifyResourcePlanResponse) { // Fill in logic. }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.7.x/data-sources/configure",
  "text": "Plugin Development - Framework: Configure Data Sources | Terraform\nData sources may require provider-level data or remote system clients to operate correctly. The framework supports the ability to configure this data and/or clients once within the provider, then pass that information to data sources by adding the Configure method.\nImplement the provider.ConfigureResponse.DataSourceData field in the Provider interface Configure method. This value can be set to any type, whether an existing client or vendor SDK type, a provider-defined custom type, or the provider implementation itself. It is recommended to use pointer types so that data sources can determine if this value was configured before attempting to use it.\nDuring execution of the terraform plan and terraform apply commands, Terraform calls the ConfigureProvider RPC, in which the framework calls the provider.Provider interface Configure method.\nIn this example, the Go standard library net/http.Client is configured in the provider, and made available for data sources:\n// With the provider.Provider implementation func (p *ExampleCloudProvider) Configure(ctx context.Context, req provider.ConfigureRequest, resp *provider.ConfigureResponse) { resp.DataSourceData = &http.Client{/* ... */} } \nIn this example, the code defines an ExampleClient type that is made available for data sources:\ntype ExampleClient struct { /* ... */ } // With the provider.Provider implementation func (p *ExampleCloudProvider) Configure(ctx context.Context, req provider.ConfigureRequest, resp *provider.ConfigureResponse) { resp.DataSourceData = &ExampleClient{/* ... */} } \nIn this example, the ExampleCloudProvider type itself is made available for data sources:\n// With the provider.Provider implementation type ExampleCloudProvider struct { /* ... */ } func (p *ExampleCloudProvider) Configure(ctx context.Context, req provider.ConfigureRequest, resp *provider.ConfigureResponse) { resp.DataSourceData = p } \nImplement the datasource.DataSourceWithConfigure interface which receives the provider configured data from the Provider interface Configure method and saves it into the datasource.DataSource interface implementation.\nThe datasource.DataSourceWithConfigure interface Configure method is called during execution of the terraform validate, terraform plan and terraform apply commands when the ValidateDataResourceConfig RPC is sent. Additionally, the datasource.DataSourceWithConfigure interface Configure method is called during execution of the terraform plan and terraform apply commands when the ReadDataSource RPC is sent.\nNote that Terraform calling the ValidateDataResourceConfig RPC would not call the ConfigureProvider RPC first, so implementations need to account for that situation. Configuration validation in Terraform occurs without provider configuration (\"offline\").\nIn this example, the provider configured the Go standard library net/http.Client which the data source uses during Read:\n// With the datasource.DataSource implementation type ThingDataSource struct { client *http.Client } func (d *ThingDataSource) Configure(ctx context.Context, req datasource.ConfigureRequest, resp *datasource.ConfigureResponse) { // A nil check should always be performed when handling ProviderData as it // is only set after the ConfigureProvider RPC has been called by Terraform. if req.ProviderData == nil { return } client, ok := req.ProviderData.(*http.Client) if !ok { resp.Diagnostics.AddError( \"Unexpected Data Source Configure Type\", fmt.Sprintf(\"Expected *http.Client, got: %T. Please report this issue to the provider developers.\", req.ProviderData), ) return } d.client = client } func (d *ThingDataSource) Read(ctx context.Context, req datasource.ReadRequest, resp *datasource.ReadResponse) { // Prevent panic if the provider has not been configured. if d.client == nil { resp.Diagnostics.AddError( \"Unconfigured HTTP Client\", \"Expected configured HTTP client. Please report this issue to the provider developers.\", ) return } httpResp, err := d.client.Get(\"https://example.com\") /* ... */ }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.7.x/resources/configure",
  "text": "Plugin Development - Framework: Configure Resources | Terraform\nResources may require provider-level data or remote system clients to operate correctly. The framework supports the ability to configure this data and/or clients once within the provider, then pass that information to resources by adding the Configure method.\nImplement the provider.ConfigureResponse.ResourceData field in the Provider interface Configure method. This value can be set to any type, whether an existing client or vendor SDK type, a provider-defined custom type, or the provider implementation itself. It is recommended to use pointer types so that resources can determine if this value was configured before attempting to use it.\nDuring execution of the terraform plan and terraform apply commands, Terraform calls the provider ConfigureProvider RPC, in which the framework calls the provider.Provider interface Configure method.\nIn this example, the Go standard library net/http.Client is configured in the provider, and made available for resources:\n// With the provider.Provider implementation func (p *ExampleCloudProvider) Configure(ctx context.Context, req provider.ConfigureRequest, resp *provider.ConfigureResponse) { resp.ResourceData = &http.Client{/* ... */} } \nIn this example, the code defines an ExampleClient type that is made available for resources:\ntype ExampleClient struct { /* ... */ } // With the provider.Provider implementation func (p *ExampleCloudProvider) Configure(ctx context.Context, req provider.ConfigureRequest, resp *provider.ConfigureResponse) { resp.ResourceData = &ExampleClient{/* ... */} } \nIn this example, the ExampleCloudProvider type itself is made available for resources:\n// With the provider.Provider implementation type ExampleCloudProvider struct { /* ... */ } func (p *ExampleCloudProvider) Configure(ctx context.Context, req provider.ConfigureRequest, resp *provider.ConfigureResponse) { resp.ResourceData = p } \nImplement the resource.ResourceWithConfigure interface which receives the provider configured data from the Provider interface Configure method and saves it into the resource.Resource interface implementation.\nThe resource.ResourceWithConfigure interface Configure method is called during execution of the terraform validate, terraform plan and terraform apply commands when the ValidateResourceConfig RPC is sent. Additionally, the resource.ResourceWithConfigure interface Configure method is called during execution of the terraform plan and terraform apply commands when the ReadResource RPC is sent.\nNote that Terraform calling the ValidateResourceConfig RPC would not call the ConfigureProvider RPC first, so implementations need to account for that situation. Configuration validation in Terraform occurs without provider configuration (\"offline\").\nIn this example, the provider configured the Go standard library net/http.Client which the resource uses during Read:\n// With the resource.Resource implementation type ThingResource struct { client *http.Client } func (r *ThingResource) Configure(ctx context.Context, req resource.ConfigureRequest, resp *resource.ConfigureResponse) { // A nil check should always be performed when handling ProviderData as it // is only set after the ConfigureProvider RPC has been called by Terraform. if req.ProviderData == nil { return } client, ok := req.ProviderData.(*http.Client) if !ok { resp.Diagnostics.AddError( \"Unexpected Resource Configure Type\", fmt.Sprintf(\"Expected *http.Client, got: %T. Please report this issue to the provider developers.\", req.ProviderData), ) return } r.client = client } func (r *ThingResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { // Prevent panic if the provider has not been configured. if r.client == nil { resp.Diagnostics.AddError( \"Unconfigured HTTP Client\", \"Expected configured HTTP client. Please report this issue to the provider developers.\", ) return } httpResp, err := r.client.Get(\"https://example.com\") /* ... */ }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.7.x/functions/concepts",
  "text": "Plugin Development - Framework: Function Concepts | Terraform\nThis page describes Terraform concepts relating to provider-defined functions within framework-based provider code. Provider-defined functions are supported in Terraform 1.8 and later. The What is Terraform, Terraform language, and Plugin Development documentation covers more general concepts behind Terraform's workflow, its configuration, and how it interacts with providers.\nThe purpose of provider-defined functions is to encapsulate offline, computational logic beyond Terraform's built-in functions to simplify practitioner configurations. Terraform expects that provider-defined functions are implemented without side-effects and as pure functions where given the same input data that they always return the same output. Refer to HashiCorp Provider Design Principles for additional best practice details.\nExample use cases include:\nTransforming existing data, such as merging complex data structures using a specific algorithm or converting between encodings.\nParsing combined data into individual, referenceable components, such as taking an Amazon Resource Name (ARN) and returning an object of region, account identifier, etc. attributes.\nBuilding combined data from individual components, such as returning an Amazon Resource Name (ARN) based on given region, account identifier, etc. data.\nStatic data lookups when there is no remote system query available, such as returning a data value typically necessary for a practitioner configuration.\nDifferences from other provider-defined concepts include:\nData Sources: Intended to perform online or provider configuration dependent data lookup, which participate in Terraform's operational graph.\nResources: Intended to manage the full lifecycle (create, update, destroy) of a remote system component, which participate in Terraform's operational graph.\nThere are two main components of provider-defined functions:\nDefinition: Defines the expected input and output data along with documentation descriptions.\nCall: When a practioner configuration causes a function's logic to be run.\nWithin a function definition the components are:\nParameters: An ordered list of definitions for input data.\nVariadic Parameter: An optional, final parameter which accepts zero, one, or multiple parts of input data.\nReturn: The definition for output data.\nSimilar to many programming languages, when the function is called, the terminology for the data is slightly different than the terminology for the definition.\nArguments: Positionally ordered data based on the definitions of the parameters.\nResult: Data based on the definition of the return.\nFor each provider listed as a required provider, Terraform will query the provider for its function definitions. If a configuration attempts to call a provider-defined function without listing the provider as required, Terraform will return an error.\nTerraform will typically call functions before other provider concepts are evaluated. This includes before provider configuration being evaluated, which the framework enforces by not exposing provider configuration data to function implementations.\nNaming\nTerraform requires that function names must be valid identifiers.\nArgument Handling\nTerraform will statically validate that the number and types of arguments in a configuration match the definitions of parameters, otherwise returning an error.\nIf a null value is given as an argument, without individual parameter definition opt-in, Terraform will return an error. If an unknown value is given as an argument, without individual parameter definition opt-in, Terraform will skip calling the provider logic entirely and set the function result to an unknown value matching the return type.\nResult Handling\nTerraform will statically validate that the return type is appropriately used in consuming configuration, otherwise returning an error.\nFunction logic must always set the result to the return type, otherwise Terraform will return an error.\nFunction logic can only set the result to an unknown value if there is a parameter that opted into unknown value handling and an unknown value argument was received for one of those parameters."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.5.x/functions/concepts",
  "text": "Plugin Development - Framework: Function Concepts | Terraform\nThis page describes Terraform concepts relating to provider-defined functions within framework-based provider code. Provider-defined functions are supported in Terraform 1.8 and later. The What is Terraform, Terraform language, and Plugin Development documentation covers more general concepts behind Terraform's workflow, its configuration, and how it interacts with providers.\nThe purpose of provider-defined functions is to encapsulate offline, computational logic beyond Terraform's built-in functions to simplify practitioner configurations. Terraform expects that provider-defined functions are implemented without side-effects and as pure functions where given the same input data that they always return the same output. Refer to HashiCorp Provider Design Principles for additional best practice details.\nExample use cases include:\nTransforming existing data, such as merging complex data structures using a specific algorithm or converting between encodings.\nParsing combined data into individual, referenceable components, such as taking an Amazon Resource Name (ARN) and returning an object of region, account identifier, etc. attributes.\nBuilding combined data from individual components, such as returning an Amazon Resource Name (ARN) based on given region, account identifier, etc. data.\nStatic data lookups when there is no remote system query available, such as returning a data value typically necessary for a practitioner configuration.\nDifferences from other provider-defined concepts include:\nData Sources: Intended to perform online or provider configuration dependent data lookup, which participate in Terraform's operational graph.\nResources: Intended to manage the full lifecycle (create, update, destroy) of a remote system component, which participate in Terraform's operational graph.\nThere are two main components of provider-defined functions:\nDefinition: Defines the expected input and output data along with documentation descriptions.\nCall: When a practioner configuration causes a function's logic to be run.\nWithin a function definition the components are:\nParameters: An ordered list of definitions for input data.\nVariadic Parameter: An optional, final parameter which accepts zero, one, or multiple parts of input data.\nReturn: The definition for output data.\nSimilar to many programming languages, when the function is called, the terminology for the data is slightly different than the terminology for the definition.\nArguments: Positionally ordered data based on the definitions of the parameters.\nResult: Data based on the definition of the return.\nFor each provider listed as a required provider, Terraform will query the provider for its function definitions. If a configuration attempts to call a provider-defined function without listing the provider as required, Terraform will return an error.\nTerraform will typically call functions before other provider concepts are evaluated. This includes before provider configuration being evaluated, which the framework enforces by not exposing provider configuration data to function implementations.\nNaming\nTerraform requires that function names must be valid identifiers.\nArgument Handling\nTerraform will statically validate that the number and types of arguments in a configuration match the definitions of parameters, otherwise returning an error.\nIf a null value is given as an argument, without individual parameter definition opt-in, Terraform will return an error. If an unknown value is given as an argument, without individual parameter definition opt-in, Terraform will skip calling the provider logic entirely and set the function result to an unknown value matching the return type.\nResult Handling\nTerraform will statically validate that the return type is appropriately used in consuming configuration, otherwise returning an error.\nFunction logic must always set the result to the return type, otherwise Terraform will return an error.\nFunction logic can only set the result to an unknown value if there is a parameter that opted into unknown value handling and an unknown value argument was received for one of those parameters."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.7.x/resources/state-upgrade",
  "text": "Ensure the schema.Schema type Version field for the resource.Resource is greater than 0, then implement the resource.ResourceWithStateUpgrade interface for the resource.Resource. Conventionally the version is incremented by 1 for each state upgrade.\n// Other Resource methods are omitted in this example var _ resource.Resource = &ThingResource{} var _ resource.ResourceWithUpgradeState = &ThingResource{} type ThingResource struct{/* ... */} func (r *ThingResource) Schema(_ context.Context, _ resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ // ... other fields ... // This example conventionally declares that the resource has prior // state versions of 0 and 1, while the current version is 2. Version: 2, } } func (r *ThingResource) UpgradeState(ctx context.Context) map[int64]resource.StateUpgrader { return map[int64]resource.StateUpgrader{ // State upgrade implementation from 0 (prior state version) to 2 (Schema.Version) 0: { // Optionally, the PriorSchema field can be defined. StateUpgrader: func(ctx context.Context, req resource.UpgradeStateRequest, resp *resource.UpgradeStateResponse) { /* ... */ }, }, // State upgrade implementation from 1 (prior state version) to 2 (Schema.Version) 1: { // Optionally, the PriorSchema field can be defined. StateUpgrader: func(ctx context.Context, req resource.UpgradeStateRequest, resp *resource.UpgradeStateResponse) { /* ... */ }, }, } } \nEach resource.StateUpgrader implementation is expected to wholly upgrade the resource state from the prior version to the current version. The framework does not iterate through intermediate version implementations as incrementing versions by 1 is only conventional and not required.\nAll state data must be populated in the resource.UpgradeStateResponse. The framework does not copy any prior state data from the resource.UpgradeStateRequest.\nThere are two approaches to implementing the provider logic for state upgrades in StateUpgrader. The recommended approach is defining the prior schema matching the resource state, which allows for prior state access similar to other parts of the framework. The second, more advanced, approach is accessing the prior resource state using lower level data handlers.\nStateUpgrader With PriorSchema\nImplement the StateUpgrader type PriorSchema field to enable the framework to populate the resource.UpgradeStateRequest type State field for the provider defined state upgrade logic. Access the request State using methods such as Get() or GetAttribute(). Write the resource.UpgradeStateResponse type State field using methods such as Set() or SetAttribute().\n// Other Resource methods are omitted in this example var _ resource.Resource = &ThingResource{} var _ resource.ResourceWithUpgradeState = &ThingResource{} type ThingResource struct{/* ... */} type ThingResourceModelV0 struct { Id string `tfsdk:\"id\"` OptionalAttribute *bool `tfsdk:\"optional_attribute\"` RequiredAttribute bool `tfsdk:\"required_attribute\"` } type ThingResourceModelV1 struct { Id string `tfsdk:\"id\"` OptionalAttribute *string `tfsdk:\"optional_attribute\"` RequiredAttribute string `tfsdk:\"required_attribute\"` } func (r *ThingResource) Schema(_ context.Context, _ resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"id\": schema.StringAttribute{ Computed: true, }, \"optional_attribute\": schema.StringAttribute{ // As compared to prior schema.BoolAttribute below Optional: true, }, \"required_attribute\": schema.StringAttribute{ // As compared to prior schema.BoolAttribute below Required: true, }, }, // The resource has a prior state version of 0, which had the attribute // types of types.BoolType as shown below. Version: 1, } } func (r *ThingResource) UpgradeState(ctx context.Context) map[int64]resource.StateUpgrader { return map[int64]resource.StateUpgrader{ // State upgrade implementation from 0 (prior state version) to 1 (Schema.Version) 0: { PriorSchema: &schema.Schema{ Attributes: map[string]schema.Attribute{ \"id\": schema.StringAttribute{ Computed: true, }, \"optional_attribute\": schema.BoolAttribute{ // As compared to current schema.StringAttribute above Optional: true, }, \"required_attribute\": schema.BoolAttribute{ // As compared to current schema.StringAttribute above Required: true, }, }, }, StateUpgrader: func(ctx context.Context, req resource.UpgradeStateRequest, resp *resource.UpgradeStateResponse) { var priorStateData ThingResourceModelV0 resp.Diagnostics.Append(req.State.Get(ctx, &priorStateData)...) if resp.Diagnostics.HasError() { return } upgradedStateData := ThingResourceModelV1{ Id: priorStateData.Id, RequiredAttribute: fmt.Sprintf(\"%t\", priorStateData.RequiredAttribute), } if priorStateData.OptionalAttribute != nil { v := fmt.Sprintf(\"%t\", *priorStateData.OptionalAttribute) upgradedStateData.OptionalAttribute = &v } resp.Diagnostics.Append(resp.State.Set(ctx, upgradedStateData)...) }, }, } } \nStateUpgrader Without PriorSchema\nRead prior state data from the resource.UpgradeStateRequest type RawState field. Write the resource.UpgradeStateResponse type State field using methods such as Set() or SetAttribute(), or for more advanced use cases, write the resource.UpgradeStateResponse type DynamicValue field.\n// Other Resource methods are omitted in this example var _ resource.Resource = &ThingResource{} var _ resource.ResourceWithUpgradeState = &ThingResource{} var ThingResourceTftypesDataV0 = tftypes.Object{ AttributeTypes: map[string]tftypes.Type{ \"id\": tftypes.String, \"optional_attribute\": tftypes.Bool, \"required_attribute\": tftypes.Bool, }, } var ThingResourceTftypesDataV1 = tftypes.Object{ AttributeTypes: map[string]tftypes.Type{ \"id\": tftypes.String, \"optional_attribute\": tftypes.String, \"required_attribute\": tftypes.String, }, } type ThingResource struct{/* ... */} func (r *ThingResource) Schema(_ context.Context, _ resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"id\": schema.StringAttribute{ Computed: true, }, \"optional_attribute\": schema.StringAttribute{ // As compared to prior schema.BoolAttribute below Optional: true, }, \"required_attribute\": schema.StringAttribute{ // As compared to prior schema.BoolAttribute below Required: true, }, }, // The resource has a prior state version of 0, which had the attribute // types of types.BoolType as shown below. Version: 1, } } func (r *ThingResource) UpgradeState(ctx context.Context) map[int64]resource.StateUpgrader { return map[int64]resource.StateUpgrader{ // State upgrade implementation from 0 (prior state version) to 1 (Schema.Version) 0: { StateUpgrader: func(ctx context.Context, req resource.UpgradeStateRequest, resp *resource.UpgradeStateResponse) { // Refer also to the RawState type JSON field which can be used // with json.Unmarshal() rawStateValue, err := req.RawState.Unmarshal(ThingResourceTftypesDataV0) if err != nil { resp.Diagnostics.AddError( \"Unable to Unmarshal Prior State\", err.Error(), ) return } var rawState map[string]tftypes.Value if err := rawStateValue.As(&rawState); err != nil { resp.Diagnostics.AddError( \"Unable to Convert Prior State\", err.Error(), ) return } var optionalAttributeString *string if !rawState[\"optional_attribute\"].IsNull() { var optionalAttribute bool if err := rawState[\"optional_attribute\"].As(&optionalAttribute); err != nil { resp.Diagnostics.AddAttributeError( path.Root(\"optional_attribute\"), \"Unable to Convert Prior State\", err.Error(), ) return } v := fmt.Sprintf(\"%t\", optionalAttribute) optionalAttributeString = &v } var requiredAttribute bool if err := rawState[\"required_attribute\"].As(&requiredAttribute); err != nil { resp.Diagnostics.AddAttributeError( path.Root(\"required_attribute\"), \"Unable to Convert Prior State\", err.Error(), ) return } dynamicValue, err := tfprotov6.NewDynamicValue( ThingResourceTftypesDataV1, tftypes.NewValue(ThingResourceTftypesDataV1, map[string]tftypes.Value{ \"id\": rawState[\"id\"], \"optional_attribute\": tftypes.NewValue(tftypes.String, optionalAttributeString), \"required_attribute\": tftypes.NewValue(tftypes.String, fmt.Sprintf(\"%t\", requiredAttribute)), }), ) if err != nil { resp.Diagnostics.AddError( \"Unable to Convert Upgraded State\", err.Error(), ) return } resp.DynamicValue = &dynamicValue }, }, } } \nNote these caveats when implementing the UpgradeState method:\nAn error is returned if the response state contains unknown values. Set all attributes to either null or known values in the response.\nAny response errors will cause Terraform to keep the prior resource state."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/functions/returns",
  "text": "Plugin Development - Framework: Function Returns | Terraform\nA return in a function definition describes the result data value from function logic. Every return type has an associated value type, although this data handling is simplified for function implementations over other provider concepts, such as resource implementations.\nFunction definitions support the following return types:\nPrimitive: Return that expects a single value, such as a boolean, number, or string.\nCollection: Return that expects multiple values of a single element type, such as a list, map, or set.\nObject: Return that expects a structure of explicit attribute names.\nDynamic: Return that can be any value type.\nPrimitive Return Types\nReturn types that expect a single data value, such as a boolean, number, or string.\nReturn TypeUse Case\nBool\tBoolean true or false\t\nFloat32\t32-bit floating point number\t\nFloat64\t64-bit floating point number\t\nInt32\t32-bit integer number\t\nInt64\t64-bit integer number\t\nNumber\tArbitrary precision (generally over 64-bit, up to 512-bit) number\t\nString\tCollection of UTF-8 encoded characters\t\nCollection Return Types\nReturn types that expect multiple values of a single element type, such as a list, map, or set.\nReturn TypeUse Case\nList\tOrdered collection of single element type\t\nMap\tMapping of arbitrary string keys to values of single element type\t\nSet\tUnordered, unique collection of single element type\t\nObject Return Type\nReturn type that expects a structure of explicit attribute names.\nReturn TypeUse Case\nObject\tSingle structure mapping explicit attribute names\t\nDynamic Return Type\nReturn type that can be any value type, determined by the provider at runtime.\nReturn TypeUse Case\nDynamic\tReturn any value type of data, determined at runtime."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.7.x/functions/implementation",
  "text": "Plugin Development - Framework: Implement Functions | Terraform\nProvider-defined function support is in technical preview and offered without compatibility promises until Terraform 1.8 is generally available.\nThe framework supports implementing functions based on Terraform's concepts for provider-defined functions. It is recommended to understand those concepts before implementing a function since the terminology is used throughout this page and there are details that simplify function handling as compared to other provider concepts.\nThe main code components of a function implementation are:\nDefining the function including its name, expected data types, descriptions, and logic.\nAdding the function to the provider so it is accessible by Terraform and practitioners.\nOnce the code is implemented, it is always recommended to also add:\nTesting to ensure expected function behaviors.\nDocumentation to ensure the function is discoverable by practitioners with usage information.\nImplement the function.Function interface. Each of the methods is described in more detail below.\nIn this example, a function named echo is defined, which takes a string argument and returns that value as the result:\nimport ( \"github.com/hashicorp/terraform-plugin-framework/function\" ) // Ensure the implementation satisfies the desired interfaces. var _ function.Function = &EchoFunction{} type EchoFunction struct {} func (f *EchoFunction) Metadata(ctx context.Context, req function.MetadataRequest, resp *function.MetadataResponse) { resp.Name = \"echo\" } func (f *EchoFunction) Definition(ctx context.Context, req function.DefinitionRequest, resp *function.DefinitionResponse) { resp.Definition = function.Definition{ Summary: \"Echo a string\", Description: \"Given a string value, returns the same value.\", Parameters: []function.Parameter{ function.StringParameter{ Name: \"input\", Description: \"Value to echo\", }, }, Return: function.StringReturn{}, } } func (f *EchoFunction) Run(ctx context.Context, req function.RunRequest, resp *function.RunResponse) { var input string // Read Terraform argument data into the variable resp.Error = function.ConcatFuncErrors(resp.Error, req.Arguments.Get(ctx, &input)) // Set the result to the same data resp.Error = function.ConcatFuncErrors(resp.Error, resp.Result.Set(ctx, input)) } \nMetadata Method\nThe function.Function interface Metadata method defines the function name as it would appear in Terraform configurations. Unlike resources and data sources, this name should NOT include the provider name as the configuration language syntax for calling functions will separately include the provider name. Refer to naming for additional best practice details.\nIn this example, the function name is set to example:\n// With the function.Function implementation func (f *ExampleFunction) Metadata(ctx context.Context, req function.MetadataRequest, resp *function.MetadataResponse) { resp.Name = \"example\" } \nDefinition Method\nThe function.Function interface Definition method defines the parameters, return, and various descriptions for documentation of the function.\nIn this example, the function definition includes one string parameter, a string return, and descriptions for documentation:\nfunc (f *ExampleFunction) Definition(ctx context.Context, req function.DefinitionRequest, resp *function.DefinitionResponse) { resp.Definition = function.Definition{ Summary: \"Echo a string\", Description: \"Given a string value, returns the same value.\", Parameters: []function.Parameter{ function.StringParameter{ Description: \"Value to echo\", Name: \"input\", }, }, Return: function.StringReturn{}, } } \nReturn\nThe Return field must be defined as all functions must return a result. This influences how the Run method must set the result data. Refer to the returns documentation for details about all available types and how to handle data with each type.\nParameters\nThere may be zero or more parameters, which are defined with the Parameters field. They are ordered, which influences how practitioners call the function in their configurations and how the Run method must read the argument data. Refer to the parameters documentation for details about all available types and how to handle data with each type.\nAn optional VariadicParameter field enables a final variadic parameter which accepts zero, one, or more values of the same type. It may be optionally combined with Parameters, meaning it represents the any argument data after the final parameter. When reading argument data, a VariadicParameter is represented as a tuple, with each element matching the parameter type; the tuple has zero or more elements to match the given arguments.\nBy default, Terraform will not pass null or unknown values to the provider logic when a function is called. Within each parameter, use the AllowNullValue and/or AllowUnknownValues fields to explicitly allow those kinds of values. Enabling AllowNullValue requires using a pointer type or framework type when reading argument data. Enabling AllowUnknownValues requires using a framework type when reading argument data.\nThe function documentation page describes how to implement documentation so it is available to Terraform, downstream tooling such as practitioner configuration editor integrations, and in the Terraform Registry.\nDeprecation\nIf a function is being deprecated, such as for future removal, the DeprecationMessage field should be set. The message should be actionable for practitioners, such as telling them what to do with their configuration instead of calling this function.\nRun Method\nThe function.Function interface Run method defines the logic that is invoked when Terraform calls the function. Only argument data is provided when a function is called. Refer to HashiCorp Provider Design Principles for additional best practice details.\nImplement the Run method by:\nCreating variables for argument data, based on the parameter definitions. Refer to the parameters documentation for details about all available parameter types and how to handle data with each type.\nReading argument data from the function.RunRequest.Arguments field.\nPerforming any computational logic.\nSetting the result value, based on the return definition, into the function.RunResponse.Result field. Refer to the returns documentation for details about all available return types and how to handle data with each type.\nIf the logic needs to return a function error, it can be added into the function.RunResponse.Error field.\nReading Argument Data\nThe framework supports two methodologies for reading argument data from the function.RunRequest.Arguments field, which is of the function.ArgumentsData type.\nThe first option is using the (function.ArgumentsData).Get() method to read all arguments at once. The framework will return an error if the number and types of target variables does not match the argument data.\nIn this example, the parameters are defined as a boolean and string which are read into Go built-in bool and string variables since they do not opt into null or unknown value handling:\nfunc (f *ExampleFunction) Definition(ctx context.Context, req function.DefinitionRequest, resp *function.DefinitionResponse) { resp.Definition = function.Definition{ // ... other fields ... Parameters: []function.Parameter{ function.BoolParameter{ Name: \"bool_param\", // ... other fields ... }, function.StringParameter{ Name: \"string_param\", // ... other fields ... }, }, } } func (f *ExampleFunction) Run(ctx context.Context, req function.RunRequest, resp *function.RunResponse) { var boolArg bool var stringArg string resp.Error = function.ConcatFuncErrors(resp.Error, req.Arguments.Get(ctx, &boolArg, &stringArg)) // ... other logic ... } \nThe second option is using (function.ArgumentsData).GetArgument() method to read individual arguments. The framework will return an error if the argument position does not exist or if the type of the target variable does not match the argument data.\nIn this example, the parameters are defined as a boolean and string and the first argument is read into a Go built-in bool variable since it does not opt into null or unknown value handling:\nfunc (f *ExampleFunction) Definition(ctx context.Context, req function.DefinitionRequest, resp *function.DefinitionResponse) { resp.Definition = function.Definition{ // ... other fields ... Parameters: []function.Parameter{ function.BoolParameter{ Name: \"bool_param\", // ... other fields ... }, function.StringParameter{ Name: \"string_param\", // ... other fields ... }, }, } } func (f *ExampleFunction) Run(ctx context.Context, req function.RunRequest, resp *function.RunResponse) { var boolArg bool resp.Error = function.ConcatFuncErrors(resp.Error, req.Arguments.GetArgument(ctx, 0, &boolArg)) // ... other logic ... } \nReading Variadic Parameter Argument Data\nThe optional VariadicParameter field in a function definition enables a final variadic parameter which accepts zero, one, or more values of the same type. It may be optionally combined with Parameters, meaning it represents the argument data after the final parameter. When reading argument data, a VariadicParameter is represented as a tuple, with each element matching the parameter type; the tuple has zero or more elements to match the given arguments.\nUse either the framework tuple type or a Go slice of an appropriate type to match the variadic parameter []T.\nIn this example, there is a boolean parameter and string variadic parameter, where the variadic parameter argument data is always fetched as a slice of string:\nfunc (f *ExampleFunction) Definition(ctx context.Context, req function.DefinitionRequest, resp *function.DefinitionResponse) { resp.Definition = function.Definition{ // ... other fields ... Parameters: []function.Parameter{ function.BoolParameter{ Name: \"bool_param\", // ... other fields ... }, }, VariadicParameter: function.StringParameter{ Name: \"variadic_param\", // ... other fields ... }, } } func (f *ExampleFunction) Run(ctx context.Context, req function.RunRequest, resp *function.RunResponse) { var boolArg bool var stringVarg []string resp.Error = function.ConcatFuncErrors(resp.Error, req.Arguments.Get(ctx, &boolArg, &stringVarg)) // ... other logic ... } \nIf it is necessary to return a function error for a specific variadic argument, note that Terraform treats each zero-based argument position individually unlike how the framework exposes the argument data. Add the number of non-variadic parameters (if any) to the variadic argument tuple element index to ensure the error is aligned to the correct argument in the configuration.\nIn this example with two parameters and one variadic parameter, an error is returned for variadic arguments:\nfunc (f *ExampleFunction) Definition(ctx context.Context, req function.DefinitionRequest, resp *function.DefinitionResponse) { resp.Definition = function.Definition{ // ... other fields ... Parameters: []function.Parameter{ function.BoolParameter{ Name: \"bool_param\", // ... other fields ... }, function.Int64Parameter{ Name: \"int64_param\", // ... other fields ... }, }, VariadicParameter: function.StringParameter{ Name: \"variadic_param\", // ... other fields ... }, } } func (f *ExampleFunction) Run(ctx context.Context, req function.RunRequest, resp *function.RunResponse) { var boolArg bool var int64Arg int64 var stringVarg []string resp.Error = function.ConcatFuncErrors(resp.Error, req.Arguments.Get(ctx, &boolArg, &int64arg, &stringVarg)) for index, element := range stringVarg { // Added by 2 to match the definition including two parameters. resp.Error = function.ConcatFuncErrors(resp.Error, function.NewArgumentFuncError(2+index, \"example summary: example detail\")) } // ... other logic ... } \nSetting Result Data\nThe framework supports setting a result value into the function.RunResponse.Result field, which is of the function.ResultData type. The result value must match the return type, otherwise the framework or Terraform will return an error.\nIn this example, the return is defined as a string and a string value is set:\nfunc (f *ExampleFunction) Definition(ctx context.Context, req function.DefinitionRequest, resp *function.DefinitionResponse) { resp.Definition = function.Definition{ // ... other fields ... Return: function.StringReturn{}, } } func (f *ExampleFunction) Run(ctx context.Context, req function.RunRequest, resp *function.RunResponse) { // ... other logic ... // Value based on the return type. Returns can also use the framework type system. result := \"hardcoded example\" resp.Error = function.ConcatFuncErrors(resp.Error, resp.Result.Set(ctx, result)) } \nFunctions become available to practitioners when they are included in the provider implementation via the provider.ProviderWithFunctions interface Functions method.\nIn this example, the EchoFunction type, which implements the function.Function interface, is added to the provider implementation:\n// With the provider.Provider implementation func (p *ExampleCloudProvider) Functions(_ context.Context) []func() function.Function { return []func() function.Function{ func() function.Function { return &EchoFunction{}, }, } } \nTo simplify provider implementations, a named function can be created with the function implementation.\nIn this example, the EchoFunction code includes an additional NewEchoFunction function, which simplifies the provider implementation:\n// With the provider.Provider implementation func (p *ExampleCloudProvider) Functions(_ context.Context) []func() function.Function { return []func() function.Function{ NewEchoFunction, } } // With the function.Function implementation func NewEchoFunction() function.Function { return &EchoFunction{} }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.5.x/functions/implementation",
  "text": "Plugin Development - Framework: Implement Functions | Terraform\nProvider-defined function support is in technical preview and offered without compatibility promises until Terraform 1.8 is generally available.\nThe framework supports implementing functions based on Terraform's concepts for provider-defined functions. It is recommended to understand those concepts before implementing a function since the terminology is used throughout this page and there are details that simplify function handling as compared to other provider concepts.\nThe main code components of a function implementation are:\nDefining the function including its name, expected data types, descriptions, and logic.\nAdding the function to the provider so it is accessible by Terraform and practitioners.\nOnce the code is implemented, it is always recommended to also add:\nTesting to ensure expected function behaviors.\nDocumentation to ensure the function is discoverable by practitioners with usage information.\nImplement the function.Function interface. Each of the methods is described in more detail below.\nIn this example, a function named echo is defined, which takes a string argument and returns that value as the result:\nimport ( \"github.com/hashicorp/terraform-plugin-framework/function\" ) // Ensure the implementation satisfies the desired interfaces. var _ function.Function = &EchoFunction{} type EchoFunction struct {} func (f *EchoFunction) Metadata(ctx context.Context, req function.MetadataRequest, resp *function.MetadataResponse) { resp.Name = \"echo\" } func (f *EchoFunction) Definition(ctx context.Context, req function.DefinitionRequest, resp *function.DefinitionResponse) { resp.Definition = function.Definition{ Summary: \"Echo a string\", Description: \"Given a string value, returns the same value.\", Parameters: []function.Parameter{ function.StringParameter{ Name: \"input\", Description: \"Value to echo\", }, }, Return: function.StringReturn{}, } } func (f *EchoFunction) Run(ctx context.Context, req function.RunRequest, resp *function.RunResponse) { var input string // Read Terraform argument data into the variable resp.Diagnostics.Append(req.Arguments.Get(ctx, &input)...) // Set the result to the same data resp.Diagnostics.Append(resp.Result.Set(ctx, &input)...) } \nMetadata Method\nThe function.Function interface Metadata method defines the function name as it would appear in Terraform configurations. Unlike resources and data sources, this name should NOT include the provider name as the configuration language syntax for calling functions will separately include the provider name. Refer to naming for additional best practice details.\nIn this example, the function name is set to example:\n// With the function.Function implementation func (f *ExampleFunction) Metadata(ctx context.Context, req function.MetadataRequest, resp *function.MetadataResponse) { resp.Name = \"example\" } \nDefinition Method\nThe function.Function interface Definition method defines the parameters, return, and various descriptions for documentation of the function.\nIn this example, the function definition includes one string parameter, a string return, and descriptions for documentation:\nfunc (f *ExampleFunction) Definition(ctx context.Context, req function.DefinitionRequest, resp *function.DefinitionResponse) { resp.Definition = function.Definition{ Summary: \"Echo a string\", Description: \"Given a string value, returns the same value.\", Parameters: []function.Parameter{ function.StringParameter{ Description: \"Value to echo\", Name: \"input\", }, }, Return: function.StringReturn{}, } } \nReturn\nThe Return field must be defined as all functions must return a result. This influences how the Run method must set the result data. Refer to the returns documentation for details about all available types and how to handle data with each type.\nParameters\nThere may be zero or more parameters, which are defined with the Parameters field. They are ordered, which influences how practitioners call the function in their configurations and how the Run method must read the argument data. Refer to the parameters documentation for details about all available types and how to handle data with each type.\nAn optional VariadicParameter field enables a final variadic parameter which accepts zero, one, or more values of the same type. It may be optionally combined with Parameters, meaning it represents the any argument data after the final parameter. When reading argument data, a VariadicParameter is represented as an ordered list of the parameter type, where the list has zero or more elements to match the given arguments.\nBy default, Terraform will not pass null or unknown values to the provider logic when a function is called. Within each parameter, use the AllowNullValue and/or AllowUnknownValues fields to explicitly allow those kinds of values. Enabling AllowNullValue requires using a pointer type or framework type when reading argument data. Enabling AllowUnknownValues requires using a framework type when reading argument data.\nThe function documentation page describes how to implement documentation so it is available to Terraform, downstream tooling such as practitioner configuration editor integrations, and in the Terraform Registry.\nDeprecation\nIf a function is being deprecated, such as for future removal, the DeprecationMessage field should be set. The message should be actionable for practitioners, such as telling them what to do with their configuration instead of calling this function.\nRun Method\nThe function.Function interface Run method defines the logic that is invoked when Terraform calls the function. Only argument data is provided when a function is called. Refer to HashiCorp Provider Design Principles for additional best practice details.\nImplement the Run method by:\nCreating variables for argument data, based on the parameter definitions. Refer to the parameters documentation for details about all available parameter types and how to handle data with each type.\nReading argument data from the function.RunRequest.Arguments field.\nPerforming any computational logic.\nSetting the result value, based on the return definition, into the function.RunResponse.Result field. Refer to the returns documentation for details about all available return types and how to handle data with each type.\nIf the logic needs to return warning or error diagnostics, they can be added into the function.RunResponse.Diagnostics field.\nReading Argument Data\nThe framework supports two methodologies for reading argument data from the function.RunRequest.Arguments field, which is of the function.ArgumentsData type.\nThe first option is using the (function.ArgumentsData).Get() method to read all arguments at once. The framework will return errors if the number and types of target variables does not match the argument data.\nIn this example, the parameters are defined as a boolean and string which are read into Go built-in bool and string variables since they do not opt into null or unknown value handling:\nfunc (f *ExampleFunction) Definition(ctx context.Context, req function.DefinitionRequest, resp *function.DefinitionResponse) { resp.Definition = function.Definition{ // ... other fields ... Parameters: []function.Parameter{ function.BoolParameter{}, function.StringParameter{}, }, } } func (f *ExampleFunction) Run(ctx context.Context, req function.RunRequest, resp *function.RunResponse) { var boolArg bool var stringArg string resp.Diagnostics.Append(req.Arguments.Get(ctx, &boolArg, &stringArg)...) // ... other logic ... } \nThe second option is using (function.ArgumentsData).GetArgument() method to read individual arguments. The framework will return errors if the argument position does not exist or if the type of the target variable does not match the argument data.\nIn this example, the parameters are defined as a boolean and string and the first argument is read into a Go built-in bool variable since it does not opt into null or unknown value handling:\nfunc (f *ExampleFunction) Definition(ctx context.Context, req function.DefinitionRequest, resp *function.DefinitionResponse) { resp.Definition = function.Definition{ // ... other fields ... Parameters: []function.Parameter{ function.BoolParameter{}, function.StringParameter{}, }, } } func (f *ExampleFunction) Run(ctx context.Context, req function.RunRequest, resp *function.RunResponse) { var boolArg bool resp.Diagnostics.Append(req.Arguments.GetArgument(ctx, 0, &boolArg)...) // ... other logic ... } \nReading Variadic Parameter Argument Data\nThe optional VariadicParameter field in a function definition enables a final variadic parameter which accepts zero, one, or more values of the same type. It may be optionally combined with Parameters, meaning it represents the argument data after the final parameter. When reading argument data, a VariadicParameter is represented as an ordered list of the parameter type, where the list has zero or more elements to match the given arguments.\nUse either the framework list type or a Go slice of an appropriate type to match the variadic parameter []T.\nIn this example, there is a boolean parameter and string variadic parameter, where the variadic parameter argument data is always fetched as a list:\nfunc (f *ExampleFunction) Definition(ctx context.Context, req function.DefinitionRequest, resp *function.DefinitionResponse) { resp.Definition = function.Definition{ // ... other fields ... Parameters: []function.Parameter{ function.BoolParameter{}, }, VariadicParameter: function.StringParameter{}, } } func (f *ExampleFunction) Run(ctx context.Context, req function.RunRequest, resp *function.RunResponse) { var boolArg bool var stringVarg []string resp.Diagnostics.Append(req.Arguments.Get(ctx, &boolArg, &stringVarg)...) // ... other logic ... } \nIf necessary to return diagnostics for a specific variadic argument, note that Terraform treats each zero-based argument position individually unlike how the framework exposes the argument data. Add the number of non-variadic parameters (if any) to the variadic argument list element index to ensure the diagnostic is aligned to the correct argument in the configuration.\nIn this example with two parameters and one variadic parameter, warning diagnostics are returned for all variadic arguments:\nfunc (f *ExampleFunction) Definition(ctx context.Context, req function.DefinitionRequest, resp *function.DefinitionResponse) { resp.Definition = function.Definition{ // ... other fields ... Parameters: []function.Parameter{ function.BoolParameter{}, function.Int64Parameter{}, }, VariadicParameter: function.StringParameter{}, } } func (f *ExampleFunction) Run(ctx context.Context, req function.RunRequest, resp *function.RunResponse) { var boolArg bool var int64Arg int64 var stringVarg []string resp.Diagnostics.Append(req.Arguments.Get(ctx, &boolArg, &int64arg, &stringVarg)...) for index, element := range stringVarg { // Added by 2 to match the definition including two parameters. resp.Diagnostic.AddArgumentWarning(2+index, \"example summary\", \"example detail\") } // ... other logic ... } \nSetting Result Data\nThe framework supports setting a result value into the function.RunResponse.Result field, which is of the function.ResultData type. The result value must match the return type, otherwise the framework or Terraform will return an error.\nIn this example, the return is defined as a string and a string value is set:\nfunc (f *ExampleFunction) Definition(ctx context.Context, req function.DefinitionRequest, resp *function.DefinitionResponse) { resp.Definition = function.Definition{ // ... other fields ... Return: function.StringReturn{}, } } func (f *ExampleFunction) Run(ctx context.Context, req function.RunRequest, resp *function.RunResponse) { // ... other logic ... // Value based on the return type. Returns can also use the framework type system. result := \"hardcoded example\" resp.Diagnostics.Append(resp.Result.Set(ctx, result)...) } \nFunctions become available to practitioners when they are included in the provider implementation via the provider.ProviderWithFunctions interface Functions method.\nIn this example, the EchoFunction type, which implements the function.Function interface, is added to the provider implementation:\n// With the provider.Provider implementation func (p *ExampleCloudProvider) Functions(_ context.Context) []func() function.Function { return []func() function.Function{ func() function.Function { return &EchoFunction{}, }, } } \nTo simplify provider implementations, a named function can be created with the function implementation.\nIn this example, the EchoFunction code includes an additional NewEchoFunction function, which simplifies the provider implementation:\n// With the provider.Provider implementation func (p *ExampleCloudProvider) Functions(_ context.Context) []func() function.Function { return []func() function.Function{ NewEchoFunction, } } // With the function.Function implementation func NewEchoFunction() function.Function { return &EchoFunction{} }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/functions/parameters",
  "text": "Plugin Development - Framework: Function Parameters | Terraform\nParameters in function definitions describes how data values are passed to the function logic. Every parameter type has an associated value type, although this data handling is simplified for function implementations over other provider concepts, such as resource implementations.\nFunction definitions support the following parameter types:\nPrimitive: Parameter that accepts a single value, such as a boolean, number, or string.\nCollection: Parameter that accepts multiple values of a single element type, such as a list, map, or set.\nObject: Parameter that accepts a structure of explicit attribute names.\nDynamic: Parameter that accepts any value type.\nPrimitive Parameter Types\nParameter types that accepts a single data value, such as a boolean, number, or string.\nParameter TypeUse Case\nBool\tBoolean true or false\t\nFloat32\t32-bit floating point number\t\nFloat64\t64-bit floating point number\t\nInt32\t32-bit integer number\t\nInt64\t64-bit integer number\t\nNumber\tArbitrary precision (generally over 64-bit, up to 512-bit) number\t\nString\tCollection of UTF-8 encoded characters\t\nCollection Parameter Types\nParameter types that accepts multiple values of a single element type, such as a list, map, or set.\nParameter TypeUse Case\nList\tOrdered collection of single element type\t\nMap\tMapping of arbitrary string keys to values of single element type\t\nSet\tUnordered, unique collection of single element type\t\nObject Parameter Type\nParameter type that accepts a structure of explicit attribute names.\nParameter TypeUse Case\nObject\tSingle structure mapping explicit attribute names\t\nDynamic Parameter Type\nNote\nDynamic value handling is an advanced use case. Prefer static parameter types when possible unless absolutely necessary for your use case.\nParameter that accepts any value type, determined by Terraform at runtime.\nParameter TypeUse Case\nDynamic\tAccept any value type of data, determined at runtime.\t\nAll parameter types have a Name field that is required.\nMissing Parameter Names\nAttempting to use unnamed parameters will generate runtime errors of the following form:\n Error: Failed to load plugin schemas   Error while loading schemas for plugin components: Failed to obtain provider schema: Could not load the schema for provider registry.terraform.io/cloud_provider/cloud_resource: failed to  retrieve schema from provider \"registry.terraform.io/cloud_provider/cloud_resource\": Invalid Function Definition: When validating the function definition, an implementation issue was  found. This is always an issue with the provider and should be reported to the provider developers.   Function \"example_function\" - Parameter at position 0 does not have a name. \nParameter Errors\nParameter names are used in runtime errors to highlight which parameter is causing the issue. For example, using a value that is incompatible with the parameter type will generate an error message such as the following:\n Error: Invalid function argument   on resource.tf line 10, in resource \"example_resource\" \"example\":  10: configurable_attribute = provider::example::example_function(\"string\")     while calling provider::example::example_function(bool_param)   Invalid value for \"bool_param\" parameter: a bool is required. \nValidation handling for provider-defined function parameters can be enabled by using custom types.\nImplement the function.ValidateableParameter interface on the custom value type to define and enable validation handling for a provider-defined function parameter, which will automatically raise an error when a value is determined to be invalid.\n// Implementation of the function.ValidateableParameter interface func (v CustomStringValue) ValidateParameter(ctx context.Context, req function.ValidateParameterRequest, resp *function.ValidateParameterResponse) { if v.IsNull() || v.IsUnknown() { return } _, err := time.Parse(time.RFC3339, v.ValueString()) if err != nil { resp.Error = function.NewArgumentFuncError( req.Position, \"Invalid RFC 3339 String Value: \"+ \"An unexpected error occurred while converting a string value that was expected to be RFC 3339 format. \"+ \"The RFC 3339 string format is YYYY-MM-DDTHH:MM:SSZ, such as 2006-01-02T15:04:05Z or 2006-01-02T15:04:05+07:00.\\n\\n\"+ fmt.Sprintf(\"Position: %d\", req.Position)+\"\\n\"+ \"Given Value: \"+v.ValueString()+\"\\n\"+ \"Error: \"+err.Error(), ) } } \nRefer to Custom Types for further details on creating provider-defined types and values"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.7.x/functions/documentation",
  "text": "Plugin Development - Framework: Document Functions | Terraform\nWhen a function is implemented, ensure the function is discoverable by practitioners with usage information.\nThere are two main components for function documentation:\nImplementation-Based Documentation: Exposes function documentation to Terraform and downstream tooling, such as practitioner configuration editor integrations.\nRegistry-Based Documentation: Exposes function documentation to the Terraform Registry when the provider is published, making it displayed and discoverable on the web.\nAdd documentation directly inside the function definition. All implementation-based documentation is passed to Terraform, which downstream tooling such as pracitioner configuration editor integrations will automatically display.\nDefinition\nThe function.Definition type implements the following fields:\nField NameDescription\nSummary\tA short description of the function and its return, preferably a single sentence.\t\nDescription\tLonger documentation about the function, its return, and pertinent implementation details in plaintext format.\t\nMarkdownDescription\tLonger documentation about the function, its return, and pertinent implementation details in Markdown format.\t\nIf there are no description formatting differences, set only one of Description or MarkdownDescription. When Terraform has not sent a preference for the description formatting, the framework will return MarkdownDescription if both are defined.\nIn this example, the function definition sets summary and description documentation:\nfunc (f *CidrContainsIpFunction) Definition(ctx context.Context, req function.DefinitionRequest, resp *function.DefinitionResponse) { resp.Definition = function.Definition{ // ... other fields ... Summary: \"Check if a network CIDR contains an IP\", Description: \"Returns a boolean whether a RFC4632 CIDR contains an IP address\", } } \nParameters\nEach parameter type, whether in the definition Parameters or VariadicParameter field, implements the following fields:\nField NameDescription\nName\tRequired: Single word or abbreviation of parameter for function signature generation. If name is not provided, a runtime error will be generated.\t\nDescription\tDocumentation about the parameter and its expected values in plaintext format.\t\nMarkdownDescription\tDocumentation about the parameter and its expected values in Markdown format.\t\nThe name must be unique in the context of the function definition. It is used for documentation purposes and displayed in error diagnostics presented to practitioners. The name should delineate the purpose of the parameter, especially to disambiguate between multiple parameters, such as the words cidr and ip in a generated function signature like cidr_contains_ip(cidr string, ip string) bool.\nIf there are no description formatting differences, set only one of Description or MarkdownDescription. When Terraform has not sent a preference for the description formatting, the framework will return MarkdownDescription if both are defined.\nIn this example, the function parameters set name and description documentation:\nfunc (f *CidrContainsIpFunction) Definition(ctx context.Context, req function.DefinitionRequest, resp *function.DefinitionResponse) { resp.Definition = function.Definition{ // ... other fields ... Parameters: []function.Parameter{ function.StringParameter{ Name: \"cidr\", Description: \"RFC4632 CIDR to check whether it contains the given IP address\", }, function.StringParameter{ Name: \"ip\", Description: \"IP address to check whether its contained in the RFC4632 CIDR\", }, }, } } \nAdd Markdown documentation files in conventional provider codebase locations before publishing to the Terraform Registry. The documentation is displayed and discoverable on the web. These files can be manually created or automatically generated using tooling such as terraform-plugin-docs.\nThe Registry provider documentation covers the overall requirements, conventional file layout details, and how to enable additional features such as sub-categories for the navigation sidebar. Function documentation for most providers is expected under the docs/functions/ directory with a file named after the function and with the extension .md. Older providers using the legacy file layout use website/docs/functions/ and .html.md.\nFunctions are conventionally documented with the following:\nDescription\nExample Usage\nSignature\nArguments\nIn this example, a docs/functions/contains_ip.md file (either manually or automatically created) will be displayed in the Terraform Registry after provider publishing:\n--- page_title: contains_ip Function - terraform-provider-cidr description: |- Returns a boolean whether a RFC4632 CIDR contains an IP address. --- # Function: contains_ip Returns a boolean whether a RFC4632 CIDR contains an IP address. ## Example Usage ```terraform # result: true provider::cidr::contains_ip(\"10.0.0.0/8\", \"10.0.0.1\") ``` ## Signature ```text contains_ip(cidr string, ip string) bool ``` ## Arguments 1. `cidr` (String) RFC4632 CIDR to check whether it contains the given IP address. 2. `ip` (String) IP address to check whether its contained in the RFC4632 CIDR."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.5.x/functions/documentation",
  "text": "Plugin Development - Framework: Document Functions | Terraform\nWhen a function is implemented, ensure the function is discoverable by practitioners with usage information.\nThere are two main components for function documentation:\nImplementation-Based Documentation: Exposes function documentation to Terraform and downstream tooling, such as practitioner configuration editor integrations.\nRegistry-Based Documentation: Exposes function documentation to the Terraform Registry when the provider is published, making it displayed and discoverable on the web.\nAdd documentation directly inside the function definition. All implementation-based documentation is passed to Terraform, which downstream tooling such as pracitioner configuration editor integrations will automatically display.\nDefinition\nThe function.Definition type implements the following fields:\nField NameDescription\nSummary\tA short description of the function and its return, preferably a single sentence.\t\nDescription\tLonger documentation about the function, its return, and pertinent implementation details in plaintext format.\t\nMarkdownDescription\tLonger documentation about the function, its return, and pertinent implementation details in Markdown format.\t\nIf there are no description formatting differences, set only one of Description or MarkdownDescription. When Terraform has not sent a preference for the description formatting, the framework will return MarkdownDescription if both are defined.\nIn this example, the function definition sets summary and description documentation:\nfunc (f *CidrContainsIpFunction) Definition(ctx context.Context, req function.DefinitionRequest, resp *function.DefinitionResponse) { resp.Definition = function.Definition{ // ... other fields ... Summary: \"Check if a network CIDR contains an IP\", Description: \"Returns a boolean whether a RFC4632 CIDR contains an IP address\", } } \nParameters\nEach parameter type, whether in the definition Parameters or VariadicParameter field, implements the following fields:\nField NameDescription\nName\tSingle word or abbreviation of parameter for function signature generation, defaults to param.\t\nDescription\tDocumentation about the parameter and its expected values in plaintext format.\t\nMarkdownDescription\tDocumentation about the parameter and its expected values in Markdown format.\t\nThe name is only for documentation purposes and helpful when there is a need to disambiguate between multiple parameters, such as the words cidr and ip in a generated function signature like cidr_contains_ip(cidr string, ip string) bool.\nIf there are no description formatting differences, set only one of Description or MarkdownDescription. When Terraform has not sent a preference for the description formatting, the framework will return MarkdownDescription if both are defined.\nIn this example, the function parameters set name and description documentation:\nfunc (f *CidrContainsIpFunction) Definition(ctx context.Context, req function.DefinitionRequest, resp *function.DefinitionResponse) { resp.Definition = function.Definition{ // ... other fields ... Parameters: []function.Parameter{ function.StringParameter{ Name: \"cidr\", Description: \"RFC4632 CIDR to check whether it contains the given IP address\", }, function.StringParameter{ Name: \"ip\", Description: \"IP address to check whether its contained in the RFC4632 CIDR\", }, }, } } \nAdd Markdown documentation files in conventional provider codebase locations before publishing to the Terraform Registry. The documentation is displayed and discoverable on the web. These files can be manually created or automatically generated using tooling such as terraform-plugin-docs.\nThe Registry provider documentation covers the overall requirements, conventional file layout details, and how to enable additional features such as sub-categories for the navigation sidebar. Function documentation for most providers is expected under the docs/functions/ directory with a file named after the function and with the extension .md. Older providers using the legacy file layout use website/docs/functions/ and .html.md.\nFunctions are conventionally documented with the following:\nDescription\nExample Usage\nSignature\nArguments\nIn this example, a docs/functions/contains_ip.md file (either manually or automatically created) will be displayed in the Terraform Registry after provider publishing:\n--- page_title: contains_ip Function - terraform-provider-cidr description: |- Returns a boolean whether a RFC4632 CIDR contains an IP address. --- # Function: contains_ip Returns a boolean whether a RFC4632 CIDR contains an IP address. ## Example Usage ```terraform # result: true provider::cidr::contains_ip(\"10.0.0.0/8\", \"10.0.0.1\") ``` ## Signature ```text contains_ip(cidr string, ip string) bool ``` ## Arguments 1. `cidr` (String) RFC4632 CIDR to check whether it contains the given IP address. 2. `ip` (String) IP address to check whether its contained in the RFC4632 CIDR."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.7.x/functions/testing",
  "text": "Plugin Development - Framework: Testing Functions | Terraform\nWhen a function is implemented, ensure the function behaves as expected. Follow recommendations to cover how practitioner configurations may call the function.\nThere are two methodologies for testing provider-defined functions:\nAcceptance Testing: Verify implementation using real Terraform configurations and commands.\nUnit Testing: Verify implementation using with Terraform and framework implementation details.\nSimilar to other provider concepts, many provider developers prefer acceptance testing over unit testing. Acceptance testing guarantees the function implementation works exactly as expected in real world use cases without trying to determine Terraform or framework implementation details. Unit testing details are provided, however, for function implementations which warrant a broad amount of input value testing, such as generic data handling functions or to perform fuzzing.\nTesting examples on this page are dependent on the example echo function implementation.\nTesting a provider-defined function should ensure at least the following behaviors are covered:\nKnown values return the expected results.\nFor any list, map, object, and set parameters, null values for collection elements or object attributes. The AllowNullValue parameter setting does not affect Terraform sending these types of null values.\nIf any parameters enable AllowNullValue, null values for those arguments.\nIf any parameters enable AllowUnknownValues, unknown values for those arguments.\nAny errors, such as argument validation errors.\nUse the plugin testing Go module to implement real world testing with Terraform configurations and commands. The documentation for that Go module covers many more available testing features, however this section example gives a high level overview of how to start writing these tests.\nIn this example, a echo_function_test.go file is created:\npackage provider_test import ( \"testing\" \"example.com/terraform-provider-example/internal/provider\" \"github.com/hashicorp/terraform-plugin-framework/providerserver\" \"github.com/hashicorp/terraform-plugin-go/tfprotov6\" \"github.com/hashicorp/terraform-plugin-testing/helper/resource\" \"github.com/hashicorp/terraform-plugin-testing/tfversion\" ) func TestEchoFunction_Valid(t *testing.T) { t.Parallel() resource.UnitTest(t, resource.TestCase{ TerraformVersionChecks: []tfversion.TerraformVersionCheck{ tfversion.SkipBelow(tfversion.Version1_8_0), }, ProtoV6ProviderFactories: map[string]func() (tfprotov6.ProviderServer, error) { \"example\": providerserver.NewProtocol6WithError(provider.New()), }, Steps: []resource.TestStep{ { Config: ` output \"test\" { value = provider::example::echo(\"test-value\") } `, Check: resource.TestCheckOutput(\"test\", \"test-value\"), }, }, }) } // The example implementation does not return any errors, however // this acceptance test verifies how the function should behave if it did. func TestEchoFunction_Invalid(t *testing.T) { t.Parallel() resource.UnitTest(t, resource.TestCase{ TerraformVersionChecks: []tfversion.TerraformVersionCheck{ tfversion.SkipBelow(tfversion.Version1_8_0), }, ProtoV6ProviderFactories: map[string]func() (tfprotov6.ProviderServer, error) { \"example\": providerserver.NewProtocol6WithError(provider.New()), }, Steps: []resource.TestStep{ { Config: ` output \"test\" { value = provider::example::echo(\"invalid\") } `, ExpectError: regexp.MustCompile(`error summary`), }, }, }) } // The example implementation does not enable AllowNullValue, however this // acceptance test shows how to verify the behavior. func TestEchoFunction_Null(t *testing.T) { t.Parallel() resource.UnitTest(t, resource.TestCase{ TerraformVersionChecks: []tfversion.TerraformVersionCheck{ tfversion.SkipBelow(tfversion.Version1_8_0), }, ProtoV6ProviderFactories: map[string]func() (tfprotov6.ProviderServer, error) { \"example\": providerserver.NewProtocol6WithError(provider.New()), }, Steps: []resource.TestStep{ { Config: ` output \"test\" { value = provider::example::echo(null) } `, ExpectError: regexp.MustCompile(`Invalid Function Call`), }, }, }) } // The example implementation does not enable AllowUnknownValues, however this // acceptance test shows how to verify the behavior. func TestEchoFunction_Unknown(t *testing.T) { resource.UnitTest(t, resource.TestCase{ TerraformVersionChecks: []tfversion.TerraformVersionCheck{ tfversion.SkipBelow(tfversion.Version1_8_0), }, ProtoV6ProviderFactories: map[string]func() (tfprotov6.ProviderServer, error) { \"example\": providerserver.NewProtocol6WithError(provider.New()), }, Steps: []resource.TestStep{ { Config: ` terraform_data \"test\" { input = \"test-value\" } output \"test\" { value = provider::example::echo(terraform_data.test.output) } `, Check: resource.TestCheckOutput(\"test\", \"test-value\"), ConfigPlanChecks: resource.ConfigPlanChecks{ PreApply: []plancheck.PlanCheck{ plancheck.ExpectUnknownOutputValue(\"test\"), }, }, }, }, }) } \nUse the function.NewArgumentsData() function and function.NewResultData() function as part of implementing a Go test.\nIn this example, a echo_function_test.go file is created:\npackage provider_test import ( \"context\" \"testing\" \"example.com/terraform-provider-example/internal/provider\" \"github.com/google/go-cmp/cmp\" \"github.com/hashicorp/terraform-plugin-framework/function\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) func TestEchoFunctionRun(t *testing.T) { t.Parallel() testCases := map[string]struct { request function.RunRequest expected function.RunResponse }{ // The example implementation uses the Go built-in string type, however // if AllowNullValue was enabled and *string or types.String was used, // this test case shows how the function would be expected to behave. \"null\": { request: function.RunRequest{ Arguments: function.NewArgumentsData([]attr.Value{types.StringNull()}), }, expected: function.RunResponse{ Result: function.NewResultData(types.StringNull()), }, }, // The example implementation uses the Go built-in string type, however // if AllowUnknownValues was enabled and types.String was used, // this test case shows how the function would be expected to behave. \"unknown\": { request: function.RunRequest{ Arguments: function.NewArgumentsData([]attr.Value{types.StringUnknown()}), }, expected: function.RunResponse{ Result: function.NewResultData(types.StringUnknown()), }, }, \"value-valid\": { request: function.RunRequest{ Arguments: function.NewArgumentsData([]attr.Value{types.StringValue(\"test-value\")}), }, expected: function.RunResponse{ Result: function.NewResultData(types.StringValue(\"test-value\")), }, }, // The example implementation does not return an error, however // this test case shows how the function would be expected to behave if // it did. \"value-invalid\": { request: function.RunRequest{ Arguments: function.NewArgumentsData([]attr.Value{types.StringValue(\"\")}), }, expected: function.RunResponse{ Error: function.NewArgumentFuncError(0, \"error summary: error detail\"), Result: function.NewResultData(types.StringUnknown()), }, }, } for name, testCase := range testCases { name, testCase := name, testCase t.Run(name, func(t *testing.T) { t.Parallel() got := function.RunResponse{ Result: function.NewResultData(types.StringUnknown()), } provider.EchoFunction{}.Run(context.Background(), testCase.request, &got) if diff := cmp.Diff(got, testCase.expected); diff != \"\" { t.Errorf(\"unexpected difference: %s\", diff) } }) } }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/data-handling/schemas",
  "text": "HashiCorp Developer\nWe couldn't find the page you're looking for.\nPlease check the url you entered for typos, go back to the page you came from, or go to one of the pages below.\nHashiCorp Cloud Platform\nTerraform\nPacker\nConsul\nVault\nBoundary\nNomad\nWaypoint\nVagrant\nHashiCorp Developer"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.7.x/resources/create",
  "text": "Plugin Development - Framework: Create Resources | Terraform\nCreation is part of the basic Terraform lifecycle for managing resources. During the terraform apply command, Terraform calls the provider ApplyResourceChange RPC, in which the framework calls the resource.Resource interface Create method. The request contains Terraform configuration and plan data. The response expects the applied Terraform state data, including any computed values. The data is defined by the schema of the resource.\nOther resource lifecycle implementations include:\nRead resources by receiving Terraform prior state data, performing read logic, and saving refreshed Terraform state data.\nUpdate resources in-place by receiving Terraform prior state, configuration, and plan data, performing update logic, and saving updated Terraform state data.\nDelete resources by receiving Terraform prior state data and performing deletion logic.\nImplement the Create method by:\nAccessing the data from the resource.CreateRequest type. Most use cases should access the plan data in the resource.CreateRequest.Plan field.\nPerforming logic or external calls to create and/or run the resource.\nWriting state data into the resource.CreateResponse.State field.\nIf the logic needs to return warning or error diagnostics, they can added into the resource.CreateResponse.Diagnostics field.\nIn this example, the resource is setup to accept a configuration value that is sent in a service API creation call:\n// ThingResource defines the resource implementation. // Some resource.Resource interface methods are omitted for brevity. type ThingResource struct { // client is configured via a Configure method, which is not shown in this // example for brevity. Refer to the Configure Resources documentation for // additional details for setting up resources with external clients. client *http.Client } // ThingResourceModel describes the Terraform resource data model to match the // resource schema. type ThingResourceModel struct { Name types.String `tfsdk:\"name\"` Id types.String `tfsdk:\"id\"` } // ThingResourceAPIModel describes the API data model. type ThingResourceAPIModel struct { Name string `json:\"name\"` Id string `json:\"id\"` } func (r ThingResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"name\": schema.StringAttribute{ MarkdownDescription: \"Name of the thing to be saved in the service.\", Required: true, }, \"id\": schema.StringAttribute{ Computed: true, MarkdownDescription: \"Service generated identifier for the thing.\", PlanModifiers: planmodifier.String{ stringplanmodifier.UseStateForUnknown(), }, }, }, MarkdownDescription: \"Manages a thing.\", } } func (r ThingResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data ThingResourceModel // Read Terraform plan data into the model resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) if resp.Diagnostics.HasError() { return } // Convert from Terraform data model into API data model createReq := ThingResourceAPIModel{ Name: data.Name.ValueString(), } httpReqBody, err := json.Marshal(createReq) if err != nil { resp.Diagnostics.AddError( \"Unable to Create Resource\", \"An unexpected error occurred while creating the resource create request. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Create HTTP request httpReq := http.NewRequestWithContext( ctx, http.MethodPost, \"http://example.com/things\", bytes.NewBuffer(httpReqBody), ) // Send HTTP request httpResp, err := d.client.Do(httpReq) defer httpResp.Body.Close() if err != nil { resp.Diagnostics.AddError( \"Unable to Create Resource\", \"An unexpected error occurred while attempting to create the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Error: \"+err.Error(), ) return } // Return error if the HTTP status code is not 200 OK if httpResp.StatusCode != http.StatusOK { resp.Diagnostics.AddError( \"Unable to Create Resource\", \"An unexpected error occurred while attempting to create the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Status: \"+httpResp.Status, ) return } var createResp ThingResourceAPIModel err := json.NewDecoder(httpResp.Body).Decode(&createResp) if err != nil { resp.Diagnostics.AddError( \"Unable to Create Resource\", \"An unexpected error occurred while parsing the resource creation response. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Convert from the API data model to the Terraform data model // and set any unknown attribute values. data.Id = types.StringValue(createResp.Id) // Save data into Terraform state resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } \nNote these caveats when implementing the Create method:\nAn error is returned if the response state contains unknown values. Set all attributes to either null or known values in the response.\nAn error is returned if the response state has the RemoveResource() method called. This method is not valid during creation.\nAn error is returned unless every null or known value in the request plan is saved exactly as-is into the response state. Only unknown plan values can be modified.\nAny response errors will cause Terraform to mark the resource as tainted for recreation on the next Terraform plan.\nNote these recommendations when implementing the Create method:\nGet request data from the Terraform plan data over configuration data as the schema or resource may include plan modification logic which sets plan values.\nReturn errors that signify there is an existing resource. Terraform practitioners expect to be notified if an existing resource needs to be imported into Terraform rather than created. This prevents situations where multiple Terraform configurations unexpectedly manage the same underlying resource."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.7.x/resources/read",
  "text": "Plugin Development - Framework: Read Resources | Terraform\nRead (refresh) is part of the basic Terraform lifecycle for managing resources. During the terraform apply, terraform plan, and terraform refresh commands, Terraform calls the provider ReadResource RPC, in which the framework calls the resource.Resource interface Read method. The Read method is also executed after resource import. The request contains Terraform prior state data. The response contains the refreshed state data. The data is defined by the schema of the resource.\nOther resource lifecycle implementations include:\nCreate resources by receiving Terraform configuration and plan data, performing creation logic, and saving Terraform state data.\nUpdate resources in-place by receiving Terraform prior state, configuration, and plan data, performing update logic, and saving updated Terraform state data.\nDelete resources by receiving Terraform prior state data and performing deletion logic.\nImplement the Read method by:\nAccessing prior state data from the resource.ReadRequest.State field.\nRetriving updated resource state, such as remote system information.\nWriting state data into the resource.ReadResponse.State field.\nIf the logic needs to return warning or error diagnostics, they can added into the resource.ReadResponse.Diagnostics field.\nIn this example, the Read function makes a HTTP call and refreshes the state data if the status code was 200 OK or removes the resource if 404 Not Found:\n// ThingResource defines the resource implementation. // Some resource.Resource interface methods are omitted for brevity. type ThingResource struct { // client is configured via a Configure method, which is not shown in this // example for brevity. Refer to the Configure Resources documentation for // additional details for setting up resources with external clients. client *http.Client } // ThingResourceModel describes the Terraform resource data model to match the // resource schema. type ThingResourceModel struct { Name types.String `tfsdk:\"name\"` Id types.String `tfsdk:\"id\"` } // ThingResourceAPIModel describes the API data model. type ThingResourceAPIModel struct { Name string `json:\"name\"` Id string `json:\"id\"` } func (r ThingResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"name\": schema.StringAttribute{ MarkdownDescription: \"Name of the thing to be saved in the service.\", Required: true, }, \"id\": schema.StringAttribute{ Computed: true, MarkdownDescription: \"Service generated identifier for the thing.\", PlanModifiers: planmodifier.String{ stringplanmodifier.UseStateForUnknown(), }, }, }, MarkdownDescription: \"Manages a thing.\", } } func (r *ThingResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { var data ThingResourceModel // Read Terraform prior state data into the model resp.Diagnostics.Append(req.State.Get(ctx, &data)...) // Convert from Terraform data model into API data model readReq := ThingResourceAPIModel{ Id: data.Id.ValueString(), Name: data.Name.ValueString(), } httpReqBody, err := json.Marshal(readReq) if err != nil { resp.Diagnostics.AddError( \"Unable to Refresh Resource\", \"An unexpected error occurred while creating the resource read request. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Create HTTP request httpReq := http.NewRequestWithContext( ctx, http.MethodPut, \"http://example.com/things\", bytes.NewBuffer(httpReqBody), ) httpResp, err := d.client.Do(httpReq) defer httpResp.Body.Close() if err != nil { resp.Diagnostics.AddError( \"Unable to Refresh Resource\", \"An unexpected error occurred while attempting to refresh resource state. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Error: \"+err.Error(), ) return } // Treat HTTP 404 Not Found status as a signal to recreate resource // and return early if httpResp.StatusCode == http.StatusNotFound { resp.State.RemoveResource(ctx) return } var readResp ThingResourceAPIModel err := json.NewDecoder(httpResp.Body).Decode(&readResp) if err != nil { resp.Diagnostics.AddError( \"Unable to Refresh Resource\", \"An unexpected error occurred while parsing the resource read response. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Convert from the API data model to the Terraform data model // and refresh any attribute values. data.Name = types.StringValue(readResp.Name) // Save updated data into Terraform state resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } \nNote these caveats when implementing the Read method:\nAn error is returned if the response state contains unknown values. Set all attributes to either null or known values in the response.\nAny response errors will cause Terraform to keep the prior resource state.\nNote these recommendations when implementing the Read method:\nIgnore returning errors that signify the resource is no longer existent, call the response state RemoveResource() method, and return early. The next Terraform plan will recreate the resource.\nRefresh all possible values. This will ensure Terraform shows configuration drift and reduces import logic.\nPreserve the prior state value if the updated value is semantically equal. For example, JSON strings that have inconsequential object property reordering or whitespace differences. This prevents Terraform from showing extraneous drift in plans."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.7.x/resources/update",
  "text": "Plugin Development - Framework: Update Resources | Terraform\nIn-place update is part of the basic Terraform lifecycle for managing resources. During the terraform apply command, Terraform calls the provider ApplyResourceChange RPC, in which the framework calls the resource.Resource interface Update method. The request contains Terraform prior state, configuration, and plan data. The response contains updated state data. The data is defined by the schema of the resource.\nTo ensure that Terraform plans replacement of a resource, rather than perform an in-place update, use the resource.RequiresReplace() attribute plan modifier in the schema or implement resource plan modification.\nOther resource lifecycle implementations include:\nCreate resources by receiving Terraform configuration and plan data, performing creation logic, and saving Terraform state data.\nRead resources by receiving Terraform prior state data, performing read logic, and saving refreshed Terraform state data.\nDelete resources by receiving Terraform prior state data and performing deletion logic.\nImplement the Update method by:\nAccessing the data from the resource.UpdateRequest type. Most use cases should access the plan data in the resource.UpdateRequest.Plan field.\nPerforming logic or external calls to modify the resource.\nWriting state data into the resource.UpdateResponse.State field.\nIf the logic needs to return warning or error diagnostics, they can added into the resource.UpdateResponse.Diagnostics field.\nIn this example, the Update function makes a HTTP call and returns successfully if the status code was 200 OK:\n// ThingResource defines the resource implementation. // Some resource.Resource interface methods are omitted for brevity. type ThingResource struct { // client is configured via a Configure method, which is not shown in this // example for brevity. Refer to the Configure Resources documentation for // additional details for setting up resources with external clients. client *http.Client } // ThingResourceModel describes the Terraform resource data model to match the // resource schema. type ThingResourceModel struct { Name types.String `tfsdk:\"name\"` Id types.String `tfsdk:\"id\"` } // ThingResourceAPIModel describes the API data model. type ThingResourceAPIModel struct { Name string `json:\"name\"` Id string `json:\"id\"` } func (r ThingResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"name\": schema.StringAttribute{ MarkdownDescription: \"Name of the thing to be saved in the service.\", Required: true, }, \"id\": schema.StringAttribute{ Computed: true, MarkdownDescription: \"Service generated identifier for the thing.\", PlanModifiers: planmodifier.String{ stringplanmodifier.UseStateForUnknown(), }, }, }, MarkdownDescription: \"Manages a thing.\", } } func (r ThingResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data ThingResourceModel // Read Terraform plan data into the model resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...) // Convert from Terraform data model into API data model updateReq := ThingResourceAPIModel{ Id: data.Id.ValueString(), Name: data.Name.ValueString(), } httpReqBody, err := json.Marshal(updateReq) if err != nil { resp.Diagnostics.AddError( \"Unable to Update Resource\", \"An unexpected error occurred while creating the resource update request. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Create HTTP request httpReq := http.NewRequestWithContext( ctx, http.MethodPut, \"http://example.com/things\", bytes.NewBuffer(httpReqBody), ) // Send HTTP request httpResp, err := d.client.Do(httpReq) defer httpResp.Body.Close() if err != nil { resp.Diagnostics.AddError( \"Unable to Update Resource\", \"An unexpected error occurred while attempting to update the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Error: \"+err.Error(), ) return } // Return error if the HTTP status code is not 200 OK if httpResp.StatusCode != http.StatusOK { resp.Diagnostics.AddError( \"Unable to Update Resource\", \"An unexpected error occurred while attempting to update the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Status: \"+httpResp.Status, ) return } // Save updated data into Terraform state resp.Diagnostics.Append(resp.State.Set(ctx, &data)...) } \nNote these caveats when implementing the Update method:\nAn error is returned if the response state is not set when Update is called by the framework. If the resource does not support modification and should always be recreated on configuration value updates, the Update logic can be left empty and ensure all configurable schema attributes implement the resource.RequiresReplace() attribute plan modifier.\nAn error is returned if the response state contains unknown values. Set all attributes to either null or known values in the response.\nAn error is returned if the response state has the RemoveResource() method called. This method is not valid during update. Return an error if the resource is no longer exists.\nAn error is returned unless every null or known value in the request plan is saved exactly as-is into the response state. Only unknown plan values can be modified.\nNote these recommendations when implementing the Update method:\nGet request data from the Terraform plan data over configuration data as the schema or resource may include plan modification logic which sets plan values.\nOnly successfully modified parts of the resource should be return updated data in the state response.\nUse the resource.UseStateForUnknown() attribute plan modifier for Computed attributes that are known to not change during resource updates. This will enhance the Terraform plan to not show => (known after apply) differences.\nThis section highlights implementation details for specific use cases.\nDetect Specific Attribute Changes\nCertain update APIs require that only value changes are sent in the update request or require individual update API calls. Compare the attribute plan value to the attribute prior state value to determine individual attribute changes.\nIn this example, the entire plan and prior state are fetched then the attribute values are compared:\ntype ThingResourceModel struct { Name types.String `tfsdk:\"name\"` // ... other attributes ... } func (r ThingResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var plan, state ThingResourceModel resp.Diagnostics.Append(req.Plan.Get(ctx, &plan)...) resp.Diagnostics.Append(req.State.Get(ctx, &state)...) if resp.Diagnostics.HasError() { return } // Compare name attribute value between plan and prior state if !plan.Name.Equal(state.Name) { // name attribute was changed } // ... further logic ... } \nIn this example, the attribute is individually fetched from the plan and prior state then the values are compared:\nfunc (r ThingResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var namePlan, nameState types.String resp.Diagnostics.Append(req.Plan.GetAttribute(ctx, path.Root(\"name\"), &namePlan)...) resp.Diagnostics.Append(req.State.GetAttribute(ctx, path.Root(\"name\"), &nameState)...) if resp.Diagnostics.HasError() { return } // Compare name attribute value between plan and prior state if !namePlan.Equal(nameState) { // name attribute was changed } // ... further logic ... }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.7.x/getting-started/code-walkthrough",
  "text": "Plugin Development - Framework: Getting Started - Code Walkthrough | Terraform\nTerraform providers let Terraform communicate with third parties, such as cloud providers, SaaS providers, and other APIs. Terraform and Terraform providers use gRPC to communicate. Terraform operates as a gRPC client and providers operate as gRPC servers.\nEach provider defines resources that let Terraform manage infrastructure objects and data sources that let Terraform read data. Terraform practitioners then write configuration to define resources, such as compute storage or networking resources. Terraform then communicates this configuration to the provider, and the provider creates the infrastructure.\nThis example provider shows the relationship between the required provider components. The resources and data sources in a typical provider interact with a cloud provider through an API, but the example only stores values in state.\nA Terraform plugin provider requires at least the following components:\nprovider server\nprovider\nresource or data source\nThe provider wraps the resource(s) and/or data source(s), and can be used to configure a client which communicates with a 3rd party service via an API. Resources are used to manage infrastructure objects. Data sources are used to read infrastructure objects.\nEach provider must implement a gRPC server that supports Terraform-specific connection and handshake handling on startup. A provider server is required in order for a Terraform provider to:\nexpose resources that can be managed by Terraform core.\nexpose data sources that can be read by Terraform core.\nThe main() function is used for defining a provider server.\nThe provider.New() returns a function which returns a type that satisfies the provider.Provider interface. The provider.Provider interface defines functions for obtaining the resource(s) and/or data source(s) from a provider.\npackage main import ( \"context\" \"flag\" \"log\" \"github.com/hashicorp/terraform-plugin-framework/providerserver\" \"github.com/example_namespace/terraform-provider-example/internal/provider\" ) func main() { var debug bool flag.BoolVar(&debug, \"debug\", false, \"set to true to run the provider with support for debuggers like delve\") flag.Parse() opts := providerserver.ServeOpts{ Address: \"registry.terraform.io/example_namespace/example\", Debug: debug, } err := providerserver.Serve(context.Background(), provider.New(), opts) if err != nil { log.Fatal(err.Error()) } } \nRefer to Provider Servers for more details.\nThe provider wraps resources and data sources which are typically used for interacting with cloud providers, SaaS providers, or other APIs.\nIn this example the provider wraps a resource and a data source which simply interact with Terraform state. Refer to the tutorial for an example of provider configuration that configures an API client.\nNew() returns a function which returns a type that satisfies the provider.Provider interface. The New() function is called by the provider server to obtain the provider.\nThe exampleProvider struct implements the provider.Provider interface. This interface defines the following functions:\nSchema: This function returns a provider schema.Schema struct that defines the provider schema. Schemas specify the constraints of Terraform configuration blocks. They define what fields a provider, resource, or data source configuration block has, and give Terraform metadata about those fields.\nConfigure: This function lets you configure provider-level data or clients. These configuration values may be from the practitioner Terraform configuration as defined by the schema, environment variables, or other means such as reading vendor-specific configuration files.\nResources: This function returns a slice of functions that return types that implement the resource.Resource interface. Resources let Terraform manage infrastructure objects, such as a compute instance, an access policy, or disk.\nData Sources: This function returns a slice of functions that return types which implement the datasource.DataSource interface. Data sources let Terraform reference external data. For example a database instance.\nThe exampleProvider struct also implements the provider.ProviderWithMetadata interface which defines the Metadata function. The Metadata function returns metadata for the provider such as a TypeName and Version. The TypeName is used as a prefix within a provider by for naming resources and data sources.\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/datasource\" \"github.com/hashicorp/terraform-plugin-framework/provider\" \"github.com/hashicorp/terraform-plugin-framework/provider/schema\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ provider.Provider = (*exampleProvider)(nil) var _ provider.ProviderWithMetadata = (*exampleProvider)(nil) type exampleProvider struct{} func New() func() provider.Provider { return func() provider.Provider { return &exampleProvider{} } } func (p *exampleProvider) Configure(ctx context.Context, req provider.ConfigureRequest, resp *provider.ConfigureResponse) { } func (p *exampleProvider) Metadata(ctx context.Context, req provider.MetadataRequest, resp *provider.MetadataResponse) { resp.TypeName = \"example\" } func (p *exampleProvider) DataSources(ctx context.Context) []func() datasource.DataSource { return []func() datasource.DataSource{ NewDataSource, } } func (p *exampleProvider) Resources(ctx context.Context) []func() resource.Resource { return []func() resource.Resource{ NewResource, } } func (p *exampleProvider) Schema(ctx context.Context, req provider.SchemaRequest, resp *provider.SchemaResponse) { } \nRefer to Providers for more details and configuration examples.\nA resource is typically used to manage infrastructure objects such as virtual networks and compute instances.\nIn this example the resource simply interacts with Terraform state.\nNewResource() returns a function which returns a type that satisfies the resource.Resource interface. The provider calls the NewResource() function within provider.Resources to obtain an instance of the resource.\nThe exampleResource struct implements the resource.Resource interface. This interface defines the following functions:\nMetadata: This function returns the full name (TypeName) of the resource. The full name is used in Terraform configuration as resource <full name> <alias>.\nSchema: This function returns a resource schema.Schema struct that defines the resource schema. The schema specifies the constraints of the resource Terraform configuration block. It defines what fields a resource configuration block has, and gives Terraform metadata about those fields. For instance, defining whether a field is required.\nCreate: This function lets the provider create a new resource of this type.\nRead: This function lets the provider read resource values in order to update state.\nUpdate: This function lets the provider update the resource and state.\nDelete: This function lets the provider delete the resource.\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/path\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" \"github.com/hashicorp/terraform-plugin-log/tflog\" ) var _ resource.Resource = (*exampleResource)(nil) type exampleResource struct { provider exampleProvider } func NewResource() resource.Resource { return &exampleResource{} } func (e *exampleResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_resource\" } func (e *exampleResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"configurable_attribute\": schema.StringAttribute{ Optional: true, }, \"id\": schema.StringAttribute{ Computed: true, PlanModifiers: []planmodifier.String{ stringplanmodifier.UseStateForUnknown(), }, }, }, } } type exampleResourceData struct { ConfigurableAttribute types.String `tfsdk:\"configurable_attribute\"` Id types.String `tfsdk:\"id\"` } func (e *exampleResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleResourceData diags := req.Config.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } // Create resource using 3rd party API. data.Id = types.StringValue(\"example-id\") tflog.Trace(ctx, \"created a resource\") diags = resp.State.Set(ctx, &data) resp.Diagnostics.Append(diags...) } func (e *exampleResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { var data exampleResourceData diags := req.State.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } // Read resource using 3rd party API. diags = resp.State.Set(ctx, &data) resp.Diagnostics.Append(diags...) } func (e *exampleResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleResourceData diags := req.Plan.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } // Update resource using 3rd party API. diags = resp.State.Set(ctx, &data) resp.Diagnostics.Append(diags...) } func (e *exampleResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { var data exampleResourceData diags := req.State.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } // Delete resource using 3rd party API. } \nRefer to Resources for more details and configuration examples.\nA data source is typically used to provide a read-only view of infrastructure objects.\nIn this example the data source simply interacts with Terraform state.\nNewDataSource() returns a function which returns a type that satisfies the datasource.DataSource interface. The NewDataSource() function is used within the provider.DataSources function to make the data source available to the provider.\nThe exampleDataSource struct implements the datasource.DataSource interface. This interface defines the following functions:\nMetadata: This function returns the full name (TypeName) of the data source. The full name is used in Terraform configuration as data <full name> <alias>.\nSchema: This function returns a data source schema.Schema struct that defines the data source schema. The schema specifies the constraints of the data source Terraform configuration block. It defines what fields a data source configuration block has, and gives Terraform metadata about those fields. For instance, defining whether a field is optional.\nRead: This function lets the provider read data source values in order to update state.\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/datasource\" \"github.com/hashicorp/terraform-plugin-framework/datasource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" \"github.com/hashicorp/terraform-plugin-log/tflog\" ) var _ datasource.DataSource = (*exampleDataSource)(nil) type exampleDataSource struct { provider exampleProvider } func NewDataSource() datasource.DataSource { return &exampleDataSource{} } func (e *exampleDataSource) Metadata(ctx context.Context, req datasource.MetadataRequest, resp *datasource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_datasource\" } func (e *exampleDataSource) Schema(ctx context.Context, req datasource.SchemaRequest, resp *datasource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"configurable_attribute\": schema.StringAttribute{ MarkdownDescription: \"Example configurable attribute\", Optional: true, }, \"id\": schema.StringAttribute{ MarkdownDescription: \"Example identifier\", Computed: true, }, }, } } type exampleDataSourceData struct { ConfigurableAttribute types.String `tfsdk:\"configurable_attribute\"` Id types.String `tfsdk:\"id\"` } func (e *exampleDataSource) Read(ctx context.Context, req datasource.ReadRequest, resp *datasource.ReadResponse) { var data exampleDataSourceData diags := req.Config.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } // Interact with 3rd party API to read data source. data.Id = types.StringValue(\"example-id\") tflog.Trace(ctx, \"read a data source\") diags = resp.State.Set(ctx, &data) resp.Diagnostics.Append(diags...) } \nRefer to Data Sources for more details and configuration examples.\nRefer to terraform-provider-scaffolding-framework for details on how to wire together a provider server, provider, resource and data source.\nOnce wired together, run the provider by specifying configuration and executing terraform apply.\nResource Configuration\nresource \"example_resource\" \"example\" { configurable_attribute = \"some-value\" } \nThe configurable_attribute is defined within the schema as a string type attribute.\nExamples of the various types of attributes and their representation within Terraform configuration and schema definitions are detailed in Terraform Concepts.\nData Source Configuration\ndata \"example_datasource\" \"example\" { configurable_attribute = \"some-value\" } \nThe configurable_attribute is defined within the schema as a string type attribute.\nExamples of the various types of attributes and their representation within Terraform configuration and schema definitions are detailed in Terraform Concepts."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.4.x/getting-started/code-walkthrough",
  "text": "Plugin Development - Framework: Getting Started - Code Walkthrough | Terraform\nTerraform providers let Terraform communicate with third parties, such as cloud providers, SaaS providers, and other APIs. Terraform and Terraform providers use gRPC to communicate. Terraform operates as a gRPC client and providers operate as gRPC servers.\nEach provider defines resources that let Terraform manage infrastructure objects and data sources that let Terraform read data. Terraform practitioners then write configuration to define resources, such as compute storage or networking resources. Terraform then communicates this configuration to the provider, and the provider creates the infrastructure.\nThis example provider shows the relationship between the required provider components. The resources and data sources in a typical provider interact with a cloud provider through an API, but the example only stores values in state.\nA Terraform plugin provider requires at least the following components:\nprovider server\nprovider\nresource or data source\nThe provider wraps the resource(s) and/or data source(s), and can be used to configure a client which communicates with a 3rd party service via an API. Resources are used to manage infrastructure objects. Data sources are used to read infrastructure objects.\nEach provider must implement a gRPC server that supports Terraform-specific connection and handshake handling on startup. A provider server is required in order for a Terraform provider to:\nexpose resources that can be managed by Terraform core.\nexpose data sources that can be read by Terraform core.\nThe main() function is used for defining a provider server.\nThe provider.New() returns a function which returns a type that satisfies the provider.Provider interface. The provider.Provider interface defines functions for obtaining the resource(s) and/or data source(s) from a provider.\npackage main import ( \"context\" \"flag\" \"log\" \"github.com/hashicorp/terraform-plugin-framework/providerserver\" \"github.com/example_namespace/terraform-provider-example/internal/provider\" ) func main() { var debug bool flag.BoolVar(&debug, \"debug\", false, \"set to true to run the provider with support for debuggers like delve\") flag.Parse() opts := providerserver.ServeOpts{ Address: \"registry.terraform.io/example_namespace/example\", Debug: debug, } err := providerserver.Serve(context.Background(), provider.New(), opts) if err != nil { log.Fatal(err.Error()) } } \nRefer to Provider Servers for more details.\nThe provider wraps resources and data sources which are typically used for interacting with cloud providers, SaaS providers, or other APIs.\nIn this example the provider wraps a resource and a data source which simply interact with Terraform state. Refer to the tutorial for an example of provider configuration that configures an API client.\nNew() returns a function which returns a type that satisfies the provider.Provider interface. The New() function is called by the provider server to obtain the provider.\nThe exampleProvider struct implements the provider.Provider interface. This interface defines the following functions:\nSchema: This function returns a provider schema.Schema struct that defines the provider schema. Schemas specify the constraints of Terraform configuration blocks. They define what fields a provider, resource, or data source configuration block has, and give Terraform metadata about those fields.\nConfigure: This function lets you configure provider-level data or clients. These configuration values may be from the practitioner Terraform configuration as defined by the schema, environment variables, or other means such as reading vendor-specific configuration files.\nResources: This function returns a slice of functions that return types that implement the resource.Resource interface. Resources let Terraform manage infrastructure objects, such as a compute instance, an access policy, or disk.\nData Sources: This function returns a slice of functions that return types which implement the datasource.DataSource interface. Data sources let Terraform reference external data. For example a database instance.\nThe exampleProvider struct also implements the provider.ProviderWithMetadata interface which defines the Metadata function. The Metadata function returns metadata for the provider such as a TypeName and Version. The TypeName is used as a prefix within a provider by for naming resources and data sources.\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/datasource\" \"github.com/hashicorp/terraform-plugin-framework/provider\" \"github.com/hashicorp/terraform-plugin-framework/provider/schema\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ provider.Provider = (*exampleProvider)(nil) var _ provider.ProviderWithMetadata = (*exampleProvider)(nil) type exampleProvider struct{} func New() func() provider.Provider { return func() provider.Provider { return &exampleProvider{} } } func (p *exampleProvider) Configure(ctx context.Context, req provider.ConfigureRequest, resp *provider.ConfigureResponse) { } func (p *exampleProvider) Metadata(ctx context.Context, req provider.MetadataRequest, resp *provider.MetadataResponse) { resp.TypeName = \"example\" } func (p *exampleProvider) DataSources(ctx context.Context) []func() datasource.DataSource { return []func() datasource.DataSource{ NewDataSource, } } func (p *exampleProvider) Resources(ctx context.Context) []func() resource.Resource { return []func() resource.Resource{ NewResource, } } func (p *exampleProvider) Schema(ctx context.Context, req provider.SchemaRequest, resp *provider.SchemaResponse) { } \nRefer to Providers for more details and configuration examples.\nA resource is typically used to manage infrastructure objects such as virtual networks and compute instances.\nIn this example the resource simply interacts with Terraform state.\nNewResource() returns a function which returns a type that satisfies the resource.Resource interface. The provider calls the NewResource() function within provider.Resources to obtain an instance of the resource.\nThe exampleResource struct implements the resource.Resource interface. This interface defines the following functions:\nMetadata: This function returns the full name (TypeName) of the resource. The full name is used in Terraform configuration as resource <full name> <alias>.\nSchema: This function returns a resource schema.Schema struct that defines the resource schema. The schema specifies the constraints of the resource Terraform configuration block. It defines what fields a resource configuration block has, and gives Terraform metadata about those fields. For instance, defining whether a field is required.\nCreate: This function lets the provider create a new resource of this type.\nRead: This function lets the provider read resource values in order to update state.\nUpdate: This function lets the provider update the resource and state.\nDelete: This function lets the provider delete the resource.\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/path\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" \"github.com/hashicorp/terraform-plugin-log/tflog\" ) var _ resource.Resource = (*exampleResource)(nil) type exampleResource struct { provider exampleProvider } func NewResource() resource.Resource { return &exampleResource{} } func (e *exampleResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_resource\" } func (e *exampleResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"configurable_attribute\": schema.StringAttribute{ Optional: true, }, \"id\": schema.StringAttribute{ Computed: true, PlanModifiers: []planmodifier.String{ stringplanmodifier.UseStateForUnknown(), }, }, }, } } type exampleResourceData struct { ConfigurableAttribute types.String `tfsdk:\"configurable_attribute\"` Id types.String `tfsdk:\"id\"` } func (e *exampleResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleResourceData diags := req.Config.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } // Create resource using 3rd party API. data.Id = types.StringValue(\"example-id\") tflog.Trace(ctx, \"created a resource\") diags = resp.State.Set(ctx, &data) resp.Diagnostics.Append(diags...) } func (e *exampleResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { var data exampleResourceData diags := req.State.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } // Read resource using 3rd party API. diags = resp.State.Set(ctx, &data) resp.Diagnostics.Append(diags...) } func (e *exampleResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleResourceData diags := req.Plan.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } // Update resource using 3rd party API. diags = resp.State.Set(ctx, &data) resp.Diagnostics.Append(diags...) } func (e *exampleResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { var data exampleResourceData diags := req.State.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } // Delete resource using 3rd party API. } \nRefer to Resources for more details and configuration examples.\nA data source is typically used to provide a read-only view of infrastructure objects.\nIn this example the data source simply interacts with Terraform state.\nNewDataSource() returns a function which returns a type that satisfies the datasource.DataSource interface. The NewDataSource() function is used within the provider.DataSources function to make the data source available to the provider.\nThe exampleDataSource struct implements the datasource.DataSource interface. This interface defines the following functions:\nMetadata: This function returns the full name (TypeName) of the data source. The full name is used in Terraform configuration as data <full name> <alias>.\nSchema: This function returns a data source schema.Schema struct that defines the data source schema. The schema specifies the constraints of the data source Terraform configuration block. It defines what fields a data source configuration block has, and gives Terraform metadata about those fields. For instance, defining whether a field is optional.\nRead: This function lets the provider read data source values in order to update state.\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/datasource\" \"github.com/hashicorp/terraform-plugin-framework/datasource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" \"github.com/hashicorp/terraform-plugin-log/tflog\" ) var _ datasource.DataSource = (*exampleDataSource)(nil) type exampleDataSource struct { provider exampleProvider } func NewDataSource() datasource.DataSource { return &exampleDataSource{} } func (e *exampleDataSource) Metadata(ctx context.Context, req datasource.MetadataRequest, resp *datasource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_datasource\" } func (e *exampleDataSource) Schema(ctx context.Context, req datasource.SchemaRequest, resp *datasource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"configurable_attribute\": schema.StringAttribute{ MarkdownDescription: \"Example configurable attribute\", Optional: true, }, \"id\": schema.StringAttribute{ MarkdownDescription: \"Example identifier\", Computed: true, }, }, } } type exampleDataSourceData struct { ConfigurableAttribute types.String `tfsdk:\"configurable_attribute\"` Id types.String `tfsdk:\"id\"` } func (e *exampleDataSource) Read(ctx context.Context, req datasource.ReadRequest, resp *datasource.ReadResponse) { var data exampleDataSourceData diags := req.Config.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } // Interact with 3rd party API to read data source. data.Id = types.StringValue(\"example-id\") tflog.Trace(ctx, \"read a data source\") diags = resp.State.Set(ctx, &data) resp.Diagnostics.Append(diags...) } \nRefer to Data Sources for more details and configuration examples.\nRefer to terraform-provider-scaffolding-framework for details on how to wire together a provider server, provider, resource and data source.\nOnce wired together, run the provider by specifying configuration and executing terraform apply.\nResource Configuration\nresource \"example_resource\" \"example\" { configurable_attribute = \"some-value\" } \nData Source Configuration\ndata \"example_datasource\" \"example\" { configurable_attribute = \"some-value\" } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.6.x/getting-started/code-walkthrough",
  "text": "Plugin Development - Framework: Getting Started - Code Walkthrough | Terraform\nTerraform providers let Terraform communicate with third parties, such as cloud providers, SaaS providers, and other APIs. Terraform and Terraform providers use gRPC to communicate. Terraform operates as a gRPC client and providers operate as gRPC servers.\nEach provider defines resources that let Terraform manage infrastructure objects and data sources that let Terraform read data. Terraform practitioners then write configuration to define resources, such as compute storage or networking resources. Terraform then communicates this configuration to the provider, and the provider creates the infrastructure.\nThis example provider shows the relationship between the required provider components. The resources and data sources in a typical provider interact with a cloud provider through an API, but the example only stores values in state.\nA Terraform plugin provider requires at least the following components:\nprovider server\nprovider\nresource or data source\nThe provider wraps the resource(s) and/or data source(s), and can be used to configure a client which communicates with a 3rd party service via an API. Resources are used to manage infrastructure objects. Data sources are used to read infrastructure objects.\nEach provider must implement a gRPC server that supports Terraform-specific connection and handshake handling on startup. A provider server is required in order for a Terraform provider to:\nexpose resources that can be managed by Terraform core.\nexpose data sources that can be read by Terraform core.\nThe main() function is used for defining a provider server.\nThe provider.New() returns a function which returns a type that satisfies the provider.Provider interface. The provider.Provider interface defines functions for obtaining the resource(s) and/or data source(s) from a provider.\npackage main import ( \"context\" \"flag\" \"log\" \"github.com/hashicorp/terraform-plugin-framework/providerserver\" \"github.com/example_namespace/terraform-provider-example/internal/provider\" ) func main() { var debug bool flag.BoolVar(&debug, \"debug\", false, \"set to true to run the provider with support for debuggers like delve\") flag.Parse() opts := providerserver.ServeOpts{ Address: \"registry.terraform.io/example_namespace/example\", Debug: debug, } err := providerserver.Serve(context.Background(), provider.New(), opts) if err != nil { log.Fatal(err.Error()) } } \nRefer to Provider Servers for more details.\nThe provider wraps resources and data sources which are typically used for interacting with cloud providers, SaaS providers, or other APIs.\nIn this example the provider wraps a resource and a data source which simply interact with Terraform state. Refer to the tutorial for an example of provider configuration that configures an API client.\nNew() returns a function which returns a type that satisfies the provider.Provider interface. The New() function is called by the provider server to obtain the provider.\nThe exampleProvider struct implements the provider.Provider interface. This interface defines the following functions:\nSchema: This function returns a provider schema.Schema struct that defines the provider schema. Schemas specify the constraints of Terraform configuration blocks. They define what fields a provider, resource, or data source configuration block has, and give Terraform metadata about those fields.\nConfigure: This function lets you configure provider-level data or clients. These configuration values may be from the practitioner Terraform configuration as defined by the schema, environment variables, or other means such as reading vendor-specific configuration files.\nResources: This function returns a slice of functions that return types that implement the resource.Resource interface. Resources let Terraform manage infrastructure objects, such as a compute instance, an access policy, or disk.\nData Sources: This function returns a slice of functions that return types which implement the datasource.DataSource interface. Data sources let Terraform reference external data. For example a database instance.\nThe exampleProvider struct also implements the provider.ProviderWithMetadata interface which defines the Metadata function. The Metadata function returns metadata for the provider such as a TypeName and Version. The TypeName is used as a prefix within a provider by for naming resources and data sources.\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/datasource\" \"github.com/hashicorp/terraform-plugin-framework/provider\" \"github.com/hashicorp/terraform-plugin-framework/provider/schema\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ provider.Provider = (*exampleProvider)(nil) var _ provider.ProviderWithMetadata = (*exampleProvider)(nil) type exampleProvider struct{} func New() func() provider.Provider { return func() provider.Provider { return &exampleProvider{} } } func (p *exampleProvider) Configure(ctx context.Context, req provider.ConfigureRequest, resp *provider.ConfigureResponse) { } func (p *exampleProvider) Metadata(ctx context.Context, req provider.MetadataRequest, resp *provider.MetadataResponse) { resp.TypeName = \"example\" } func (p *exampleProvider) DataSources(ctx context.Context) []func() datasource.DataSource { return []func() datasource.DataSource{ NewDataSource, } } func (p *exampleProvider) Resources(ctx context.Context) []func() resource.Resource { return []func() resource.Resource{ NewResource, } } func (p *exampleProvider) Schema(ctx context.Context, req provider.SchemaRequest, resp *provider.SchemaResponse) { } \nRefer to Providers for more details and configuration examples.\nA resource is typically used to manage infrastructure objects such as virtual networks and compute instances.\nIn this example the resource simply interacts with Terraform state.\nNewResource() returns a function which returns a type that satisfies the resource.Resource interface. The provider calls the NewResource() function within provider.Resources to obtain an instance of the resource.\nThe exampleResource struct implements the resource.Resource interface. This interface defines the following functions:\nMetadata: This function returns the full name (TypeName) of the resource. The full name is used in Terraform configuration as resource <full name> <alias>.\nSchema: This function returns a resource schema.Schema struct that defines the resource schema. The schema specifies the constraints of the resource Terraform configuration block. It defines what fields a resource configuration block has, and gives Terraform metadata about those fields. For instance, defining whether a field is required.\nCreate: This function lets the provider create a new resource of this type.\nRead: This function lets the provider read resource values in order to update state.\nUpdate: This function lets the provider update the resource and state.\nDelete: This function lets the provider delete the resource.\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/path\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" \"github.com/hashicorp/terraform-plugin-log/tflog\" ) var _ resource.Resource = (*exampleResource)(nil) type exampleResource struct { provider exampleProvider } func NewResource() resource.Resource { return &exampleResource{} } func (e *exampleResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_resource\" } func (e *exampleResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"configurable_attribute\": schema.StringAttribute{ Optional: true, }, \"id\": schema.StringAttribute{ Computed: true, PlanModifiers: []planmodifier.String{ stringplanmodifier.UseStateForUnknown(), }, }, }, } } type exampleResourceData struct { ConfigurableAttribute types.String `tfsdk:\"configurable_attribute\"` Id types.String `tfsdk:\"id\"` } func (e *exampleResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleResourceData diags := req.Config.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } // Create resource using 3rd party API. data.Id = types.StringValue(\"example-id\") tflog.Trace(ctx, \"created a resource\") diags = resp.State.Set(ctx, &data) resp.Diagnostics.Append(diags...) } func (e *exampleResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { var data exampleResourceData diags := req.State.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } // Read resource using 3rd party API. diags = resp.State.Set(ctx, &data) resp.Diagnostics.Append(diags...) } func (e *exampleResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleResourceData diags := req.Plan.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } // Update resource using 3rd party API. diags = resp.State.Set(ctx, &data) resp.Diagnostics.Append(diags...) } func (e *exampleResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { var data exampleResourceData diags := req.State.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } // Delete resource using 3rd party API. } \nRefer to Resources for more details and configuration examples.\nA data source is typically used to provide a read-only view of infrastructure objects.\nIn this example the data source simply interacts with Terraform state.\nNewDataSource() returns a function which returns a type that satisfies the datasource.DataSource interface. The NewDataSource() function is used within the provider.DataSources function to make the data source available to the provider.\nThe exampleDataSource struct implements the datasource.DataSource interface. This interface defines the following functions:\nMetadata: This function returns the full name (TypeName) of the data source. The full name is used in Terraform configuration as data <full name> <alias>.\nSchema: This function returns a data source schema.Schema struct that defines the data source schema. The schema specifies the constraints of the data source Terraform configuration block. It defines what fields a data source configuration block has, and gives Terraform metadata about those fields. For instance, defining whether a field is optional.\nRead: This function lets the provider read data source values in order to update state.\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/datasource\" \"github.com/hashicorp/terraform-plugin-framework/datasource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" \"github.com/hashicorp/terraform-plugin-log/tflog\" ) var _ datasource.DataSource = (*exampleDataSource)(nil) type exampleDataSource struct { provider exampleProvider } func NewDataSource() datasource.DataSource { return &exampleDataSource{} } func (e *exampleDataSource) Metadata(ctx context.Context, req datasource.MetadataRequest, resp *datasource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_datasource\" } func (e *exampleDataSource) Schema(ctx context.Context, req datasource.SchemaRequest, resp *datasource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"configurable_attribute\": schema.StringAttribute{ MarkdownDescription: \"Example configurable attribute\", Optional: true, }, \"id\": schema.StringAttribute{ MarkdownDescription: \"Example identifier\", Computed: true, }, }, } } type exampleDataSourceData struct { ConfigurableAttribute types.String `tfsdk:\"configurable_attribute\"` Id types.String `tfsdk:\"id\"` } func (e *exampleDataSource) Read(ctx context.Context, req datasource.ReadRequest, resp *datasource.ReadResponse) { var data exampleDataSourceData diags := req.Config.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } // Interact with 3rd party API to read data source. data.Id = types.StringValue(\"example-id\") tflog.Trace(ctx, \"read a data source\") diags = resp.State.Set(ctx, &data) resp.Diagnostics.Append(diags...) } \nRefer to Data Sources for more details and configuration examples.\nRefer to terraform-provider-scaffolding-framework for details on how to wire together a provider server, provider, resource and data source.\nOnce wired together, run the provider by specifying configuration and executing terraform apply.\nResource Configuration\nresource \"example_resource\" \"example\" { configurable_attribute = \"some-value\" } \nThe configurable_attribute is defined within the schema as a string type attribute.\nExamples of the various types of attributes and their representation within Terraform configuration and schema definitions are detailed in Terraform Concepts.\nData Source Configuration\ndata \"example_datasource\" \"example\" { configurable_attribute = \"some-value\" } \nThe configurable_attribute is defined within the schema as a string type attribute.\nExamples of the various types of attributes and their representation within Terraform configuration and schema definitions are detailed in Terraform Concepts."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.5.x/getting-started/code-walkthrough",
  "text": "Plugin Development - Framework: Getting Started - Code Walkthrough | Terraform\nTerraform providers let Terraform communicate with third parties, such as cloud providers, SaaS providers, and other APIs. Terraform and Terraform providers use gRPC to communicate. Terraform operates as a gRPC client and providers operate as gRPC servers.\nEach provider defines resources that let Terraform manage infrastructure objects and data sources that let Terraform read data. Terraform practitioners then write configuration to define resources, such as compute storage or networking resources. Terraform then communicates this configuration to the provider, and the provider creates the infrastructure.\nThis example provider shows the relationship between the required provider components. The resources and data sources in a typical provider interact with a cloud provider through an API, but the example only stores values in state.\nA Terraform plugin provider requires at least the following components:\nprovider server\nprovider\nresource or data source\nThe provider wraps the resource(s) and/or data source(s), and can be used to configure a client which communicates with a 3rd party service via an API. Resources are used to manage infrastructure objects. Data sources are used to read infrastructure objects.\nEach provider must implement a gRPC server that supports Terraform-specific connection and handshake handling on startup. A provider server is required in order for a Terraform provider to:\nexpose resources that can be managed by Terraform core.\nexpose data sources that can be read by Terraform core.\nThe main() function is used for defining a provider server.\nThe provider.New() returns a function which returns a type that satisfies the provider.Provider interface. The provider.Provider interface defines functions for obtaining the resource(s) and/or data source(s) from a provider.\npackage main import ( \"context\" \"flag\" \"log\" \"github.com/hashicorp/terraform-plugin-framework/providerserver\" \"github.com/example_namespace/terraform-provider-example/internal/provider\" ) func main() { var debug bool flag.BoolVar(&debug, \"debug\", false, \"set to true to run the provider with support for debuggers like delve\") flag.Parse() opts := providerserver.ServeOpts{ Address: \"registry.terraform.io/example_namespace/example\", Debug: debug, } err := providerserver.Serve(context.Background(), provider.New(), opts) if err != nil { log.Fatal(err.Error()) } } \nRefer to Provider Servers for more details.\nThe provider wraps resources and data sources which are typically used for interacting with cloud providers, SaaS providers, or other APIs.\nIn this example the provider wraps a resource and a data source which simply interact with Terraform state. Refer to the tutorial for an example of provider configuration that configures an API client.\nNew() returns a function which returns a type that satisfies the provider.Provider interface. The New() function is called by the provider server to obtain the provider.\nThe exampleProvider struct implements the provider.Provider interface. This interface defines the following functions:\nSchema: This function returns a provider schema.Schema struct that defines the provider schema. Schemas specify the constraints of Terraform configuration blocks. They define what fields a provider, resource, or data source configuration block has, and give Terraform metadata about those fields.\nConfigure: This function lets you configure provider-level data or clients. These configuration values may be from the practitioner Terraform configuration as defined by the schema, environment variables, or other means such as reading vendor-specific configuration files.\nResources: This function returns a slice of functions that return types that implement the resource.Resource interface. Resources let Terraform manage infrastructure objects, such as a compute instance, an access policy, or disk.\nData Sources: This function returns a slice of functions that return types which implement the datasource.DataSource interface. Data sources let Terraform reference external data. For example a database instance.\nThe exampleProvider struct also implements the provider.ProviderWithMetadata interface which defines the Metadata function. The Metadata function returns metadata for the provider such as a TypeName and Version. The TypeName is used as a prefix within a provider by for naming resources and data sources.\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/datasource\" \"github.com/hashicorp/terraform-plugin-framework/provider\" \"github.com/hashicorp/terraform-plugin-framework/provider/schema\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) var _ provider.Provider = (*exampleProvider)(nil) var _ provider.ProviderWithMetadata = (*exampleProvider)(nil) type exampleProvider struct{} func New() func() provider.Provider { return func() provider.Provider { return &exampleProvider{} } } func (p *exampleProvider) Configure(ctx context.Context, req provider.ConfigureRequest, resp *provider.ConfigureResponse) { } func (p *exampleProvider) Metadata(ctx context.Context, req provider.MetadataRequest, resp *provider.MetadataResponse) { resp.TypeName = \"example\" } func (p *exampleProvider) DataSources(ctx context.Context) []func() datasource.DataSource { return []func() datasource.DataSource{ NewDataSource, } } func (p *exampleProvider) Resources(ctx context.Context) []func() resource.Resource { return []func() resource.Resource{ NewResource, } } func (p *exampleProvider) Schema(ctx context.Context, req provider.SchemaRequest, resp *provider.SchemaResponse) { } \nRefer to Providers for more details and configuration examples.\nA resource is typically used to manage infrastructure objects such as virtual networks and compute instances.\nIn this example the resource simply interacts with Terraform state.\nNewResource() returns a function which returns a type that satisfies the resource.Resource interface. The provider calls the NewResource() function within provider.Resources to obtain an instance of the resource.\nThe exampleResource struct implements the resource.Resource interface. This interface defines the following functions:\nMetadata: This function returns the full name (TypeName) of the resource. The full name is used in Terraform configuration as resource <full name> <alias>.\nSchema: This function returns a resource schema.Schema struct that defines the resource schema. The schema specifies the constraints of the resource Terraform configuration block. It defines what fields a resource configuration block has, and gives Terraform metadata about those fields. For instance, defining whether a field is required.\nCreate: This function lets the provider create a new resource of this type.\nRead: This function lets the provider read resource values in order to update state.\nUpdate: This function lets the provider update the resource and state.\nDelete: This function lets the provider delete the resource.\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/path\" \"github.com/hashicorp/terraform-plugin-framework/resource\" \"github.com/hashicorp/terraform-plugin-framework/resource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" \"github.com/hashicorp/terraform-plugin-log/tflog\" ) var _ resource.Resource = (*exampleResource)(nil) type exampleResource struct { provider exampleProvider } func NewResource() resource.Resource { return &exampleResource{} } func (e *exampleResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_resource\" } func (e *exampleResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"configurable_attribute\": schema.StringAttribute{ Optional: true, }, \"id\": schema.StringAttribute{ Computed: true, PlanModifiers: []planmodifier.String{ stringplanmodifier.UseStateForUnknown(), }, }, }, } } type exampleResourceData struct { ConfigurableAttribute types.String `tfsdk:\"configurable_attribute\"` Id types.String `tfsdk:\"id\"` } func (e *exampleResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleResourceData diags := req.Config.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } // Create resource using 3rd party API. data.Id = types.StringValue(\"example-id\") tflog.Trace(ctx, \"created a resource\") diags = resp.State.Set(ctx, &data) resp.Diagnostics.Append(diags...) } func (e *exampleResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { var data exampleResourceData diags := req.State.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } // Read resource using 3rd party API. diags = resp.State.Set(ctx, &data) resp.Diagnostics.Append(diags...) } func (e *exampleResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) { var data exampleResourceData diags := req.Plan.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } // Update resource using 3rd party API. diags = resp.State.Set(ctx, &data) resp.Diagnostics.Append(diags...) } func (e *exampleResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { var data exampleResourceData diags := req.State.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } // Delete resource using 3rd party API. } \nRefer to Resources for more details and configuration examples.\nA data source is typically used to provide a read-only view of infrastructure objects.\nIn this example the data source simply interacts with Terraform state.\nNewDataSource() returns a function which returns a type that satisfies the datasource.DataSource interface. The NewDataSource() function is used within the provider.DataSources function to make the data source available to the provider.\nThe exampleDataSource struct implements the datasource.DataSource interface. This interface defines the following functions:\nMetadata: This function returns the full name (TypeName) of the data source. The full name is used in Terraform configuration as data <full name> <alias>.\nSchema: This function returns a data source schema.Schema struct that defines the data source schema. The schema specifies the constraints of the data source Terraform configuration block. It defines what fields a data source configuration block has, and gives Terraform metadata about those fields. For instance, defining whether a field is optional.\nRead: This function lets the provider read data source values in order to update state.\npackage provider import ( \"context\" \"github.com/hashicorp/terraform-plugin-framework/datasource\" \"github.com/hashicorp/terraform-plugin-framework/datasource/schema\" \"github.com/hashicorp/terraform-plugin-framework/types\" \"github.com/hashicorp/terraform-plugin-log/tflog\" ) var _ datasource.DataSource = (*exampleDataSource)(nil) type exampleDataSource struct { provider exampleProvider } func NewDataSource() datasource.DataSource { return &exampleDataSource{} } func (e *exampleDataSource) Metadata(ctx context.Context, req datasource.MetadataRequest, resp *datasource.MetadataResponse) { resp.TypeName = req.ProviderTypeName + \"_datasource\" } func (e *exampleDataSource) Schema(ctx context.Context, req datasource.SchemaRequest, resp *datasource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"configurable_attribute\": schema.StringAttribute{ MarkdownDescription: \"Example configurable attribute\", Optional: true, }, \"id\": schema.StringAttribute{ MarkdownDescription: \"Example identifier\", Computed: true, }, }, } } type exampleDataSourceData struct { ConfigurableAttribute types.String `tfsdk:\"configurable_attribute\"` Id types.String `tfsdk:\"id\"` } func (e *exampleDataSource) Read(ctx context.Context, req datasource.ReadRequest, resp *datasource.ReadResponse) { var data exampleDataSourceData diags := req.Config.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } // Interact with 3rd party API to read data source. data.Id = types.StringValue(\"example-id\") tflog.Trace(ctx, \"read a data source\") diags = resp.State.Set(ctx, &data) resp.Diagnostics.Append(diags...) } \nRefer to Data Sources for more details and configuration examples.\nRefer to terraform-provider-scaffolding-framework for details on how to wire together a provider server, provider, resource and data source.\nOnce wired together, run the provider by specifying configuration and executing terraform apply.\nResource Configuration\nresource \"example_resource\" \"example\" { configurable_attribute = \"some-value\" } \nData Source Configuration\ndata \"example_datasource\" \"example\" { configurable_attribute = \"some-value\" } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.5.x/functions/testing",
  "text": "Plugin Development - Framework: Testing Functions | Terraform\nWhen a function is implemented, ensure the function behaves as expected. Follow recommendations to cover how practitioner configurations may call the function.\nThere are two methodologies for testing provider-defined functions:\nAcceptance Testing: Verify implementation using real Terraform configurations and commands.\nUnit Testing: Verify implementation using with Terraform and framework implementation details.\nSimilar to other provider concepts, many provider developers prefer acceptance testing over unit testing. Acceptance testing guarantees the function implementation works exactly as expected in real world use cases without trying to determine Terraform or framework implementation details. Unit testing details are provided, however, for function implementations which warrant a broad amount of input value testing, such as generic data handling functions or to perform fuzzing.\nTesting examples on this page are dependent on the example echo function implementation.\nTesting a provider-defined function should ensure at least the following behaviors are covered:\nKnown values return the expected results.\nFor any list, map, object, and set parameters, null values for collection elements or object attributes. The AllowNullValue parameter setting does not affect Terraform sending these types of null values.\nIf any parameters enable AllowNullValue, null values for those arguments.\nIf any parameters enable AllowUnknownValues, unknown values for those arguments.\nAny diagnostics, such as argument validation errors.\nUse the plugin testing Go module to implement real world testing with Terraform configurations and commands. The documentation for that Go module covers many more available testing features, however this section example gives a high level overview of how to start writing these tests.\nIn this example, a echo_function_test.go file is created:\npackage provider_test import ( \"testing\" \"example.com/terraform-provider-example/internal/provider\" \"github.com/hashicorp/terraform-plugin-framework/providerserver\" \"github.com/hashicorp/terraform-plugin-go/tfprotov6\" \"github.com/hashicorp/terraform-plugin-testing/helper/resource\" ) func TestEchoFunction_Valid(t *testing.T) { t.Parallel() resource.UnitTest(t, resource.TestCase{ ProtoV6ProviderFactories: map[string]func() (tfprotov6.ProviderServer, error) { \"example\": providerserver.NewProtocol6WithError(provider.New()), }, Steps: []resource.TestStep{ { Config: ` output \"test\" { value = provider::example::echo(\"test-value\") } `, Check: resource.TestCheckOutput(\"test\", \"test-value\"), }, }, }) } // The example implementation does not return any error diagnostics, however // this acceptance test verifies how the function should behave if it did. func TestEchoFunction_Invalid(t *testing.T) { t.Parallel() resource.UnitTest(t, resource.TestCase{ ProtoV6ProviderFactories: map[string]func() (tfprotov6.ProviderServer, error) { \"example\": providerserver.NewProtocol6WithError(provider.New()), }, Steps: []resource.TestStep{ { Config: ` output \"test\" { value = provider::example::echo(\"invalid\") } `, ExpectError: regexp.MustCompile(`error summary`), }, }, }) } // The example implementation does not enable AllowNullValue, however this // acceptance test shows how to verify the behavior. func TestEchoFunction_Null(t *testing.T) { t.Parallel() resource.UnitTest(t, resource.TestCase{ ProtoV6ProviderFactories: map[string]func() (tfprotov6.ProviderServer, error) { \"example\": providerserver.NewProtocol6WithError(provider.New()), }, Steps: []resource.TestStep{ { Config: ` output \"test\" { value = provider::example::echo(null) } `, ExpectError: regexp.MustCompile(`Invalid Function Call`), }, }, }) } // The example implementation does not enable AllowUnknownValues, however this // acceptance test shows how to verify the behavior. func TestEchoFunction_Unknown(t *testing.T) { resource.UnitTest(t, resource.TestCase{ ProtoV6ProviderFactories: map[string]func() (tfprotov6.ProviderServer, error) { \"example\": providerserver.NewProtocol6WithError(provider.New()), }, Steps: []resource.TestStep{ { Config: ` terraform_data \"test\" { input = \"test-value\" } output \"test\" { value = provider::example::echo(terraform_data.test.output) } `, Check: resource.TestCheckOutput(\"test\", \"test-value\"), ConfigPlanChecks: resource.ConfigPlanChecks{ PreApply: []plancheck.PlanCheck{ plancheck.ExpectUnknownOutputValue(\"test\"), }, }, }, }, }) } \nUse the function.NewArgumentsData() function and function.NewResultData() function as part of implementing a Go test.\nIn this example, a echo_function_test.go file is created:\npackage provider_test import ( \"context\" \"testing\" \"example.com/terraform-provider-example/internal/provider\" \"github.com/google/go-cmp/cmp\" \"github.com/hashicorp/terraform-plugin-framework/diag\" \"github.com/hashicorp/terraform-plugin-framework/function\" \"github.com/hashicorp/terraform-plugin-framework/types\" ) func TestEchoFunctionRun(t *testing.T) { t.Parallel() testCases := map[string]struct { request function.RunRequest expected function.RunResponse }{ // The example implementation uses the Go built-in string type, however // if AllowNullValue was enabled and *string or types.String was used, // this test case shows how the function would be expected to behave. \"null\": { request: function.RunRequest{ Arguments: function.NewArgumentsData(types.StringNull()), }, Expected: function.RunResponse{ Result: function.NewResultData(types.StringNull()), }, }, // The example implementation uses the Go built-in string type, however // if AllowUnknownValues was enabled and types.String was used, // this test case shows how the function would be expected to behave. \"unknown\": { request: function.RunRequest{ Arguments: function.NewArgumentsData(types.StringUnknown()), }, Expected: function.RunResponse{ Result: function.NewResultData(types.StringUnknown()), }, }, \"value-valid\": { request: function.RunRequest{ Arguments: function.NewArgumentsData(types.StringValue(\"test-value\")), }, Expected: function.RunResponse{ Result: function.NewResultData(types.StringValue(\"test-value\")), }, }, // The example implementation does not return any diagnostics, however // this test case shows how the function would be expected to behave if // it did. \"value-invalid\": { request: function.RunRequest{ Arguments: function.NewArgumentsData(types.StringValue()), }, Expected: function.RunResponse{ Diagnostics: diag.Diagnostics{ diag.NewArgumentErrorDiagnostic(0, \"error summary\", \"error detail\"), }, Result: function.NewResultData(types.StringUnknown()), }, }, } for name, testCase := range testCases { name, testCase := name, testCase t.Run(name, func(t *testing.T) { t.Parallel() got := function.RunResponse{} provider.EchoFunction{}.Run(context.Background(), testCase.request, &got) if diff := cmp.Diff(got, testCase.expected); diff != \"\" { t.Errorf(\"unexpected difference: %s\", diff) } }) } }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.1.x/getting-started/code-walkthrough",
  "text": "Examples of the various types of attributes and their representation within Terraform configuration and schema definitions is detailed in Core Configuration Concepts.\nExamples of the various types of attributes and their representation within Terraform configuration and schema definitions is detailed in Core Configuration Concepts."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.0.x/getting-started/code-walkthrough",
  "text": "Examples of the various types of attributes and their representation within Terraform configuration and schema definitions is detailed in Core Configuration Concepts.\nExamples of the various types of attributes and their representation within Terraform configuration and schema definitions is detailed in Core Configuration Concepts."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.7.x/resources/delete",
  "text": "Plugin Development - Framework: Delete Resources | Terraform\nDeletion is part of the basic Terraform lifecycle for managing resources. During the terraform apply command, Terraform calls the provider ApplyResourceChange RPC, in which the framework calls the resource.Resource interface Delete method. The request contains Terraform prior state data. The response is only for returning diagnostics. The data is defined by the schema of the resource.\nTerraform 1.3 and later enables deletion planning, which resources can implement to return warning and error diagnostics. For additional information, refer to the resource plan modification documentation.\nOther resource lifecycle implementations include:\nCreate resources by receiving Terraform configuration and plan data, performing creation logic, and saving Terraform state data.\nRead resources by receiving Terraform prior state data, performing read logic, and saving refreshed Terraform state data.\nUpdate resources in-place by receiving Terraform prior state, configuration, and plan data, performing update logic, and saving updated Terraform state data.\nImplement the Delete method by:\nAccessing prior state data from the resource.DeleteRequest.State field.\nPerforming logic or external calls to destroy the resource.\nIf the logic needs to return warning or error diagnostics, they can added into the resource.DeleteResponse.Diagnostics field.\nIn this example, the Delete function makes a HTTP call and returns successfully if the status code was 200 OK or 404 Not Found:\n// ThingResource defines the resource implementation. // Some resource.Resource interface methods are omitted for brevity. type ThingResource struct { // client is configured via a Configure method, which is not shown in this // example for brevity. Refer to the Configure Resources documentation for // additional details for setting up resources with external clients. client *http.Client } // ThingResourceModel describes the Terraform resource data model to match the // resource schema. type ThingResourceModel struct { Name types.String `tfsdk:\"name\"` Id types.String `tfsdk:\"id\"` } // ThingResourceAPIModel describes the API data model. type ThingResourceAPIModel struct { Name string `json:\"name\"` Id string `json:\"id\"` } func (r ThingResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"name\": schema.StringAttribute{ MarkdownDescription: \"Name of the thing to be saved in the service.\", Required: true, }, \"id\": schema.StringAttribute{ Computed: true, MarkdownDescription: \"Service generated identifier for the thing.\", PlanModifiers: []planmodifier.String{ stringplanmodifier.UseStateForUnknown(), }, }, }, MarkdownDescription: \"Manages a thing.\", } } func (r ThingResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) { var data ThingResourceModel // Read Terraform prior state data into the model resp.Diagnostics.Append(req.State.Get(ctx, &data)...) // Convert from Terraform data model into API data model readReq := ThingResourceAPIModel{ Id: data.Id.ValueString(), Name: data.Name.ValueString(), } httpReqBody, err := json.Marshal(readReq) if err != nil { resp.Diagnostics.AddError( \"Unable to Delete Resource\", \"An unexpected error occurred while creating the resource d request. \"+ \"Please report this issue to the provider developers.\\n\\n\"+ \"JSON Error: \"+err.Error(), ) return } // Create HTTP request httpReq := http.NewRequestWithContext( ctx, http.MethodDelete, \"http://example.com/things\", bytes.NewBuffer(httpReqBody), ) // Send HTTP request httpResp, err := d.client.Do(httpReq) defer httpResp.Body.Close() if err != nil { resp.Diagnostics.AddError( \"Unable to Delete Resource\", \"An unexpected error occurred while attempting to delete the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Error: \"+err.Error(), ) return } // Return error if the HTTP status code is not 200 OK or 404 Not Found if httpResp.StatusCode != http.StatusNotFound && httpResp.StatusCode != http.StatusOK { resp.Diagnostics.AddError( \"Unable to Delete Resource\", \"An unexpected error occurred while attempting to delete the resource. \"+ \"Please retry the operation or report this issue to the provider developers.\\n\\n\"+ \"HTTP Status: \"+httpResp.Status, ) return } // If the logic reaches here, it implicitly succeeded and will remove // the resource from state if there are no other errors. } \nNote these caveats when implementing the Delete method:\nAn error is returned if the response state is set to anything other than null.\nAny response errors will cause Terraform to keep the resource under management.\nNote these recommendations when implementing the Delete method:\nIgnore errors that signify the resource is no longer existent.\nSkip calling the response state RemoveResource() method. The framework automatically handles this logic with the response state if there are no error diagnostics."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.7.x/resources/default",
  "text": "Plugin Development - Framework: Default | Terraform\nAfter validation and before applying configuration changes, Terraform generates a plan that describes the expected values and behaviors of those changes. Resources can then tailor the plan to set default values on computed resource attributes that are null in the configuration.\nA Default can only be added to a resource schema attribute.\nA Default is set during the planning process, immediately prior to the framework marking computed attributes that are null in the configuration as unknown in the plan.\nYou can supply the attribute type Default field with a default for that attribute. For example:\n// Typically within the schema.Schema returned by Schema() for a resource. schema.StringAttribute{ // ... other Attribute configuration ... Default: stringdefault.StaticString(\"str\"), } schema.SetAttribute{ // ... other Attribute configuration ... Default: setdefault.StaticValue( types.SetValueMust( types.StringType, []attr.Value{ types.StringValue(\"str\"), }, ), ), }, \nIf defined, a default is applied to the current attribute providing that the attribute is null in the configuration. If any nested attributes define a default, then those are applied afterwards. Any default that returns an error will prevent Terraform from applying further defaults of that attribute as well as any nested attribute defaults.\nCommon Use Case Attribute Defaults\nThe framework implements static value defaults in the typed packages under resource/schema/:\nCustom Default Implementations\nTo create an attribute default, you must implement the one of the resource/schema/defaults package interfaces. For example:\n// timeDefaultValue is a default that sets the value for a types.StringType // attribute to the current time when it is not configured. The attribute // must be marked as Optional and Computed. When setting the state during // the resource Create, Read, or Update methods, this value must also be // included or the Terraform CLI will generate an error. type timeDefaultValue struct { time time.Time } // Description returns a plain text description of the default's behavior, suitable for a practitioner to understand its impact. func (d timeDefaultValue) Description(ctx context.Context) string { return fmt.Sprintf(\"If value is not configured, defaults to a string representation of the current time\") } // MarkdownDescription returns a markdown formatted description of the default's behavior, suitable for a practitioner to understand its impact. func (d timeDefaultValue) MarkdownDescription(ctx context.Context) string { return fmt.Sprintf(\"If value is not configured, defaults to a string representation of the current time\") } // DefaultString runs the logic of the default. Access to the path is available in `req`, while // `resp` contains fields for updating the planned value, and returning diagnostics. func (d timeDefaultValue) DefaultString(_ context.Context, req defaults.StringRequest, resp *defaults.StringResponse) { resp.PlanValue = types.StringValue(d.time.Format(time.RFC3339)) } \nOptionally, you may also want to create a helper function to instantiate the default. For example:\nfunc timeDefault(t time.Time) defaults.String { return timeDefaultValue{ time: t, } }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.7.x/resources/plan-modification",
  "text": "After validation and before applying configuration changes, Terraform generates a plan that describes the expected values and behaviors of those changes. Resources can then tailor the plan to match the expected end state, prevent errant in-place updates, or return any diagnostics.\nTerraform and the framework support multiple types of plan modification on resources:\nAdjusting unknown attribute values, such as providing a known remote default value when a configuration is not present.\nReturning warning or error diagnostics on planned resource creation, update, or deletion.\nPlan modification can be added on resource schema attributes or an entire resource. Use resource-based plan modification if access to the configured resource is necessary.\nSet any attributes with a null configuration value to the default value.\nRun attribute plan modifiers.\nRun resource plan modifiers.\nDuring the terraform plan and terraform apply commands, Terraform calls the provider PlanResourceChange RPC, in which the framework calls the resource.Resource interface Schema method attribute plan modifiers and the ModifyPlan method on resources that implement the resource.ResourceWithModifyPlan interface.\nYou can supply the attribute type PlanModifiers field with a list of plan modifiers for that attribute. For example:\n// Typically within the schema.Schema returned by Schema() for a resource. schema.StringAttribute{ // ... other Attribute configuration ... PlanModifiers: []planmodifier.String{ stringplanmodifier.RequiresReplace(), }, } \nThe framework implements some common use case modifiers in the typed packages under resource/schema/, such as resource/schema/stringplanmodifier:\nRequiresReplace(): If the value of the attribute changes, in-place update is not possible and instead the resource should be replaced for the change to occur. Refer to the Go documentation for full details on its behavior.\nRequiresReplaceIf(): Similar to resource.RequiresReplace(), however it also accepts provider-defined conditional logic. Refer to the Go documentation for full details on its behavior.\nRequiresReplaceIfConfigured(): Similar to resource.RequiresReplace(), however it also will only trigger if the practitioner has configured a value. Refer to the Go documentation for full details on its behavior.\nUseStateForUnknown(): Copies the prior state value, if not null. This is useful for reducing (known after apply) plan outputs for computed attributes which are known to not change over time.\nTo create an attribute plan modifier, you must implement the one of the planmodifier package interfaces. For example:\n// useStateForUnknownModifier implements the plan modifier. type useStateForUnknownModifier struct{} // Description returns a human-readable description of the plan modifier. func (m useStateForUnknownModifier) Description(_ context.Context) string { return \"Once set, the value of this attribute in state will not change.\" } // MarkdownDescription returns a markdown description of the plan modifier. func (m useStateForUnknownModifier) MarkdownDescription(_ context.Context) string { return \"Once set, the value of this attribute in state will not change.\" } // PlanModifyBool implements the plan modification logic. func (m useStateForUnknownModifier) PlanModifyBool(_ context.Context, req planmodifier.BoolRequest, resp *planmodifier.BoolResponse) { // Do nothing if there is no state value. if req.StateValue.IsNull() { return } // Do nothing if there is a known planned value. if !req.PlanValue.IsUnknown() { return } // Do nothing if there is an unknown configuration value, otherwise interpolation gets messed up. if req.ConfigValue.IsUnknown() { return } resp.PlanValue = req.StateValue } \n// UseStateForUnknown returns a plan modifier that copies a known prior state // value into the planned value. Use this when it is known that an unconfigured // value will remain the same after a resource update. // // To prevent Terraform errors, the framework automatically sets unconfigured // and Computed attributes to an unknown value \"(known after apply)\" on update. // Using this plan modifier will instead display the prior state value in the // plan, unless a prior plan modifier adjusts the value. func UseStateForUnknown() planmodifier.Bool { return useStateForUnknownModifier{} } \nCaveats\nTerraform Data Consistency Rules\nTerraform core implements data consistency rules between configuration, plan, and state data. For example, if an attribute value is configured, it is never valid to change that value in the plan except being set to null on resource destroy. The framework does not raise its own targeted errors in many situations, so it is the responsibility of the developer to account for these rules when implementing plan modification logic.\nPrior State Under Lists and Sets\nAttribute plan modifiers under the following must take special consideration if they rely on prior state data:\nList nested attributes\nList nested blocks\nSet nested attributes\nSet nested blocks\nThese data structures are implemented based on array indexing, which the framework always sends the exact representation given across the protocol. If list/set elements are rearranged or removed, Terraform nor the framework performs any re-alignment of prior state for those elements.\nIn this example, potentially unexpected prior state may be given to attribute plan modifier request:\nA list nested attribute with two elements in configuration is saved into state\nThe configuration for the first element is removed\nThe list nested attribute with now one element still receives the prior state of the first element\nChecking Resource Change Operations\nPlan modifiers execute on all resource change operations: creation, update, and destroy. If the plan modification logic is sensitive to these details, check the request data to determine the current operation.\nImplement the following to check whether the resource is being created:\nfunc (m ExampleModifier) PlanModifyString(_ context.Context, req planmodifier.StringRequest, resp *planmodifier.StringResponse) { // Check if the resource is being created. if req.State.Raw.IsNull() { // ... } // ... } \nImplement the following to check whether the resource is being destroyed:\nfunc (m ExampleModifier) PlanModifyString(_ context.Context, req planmodifier.StringRequest, resp *planmodifier.StringResponse) { // Check if the resource is being destroyed. if req.Plan.Raw.IsNull() { // ... } // ... } \nResources also support plan modification across all attributes. This is helpful when working with logic that applies to the resource as a whole, or in Terraform 1.3 and later, to return diagnostics during resource destruction. Implement the resource.ResourceWithModifyPlan interface to support resource-level plan modification. For example:\n// Ensure the Resource satisfies the resource.ResourceWithModifyPlan interface. // Other methods to implement the resource.Resource interface are omitted for brevity var _ resource.ResourceWithModifyPlan = ThingResource{} type ThingResource struct {} func (r ThingResource) ModifyPlan(ctx context.Context, req resource.ModifyPlanRequest, resp *resource.ModifyPlanResponse) { // Fill in logic. } \nResource Destroy Plan Diagnostics\nSupport for handling resource destruction during planning is available in Terraform 1.3 and later.\nImplement the ModifyPlan method by checking if the resource.ModifyPlanRequest type Plan field is a null value:\nfunc (r ThingResource) ModifyPlan(ctx context.Context, req resource.ModifyPlanRequest, resp *resource.ModifyPlanResponse) { // If the entire plan is null, the resource is planned for destruction. if req.Plan.Raw.IsNull() { // Return an example warning diagnostic to practitioners. resp.Diagnostics.AddWarning( \"Resource Destruction Considerations\", \"Applying this resource destruction will only remove the resource from the Terraform state \"+ \"and will not call the deletion API due to API limitations. Manually use the web \"+ \"interface to fully destroy this resource.\", ) } } \nEnsure the response plan remains entirely null when the request plan is entirely null."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.7.x/resources/plan-modification",
  "text": "Terraform and the framework support two types of plan modification on resources:\n// Typically within the tfsdk.Schema returned by GetSchema() for a resource. tfsdk.Attribute{ // ... other Attribute configuration ... PlanModifiers: []AttributePlanModifiers{ tfsdk.RequiresReplace(), }, } \ntfsdk.RequiresReplace(): If the value of the attribute changes, in-place update is not possible and instead the resource should be replaced for the change to occur. Refer to the Go documentation for full details on its behavior.\ntfsdk.RequiresReplaceIf(): Similar to tfsdk.RequiresReplace(), however it also accepts provider-defined conditional logic. Refer to the Go documentation for full details on its behavior.\ntfsdk.UseStateForUnknown(): Copies the prior state value, if not null. This is useful for reducing (known after apply) plan outputs for computed attributes which are known to not change over time.\nResource schemas also support plan modification across all attributes. This is helpful when working with logic that applies to the resource as a whole. To create a resource schema plan modification, you must implement the tfsdk.ResourceWithModifyPlan interface. For example:\n// Other methods to implement the tfsdk.Resource interface are omitted for brevity type exampleResource struct {} func (r exampleResource) ModifyPlan(ctx context.Context, req ModifyResourcePlanRequest, resp *ModifyResourcePlanResponse) { // Fill in logic. }"
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.3.x/getting-started/code-walkthrough",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.2.x/getting-started/code-walkthrough",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v0.7.x/resources/import",
  "text": "Practitioners can use the terraform import command to let Terraform begin managing existing infrastructure resources. Resources can implement the ImportState method, which must either specify enough Terraform state for the Read method to refresh tfsdk.Resource or return an error.\nWhen the Read method requires a single attribute to refresh, use the tfsdk.ResourceImportStatePassthroughID function to write the import identifier argument for terraform import.\nfunc (r exampleResource) ImportState(ctx context.Context, req tfsdk.ImportResourceStateRequest, resp *tfsdk.ImportResourceStateResponse) { tfsdk.ResourceImportStatePassthroughID(ctx, tftypes.NewAttributePath().WithAttributeName(\"id\"), req, resp) } \nUse the import identifier from the tfsdk.ImportResourceStateRequest. \nSet state data in the tfsdk.ImportResourceStateResponse.\nFor example, if the tfsdk.ResourceType implementation has the following GetSchema method:\nAlong with a tfsdk.Resource implementation with the following Read method:\nfunc (r exampleResource) Read(ctx context.Context, req tfsdk.ReadResourceRequest, resp *tfsdk.ReadResourceResponse) { var attrOne, attrTwo string resp.Diagnostics.Append(req.State.GetAttribute(ctx, tftypes.NewAttributePath().WithAttributeName(\"attr_one\"), &attrOne)...) resp.Diagnostics.Append(req.State.GetAttribute(ctx, tftypes.NewAttributePath().WithAttributeName(\"attr_two\"), &attrTwo)...) if resp.Diagnostics.HasError() { return } // API call using attrOne and attrTwo } \nfunc (r exampleResource) ImportState(ctx context.Context, req tfsdk.ImportResourceStateRequest, resp *tfsdk.ImportResourceStateResponse) { idParts := strings.Split(req.ID, \",\") if len(idParts) != 2 || idParts[0] == \"\" || idParts[1] == \"\" { resp.Diagnostics.AddError( \"Unexpected Import Identifier\", fmt.Sprintf(\"Expected import identifier with format: attr_one,attr_two. Got: %q\", req.ID), ) return } resp.Diagnostics.Append(resp.State.SetAttribute(ctx, tftypes.NewAttributePath().WithAttributeName(\"attr_one\"), idParts[0])...) resp.Diagnostics.Append(resp.State.SetAttribute(ctx, tftypes.NewAttributePath().WithAttributeName(\"attr_two\"), idParts[1])...) } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.7.x/resources/state-move",
  "text": "Plugin Development - Framework: State Move | Terraform\nState move across managed resource types is supported in Terraform 1.8 and later.\nTerraform is designed with each managed resource type being distinguished from all other types. To prevent data loss or unexpected data issues, Terraform will raise an error when practitioners attempt to refactor existing resource usage across resource types via the moved configuration block since data compatibility is not guaranteed. Provider developers can opt into explicitly enabling Terraform to allow these refactoring operations for a target resource type based on source resource type criteria. This criteria can include the source provider address, resource type name, and schema version.\nExample use cases include:\nRenaming a resource type, such as API service name changes or for Terraform resource naming consistency.\nSplitting a resource type, into separate resource types for practitioner ease, such as a compute resource into Linux and Windows variants.\nHanding a resource type with API versioning quirks, such as multiple resource types representing the same real world resources with partially different configuration data/concepts.\nA managed resource type has an associated state, which captures the structure and types of data for the resource type. Enabling state move support requires the provider to handle data transformation logic which takes in source resource type state as an input and outputs the equivalent target resource type state.\nWhen a plan is generated with a moved configuration block, Terraform will send a request to the provider with all the source resource state information (provider address, resource type, schema version) and target resource type. The framework will check the target resource to see if it defines state move support.\nThe framework implementation does the following:\nIf no state move support is defined for the resource, an error diagnostic is returned.\nIf state move support is defined for the resource, each provider defined implementation is called until one responds with error diagnostics or state data.\nIf all implementations return without error diagnostics and state data, an error diagnostic is returned.\nImplement the resource.ResourceWithMoveState interface for the resource.Resource. That interface requires the MoveState method, which enables individual source resource criteria and logic for each source resource type to support.\nThis example shows a Resource with the necessary MoveState method to implement the ResourceWithMoveState interface:\n// Other Resource methods are omitted in this example var _ resource.ResourceWithMoveState = &TargetResource{} type TargetResource struct{/* ... */} func (r *TargetResource) MoveState(ctx context.Context) []resource.StateMover { return []resource.StateMover{ { // Optionally, the SourceSchema field can be defined. StateMover: func(ctx context.Context, req resource.MoveStateRequest, resp *resource.MoveStateResponse) { /* ... */ }, }, // ... potentially more StateMover for each compatible source ... } } \nEach resource.StateMover implementation is expected to:\nCheck the resource.MoveStateRequest for whether this implementation matches a known source resource. It is always recommended to check the SourceTypeName, SourceSchemaVersion, and SourceProviderAddress (without the hostname, unless needed for disambiguation).\nIf not matching, return early without diagnostics or setting state data in the resource.MoveStateResponse. The framework will try the next implementation.\nIf matching, wholly set the resource state from the source state. All state data must be populated in the resource.MoveStateResponse. The framework does not copy any source state data from the resource.MoveStateRequest.\nThere are two approaches to implementing the provider logic for state moves in StateMover. The recommended approach is defining the source schema matching the source resource state, which allows for source state access similar to other parts of the framework. The second, more advanced, approach is accessing the source resource state using lower level data handlers.\nStateMover With SourceSchema\nImplement the StateMover type SourceSchema field to enable the framework to populate the resource.MoveStateRequest type SourceState field for the provider defined state move logic. Access the request SourceState using methods such as Get() or GetAttribute(). Write the resource.MoveStateResponse type TargetState field using methods such as Set() or SetAttribute().\nThis example shows a target resource that supports moving state from a source resource, using the SourceSchema approach:\n// Other Resource methods are omitted in this example var _ resource.Resource = &TargetResource{} var _ resource.ResourceWithMoveState = &TargetResource{} type TargetResource struct{/* ... */} type TargetResourceModel struct { Id types.String `tfsdk:\"id\"` TargetAttribute types.Bool `tfsdk:\"target_attribute\"` } type SourceResourceModel struct { Id types.String `tfsdk:\"id\"` SourceAttribute types.Bool `tfsdk:\"source_attribute\"` } func (r *TargetResource) Schema(_ context.Context, _ resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"id\": schema.StringAttribute{ /* ... */ }, \"target_attribute\": schema.BoolAttribute{ /* ... */ }, }, } } func (r *TargetResource) MoveState(ctx context.Context) []resource.StateMover { return []resource.StateMover{ { SourceSchema: &schema.Schema{ Attributes: map[string]schema.Attribute{ \"id\": schema.StringAttribute{}, \"source_attribute\": schema.BoolAttribute{}, }, }, StateMover: func(ctx context.Context, req resource.MoveStateRequest, resp *resource.MoveStateResponse) { // Always verify the expected source before working with the data. if req.SourceTypeName != \"examplecloud_source\" { return } if req.SourceSchemaVersion != 0 { return } // This only checks the provider address namespace and type // since practitioners may use differing hostnames for the same // provider, such as a network mirror. If necessary though, the // hostname can be used for disambiguation. if !strings.HasSuffix(req.SourceProviderAddress, \"examplecorp/examplecloud\") { return } var sourceStateData SourceResourceModel resp.Diagnostics.Append(req.SourceState.Get(ctx, &sourceStateData)...) if resp.Diagnostics.HasError() { return } targetStateData := TargetResourceModel{ Id: sourceStateData.Id, TargetAttribute: sourceStateData.SourceAttribute, } resp.Diagnostics.Append(resp.TargetState.Set(ctx, targetStateData)...) }, }, } } \nStateMover Without SourceSchema\nRead source state data from the resource.MoveStateRequest type SourceRawState field. Write the resource.MoveStateResponse type TargetState field using methods such as Set() or SetAttribute().\nThis example shows a target resource that supports moving state from a source resource, using the SourceRawState approach for the request:\n// Other Resource methods are omitted in this example var _ resource.Resource = &TargetResource{} var _ resource.ResourceWithMoveState = &TargetResource{} type TargetResource struct{/* ... */} type TargetResourceModel struct { Id types.String `tfsdk:\"id\"` TargetAttribute types.Bool `tfsdk:\"target_attribute\"` } func (r *TargetResource) Schema(_ context.Context, _ resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"id\": schema.StringAttribute{ /* ... */ }, \"target_attribute\": schema.BoolAttribute{ /* ... */ }, }, } } func (r *TargetResource) MoveState(ctx context.Context) []resource.StateMover { return []resource.StateMover{ { StateMover: func(ctx context.Context, req resource.MoveStateRequest, resp *resource.MoveStateResponse) { // Always verify the expected source before working with the data. if req.SourceTypeName != \"examplecloud_source\" { return } if req.SourceSchemaVersion != 0 { return } // This only checks the provider address namespace and type // since practitioners may use differing hostnames for the same // provider, such as a network mirror. If necessary though, the // hostname can be used for disambiguation. if !strings.HasSuffix(req.SourceProviderAddress, \"examplecorp/examplecloud\") { return } // Refer also to the RawState type JSON field which can be used // with json.Unmarshal() rawStateValue, err := req.SourceRawState.Unmarshal(tftypes.Object{ AttributeTypes: map[string]tftypes.Type{ \"id\": tftypes.String, \"source_attribute\": tftypes.Bool, }, }) if err != nil { resp.Diagnostics.AddError( \"Unable to Unmarshal Source State\", err.Error(), ) return } var rawState map[string]tftypes.Value if err := rawStateValue.As(&rawState); err != nil { resp.Diagnostics.AddError( \"Unable to Convert Source State\", err.Error(), ) return } var id *string if err := rawState[\"id\"].As(&id); err != nil { resp.Diagnostics.AddAttributeError( path.Root(\"id\"), \"Unable to Convert Source State\", err.Error(), ) return } var sourceAttribute *bool if err := rawState[\"source_attribute\"].As(&sourceAttribute); err != nil { resp.Diagnostics.AddAttributeError( path.Root(\"source_attribute\"), \"Unable to Convert Source State\", err.Error(), ) return } targetStateData := TargetResourceModel{ Id: types.StringPointerValue(id), TargetAttribute: types.BoolPointerValue(sourceAttribute), } resp.Diagnostics.Append(resp.TargetState.Set(ctx, targetStateData)...) }, }, } } \nNote these caveats when implementing the MoveState method:\nThe SourceState will not always be nil if the schema does not match the source state. Always verify the implementation matches other request fields (SourceTypeName, etc.) beforehand.\nAny response errors will cause Terraform to keep the source resource state."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.7.x/resources/import",
  "text": "Practitioners can use the terraform import command to let Terraform begin managing existing infrastructure resources. Resources can implement the ImportState method, which must either specify enough Terraform state for the Read method to refresh resource.Resource or return an error.\nThe resource.ResourceWithImportState interface on the resource.Resource interface implementation will enable practitioner support for importing an existing resource.\nIn this example, the resource state has the id attribute set to the value passed into the terraform import command using the resource.ImportStatePassthroughID function:\nfunc (r *ThingResource) ImportState(ctx context.Context, req resource.ImportStateRequest, resp *resource.ImportStateResponse) { resource.ImportStatePassthroughID(ctx, path.Root(\"id\"), req, resp) } \nMultiple Attributes\nWhen the Read method requires multiple attributes to refresh, you must write custom logic in the ImportState method.\nPerforming the custom logic.\nThe terraform import command will need to accept the multiple attribute values as a single import identifier string. A typical convention is to use a separator character, such as a comma (,), between the values. The ImportState method will then need to parse the import identifier string into the multiple separate values and save them appropriately into the Terraform state.\nIn this example, the resource requires two attributes to refresh state and accepts them as an import identifier of attr_one,attr_two:\nfunc (r *ThingResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ \"attr_one\": schema.StringAttribute{ Required: true, }, \"attr_two\": schema.StringAttribute{ Required: true, }, /* ... */ }, } } func (r *ThingResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) { var attrOne, attrTwo string resp.Diagnostics.Append(req.State.GetAttribute(ctx, path.Root(\"attr_one\"), &attrOne)...) resp.Diagnostics.Append(req.State.GetAttribute(ctx, path.Root(\"attr_two\"), &attrTwo)...) if resp.Diagnostics.HasError() { return } // API call using attrOne and attrTwo } func (r *ThingResource) ImportState(ctx context.Context, req resource.ImportStateRequest, resp *resource.ImportStateResponse) { idParts := strings.Split(req.ID, \",\") if len(idParts) != 2 || idParts[0] == \"\" || idParts[1] == \"\" { resp.Diagnostics.AddError( \"Unexpected Import Identifier\", fmt.Sprintf(\"Expected import identifier with format: attr_one,attr_two. Got: %q\", req.ID), ) return } resp.Diagnostics.Append(resp.State.SetAttribute(ctx, path.Root(\"attr_one\"), idParts[0])...) resp.Diagnostics.Append(resp.State.SetAttribute(ctx, path.Root(\"attr_two\"), idParts[1])...) } "
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.7.x/resources/timeouts",
  "text": "Plugin Development - Framework: Timeouts | Terraform\nThe reality of cloud infrastructure is that it typically takes time to perform operations such as booting operating systems, discovering services, and replicating state across network edges. As the provider developer you should take known delays in resource APIs into account in the CRUD functions of the resource. Terraform supports configurable timeouts to assist in these situations.\nThe Framework can be used in conjunction with the terraform-plugin-framework-timeouts module in order to allow defining timeouts in configuration and have them be available in CRUD functions.\nTimeouts can be defined using either nested blocks or nested attributes.\nIf you are writing a new provider using terraform-plugin-framework then we recommend using nested attributes.\nIf you are migrating a provider from SDKv2 to the Framework and you are already using timeouts you can either continue to use block syntax, or switch to using nested attributes. However, switching to using nested attributes will require that practitioners that are using your provider update their Terraform configuration.\nBlock\nIf your configuration is using a nested block to define timeouts, such as the following:\nresource \"timeouts_example\" \"example\" { /* ... */ timeouts { create = \"60m\" } } \nImport the timeouts module.\nimport ( /* ... */ \"github.com/hashicorp/terraform-plugin-framework-timeouts/resource/timeouts\" ) \nYou can use this module to mutate the schema.Schema as follows:\nfunc (t *exampleResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ /* ... */ Blocks: map[string]schema.Block{ \"timeouts\": timeouts.Block(ctx, timeouts.Opts{ Create: true, }), }, \nAttribute\nIf your configuration is using nested attributes to define timeouts, such as the following:\nresource \"timeouts_example\" \"example\" { /* ... */ timeouts = { create = \"60m\" } } \nYou can use this module to mutate the schema.Schema as follows:\nfunc (t *exampleResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ /* ... */ \"timeouts\": timeouts.Attributes(ctx, timeouts.Opts{ Create: true, }), }, \nIn functions in which the config, state or plan is being unmarshalled, for instance, the Create function, the model will need to be updated.\nfunc (e *exampleResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleResourceData diags := req.Plan.Get(ctx, &data) resp.Diagnostics.Append(diags...) \nModify the exampleResourceData model to include a field for timeouts using a timeouts.Value type.\ntype exampleResourceData struct { /* ... */ Timeouts timeouts.Value `tfsdk:\"timeouts\"` \nOnce the model has been populated with the config, state or plan the duration of the timeout can be accessed by calling the appropriate helper function (e.g., timeouts.Create) and then used to configure timeout behaviour, for instance:\nfunc (e *exampleResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) { var data exampleResourceData diags := req.Plan.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } createTimeout, diags := data.Timeouts.Create(ctx, 20*time.Minute) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } ctx, cancel := context.WithTimeout(ctx, createTimeout) defer cancel() /* ... */ }"
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.10.x/commands/workspace/new",
  "text": "Command: workspace new | Terraform\nThe terraform workspace new command is used to create a new workspace.\nUsage: terraform workspace new [OPTIONS] NAME [DIR]\nThis command will create a new workspace with the given name. A workspace with this name must not already exist.\nIf the -state flag is given, the state specified by the given path will be copied to initialize the state for this new workspace.\nThe command-line flags are all optional. The supported flags are:\n-lock=false - Don't hold a state lock during the operation. This is dangerous if others might concurrently run commands against the same workspace.\n-lock-timeout=DURATION - Duration to retry a state lock. Default 0s.\n-state=path - Path to an existing state file to initialize the state of this environment.\n$ terraform workspace new example Created and switched to workspace \"example\"! You're now on a new, empty workspace. Workspaces isolate their state, so if you run \"terraform plan\" Terraform will not see any existing state for this configuration. \nTo create a new workspace from a pre-existing local state file:\n$ terraform workspace new -state=old.terraform.tfstate example Created and switched to workspace \"example\". You're now on a new, empty workspace. Workspaces isolate their state, so if you run \"terraform plan\" Terraform will not see any existing state for this configuration."
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.10.x/commands/workspace/list",
  "text": "Command: workspace list | Terraform\nThe terraform workspace list command is used to list all existing workspaces.\nUsage: terraform workspace list [DIR]\nThe command will list all existing workspaces. The current workspace is indicated using an asterisk (*) marker.\n$ terraform workspace list default * development jsmith-test"
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.10.x/commands/workspace/delete",
  "text": "Command: workspace delete | Terraform\nThe terraform workspace delete command is used to delete an existing workspace.\nUsage: terraform workspace delete [OPTIONS] NAME [DIR]\nThis command will delete the specified workspace.\nTo delete a workspace, it must already exist, it must not be tracking resources, and it must not be your current workspace. If the workspace is tracking resources, Terraform will not allow you to delete it unless the -force flag is specified.\nAdditionally, different backends may implement other restrictions on whether a workspace is considered safe to delete without the -force flag, such as whether the workspace is locked.\nIf you delete a workspace which is tracking resources (via -force), then resources may become \"dangling\". These are resources that physically exist but that Terraform can no longer manage. This is sometimes preferred: you may want Terraform to stop managing resources, so they can be managed some other way. Most of the time, however, this is not intended and so Terraform protects you from getting into this situation.\nThe command-line flags are all optional. The only supported flags are:\n-force - Delete the workspace even if it is tracking resources. After deletion, Terraform can no longer track or manage the workspace's infrastructure. Defaults to false.\n-lock=false - Don't hold a state lock during the operation. This is dangerous if others might concurrently run commands against the same workspace.\n-lock-timeout=DURATION - Duration to retry a state lock. Default 0s.\n$ terraform workspace delete example Deleted workspace \"example\"."
},
{
  "url": "https://developer.hashicorp.com/terraform/plugin/framework/v1.7.x/data-sources/timeouts",
  "text": "Plugin Development - Framework: Timeouts | Terraform\nThe reality of cloud infrastructure is that it typically takes time to perform operations such as booting operating systems, discovering services, and replicating state across network edges. As the provider developer you should take known delays in data source APIs into account in the Read function of the data source. Terraform supports configurable timeouts to assist in these situations.\nThe Framework can be used in conjunction with the terraform-plugin-framework-timeouts module in order to allow defining timeouts in configuration and have them be available in the Read function.\nTimeouts can be defined using either nested blocks or nested attributes.\nIf you are writing a new provider using terraform-plugin-framework then we recommend using nested attributes.\nIf you are migrating a provider from SDKv2 to the Framework and you are already using timeouts you can either continue to use block syntax, or switch to using nested attributes. However, switching to using nested attributes will require that practitioners that are using your provider update their Terraform configuration.\nBlock\nIf your configuration is using a nested block to define timeouts, such as the following:\nresource \"timeouts_example\" \"example\" { /* ... */ timeouts { read = \"60m\" } } \nImport the timeouts module.\nimport ( /* ... */ \"github.com/hashicorp/terraform-plugin-framework-timeouts/datasource/timeouts\" ) \nYou can use this module to mutate the schema.Schema as follows:\nfunc (d *ThingDataSource) Schema(ctx context.Context, req datasource.SchemaRequest, resp *datasource.SchemaResponse) { resp.Schema = schema.Schema{ /* ... */ Blocks: map[string]schema.Block{ \"timeouts\": timeouts.Block(ctx), }, \nAttribute\nIf your configuration is using nested attributes to define timeouts, such as the following:\nresource \"timeouts_example\" \"example\" { /* ... */ timeouts = { read = \"60m\" } } \nYou can use this module to mutate the schema.Schema as follows:\nfunc (d *ThingDataSource) Schema(ctx context.Context, req datasource.SchemaRequest, resp *datasource.SchemaResponse) { resp.Schema = schema.Schema{ Attributes: map[string]schema.Attribute{ /* ... */ \"timeouts\": timeouts.Attributes(ctx), }, \nGiven a Read method which fetches the entire configuration:\nfunc (e *exampleDataSource) Read(ctx context.Context, req datasource.ReadRequest, resp *datasource.ReadResponse) { var data exampleDataSourceData diags := req.Config.Get(ctx, &data) resp.Diagnostics.Append(diags...) \nModify the exampleDataSourceData model to include a field for timeouts using a timeouts.Value type.\ntype exampleDataSourceData struct { /* ... */ Timeouts timeouts.Value `tfsdk:\"timeouts\"` \nCall the timeouts.Read() function.\nfunc (e *exampleDataSource) Read(ctx context.Context, req datasource.ReadRequest, resp *datasource.ReadResponse) { var data exampleDataSourceData diags := req.Config.Get(ctx, &data) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } readTimeout, diags := data.Timeouts.Read(ctx, 20*time.Minute) resp.Diagnostics.Append(diags...) if resp.Diagnostics.HasError() { return } ctx, cancel := context.WithTimeout(ctx, readTimeout) defer cancel() /* ... */ }"
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.10.x/commands/workspace/select",
  "text": "Command: workspace select | Terraform\nThe terraform workspace select command is used to choose a different workspace to use for further operations.\nUsage\nUsage: terraform workspace select NAME [DIR]\nThis command will select another workspace. The named workspace must already exist.\nThe supported flags are:\n-or-create - If the workspace that is being selected does not exist, create it. Default is false.\nExample\n$ terraform workspace list default * development jsmith-test $ terraform workspace select default Switched to workspace \"default\"."
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.8.x/commands/workspace/list",
  "text": "Command: workspace list | Terraform\nThe terraform workspace list command is used to list all existing workspaces.\nUsage: terraform workspace list [DIR]\nThe command will list all existing workspaces. The current workspace is indicated using an asterisk (*) marker.\n$ terraform workspace list default * development jsmith-test"
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.8.x/commands/workspace/new",
  "text": "Command: workspace new | Terraform\nThe terraform workspace new command is used to create a new workspace.\nUsage: terraform workspace new [OPTIONS] NAME [DIR]\nThis command will create a new workspace with the given name. A workspace with this name must not already exist.\nIf the -state flag is given, the state specified by the given path will be copied to initialize the state for this new workspace.\nThe command-line flags are all optional. The supported flags are:\n-lock=false - Don't hold a state lock during the operation. This is dangerous if others might concurrently run commands against the same workspace.\n-lock-timeout=DURATION - Duration to retry a state lock. Default 0s.\n-state=path - Path to an existing state file to initialize the state of this environment.\n$ terraform workspace new example Created and switched to workspace \"example\"! You're now on a new, empty workspace. Workspaces isolate their state, so if you run \"terraform plan\" Terraform will not see any existing state for this configuration. \nTo create a new workspace from a pre-existing local state file:\n$ terraform workspace new -state=old.terraform.tfstate example Created and switched to workspace \"example\". You're now on a new, empty workspace. Workspaces isolate their state, so if you run \"terraform plan\" Terraform will not see any existing state for this configuration."
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.8.x/commands/workspace/delete",
  "text": "Command: workspace delete | Terraform\nThe terraform workspace delete command is used to delete an existing workspace.\nUsage: terraform workspace delete [OPTIONS] NAME [DIR]\nThis command will delete the specified workspace.\nTo delete a workspace, it must already exist, it must not be tracking resources, and it must not be your current workspace. If the workspace is tracking resources, Terraform will not allow you to delete it unless the -force flag is specified.\nAdditionally, different backends may implement other restrictions on whether a workspace is considered safe to delete without the -force flag, such as whether the workspace is locked.\nIf you delete a workspace which is tracking resources (via -force), then resources may become \"dangling\". These are resources that physically exist but that Terraform can no longer manage. This is sometimes preferred: you may want Terraform to stop managing resources, so they can be managed some other way. Most of the time, however, this is not intended and so Terraform protects you from getting into this situation.\nThe command-line flags are all optional. The only supported flags are:\n-force - Delete the workspace even if it is tracking resources. After deletion, Terraform can no longer track or manage the workspace's infrastructure. Defaults to false.\n-lock=false - Don't hold a state lock during the operation. This is dangerous if others might concurrently run commands against the same workspace.\n-lock-timeout=DURATION - Duration to retry a state lock. Default 0s.\n$ terraform workspace delete example Deleted workspace \"example\"."
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.8.x/commands/workspace/select",
  "text": "Command: workspace select | Terraform\nThe terraform workspace select command is used to choose a different workspace to use for further operations.\nUsage\nUsage: terraform workspace select NAME [DIR]\nThis command will select another workspace. The named workspace must already exist.\nThe supported flags are:\n-or-create - If the workspace that is being selected does not exist, create it. Default is false.\nExample\n$ terraform workspace list default * development jsmith-test $ terraform workspace select default Switched to workspace \"default\"."
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.7.x/commands/workspace/list",
  "text": "Command: workspace list | Terraform\nThe terraform workspace list command is used to list all existing workspaces.\nUsage: terraform workspace list [DIR]\nThe command will list all existing workspaces. The current workspace is indicated using an asterisk (*) marker.\n$ terraform workspace list default * development jsmith-test"
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.7.x/commands/workspace/new",
  "text": "Command: workspace new | Terraform\nThe terraform workspace new command is used to create a new workspace.\nUsage: terraform workspace new [OPTIONS] NAME [DIR]\nThis command will create a new workspace with the given name. A workspace with this name must not already exist.\nIf the -state flag is given, the state specified by the given path will be copied to initialize the state for this new workspace.\nThe command-line flags are all optional. The supported flags are:\n-state=path - Path to an existing state file to initialize the state of this environment.\n$ terraform workspace new example Created and switched to workspace \"example\"! You're now on a new, empty workspace. Workspaces isolate their state, so if you run \"terraform plan\" Terraform will not see any existing state for this configuration. \nTo create a new workspace from a pre-existing local state file:\n$ terraform workspace new -state=old.terraform.tfstate example Created and switched to workspace \"example\". You're now on a new, empty workspace. Workspaces isolate their state, so if you run \"terraform plan\" Terraform will not see any existing state for this configuration."
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.7.x/commands/workspace/delete",
  "text": "Command: workspace delete | Terraform\nThe terraform workspace delete command is used to delete an existing workspace.\nUsage: terraform workspace delete [OPTIONS] NAME [DIR]\nThis command will delete the specified workspace.\nTo delete a workspace, it must already exist, it must not be tracking resources, and it must not be your current workspace. If the workspace is tracking resources, Terraform will not allow you to delete it unless the -force flag is specified.\nAdditionally, different backends may implement other restrictions on whether a workspace is considered safe to delete without the -force flag, such as whether the workspace is locked.\nIf you delete a workspace which is tracking resources (via -force), then resources may become \"dangling\". These are resources that physically exist but that Terraform can no longer manage. This is sometimes preferred: you may want Terraform to stop managing resources, so they can be managed some other way. Most of the time, however, this is not intended and so Terraform protects you from getting into this situation.\nThe command-line flags are all optional. The only supported flags are:\n-force - Delete the workspace even if it is tracking resources. After deletion, Terraform can no longer track or manage the workspace's infrastructure. Defaults to false.\n$ terraform workspace delete example Deleted workspace \"example\"."
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.7.x/commands/workspace/select",
  "text": "Command: workspace select | Terraform\nThe terraform workspace select command is used to choose a different workspace to use for further operations.\nUsage\nUsage: terraform workspace select NAME [DIR]\nThis command will select another workspace. The named workspace must already exist.\nThe supported flags are:\n-or-create - If the workspace that is being selected does not exist, create it. Default is false.\nExample"
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.6.x/commands/workspace/list",
  "text": "Command: workspace list | Terraform\nThe terraform workspace list command is used to list all existing workspaces.\nUsage: terraform workspace list [DIR]\nThe command will list all existing workspaces. The current workspace is indicated using an asterisk (*) marker.\n$ terraform workspace list default * development jsmith-test"
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.6.x/commands/workspace/delete",
  "text": "Command: workspace delete | Terraform\nThe terraform workspace delete command is used to delete an existing workspace.\nUsage: terraform workspace delete [OPTIONS] NAME [DIR]\nThis command will delete the specified workspace.\nTo delete a workspace, it must already exist, it must not be tracking resources, and it must not be your current workspace. If the workspace is tracking resources, Terraform will not allow you to delete it unless the -force flag is specified.\nAdditionally, different backends may implement other restrictions on whether a workspace is considered safe to delete without the -force flag, such as whether the workspace is locked.\nIf you delete a workspace which is tracking resources (via -force), then resources may become \"dangling\". These are resources that physically exist but that Terraform can no longer manage. This is sometimes preferred: you may want Terraform to stop managing resources, so they can be managed some other way. Most of the time, however, this is not intended and so Terraform protects you from getting into this situation.\nThe command-line flags are all optional. The only supported flags are:\n-force - Delete the workspace even if it is tracking resources. After deletion, Terraform can no longer track or manage the workspace's infrastructure. Defaults to false.\n$ terraform workspace delete example Deleted workspace \"example\"."
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.6.x/commands/workspace/select",
  "text": "Command: workspace select | Terraform\nThe terraform workspace select command is used to choose a different workspace to use for further operations.\nUsage\nUsage: terraform workspace select NAME [DIR]\nThis command will select another workspace. The named workspace must already exist.\nThe supported flags are:\n-or-create - If the workspace that is being selected does not exist, create it. Default is false.\nExample"
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.5.x/commands/workspace/new",
  "text": "Command: workspace new | Terraform\nThe terraform workspace new command is used to create a new workspace.\nUsage: terraform workspace new [OPTIONS] NAME [DIR]\nThis command will create a new workspace with the given name. A workspace with this name must not already exist.\nIf the -state flag is given, the state specified by the given path will be copied to initialize the state for this new workspace.\nThe command-line flags are all optional. The supported flags are:\n-state=path - Path to an existing state file to initialize the state of this environment.\n$ terraform workspace new example Created and switched to workspace \"example\"! You're now on a new, empty workspace. Workspaces isolate their state, so if you run \"terraform plan\" Terraform will not see any existing state for this configuration. \nTo create a new workspace from a pre-existing local state file:\n$ terraform workspace new -state=old.terraform.tfstate example Created and switched to workspace \"example\". You're now on a new, empty workspace. Workspaces isolate their state, so if you run \"terraform plan\" Terraform will not see any existing state for this configuration."
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.6.x/commands/workspace/new",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.5.x/commands/workspace/list",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.5.x/commands/workspace/delete",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.5.x/commands/workspace/select",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.4.x/commands/workspace/list",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.4.x/commands/workspace/delete",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.4.x/commands/workspace/select",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.4.x/commands/workspace/new",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.3.x/commands/workspace/list",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.3.x/commands/workspace/new",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.3.x/commands/workspace/delete",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.3.x/commands/workspace/select",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.1.x/commands/workspace/select",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.1.x/commands/workspace/list",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.1.x/commands/workspace/new",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.1.x/commands/workspace/delete",
  "text": "To delete an workspace, it must already exist, it must have an empty state, and it must not be your current workspace. If the workspace state is not empty, Terraform will not allow you to delete it unless the -force flag is specified.\nIf you delete a workspace with a non-empty state (via -force), then resources may become \"dangling\". These are resources that physically exist but that Terraform can no longer manage. This is sometimes preferred: you want Terraform to stop managing resources so they can be managed some other way. Most of the time, however, this is not intended and so Terraform protects you from getting into this situation.\nThe command-line flags are all optional. The only supported flag is:\n-force - Delete the workspace even if its state is not empty. Defaults to false."
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.2.x/commands/workspace/select",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.2.x/commands/workspace/list",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.2.x/commands/workspace/new",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/v1.2.x/commands/workspace/delete",
  "text": "To delete an workspace, it must already exist, it must have an empty state, and it must not be your current workspace. If the workspace state is not empty, Terraform will not allow you to delete it unless the -force flag is specified.\nIf you delete a workspace with a non-empty state (via -force), then resources may become \"dangling\". These are resources that physically exist but that Terraform can no longer manage. This is sometimes preferred: you want Terraform to stop managing resources so they can be managed some other way. Most of the time, however, this is not intended and so Terraform protects you from getting into this situation.\nThe command-line flags are all optional. The only supported flag is:\n-force - Delete the workspace even if its state is not empty. Defaults to false."
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/commands/workspace",
  "text": "Command: workspace | Terraform | HashiCorp Developer\nThe terraform workspace command is used to manage workspaces.\nThis command is a container for further subcommands that each have their own page in the documentation.\nUsage: terraform workspace <subcommand> [options] [args]\nChoose a subcommand page for more information."
},
{
  "url": "https://developer.hashicorp.com/terraform/cli/commands/workspace/show",
  "text": "Command: workspace show | Terraform\nThe terraform workspace show command is used to output the current workspace.\nUsage: terraform workspace show\nThe command will display the current workspace.\n$ terraform workspace show development"
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202407-1/policy-enforcement/manage-policy-sets",
  "text": "Manage Policies and Policy Sets - Terraform Enterprise | Terraform\nPolicies are rules that HCP Terraform enforces on Terraform runs. You can define policies using either the Sentinel or Open Policy Agent (OPA) policy-as-code frameworks.\nPolicy sets are collections of policies you can apply globally or to specific projects and workspaces in your organization. For each run in the applicable workspaces, HCP Terraform checks the Terraform plan against the policy set. Depending on the enforcement level, failed policies can stop a run in a workspace. If you do not want to enforce a policy set on a specific workspace, you can exclude the workspace from that set.\nTo view and manage policies and policy sets, you must have manage policy permissions for your organization.\nPolicy checks and evaluations can access different types of data and enable slightly different workflows.\nPolicy checks\nOnly Sentinel policies can run as policy checks, and it is the default mode for Sentinel policy sets. Checks can access cost estimation data but can only use the latest version of Sentinel.\nPolicy evaluations\nOPA policy sets can only run as policy evaluations, and you can enable policy evaluations for Sentinel policy sets by selecting the Enhanced policy set type. Policy evaluations run within the HCP Terraform agent in HCP Terraform's infrastructure. \nFor Sentinel policy sets, using policy evaluations lets you:\nEnable overrides for soft-mandatory and hard-mandatory policies, letting any user with Manage Policy Overrides permission proceed with a run in the event of policy failure.\nSelect a specific Sentinel runtime version for the policy set.\nPolicy evaluations cannot access cost estimation data, so use policy checks if your policies rely on cost estimates.\nTip: Sentinel runtime version pinning is supported only for Sentinel 0.23.1 and above, as well as HCP Terraform agent versions 1.13.1 and above\nYou can set an enforcement level for each policy that determines what happens when a Terraform plan does not pass the policy rule. Sentinel and OPA policies have different enforcement levels available.\nSentinel\nSentinel provides three policy enforcement levels:\nadvisory: Failed policies never interrupt the run. They provide information about policy check failures in the UI.\nsoft mandatory: Failed policies stop the run, but any user with Manage Policy Overrides permission can override these failures and allow the run to complete.\nhard mandatory: Failed policies stop the run. Terraform does not apply runs with failed hard mandatory policies until a user fixes the issue that caused the failure.\nOPA\nOPA provides two policy enforcement levels:\nadvisory Failed policies never interrupt the run. They provide information about policy failures in the UI.\nmandatory: Failed policies stop the run, but any user with Manage Policy Overrides permission can override these failures and allow the run to complete.\nYou can create policies and policy sets for your HCP Terraform organization in one of three ways:\nHCP Terraform web UI: Add individually-managed policies manually in the HCP Terraform UI, and store your policy code in HCP Terraform. This workflow is ideal for initial experimentation with policy enforcement, but we do not recommend it for organizations with large numbers of policies.\nVersion control: Connect HCP Terraform to a version control repository containing a policy set. When you push changes to the repository, HCP Terraform automatically uses the updated policy set.\nAutomated: Push versions of policy sets to HCP Terraform with the HCP Terraform Policy Sets API or the tfe provider tfe_policy_set resource. This workflow is ideal for automated Continuous Integration and Deployment (CI/CD) pipelines.\nManage individual policies in the web UI\nYou can add policies directly to HCP Terraform using the web UI. This process requires you to paste completed, valid Sentinel or Rego code into the UI. We recommend validating your policy code before adding it to HCP Terraform.\nAdd managed policies\nTo add an individually managed policy:\nGo to Policies in your organizations settings. A list of managed policies in HCP Terraform appears. Each policy designates its policy framework (Sentinel or OPA) and associated policy sets.\nClick Create a new policy.\nChoose the Policy framework you want to use. You can only create a policy set from policies written using the same framework. You cannot change the framework type after you create the policy.\nComplete the following fields to define the policy:\nPolicy Name: Add a name containing letters, numbers, -, and _. HCP Terraform displays this name in the UI. The name must be unique within your organization.\nDescription: Describe the policys purpose. The description supports Markdown rendering, and HCP Terraform displays this text in the UI.\nEnforcement mode: Choose whether this policy can stop Terraform runs and whether users can override it. Refer to policy enforcement levels for more details.\n(OPA Only) Query: Write a query to identify a specific policy rule within your rego code. HCP Terraform uses this query to determine the result of the policy. The query is typically a combination of the policy package name and rule name, such as terraform.deny. The result of this query must be an array. The policy passes when the array is empty.\nPolicy code: Paste the code for the policy: either Sentinel code or Rego code for OPA policies. The UI provides syntax highlighting for the policy language.\n(Optional) Policy sets: Select one or more existing managed policy sets where you want to add the new policy. You can only select policy sets compatible with the chosen policy set framework. If there are no policy sets available, you can create a new one.\nThe policy is now available in the HCP Terraform UI for you to edit and add to one or more policy sets.\nEdit managed policies\nTo edit a managed policy:\nGo to Policies in your organizations settings.\nClick the policy you want to edit to go to its details page.\nEdit the policy's fields and then click Update policy.\nDelete managed policies\nWarning: Deleting a policy that applies to an active run causes that runs policy evaluation stage to error. We recommend warning other members of your organization before you delete widely used policies.\nYou can not restore policies after deletion. You must manually re-add them to HCP Terraform. You may want to save the policy code in a separate location before you delete the policy.\nTo delete a managed policy:\nGo to Policies in your organizations settings.\nClick the policy you want to delete to go to its details page.\nClick Delete policy and then click Yes, delete policy to confirm.\nThe policy no longer appears in HCP Terraform and in any associated policy sets.\nPolicy sets are collections of policies that you can apply globally or to specific projects and workspaces.\nTo view and manage policy sets, go to the Policy Sets section of your organizations settings. This page contains all of the policy sets available in the organization, including those added through the API.\nThe way you set up and configure a new policy set depends on your workflow and where you store policies.\nFor managed policies, you use the UI to create a policy set and add managed policies.\nFor policy sets in a version control system, you use the UI to create a policy set connected to that repository. HCP Terraform automatically refreshes the policy set when you change relevant files in that repository. Version control policy sets have specific organization and formatting requirements. Refer to Sentinel VCS Repositories and OPA VCS Repositories for details.\nFor automated workflows like continuous deployment, you can use the UI to create an empty policy set and then use the Policy Sets API to add policies. You can also use the API or the tfe provider (Sentinel Only) to add an entire, packaged policy set.\nCreate policy sets\nTo create a policy set:\nGo to Policy Sets in your organizations settings.\nClick Connect a new policy set.\nChoose your workflow.\nFor managed policies, click create a policy set with individually managed policies. HCP Terraform shows a form to create a policy set and add individually managed policies.\nFor version control policies, choose a version control provider and then select the repository with your policy set. HCP Terraform shows a form to create a policy set connected to that repository.\nFor automated workflows, click No VCS Connection. HCP Terraform shows a form to create an empty policy set. You can use the API to add policies to this empty policy set later.\nChoose a Policy framework for the policies you want to add. A policy set can only contain policies that use the same framework (OPA or Sentinel). You cannot change a policy set's framework type after creation.\nChoose a policy set scope:\nPolicies enforced globally: HCP Terraform automatically enforces this global policy set on all of an organization's existing and future workspaces.\nPolicies enforced on selected projects and workspaces: Use the text fields to find and select the workspaces and projects to enforce this policy set on. This affects all current and future workspaces for any chosen projects.\n(Optional) Add Policy exclusions for this policy set. Specify any workspaces in the policy set's scope that HCP Terraform will not enforce this policy set on.\n(Sentinel Only) Choose a policy set type:\nStandard: This is the default workflow. A Sentinel policy set uses a policy check in HCP Terraform and lets you access cost estimation data.\nEnhanced: A Sentinel policy set uses a policy evaluation in HCP Terraform. This lets you enable policy overrides and enforce a Sentinel runtime version\n(OPA Only) Select a Runtime version for this policy set.\n(OPA Only) Allow Overrides, which enables users with override policy permissions to apply plans that have mandatory policy failures.\n(VCS Only) Optionally specify the VCS branch within your VCS repository where HCP Terraform should import new versions of policies. If you do not set this field, HCP Terraform uses your selected VCS repository's default branch.\n(VCS Only) Specify where your policy set files live using the Policies path. This lets you maintain multiple policy sets within a single repository. Use a relative path from your root directory to the directory that contains either the sentinel.hcl (Sentinel) or policies.hcl (OPA) configuration files. If you do not set this field, HCP Terraform uses the repository's root directory.\n(Managed Policies Only) Select managed Policies to add to the policy set. You can only add policies written with the same policy framework you selected for this set.\nChoose a descriptive and unique Name for the policy set. You can use any combination of letters, numbers, -, and _.\nWrite an optional Description that tells other users about the purpose of the policy set and what it contains.\nEdit policy sets\nTo edit a policy set:\nGo to the Policy Sets section of your organizations settings.\nClick the policy set you want to edit to go to its settings page.\nAdjust the settings and click Update policy set.\nEvaluate a policy runtime upgrade\nYou can validate that changing a policy runtime version does not introduce any breaking changes.\nNote: On-demand policy evaluation is in beta.\nTo perform a policy evaluation:\nGo to the Policy Sets section of your organizations settings.\nClick the policy set you want to upgrade.\nClick the Evaluate tab.\nSelect the Runtime version you wish to upgrade to.\nSelect a Workspace to test the policy and upgraded version against.\nClick Evaluate.\nHCP Terraform will execute the policy set using the specified version and the latest plan data for the selected workspace. It will display the evaluation results. If the evaluation returns a Failed status, inspect the JSON output to determine whether the issue is related to a non-compliant resource or is due to a syntax issue. If the evaluation results in an error, check that the policy configuration is valid.\nDelete policy sets\nWarning: Deleting a policy set that applies to an active run causes that runs policy evaluation stage to error. We recommend warning other members of your organization before you delete widely used policy sets.\nYou can not restore policy sets after deletion. You must manually re-add them to HCP Terraform.\nTo delete a policy set:\nGo to Policy Sets in your organizations settings.\nClick the policy set you want to delete to go to its details page.\nClick Delete policy and then click Yes, delete policy set to confirm.\nThe policy set no longer appears on the UI and HCP Terraform no longer applies it to any workspaces. For managed policy sets, all of the individual policies are still available in HCP Terraform. You must delete each policy individually to remove it from your organization.\n(Sentinel only) Sentinel parameters\nSentinel parameters are a list of key/value pairs that HCP Terraform sends to the Sentinel runtime when performing policy checks on workspaces. If the value parses as JSON, HCP Terraform sends it to Sentinel as the corresponding type (string, boolean, integer, map, or list). If the value fails JSON validation, HCP Terraform sends it as a string.\nYou can set Sentinel parameters when you edit a policy set.\nYou can only set parameters for existing policy sets that you added through the tfe provider, API, or a connected VCS repository. Parameters are not available for managed policy sets."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202406-1/policy-enforcement/manage-policy-sets",
  "text": "Manage Policies and Policy Sets - Terraform Enterprise | Terraform\nPolicies are rules that HCP Terraform enforces on Terraform runs. You can define policies using either the Sentinel or Open Policy Agent (OPA) policy-as-code frameworks.\nPolicy sets are collections of policies you can apply globally or to specific projects and workspaces in your organization. For each run in the applicable workspaces, HCP Terraform checks the Terraform plan against the policy set. Depending on the enforcement level, failed policies can stop a run in a workspace. If you do not want to enforce a policy set on a specific workspace, you can exclude the workspace from that set.\nTo view and manage policies and policy sets, you must have manage policy permissions for your organization.\nPolicy checks and evaluations can access different types of data and enable slightly different workflows.\nPolicy checks\nOnly Sentinel policies can run as policy checks, and it is the default mode for Sentinel policy sets. Checks can access cost estimation data but can only use the latest version of Sentinel.\nPolicy evaluations\nOPA policy sets can only run as policy evaluations, and you can enable policy evaluations for Sentinel policy sets by selecting the Enhanced policy set type. Policy evaluations run within the HCP Terraform agent in HCP Terraform's infrastructure. \nFor Sentinel policy sets, using policy evaluations lets you:\nEnable overrides for soft-mandatory and hard-mandatory policies, letting any user with Manage Policy Overrides permission proceed with a run in the event of policy failure.\nSelect a specific Sentinel runtime version for the policy set.\nPolicy evaluations cannot access cost estimation data, so use policy checks if your policies rely on cost estimates.\nTip: Sentinel runtime version pinning is supported only for Sentinel 0.23.1 and above, as well as HCP Terraform agent versions 1.13.1 and above\nYou can set an enforcement level for each policy that determines what happens when a Terraform plan does not pass the policy rule. Sentinel and OPA policies have different enforcement levels available.\nSentinel\nSentinel provides three policy enforcement levels:\nadvisory: Failed policies never interrupt the run. They provide information about policy check failures in the UI.\nsoft mandatory: Failed policies stop the run, but any user with Manage Policy Overrides permission can override these failures and allow the run to complete.\nhard mandatory: Failed policies stop the run. Terraform does not apply runs with failed hard mandatory policies until a user fixes the issue that caused the failure.\nOPA\nOPA provides two policy enforcement levels:\nadvisory Failed policies never interrupt the run. They provide information about policy failures in the UI.\nmandatory: Failed policies stop the run, but any user with Manage Policy Overrides permission can override these failures and allow the run to complete.\nYou can create policies and policy sets for your HCP Terraform organization in one of three ways:\nHCP Terraform web UI: Add individually-managed policies manually in the HCP Terraform UI, and store your policy code in HCP Terraform. This workflow is ideal for initial experimentation with policy enforcement, but we do not recommend it for organizations with large numbers of policies.\nVersion control: Connect HCP Terraform to a version control repository containing a policy set. When you push changes to the repository, HCP Terraform automatically uses the updated policy set.\nAutomated: Push versions of policy sets to HCP Terraform with the HCP Terraform Policy Sets API or the tfe provider tfe_policy_set resource. This workflow is ideal for automated Continuous Integration and Deployment (CI/CD) pipelines.\nManage individual policies in the web UI\nYou can add policies directly to HCP Terraform using the web UI. This process requires you to paste completed, valid Sentinel or Rego code into the UI. We recommend validating your policy code before adding it to HCP Terraform.\nAdd managed policies\nTo add an individually managed policy:\nGo to Policies in your organizations settings. A list of managed policies in HCP Terraform appears. Each policy designates its policy framework (Sentinel or OPA) and associated policy sets.\nClick Create a new policy.\nChoose the Policy framework you want to use. You can only create a policy set from policies written using the same framework. You cannot change the framework type after you create the policy.\nComplete the following fields to define the policy:\nPolicy Name: Add a name containing letters, numbers, -, and _. HCP Terraform displays this name in the UI. The name must be unique within your organization.\nDescription: Describe the policys purpose. The description supports Markdown rendering, and HCP Terraform displays this text in the UI.\nEnforcement mode: Choose whether this policy can stop Terraform runs and whether users can override it. Refer to policy enforcement levels for more details.\n(OPA Only) Query: Write a query to identify a specific policy rule within your rego code. HCP Terraform uses this query to determine the result of the policy. The query is typically a combination of the policy package name and rule name, such as terraform.deny. The result of this query must be an array. The policy passes when the array is empty.\nPolicy code: Paste the code for the policy: either Sentinel code or Rego code for OPA policies. The UI provides syntax highlighting for the policy language.\n(Optional) Policy sets: Select one or more existing managed policy sets where you want to add the new policy. You can only select policy sets compatible with the chosen policy set framework. If there are no policy sets available, you can create a new one.\nThe policy is now available in the HCP Terraform UI for you to edit and add to one or more policy sets.\nEdit managed policies\nTo edit a managed policy:\nGo to Policies in your organizations settings.\nClick the policy you want to edit to go to its details page.\nEdit the policy's fields and then click Update policy.\nDelete managed policies\nWarning: Deleting a policy that applies to an active run causes that runs policy evaluation stage to error. We recommend warning other members of your organization before you delete widely used policies.\nYou can not restore policies after deletion. You must manually re-add them to HCP Terraform. You may want to save the policy code in a separate location before you delete the policy.\nTo delete a managed policy:\nGo to Policies in your organizations settings.\nClick the policy you want to delete to go to its details page.\nClick Delete policy and then click Yes, delete policy to confirm.\nThe policy no longer appears in HCP Terraform and in any associated policy sets.\nPolicy sets are collections of policies that you can apply globally or to specific projects and workspaces.\nTo view and manage policy sets, go to the Policy Sets section of your organizations settings. This page contains all of the policy sets available in the organization, including those added through the API.\nThe way you set up and configure a new policy set depends on your workflow and where you store policies.\nFor managed policies, you use the UI to create a policy set and add managed policies.\nFor policy sets in a version control system, you use the UI to create a policy set connected to that repository. HCP Terraform automatically refreshes the policy set when you change relevant files in that repository. Version control policy sets have specific organization and formatting requirements. Refer to Sentinel VCS Repositories and OPA VCS Repositories for details.\nFor automated workflows like continuous deployment, you can use the UI to create an empty policy set and then use the Policy Sets API to add policies. You can also use the API or the tfe provider (Sentinel Only) to add an entire, packaged policy set.\nCreate policy sets\nTo create a policy set:\nGo to Policy Sets in your organizations settings.\nClick Connect a new policy set.\nChoose your workflow.\nFor managed policies, click create a policy set with individually managed policies. HCP Terraform shows a form to create a policy set and add individually managed policies.\nFor version control policies, choose a version control provider and then select the repository with your policy set. HCP Terraform shows a form to create a policy set connected to that repository.\nFor automated workflows, click No VCS Connection. HCP Terraform shows a form to create an empty policy set. You can use the API to add policies to this empty policy set later.\nChoose a Policy framework for the policies you want to add. A policy set can only contain policies that use the same framework (OPA or Sentinel). You cannot change a policy set's framework type after creation.\nChoose a policy set scope:\nPolicies enforced globally: HCP Terraform automatically enforces this global policy set on all of an organization's existing and future workspaces.\nPolicies enforced on selected projects and workspaces: Use the text fields to find and select the workspaces and projects to enforce this policy set on. This affects all current and future workspaces for any chosen projects.\n(Optional) Add Policy exclusions for this policy set. Specify any workspaces in the policy set's scope that HCP Terraform will not enforce this policy set on.\n(Sentinel Only) Choose a policy set type:\nStandard: This is the default workflow. A Sentinel policy set uses a policy check in HCP Terraform and lets you access cost estimation data.\nEnhanced: A Sentinel policy set uses a policy evaluation in HCP Terraform. This lets you enable policy overrides and enforce a Sentinel runtime version\n(OPA Only) Select a Runtime version for this policy set.\n(OPA Only) Allow Overrides, which enables users with override policy permissions to apply plans that have mandatory policy failures.\n(VCS Only) Optionally specify the VCS branch within your VCS repository where HCP Terraform should import new versions of policies. If you do not set this field, HCP Terraform uses your selected VCS repository's default branch.\n(VCS Only) Specify where your policy set files live using the Policies path. This lets you maintain multiple policy sets within a single repository. Use a relative path from your root directory to the directory that contains either the sentinel.hcl (Sentinel) or policies.hcl (OPA) configuration files. If you do not set this field, HCP Terraform uses the repository's root directory.\n(Managed Policies Only) Select managed Policies to add to the policy set. You can only add policies written with the same policy framework you selected for this set.\nChoose a descriptive and unique Name for the policy set. You can use any combination of letters, numbers, -, and _.\nWrite an optional Description that tells other users about the purpose of the policy set and what it contains.\nEdit policy sets\nTo edit a policy set:\nGo to the Policy Sets section of your organizations settings.\nClick the policy set you want to edit to go to its settings page.\nAdjust the settings and click Update policy set.\nEvaluate a policy runtime upgrade\nYou can validate that changing a policy runtime version does not introduce any breaking changes.\nNote: On-demand policy evaluation is in beta.\nTo perform a policy evaluation:\nGo to the Policy Sets section of your organizations settings.\nClick the policy set you want to upgrade.\nClick the Evaluate tab.\nSelect the Runtime version you wish to upgrade to.\nSelect a Workspace to test the policy and upgraded version against.\nClick Evaluate.\nHCP Terraform will execute the policy set using the specified version and the latest plan data for the selected workspace. It will display the evaluation results. If the evaluation returns a Failed status, inspect the JSON output to determine whether the issue is related to a non-compliant resource or is due to a syntax issue. If the evaluation results in an error, check that the policy configuration is valid.\nDelete policy sets\nWarning: Deleting a policy set that applies to an active run causes that runs policy evaluation stage to error. We recommend warning other members of your organization before you delete widely used policy sets.\nYou can not restore policy sets after deletion. You must manually re-add them to HCP Terraform.\nTo delete a policy set:\nGo to Policy Sets in your organizations settings.\nClick the policy set you want to delete to go to its details page.\nClick Delete policy and then click Yes, delete policy set to confirm.\nThe policy set no longer appears on the UI and HCP Terraform no longer applies it to any workspaces. For managed policy sets, all of the individual policies are still available in HCP Terraform. You must delete each policy individually to remove it from your organization.\n(Sentinel only) Sentinel parameters\nSentinel parameters are a list of key/value pairs that HCP Terraform sends to the Sentinel runtime when performing policy checks on workspaces. If the value parses as JSON, HCP Terraform sends it to Sentinel as the corresponding type (string, boolean, integer, map, or list). If the value fails JSON validation, HCP Terraform sends it as a string.\nYou can set Sentinel parameters when you edit a policy set.\nYou can only set parameters for existing policy sets that you added through the tfe provider, API, or a connected VCS repository. Parameters are not available for managed policy sets."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202405-1/policy-enforcement/manage-policy-sets",
  "text": "Manage Policies and Policy Sets - Terraform Enterprise | Terraform\nPolicies are rules that HCP Terraform enforces on Terraform runs. You can define policies using either the Sentinel or Open Policy Agent (OPA) policy-as-code frameworks.\nPolicy sets are collections of policies you can apply globally or to specific projects and workspaces in your organization. For each run in the applicable workspaces, HCP Terraform checks the Terraform plan against the policy set. Depending on the enforcement level, failed policies can stop a run in a workspace. If you do not want to enforce a policy set on a specific workspace, you can exclude the workspace from that set.\nTo view and manage policies and policy sets, you must have manage policy permissions for your organization.\nPolicy checks and evaluations can access different types of data and enable slightly different workflows.\nPolicy checks\nOnly Sentinel policies can run as policy checks, and it is the default mode for Sentinel policy sets. Checks can access cost estimation data but can only use the latest version of Sentinel.\nPolicy evaluations\nOPA policy sets can only run as policy evaluations, and you can enable policy evaluations for Sentinel policy sets by selecting the Enhanced policy set type. Policy evaluations run within the Terraform Cloud agent in HCP Terraform's infrastructure. \nFor Sentinel policy sets, using policy evaluations lets you:\nEnable overrides for soft-mandatory and hard-mandatory policies, letting any user with Manage Policy Overrides permission proceed with a run in the event of policy failure.\nSelect a specific Sentinel runtime version for the policy set.\nPolicy evaluations cannot access cost estimation data, so use policy checks if your policies rely on cost estimates.\nTip: Sentinel runtime version pinning is supported only for Sentinel 0.23.1 and above, as well as Terraform Cloud agent versions 1.13.1 and above\nYou can set an enforcement level for each policy that determines what happens when a Terraform plan does not pass the policy rule. Sentinel and OPA policies have different enforcement levels available.\nSentinel\nSentinel provides three policy enforcement levels:\nadvisory: Failed policies never interrupt the run. They provide information about policy check failures in the UI.\nsoft mandatory: Failed policies stop the run, but any user with Manage Policy Overrides permission can override these failures and allow the run to complete.\nhard mandatory: Failed policies stop the run. Terraform does not apply runs with failed hard mandatory policies until a user fixes the issue that caused the failure.\nOPA\nOPA provides two policy enforcement levels:\nadvisory Failed policies never interrupt the run. They provide information about policy failures in the UI.\nmandatory: Failed policies stop the run, but any user with Manage Policy Overrides permission can override these failures and allow the run to complete.\nYou can create policies and policy sets for your HCP Terraform organization in one of three ways:\nHCP Terraform web UI: Add individually-managed policies manually in the HCP Terraform UI, and store your policy code in HCP Terraform. This workflow is ideal for initial experimentation with policy enforcement, but we do not recommend it for organizations with large numbers of policies.\nVersion control: Connect HCP Terraform to a version control repository containing a policy set. When you push changes to the repository, HCP Terraform automatically uses the updated policy set.\nAutomated: Push versions of policy sets to HCP Terraform with the HCP Terraform Policy Sets API or the tfe provider tfe_policy_set resource. This workflow is ideal for automated Continuous Integration and Deployment (CI/CD) pipelines.\nManage individual policies in the web UI\nYou can add policies directly to HCP Terraform using the web UI. This process requires you to paste completed, valid Sentinel or Rego code into the UI. We recommend validating your policy code before adding it to HCP Terraform.\nAdd managed policies\nTo add an individually managed policy:\nGo to Policies in your organizations settings. A list of managed policies in HCP Terraform appears. Each policy designates its policy framework (Sentinel or OPA) and associated policy sets.\nClick Create a new policy.\nChoose the Policy framework you want to use. You can only create a policy set from policies written using the same framework. You cannot change the framework type after you create the policy.\nComplete the following fields to define the policy:\nPolicy Name: Add a name containing letters, numbers, -, and _. HCP Terraform displays this name in the UI. The name must be unique within your organization.\nDescription: Describe the policys purpose. The description supports Markdown rendering, and HCP Terraform displays this text in the UI.\nEnforcement mode: Choose whether this policy can stop Terraform runs and whether users can override it. Refer to policy enforcement levels for more details.\n(OPA Only) Query: Write a query to identify a specific policy rule within your rego code. HCP Terraform uses this query to determine the result of the policy. The query is typically a combination of the policy package name and rule name, such as terraform.deny. The result of this query must be an array. The policy passes when the array is empty.\nPolicy code: Paste the code for the policy: either Sentinel code or Rego code for OPA policies. The UI provides syntax highlighting for the policy language.\n(Optional) Policy sets: Select one or more existing managed policy sets where you want to add the new policy. You can only select policy sets compatible with the chosen policy set framework. If there are no policy sets available, you can create a new one.\nThe policy is now available in the HCP Terraform UI for you to edit and add to one or more policy sets.\nEdit managed policies\nTo edit a managed policy:\nClick the policy you want to edit to go to its details page.\nEdit the policy's fields and then click Update policy.\nDelete managed policies\nWarning: Deleting a policy that applies to an active run causes that runs policy evaluation stage to error. We recommend warning other members of your organization before you delete widely used policies.\nYou can not restore policies after deletion. You must manually re-add them to HCP Terraform. You may want to save the policy code in a separate location before you delete the policy.\nTo delete a managed policy:\nClick the policy you want to delete to go to its details page.\nClick Delete policy and then click Yes, delete policy to confirm.\nThe policy no longer appears in HCP Terraform and in any associated policy sets.\nPolicy sets are collections of policies that you can apply globally or to specific projects and workspaces.\nTo view and manage policy sets, go to the Policy Sets section of your organizations settings. This page contains all of the policy sets available in the organization, including those added through the API.\nThe way you set up and configure a new policy set depends on your workflow and where you store policies.\nFor managed policies, you use the UI to create a policy set and add managed policies.\nFor policy sets in a version control system, you use the UI to create a policy set connected to that repository. HCP Terraform automatically refreshes the policy set when you change relevant files in that repository. Version control policy sets have specific organization and formatting requirements. Refer to Sentinel VCS Repositories and OPA VCS Repositories for details.\nFor automated workflows like continuous deployment, you can use the UI to create an empty policy set and then use the Policy Sets API to add policies. You can also use the API or the tfe provider (Sentinel Only) to add an entire, packaged policy set.\nCreate policy sets\nTo create a policy set:\nClick Connect a new policy set.\nChoose your workflow.\nFor managed policies, click create a policy set with individually managed policies. HCP Terraform shows a form to create a policy set and add individually managed policies.\nFor version control policies, choose a version control provider and then select the repository with your policy set. HCP Terraform shows a form to create a policy set connected to that repository.\nFor automated workflows, click No VCS Connection. HCP Terraform shows a form to create an empty policy set. You can use the API to add policies to this empty policy set later.\nChoose a Policy framework for the policies you want to add. A policy set can only contain policies that use the same framework (OPA or Sentinel). You cannot change a policy set's framework type after creation.\nChoose a policy set scope:\nPolicies enforced globally: HCP Terraform automatically enforces this global policy set on all of an organization's existing and future workspaces.\nPolicies enforced on selected projects and workspaces: Use the text fields to find and select the workspaces and projects to enforce this policy set on. This affects all current and future workspaces for any chosen projects.\n(Optional) Add Policy exclusions for this policy set. Specify any workspaces in the policy set's scope that HCP Terraform will not enforce this policy set on.\n(Sentinel Only) Choose a policy set type:\nStandard: This is the default workflow. A Sentinel policy set uses a policy check in HCP Terraform and lets you access cost estimation data.\nEnhanced: A Sentinel policy set uses a policy evaluation in HCP Terraform. This lets you enable policy overrides and enforce a Sentinel runtime version\n(OPA Only) Select a Runtime version for this policy set.\n(OPA Only) Allow Overrides, which enables users with override policy permissions to apply plans that have mandatory policy failures.\n(VCS Only) Optionally specify the VCS branch within your VCS repository where HCP Terraform should import new versions of policies. If you do not set this field, HCP Terraform uses your selected VCS repository's default branch.\n(VCS Only) Specify where your policy set files live using the Policies path. This lets you maintain multiple policy sets within a single repository. Use a relative path from your root directory to the directory that contains either the sentinel.hcl (Sentinel) or policies.hcl (OPA) configuration files. If you do not set this field, HCP Terraform uses the repository's root directory.\n(Managed Policies Only) Select managed Policies to add to the policy set. You can only add policies written with the same policy framework you selected for this set.\nChoose a descriptive and unique Name for the policy set. You can use any combination of letters, numbers, -, and _.\nWrite an optional Description that tells other users about the purpose of the policy set and what it contains.\nEdit policy sets\nTo edit a policy set:\nClick the policy set you want to edit to go to its settings page.\nAdjust the settings and click Update policy set.\nEvaluate a policy runtime upgrade\nYou can validate that changing a policy runtime version does not introduce any breaking changes.\nNote: On-demand policy evaluation is in beta.\nTo perform a policy evaluation:\nClick the policy set you want to upgrade.\nClick the Evaluate tab.\nSelect the Runtime version you wish to upgrade to.\nSelect a Workspace to test the policy and upgraded version against.\nClick Evaluate.\nHCP Terraform will execute the policy set using the specified version and the latest plan data for the selected workspace. It will display the evaluation results. If the evaluation returns a Failed status, inspect the JSON output to determine whether the issue is related to a non-compliant resource or is due to a syntax issue. If the evaluation results in an error, check that the policy configuration is valid.\nDelete policy sets\nWarning: Deleting a policy set that applies to an active run causes that runs policy evaluation stage to error. We recommend warning other members of your organization before you delete widely used policy sets.\nYou can not restore policy sets after deletion. You must manually re-add them to HCP Terraform.\nTo delete a policy set:\nClick the policy set you want to delete to go to its details page.\nClick Delete policy and then click Yes, delete policy set to confirm.\nThe policy set no longer appears on the UI and HCP Terraform no longer applies it to any workspaces. For managed policy sets, all of the individual policies are still available in HCP Terraform. You must delete each policy individually to remove it from your organization.\n(Sentinel only) Sentinel parameters\nSentinel parameters are a list of key/value pairs that HCP Terraform sends to the Sentinel runtime when performing policy checks on workspaces. If the value parses as JSON, HCP Terraform sends it to Sentinel as the corresponding type (string, boolean, integer, map, or list). If the value fails JSON validation, HCP Terraform sends it as a string.\nYou can set Sentinel parameters when you edit a policy set.\nYou can only set parameters for existing policy sets that you added through the tfe provider, API, or a connected VCS repository. Parameters are not available for managed policy sets."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202404-2/policy-enforcement/manage-policy-sets",
  "text": "Manage Policies and Policy Sets - Terraform Enterprise | Terraform\nPolicies are rules that HCP Terraform enforces on Terraform runs. You can define policies using either the Sentinel or Open Policy Agent (OPA) policy-as-code frameworks.\nPolicy sets are collections of policies you can apply globally or to specific projects and workspaces in your organization. For each run in the applicable workspaces, HCP Terraform checks the Terraform plan against the policy set. Depending on the enforcement level, failed policies can stop a run in a workspace. If you do not want to enforce a policy set on a specific workspace, you can exclude the workspace from that set.\nTo view and manage policies and policy sets, you must have manage policy permissions for your organization.\nPolicy checks and evaluations can access different types of data and enable slightly different workflows.\nPolicy checks\nOnly Sentinel policies can run as policy checks, and it is the default mode for Sentinel policy sets. Checks can access cost estimation data but can only use the latest version of Sentinel.\nPolicy evaluations\nOPA policy sets can only run as policy evaluations, and you can enable policy evaluations for Sentinel policy sets by selecting the Enhanced policy set type. Policy evaluations run within the HCP Terraform agent in HCP Terraform's infrastructure. \nFor Sentinel policy sets, using policy evaluations lets you:\nEnable overrides for soft-mandatory and hard-mandatory policies, letting any user with Manage Policy Overrides permission proceed with a run in the event of policy failure.\nSelect a specific Sentinel runtime version for the policy set.\nPolicy evaluations cannot access cost estimation data, so use policy checks if your policies rely on cost estimates.\nTip: Sentinel runtime version pinning is supported only for Sentinel 0.23.1 and above, as well as HCP Terraform agent versions 1.13.1 and above\nYou can set an enforcement level for each policy that determines what happens when a Terraform plan does not pass the policy rule. Sentinel and OPA policies have different enforcement levels available.\nSentinel\nSentinel provides three policy enforcement levels:\nadvisory: Failed policies never interrupt the run. They provide information about policy check failures in the UI.\nsoft mandatory: Failed policies stop the run, but any user with Manage Policy Overrides permission can override these failures and allow the run to complete.\nhard mandatory: Failed policies stop the run. Terraform does not apply runs with failed hard mandatory policies until a user fixes the issue that caused the failure.\nOPA\nOPA provides two policy enforcement levels:\nadvisory Failed policies never interrupt the run. They provide information about policy failures in the UI.\nmandatory: Failed policies stop the run, but any user with Manage Policy Overrides permission can override these failures and allow the run to complete.\nYou can create policies and policy sets for your HCP Terraform organization in one of three ways:\nHCP Terraform web UI: Add individually-managed policies manually in the HCP Terraform UI, and store your policy code in HCP Terraform. This workflow is ideal for initial experimentation with policy enforcement, but we do not recommend it for organizations with large numbers of policies.\nVersion control: Connect HCP Terraform to a version control repository containing a policy set. When you push changes to the repository, HCP Terraform automatically uses the updated policy set.\nAutomated: Push versions of policy sets to HCP Terraform with the HCP Terraform Policy Sets API or the tfe provider tfe_policy_set resource. This workflow is ideal for automated Continuous Integration and Deployment (CI/CD) pipelines.\nManage individual policies in the web UI\nYou can add policies directly to HCP Terraform using the web UI. This process requires you to paste completed, valid Sentinel or Rego code into the UI. We recommend validating your policy code before adding it to HCP Terraform.\nAdd managed policies\nTo add an individually managed policy:\nGo to Policies in your organizations settings. A list of managed policies in HCP Terraform appears. Each policy designates its policy framework (Sentinel or OPA) and associated policy sets.\nClick Create a new policy.\nChoose the Policy framework you want to use. You can only create a policy set from policies written using the same framework. You cannot change the framework type after you create the policy.\nComplete the following fields to define the policy:\nPolicy Name: Add a name containing letters, numbers, -, and _. HCP Terraform displays this name in the UI. The name must be unique within your organization.\nDescription: Describe the policys purpose. The description supports Markdown rendering, and HCP Terraform displays this text in the UI.\nEnforcement mode: Choose whether this policy can stop Terraform runs and whether users can override it. Refer to policy enforcement levels for more details.\n(OPA Only) Query: Write a query to identify a specific policy rule within your rego code. HCP Terraform uses this query to determine the result of the policy. The query is typically a combination of the policy package name and rule name, such as terraform.deny. The result of this query must be an array. The policy passes when the array is empty.\nPolicy code: Paste the code for the policy: either Sentinel code or Rego code for OPA policies. The UI provides syntax highlighting for the policy language.\n(Optional) Policy sets: Select one or more existing managed policy sets where you want to add the new policy. You can only select policy sets compatible with the chosen policy set framework. If there are no policy sets available, you can create a new one.\nThe policy is now available in the HCP Terraform UI for you to edit and add to one or more policy sets.\nEdit managed policies\nTo edit a managed policy:\nClick the policy you want to edit to go to its details page.\nEdit the policy's fields and then click Update policy.\nDelete managed policies\nWarning: Deleting a policy that applies to an active run causes that runs policy evaluation stage to error. We recommend warning other members of your organization before you delete widely used policies.\nYou can not restore policies after deletion. You must manually re-add them to HCP Terraform. You may want to save the policy code in a separate location before you delete the policy.\nTo delete a managed policy:\nClick the policy you want to delete to go to its details page.\nClick Delete policy and then click Yes, delete policy to confirm.\nThe policy no longer appears in HCP Terraform and in any associated policy sets.\nPolicy sets are collections of policies that you can apply globally or to specific projects and workspaces.\nTo view and manage policy sets, go to the Policy Sets section of your organizations settings. This page contains all of the policy sets available in the organization, including those added through the API.\nThe way you set up and configure a new policy set depends on your workflow and where you store policies.\nFor managed policies, you use the UI to create a policy set and add managed policies.\nFor policy sets in a version control system, you use the UI to create a policy set connected to that repository. HCP Terraform automatically refreshes the policy set when you change relevant files in that repository. Version control policy sets have specific organization and formatting requirements. Refer to Sentinel VCS Repositories and OPA VCS Repositories for details.\nFor automated workflows like continuous deployment, you can use the UI to create an empty policy set and then use the Policy Sets API to add policies. You can also use the API or the tfe provider (Sentinel Only) to add an entire, packaged policy set.\nCreate policy sets\nTo create a policy set:\nClick Connect a new policy set.\nChoose your workflow.\nFor managed policies, click create a policy set with individually managed policies. HCP Terraform shows a form to create a policy set and add individually managed policies.\nFor version control policies, choose a version control provider and then select the repository with your policy set. HCP Terraform shows a form to create a policy set connected to that repository.\nFor automated workflows, click No VCS Connection. HCP Terraform shows a form to create an empty policy set. You can use the API to add policies to this empty policy set later.\nChoose a Policy framework for the policies you want to add. A policy set can only contain policies that use the same framework (OPA or Sentinel). You cannot change a policy set's framework type after creation.\nChoose a policy set scope:\nPolicies enforced globally: HCP Terraform automatically enforces this global policy set on all of an organization's existing and future workspaces.\nPolicies enforced on selected projects and workspaces: Use the text fields to find and select the workspaces and projects to enforce this policy set on. This affects all current and future workspaces for any chosen projects.\n(Optional) Add Policy exclusions for this policy set. Specify any workspaces in the policy set's scope that HCP Terraform will not enforce this policy set on.\n(Sentinel Only) Choose a policy set type:\nStandard: This is the default workflow. A Sentinel policy set uses a policy check in HCP Terraform and lets you access cost estimation data.\nEnhanced: A Sentinel policy set uses a policy evaluation in HCP Terraform. This lets you enable policy overrides and enforce a Sentinel runtime version\n(OPA Only) Select a Runtime version for this policy set.\n(OPA Only) Allow Overrides, which enables users with override policy permissions to apply plans that have mandatory policy failures.\n(VCS Only) Optionally specify the VCS branch within your VCS repository where HCP Terraform should import new versions of policies. If you do not set this field, HCP Terraform uses your selected VCS repository's default branch.\n(VCS Only) Specify where your policy set files live using the Policies path. This lets you maintain multiple policy sets within a single repository. Use a relative path from your root directory to the directory that contains either the sentinel.hcl (Sentinel) or policies.hcl (OPA) configuration files. If you do not set this field, HCP Terraform uses the repository's root directory.\n(Managed Policies Only) Select managed Policies to add to the policy set. You can only add policies written with the same policy framework you selected for this set.\nChoose a descriptive and unique Name for the policy set. You can use any combination of letters, numbers, -, and _.\nWrite an optional Description that tells other users about the purpose of the policy set and what it contains.\nEdit policy sets\nTo edit a policy set:\nClick the policy set you want to edit to go to its settings page.\nAdjust the settings and click Update policy set.\nEvaluate a policy runtime upgrade\nYou can validate that changing a policy runtime version does not introduce any breaking changes.\nNote: On-demand policy evaluation is in beta.\nTo perform a policy evaluation:\nClick the policy set you want to upgrade.\nClick the Evaluate tab.\nSelect the Runtime version you wish to upgrade to.\nSelect a Workspace to test the policy and upgraded version against.\nClick Evaluate.\nHCP Terraform will execute the policy set using the specified version and the latest plan data for the selected workspace. It will display the evaluation results. If the evaluation returns a Failed status, inspect the JSON output to determine whether the issue is related to a non-compliant resource or is due to a syntax issue. If the evaluation results in an error, check that the policy configuration is valid.\nDelete policy sets\nWarning: Deleting a policy set that applies to an active run causes that runs policy evaluation stage to error. We recommend warning other members of your organization before you delete widely used policy sets.\nYou can not restore policy sets after deletion. You must manually re-add them to HCP Terraform.\nTo delete a policy set:\nClick the policy set you want to delete to go to its details page.\nClick Delete policy and then click Yes, delete policy set to confirm.\nThe policy set no longer appears on the UI and HCP Terraform no longer applies it to any workspaces. For managed policy sets, all of the individual policies are still available in HCP Terraform. You must delete each policy individually to remove it from your organization.\n(Sentinel only) Sentinel parameters\nSentinel parameters are a list of key/value pairs that HCP Terraform sends to the Sentinel runtime when performing policy checks on workspaces. If the value parses as JSON, HCP Terraform sends it to Sentinel as the corresponding type (string, boolean, integer, map, or list). If the value fails JSON validation, HCP Terraform sends it as a string.\nYou can set Sentinel parameters when you edit a policy set.\nYou can only set parameters for existing policy sets that you added through the tfe provider, API, or a connected VCS repository. Parameters are not available for managed policy sets."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202404-1/policy-enforcement/manage-policy-sets",
  "text": "OPA policy sets can only run as policy evaluations, and you can enable policy evaluations for Sentinel policy sets by selecting the Enhanced policy set type. Policy evaluations run within the Terraform Cloud agent in HCP Terraform's infrastructure. \nTip: Sentinel runtime version pinning is supported only for Sentinel 0.23.1 and above, as well as Terraform Cloud agent versions 1.13.1 and above"
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202402-2/policy-enforcement/manage-policy-sets",
  "text": "OPA policy sets can only run as policy evaluations, and you can enable policy evaluations for Sentinel policy sets by selecting the Enhanced policy set type. Policy evaluations run within the Terraform Cloud agent in HCP Terraform's infrastructure. \nTip: Sentinel runtime version pinning is supported only for Sentinel 0.23.1 and above, as well as Terraform Cloud agent versions 1.13.1 and above"
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202401-2/policy-enforcement/manage-policy-sets",
  "text": "Policies are rules that Terraform Cloud enforces on Terraform runs. You can define policies using either the Sentinel or Open Policy Agent (OPA) policy-as-code frameworks.\nPolicy sets are collections of policies you can apply globally or to specific projects and workspaces in your organization. For each run in the applicable workspaces, Terraform Cloud checks the Terraform plan against the policy set. Depending on the enforcement level, failed policies can stop a run in a workspace. If you do not want to enforce a policy set on a specific workspace, you can exclude the workspace from that set.\nOPA policy sets can only run as policy evaluations, and you can enable policy evaluations for Sentinel policy sets by selecting the Enhanced policy set type. Policy evaluations run within the Terraform Cloud Agent in Terraform Cloud's infrastructure. \nTip: Sentinel runtime version pinning is supported only for Sentinel 0.23.1 and above, as well as Terraform Cloud agent versions 1.13.1 and above\nYou can create policies and policy sets for your Terraform Cloud organization in one of three ways:\nTerraform Cloud web UI: Add individually-managed policies manually in the Terraform Cloud UI, and store your policy code in Terraform Cloud. This workflow is ideal for initial experimentation with policy enforcement, but we do not recommend it for organizations with large numbers of policies.\nVersion control: Connect Terraform Cloud to a version control repository containing a policy set. When you push changes to the repository, Terraform Cloud automatically uses the updated policy set.\nAutomated: Push versions of policy sets to Terraform Cloud with the Terraform Cloud Policy Sets API or the tfe provider tfe_policy_set resource. This workflow is ideal for automated Continuous Integration and Deployment (CI/CD) pipelines.\nYou can add policies directly to Terraform Cloud using the web UI. This process requires you to paste completed, valid Sentinel or Rego code into the UI. We recommend validating your policy code before adding it to Terraform Cloud.\nGo to Policies in your organizations settings. A list of managed policies in Terraform Cloud appears. Each policy designates its policy framework (Sentinel or OPA) and associated policy sets.\nPolicy Name: Add a name containing letters, numbers, -, and _. Terraform Cloud displays this name in the UI. The name must be unique within your organization.\nDescription: Describe the policys purpose. The description supports Markdown rendering, and Terraform Cloud displays this text in the UI.\n(OPA Only) Query: Write a query to identify a specific policy rule within your rego code. Terraform Cloud uses this query to determine the result of the policy. The query is typically a combination of the policy package name and rule name, such as terraform.deny. The result of this query must be an array. The policy passes when the array is empty.\nThe policy is now available in the Terraform Cloud UI for you to edit and add to one or more policy sets.\nYou can not restore policies after deletion. You must manually re-add them to Terraform Cloud. You may want to save the policy code in a separate location before you delete the policy.\nThe policy no longer appears in Terraform Cloud and in any associated policy sets.\nFor policy sets in a version control system, you use the UI to create a policy set connected to that repository. Terraform Cloud automatically refreshes the policy set when you change relevant files in that repository. Version control policy sets have specific organization and formatting requirements. Refer to Sentinel VCS Repositories and OPA VCS Repositories for details.\nFor managed policies, click create a policy set with individually managed policies. Terraform Cloud shows a form to create a policy set and add individually managed policies.\nFor version control policies, choose a version control provider and then select the repository with your policy set. Terraform Cloud shows a form to create a policy set connected to that repository.\nFor automated workflows, click No VCS Connection. Terraform Cloud shows a form to create an empty policy set. You can use the API to add policies to this empty policy set later.\nPolicies enforced globally: Terraform Cloud automatically enforces this global policy set on all of an organization's existing and future workspaces.\n(Optional) Add Policy exclusions for this policy set. Specify any workspaces in the policy set's scope that Terraform Cloud will not enforce this policy set on.\nStandard: This is the default workflow. A Sentinel policy set uses a policy check in Terraform Cloud and lets you access cost estimation data.\nEnhanced: A Sentinel policy set uses a policy evaluation in Terraform Cloud. This lets you enable policy overrides and enforce a Sentinel runtime version\n(VCS Only) Optionally specify the VCS branch within your VCS repository where Terraform Cloud should import new versions of policies. If you do not set this field, Terraform Cloud uses your selected VCS repository's default branch.\n(VCS Only) Specify where your policy set files live using the Policies path. This lets you maintain multiple policy sets within a single repository. Use a relative path from your root directory to the directory that contains either the sentinel.hcl (Sentinel) or policies.hcl (OPA) configuration files. If you do not set this field, Terraform Cloud uses the repository's root directory.\nTerraform Cloud will execute the policy set using the specified version and the latest plan data for the selected workspace. It will display the evaluation results. If the evaluation returns a Failed status, inspect the JSON output to determine whether the issue is related to a non-compliant resource or is due to a syntax issue. If the evaluation results in an error, check that the policy configuration is valid.\nYou can not restore policy sets after deletion. You must manually re-add them to Terraform Cloud.\nThe policy set no longer appears on the UI and Terraform Cloud no longer applies it to any workspaces. For managed policy sets, all of the individual policies are still available in Terraform Cloud. You must delete each policy individually to remove it from your organization.\nSentinel parameters are a list of key/value pairs that Terraform Cloud sends to the Sentinel runtime when performing policy checks on workspaces. If the value parses as JSON, Terraform Cloud sends it to Sentinel as the corresponding type (string, boolean, integer, map, or list). If the value fails JSON validation, Terraform Cloud sends it as a string."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202402-1/policy-enforcement/manage-policy-sets",
  "text": "Policies are rules that Terraform Cloud enforces on Terraform runs. You can define policies using either the Sentinel or Open Policy Agent (OPA) policy-as-code frameworks.\nPolicy sets are collections of policies you can apply globally or to specific projects and workspaces in your organization. For each run in the applicable workspaces, Terraform Cloud checks the Terraform plan against the policy set. Depending on the enforcement level, failed policies can stop a run in a workspace. If you do not want to enforce a policy set on a specific workspace, you can exclude the workspace from that set.\nOPA policy sets can only run as policy evaluations, and you can enable policy evaluations for Sentinel policy sets by selecting the Enhanced policy set type. Policy evaluations run within the Terraform Cloud Agent in Terraform Cloud's infrastructure. \nYou can create policies and policy sets for your Terraform Cloud organization in one of three ways:\nTerraform Cloud web UI: Add individually-managed policies manually in the Terraform Cloud UI, and store your policy code in Terraform Cloud. This workflow is ideal for initial experimentation with policy enforcement, but we do not recommend it for organizations with large numbers of policies.\nVersion control: Connect Terraform Cloud to a version control repository containing a policy set. When you push changes to the repository, Terraform Cloud automatically uses the updated policy set.\nAutomated: Push versions of policy sets to Terraform Cloud with the Terraform Cloud Policy Sets API or the tfe provider tfe_policy_set resource. This workflow is ideal for automated Continuous Integration and Deployment (CI/CD) pipelines.\nYou can add policies directly to Terraform Cloud using the web UI. This process requires you to paste completed, valid Sentinel or Rego code into the UI. We recommend validating your policy code before adding it to Terraform Cloud.\nGo to Policies in your organizations settings. A list of managed policies in Terraform Cloud appears. Each policy designates its policy framework (Sentinel or OPA) and associated policy sets.\nPolicy Name: Add a name containing letters, numbers, -, and _. Terraform Cloud displays this name in the UI. The name must be unique within your organization.\nDescription: Describe the policys purpose. The description supports Markdown rendering, and Terraform Cloud displays this text in the UI.\n(OPA Only) Query: Write a query to identify a specific policy rule within your rego code. Terraform Cloud uses this query to determine the result of the policy. The query is typically a combination of the policy package name and rule name, such as terraform.deny. The result of this query must be an array. The policy passes when the array is empty.\nThe policy is now available in the Terraform Cloud UI for you to edit and add to one or more policy sets.\nYou can not restore policies after deletion. You must manually re-add them to Terraform Cloud. You may want to save the policy code in a separate location before you delete the policy.\nThe policy no longer appears in Terraform Cloud and in any associated policy sets.\nFor policy sets in a version control system, you use the UI to create a policy set connected to that repository. Terraform Cloud automatically refreshes the policy set when you change relevant files in that repository. Version control policy sets have specific organization and formatting requirements. Refer to Sentinel VCS Repositories and OPA VCS Repositories for details.\nFor managed policies, click create a policy set with individually managed policies. Terraform Cloud shows a form to create a policy set and add individually managed policies.\nFor version control policies, choose a version control provider and then select the repository with your policy set. Terraform Cloud shows a form to create a policy set connected to that repository.\nFor automated workflows, click No VCS Connection. Terraform Cloud shows a form to create an empty policy set. You can use the API to add policies to this empty policy set later.\nPolicies enforced globally: Terraform Cloud automatically enforces this global policy set on all of an organization's existing and future workspaces.\n(Optional) Add Policy exclusions for this policy set. Specify any workspaces in the policy set's scope that Terraform Cloud will not enforce this policy set on.\nStandard: This is the default workflow. A Sentinel policy set uses a policy check in Terraform Cloud and lets you access cost estimation data.\nEnhanced: A Sentinel policy set uses a policy evaluation in Terraform Cloud. This lets you enable policy overrides and enforce a Sentinel runtime version\n(VCS Only) Optionally specify the VCS branch within your VCS repository where Terraform Cloud should import new versions of policies. If you do not set this field, Terraform Cloud uses your selected VCS repository's default branch.\n(VCS Only) Specify where your policy set files live using the Policies path. This lets you maintain multiple policy sets within a single repository. Use a relative path from your root directory to the directory that contains either the sentinel.hcl (Sentinel) or policies.hcl (OPA) configuration files. If you do not set this field, Terraform Cloud uses the repository's root directory.\nTerraform Cloud will execute the policy set using the specified version and the latest plan data for the selected workspace. It will display the evaluation results. If the evaluation returns a Failed status, inspect the JSON output to determine whether the issue is related to a non-compliant resource or is due to a syntax issue. If the evaluation results in an error, check that the policy configuration is valid.\nYou can not restore policy sets after deletion. You must manually re-add them to Terraform Cloud.\nThe policy set no longer appears on the UI and Terraform Cloud no longer applies it to any workspaces. For managed policy sets, all of the individual policies are still available in Terraform Cloud. You must delete each policy individually to remove it from your organization.\nSentinel parameters are a list of key/value pairs that Terraform Cloud sends to the Sentinel runtime when performing policy checks on workspaces. If the value parses as JSON, Terraform Cloud sends it to Sentinel as the corresponding type (string, boolean, integer, map, or list). If the value fails JSON validation, Terraform Cloud sends it as a string."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202401-1/policy-enforcement/manage-policy-sets",
  "text": "Policies are rules that Terraform Cloud enforces on Terraform runs. You can define policies using either the Sentinel or Open Policy Agent (OPA) policy-as-code frameworks.\nPolicy sets are collections of policies you can apply globally or to specific projects and workspaces in your organization. For each run in the applicable workspaces, Terraform Cloud checks the Terraform plan against the policy set. Depending on the enforcement level, failed policies can stop a run in a workspace. If you do not want to enforce a policy set on a specific workspace, you can exclude the workspace from that set.\nOPA policy sets can only run as policy evaluations, and you can enable policy evaluations for Sentinel policy sets by selecting the Enhanced policy set type. Policy evaluations run within the Terraform Cloud Agent in Terraform Cloud's infrastructure. \nYou can create policies and policy sets for your Terraform Cloud organization in one of three ways:\nTerraform Cloud web UI: Add individually-managed policies manually in the Terraform Cloud UI, and store your policy code in Terraform Cloud. This workflow is ideal for initial experimentation with policy enforcement, but we do not recommend it for organizations with large numbers of policies.\nVersion control: Connect Terraform Cloud to a version control repository containing a policy set. When you push changes to the repository, Terraform Cloud automatically uses the updated policy set.\nAutomated: Push versions of policy sets to Terraform Cloud with the Terraform Cloud Policy Sets API or the tfe provider tfe_policy_set resource. This workflow is ideal for automated Continuous Integration and Deployment (CI/CD) pipelines.\nYou can add policies directly to Terraform Cloud using the web UI. This process requires you to paste completed, valid Sentinel or Rego code into the UI. We recommend validating your policy code before adding it to Terraform Cloud.\nGo to Policies in your organizations settings. A list of managed policies in Terraform Cloud appears. Each policy designates its policy framework (Sentinel or OPA) and associated policy sets.\nPolicy Name: Add a name containing letters, numbers, -, and _. Terraform Cloud displays this name in the UI. The name must be unique within your organization.\nDescription: Describe the policys purpose. The description supports Markdown rendering, and Terraform Cloud displays this text in the UI.\n(OPA Only) Query: Write a query to identify a specific policy rule within your rego code. Terraform Cloud uses this query to determine the result of the policy. The query is typically a combination of the policy package name and rule name, such as terraform.deny. The result of this query must be an array. The policy passes when the array is empty.\nThe policy is now available in the Terraform Cloud UI for you to edit and add to one or more policy sets.\nYou can not restore policies after deletion. You must manually re-add them to Terraform Cloud. You may want to save the policy code in a separate location before you delete the policy.\nThe policy no longer appears in Terraform Cloud and in any associated policy sets.\nFor policy sets in a version control system, you use the UI to create a policy set connected to that repository. Terraform Cloud automatically refreshes the policy set when you change relevant files in that repository. Version control policy sets have specific organization and formatting requirements. Refer to Sentinel VCS Repositories and OPA VCS Repositories for details.\nFor managed policies, click create a policy set with individually managed policies. Terraform Cloud shows a form to create a policy set and add individually managed policies.\nFor version control policies, choose a version control provider and then select the repository with your policy set. Terraform Cloud shows a form to create a policy set connected to that repository.\nFor automated workflows, click No VCS Connection. Terraform Cloud shows a form to create an empty policy set. You can use the API to add policies to this empty policy set later.\nPolicies enforced globally: Terraform Cloud automatically enforces this global policy set on all of an organization's existing and future workspaces.\n(Optional) Add Policy exclusions for this policy set. Specify any workspaces in the policy set's scope that Terraform Cloud will not enforce this policy set on.\nStandard: This is the default workflow. A Sentinel policy set uses a policy check in Terraform Cloud and lets you access cost estimation data.\nEnhanced: A Sentinel policy set uses a policy evaluation in Terraform Cloud. This lets you enable policy overrides and enforce a Sentinel runtime version\n(VCS Only) Optionally specify the VCS branch within your VCS repository where Terraform Cloud should import new versions of policies. If you do not set this field, Terraform Cloud uses your selected VCS repository's default branch.\n(VCS Only) Specify where your policy set files live using the Policies path. This lets you maintain multiple policy sets within a single repository. Use a relative path from your root directory to the directory that contains either the sentinel.hcl (Sentinel) or policies.hcl (OPA) configuration files. If you do not set this field, Terraform Cloud uses the repository's root directory.\nTerraform Cloud will execute the policy set using the specified version and the latest plan data for the selected workspace. It will display the evaluation results. If the evaluation returns a Failed status, inspect the JSON output to determine whether the issue is related to a non-compliant resource or is due to a syntax issue. If the evaluation results in an error, check that the policy configuration is valid.\nYou can not restore policy sets after deletion. You must manually re-add them to Terraform Cloud.\nThe policy set no longer appears on the UI and Terraform Cloud no longer applies it to any workspaces. For managed policy sets, all of the individual policies are still available in Terraform Cloud. You must delete each policy individually to remove it from your organization.\nSentinel parameters are a list of key/value pairs that Terraform Cloud sends to the Sentinel runtime when performing policy checks on workspaces. If the value parses as JSON, Terraform Cloud sends it to Sentinel as the corresponding type (string, boolean, integer, map, or list). If the value fails JSON validation, Terraform Cloud sends it as a string."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202311-1/policy-enforcement/manage-policy-sets",
  "text": "Managing Policy Sets - Terraform Enterprise | Terraform\nPolicies are rules that Terraform Cloud enforces on Terraform runs. You can define policies using either the Sentinel or Open Policy Agent (OPA) policy-as-code frameworks.\nPolicy sets are collections of policies you can apply globally or to specific projects and workspaces in your organization. For each run in the applicable workspaces, Terraform Cloud checks the Terraform plan against the policy set. Depending on the enforcement level, failed policies can stop a run in a workspace. If you do not want to enforce a policy set on a specific workspace, you can exclude the workspace from that set.\nPolicy checks and policy evaluations serve the same purpose, but have different workflows for enforcing policies.\nPolicy checks run Sentinel policies. This is the default workflow for Sentinel. Policy checks use the latest version of the Sentinel runtime and have access to cost estimation data.\nPolicy evaluations run in the Terraform Cloud Agent in Terraform Cloud's infrastructure. Policy evaluations do not have access to cost estimation data.\nNote: Sentinel policy evaluations are in beta. Sentinel policies can run as a policy evaluation on an opt-in basis.\nYou can use the following workflows to manage policies and policy sets for your Terraform Cloud organization:\nIndividually Managed: Add policies directly in the Terraform Cloud UI, and Terraform Cloud stores your policy code. This workflow is ideal for initial experimentation with policy enforcement, but we do not recommend it for organizations with large numbers of policies.\nVersion Control: Connect Terraform Cloud to a version control repository containing a policy set. When you push changes to the repository, Terraform Cloud automatically uses the updated policy set.\nAutomated: Push versions of policy sets to Terraform Cloud with the Terraform Cloud Policy Sets API. For Sentinel only, you can also use the tfe provider tfe_policy_set resource. This workflow is ideal for automated Continuous Integration and Deployment (CI/CD) pipelines.\nYou can add policies directly to Terraform Cloud. This process requires you to paste completed, valid Sentinel or Rego code into the UI. We recommend validating your policy code before adding it to Terraform Cloud.\nAdd Managed Policies\nGo to Policies in your organizations settings. A list of managed policies in Terraform Cloud appears. Each policy designates its policy framework (Sentinel or OPA) and associated policy sets.\nPolicy Name: Add a name containing letters, numbers, -, and _. Terraform Cloud displays this name in the UI. The name must be unique within your organization.\nDescription: Describe the policys purpose. The description supports Markdown rendering, and Terraform Cloud displays this text in the UI.\n(OPA Only) Query: Write a query to identify a specific policy rule within your rego code. Terraform Cloud uses this query to determine the result of the policy. The query is typically a combination of the policy package name and rule name, such as terraform.deny. The result of this query must be an array. The policy passes when the array is empty.\nThe policy is now available in the Terraform Cloud UI for you to edit and add to one or more policy sets.\nEdit Managed Policies\nDelete Managed Policies\nYou can not restore policies after deletion. You must manually re-add them to Terraform Cloud. You may want to save the policy code in a separate location before you delete the policy.\nThe policy no longer appears in Terraform Cloud and in any associated policy sets.\nFor policy sets in a version control system, you use the UI to create a policy set connected to that repository. Terraform Cloud automatically refreshes the policy set when you change relevant files in that repository. Version control policy sets have specific organization and formatting requirements. Refer to Sentinel VCS Repositories and OPA VCS Repositories for details.\nCreate Policy Sets\nFor managed policies, click create a policy set with individually managed policies. Terraform Cloud shows a form to create a policy set and add individually managed policies.\nFor version control policies, choose a version control provider and then select the repository with your policy set. Terraform Cloud shows a form to create a policy set connected to that repository.\nFor automated workflows, click No VCS Connection. Terraform Cloud shows a form to create an empty policy set. You can use the API to add policies to this empty policy set later.\nPolicies enforced globally: Terraform Cloud automatically enforces this global policy set on all of an organization's existing and future workspaces.\nStandard: This is the default workflow. A Sentinel policy set uses a policy check in Terraform Cloud.\nEnhanced: A Sentinel policy set uses a policy evaluation in Terraform Cloud.\n(Optional) Add Policy exclusions for this policy set. Terraform does not enforce this policy on workspaces in the policy set's scope that you specify in the Policy exclusions field. \n(VCS Only) Optionally specify the VCS branch within your VCS repository where Terraform Cloud should import new versions of policies. If you do not set this field, Terraform Cloud uses your selected VCS repository's default branch.\n(VCS Only) Specify where your policy set files live using the Policies path. This lets you maintain multiple policy sets within a single repository. Use a relative path from your root directory to the directory that contains either the sentinel.hcl (Sentinel) or policies.hcl (OPA) configuration files. If you do not set this field, Terraform Cloud uses the repository's root directory.\nEdit Policy Sets\nDelete Policy Sets\nYou can not restore policy sets after deletion. You must manually re-add them to Terraform Cloud.\nThe policy set no longer appears on the UI and Terraform Cloud no longer applies it to any workspaces. For managed policy sets, all of the individual policies are still available in Terraform Cloud. You must delete each policy individually to remove it from your organization.\n(Sentinel Only) Sentinel Parameters\nSentinel parameters are a list of key/value pairs that Terraform Cloud sends to the Sentinel runtime when performing policy checks on workspaces. If the value parses as JSON, Terraform Cloud sends it to Sentinel as the corresponding type (string, boolean, integer, map, or list). If the value fails JSON validation, Terraform Cloud sends it as a string.\nNote: Policy runtime version management is currently in beta.\nYou can pin a policy set to a specific runtime version. This provides control over the versioning of the policy-as-code runtime.\nOPA Policy sets\nFor OPA policy sets, choose a supported OPA version from the Runtime version drop-down menu. All policies within this policy set are evaluated against the policy runtime version.\nSentinel Policy sets\nFor Sentinel Policy sets, policy runtime version selection is available on an opt-in basis. Sentinel policy sets default to using the policy check flow. To opt into the policy evaluation flow for each policy set individually, enable the radio button in the Sentinel policy set configuration form.\nAfter opting into the enhanced workflow, choose the Sentinel runtime version from the Runtime version drop-down menu. Terraform evaluates the policies in the policy set against the selected runtime version.\nAll policy sets that are configured to use the enhanced workflows run as a policy evaluation.\nSentinel policy sets also have an override option. Failed policies stop the run, but any user with Manage Policy Overrides permission can override these failures and allow the run to complete. Both soft-mandatory and hard-mandatory policies can be overriden.\nNote: Sentinel runtime version pinning is supported only for Sentinel 0.23.0 and above, as well as Terraform Cloud agent versions 1.13.0 and above"
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202310-1/policy-enforcement/manage-policy-sets",
  "text": "Managing Policy Sets - Terraform Enterprise | Terraform\nYou can use the following workflows to manage policies and policy sets for your Terraform Cloud organization:\nIndividually Managed: Add policies directly in the Terraform Cloud UI, and Terraform Cloud stores your policy code. This workflow is ideal for initial experimentation with policy enforcement, but we do not recommend it for organizations with large numbers of policies.\nVersion Control: Connect Terraform Cloud to a version control repository containing a policy set. When you push changes to the repository, Terraform Cloud automatically uses the updated policy set.\nAutomated: Push versions of policy sets to Terraform Cloud with the Terraform Cloud Policy Sets API. For Sentinel only, you can also use the tfe provider tfe_policy_set resource. This workflow is ideal for automated Continuous Integration and Deployment (CI/CD) pipelines.\nYou can add policies directly to Terraform Cloud. This process requires you to paste completed, valid Sentinel or Rego code into the UI. We recommend validating your policy code before adding it to Terraform Cloud.\nAdd Managed Policies\nEdit Managed Policies\nDelete Managed Policies\nCreate Policy Sets\n(Optional) Add Policy exclusions for this policy set. Specify any workspaces in the policy set's scope that Terraform Cloud will not enforce this policy set on.\nEdit Policy Sets\nDelete Policy Sets\n(Sentinel Only) Sentinel Parameters"
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202312-1/policy-enforcement/manage-policy-sets",
  "text": "Managing Policy Sets - Terraform Enterprise | Terraform\nPolicy checks and policy evaluations serve the same purpose, but have different workflows for enforcing policies.\nPolicy checks run Sentinel policies. This is the default workflow for Sentinel. Policy checks use the latest version of the Sentinel runtime and have access to cost estimation data.\nPolicy evaluations run in the Terraform Cloud Agent in Terraform Cloud's infrastructure. Policy evaluations do not have access to cost estimation data.\nNote: Sentinel policy evaluations are currently in beta.\nSentinel policies can run as a policy evaluation on an opt-in basis.\nYou can use the following workflows to manage policies and policy sets for your Terraform Cloud organization:\nIndividually Managed: Add policies directly in the Terraform Cloud UI, and Terraform Cloud stores your policy code. This workflow is ideal for initial experimentation with policy enforcement, but we do not recommend it for organizations with large numbers of policies.\nVersion Control: Connect Terraform Cloud to a version control repository containing a policy set. When you push changes to the repository, Terraform Cloud automatically uses the updated policy set.\nAutomated: Push versions of policy sets to Terraform Cloud with the Terraform Cloud Policy Sets API. For Sentinel only, you can also use the tfe provider tfe_policy_set resource. This workflow is ideal for automated Continuous Integration and Deployment (CI/CD) pipelines.\nYou can add policies directly to Terraform Cloud. This process requires you to paste completed, valid Sentinel or Rego code into the UI. We recommend validating your policy code before adding it to Terraform Cloud.\nStandard: This is the default workflow. A Sentinel policy set uses a policy check in Terraform Cloud.\nEnhanced: A Sentinel policy set uses a policy evaluation in Terraform Cloud.\n(Optional) Add Policy exclusions for this policy set. Terraform Cloud does not enforce this policy on workspaces in the policy set's scope that you specify in the policy exclusions field. \n(Sentinel Only) Sentinel Parameters\nNote: Policy runtime version management is currently in beta.\nYou can pin a policy set to a specific runtime version. This provides control over the versioning of the policy-as-code runtime.\nOPA policy sets\nFor OPA policy sets, choose a supported OPA version from the Runtime version drop-down menu. All policies within this policy set are evaluated against the policy runtime version.\nSentinel policy sets\nFor Sentinel Policy sets, policy runtime version selection is available on an opt-in basis. Sentinel policy sets default to using the policy check flow. To opt into the policy evaluation flow for each policy set individually, enable the radio button in the Sentinel policy set configuration form.\nAfter opting into the enhanced workflow, choose the Sentinel runtime version from the Runtime version drop-down menu. Terraform evaluates the policies in the policy set against the selected runtime version.\nAll policy sets that are configured to use the enhanced workflows run as a policy evaluation.\nSentinel policy sets also have an override option. Failed policies stop the run, but any user with Manage Policy Overrides permission can override these failures and allow the run to complete. Both soft-mandatory and hard-mandatory policies can be overriden.\nNote: Sentinel runtime version pinning is supported only for Sentinel 0.23.0 and above, as well as Terraform Cloud agent versions 1.13.0 and above."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202308-1/policy-enforcement/manage-policy-sets",
  "text": "Managing Policy Sets - Terraform Enterprise | Terraform\nYou group individual policies into policy sets and apply those policy sets to one or more workspaces in your organization. For each run in those workspaces, Terraform Cloud checks the Terraform plan against the policy set. Depending on the enforcement level, failed policies can stop the run.\nYou can use the following workflows to manage policies and policy sets for your Terraform Cloud organization:\nIndividually Managed: Add policies directly in the Terraform Cloud UI, and Terraform Cloud stores your policy code. This workflow is ideal for initial experimentation with policy enforcement, but we do not recommend it for organizations with large numbers of policies.\nVersion Control: Connect Terraform Cloud to a version control repository containing a policy set. When you push changes to the repository, Terraform Cloud automatically uses the updated policy set.\nAutomated: Push versions of policy sets to Terraform Cloud with the Terraform Cloud Policy Sets API. For Sentinel only, you can also use the tfe provider tfe_policy_set resource. This workflow is ideal for automated Continuous Integration and Deployment (CI/CD) pipelines.\nYou can add policies directly to Terraform Cloud. This process requires you to paste completed, valid Sentinel or Rego code into the UI. We recommend validating your policy code before adding it to Terraform Cloud.\nAdd Managed Policies\nEdit Managed Policies\nDelete Managed Policies\nYou group policies into policy sets. Then you can assign those policy sets to workspaces.\nCreate Policy Sets\nTo add a policy set in the Terraform Cloud UI:\nConfigure the policy set settings:\nPolicy Framework: Choose the policy framework for the policies you want to add. You can only create a policy set from policies written using the same framework. You cannot change the framework type after creation.\nName: Add a name containing letters, numbers, -, and _. Terraform Cloud displays this name in the UI. The name must be unique to your organization.\nDescription: Describe the policy sets purpose. The description supports Markdown rendering, and Terraform Cloud displays this text in the UI.\nScope of policies: Choose whether Terraform Cloud should automatically enforce the policy set on all workspaces, or only on a specific subset.\n(Optional) Workspaces: A Workspaces section appears on the bottom of the form when you scope the policy set to selected workspaces. Select workspaces where Terraform Cloud should apply the policy set.\n(OPA Only) Overrides: Choose whether users with override policy permissions can let Terraform apply plans that have mandatory policy failures.\n(VCS Only) VCS Branch: Specify the branch within your VCS repository where Terraform Cloud should import new versions of policies. If you do not set this field, Terraform Cloud uses the default branch of the VCS repository you selected.\n(VCS Only) Policies Path: Specify the sub-directory in your VCS repository containing the policy set files. This action lets you maintain multiple policy sets within a single repository. Set this field to the directory path that contains the sentinel.hcl (Sentinel) or policies.hcl (OPA) configuration file for the policy set. You can start the path with /, but this is optional. Terraform Cloud assumes that relative paths originate from the root of the repository. If you do not set this field, Terraform Cloud uses the repository root directory.\n(Managed Policies Only) Policies: Select managed policies to add to the policy set. You can only add policies written with the policy framework you selected for the policy set.\nEdit Policy Sets\nTo edit policy sets in the Terraform Cloud UI:\nDelete Policy Sets\nTo delete a policy set in the Terraform Cloud UI:\n(Sentinel Only) Sentinel Parameters"
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202309-1/policy-enforcement/manage-policy-sets",
  "text": "Policy sets are collections of policies you can apply globally or to specific projects and workspaces in your organization. For each run in the selected workspaces, Terraform Cloud checks the Terraform plan against the policy set. Depending on the enforcement level, failed policies can stop a run in a workspace.\nAdd Managed Policies\nEdit Managed Policies\nDelete Managed Policies\nCreate Policy Sets\nEdit Policy Sets\nDelete Policy Sets"
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202307-1/policy-enforcement/manage-policy-sets",
  "text": "You group individual policies into policy sets and apply those policy sets to one or more workspaces in your organization. For each run in those workspaces, Terraform Cloud checks the Terraform plan against the policy set. Depending on the enforcement level, failed policies can stop the run.\nYou group policies into policy sets. Then you can assign those policy sets to workspaces.\nTo add a policy set in the Terraform Cloud UI:\nConfigure the policy set settings:\nPolicy Framework: Choose the policy framework for the policies you want to add. You can only create a policy set from policies written using the same framework. You cannot change the framework type after creation.\nName: Add a name containing letters, numbers, -, and _. Terraform Cloud displays this name in the UI. The name must be unique to your organization.\nDescription: Describe the policy sets purpose. The description supports Markdown rendering, and Terraform Cloud displays this text in the UI.\nScope of policies: Choose whether Terraform Cloud should automatically enforce the policy set on all workspaces, or only on a specific subset.\n(Optional) Workspaces: A Workspaces section appears on the bottom of the form when you scope the policy set to selected workspaces. Select workspaces where Terraform Cloud should apply the policy set.\n(OPA Only) Overrides: Choose whether users with override policy permissions can let Terraform apply plans that have mandatory policy failures.\n(VCS Only) VCS Branch: Specify the branch within your VCS repository where Terraform Cloud should import new versions of policies. If you do not set this field, Terraform Cloud uses the default branch of the VCS repository you selected.\n(VCS Only) Policies Path: Specify the sub-directory in your VCS repository containing the policy set files. This action lets you maintain multiple policy sets within a single repository. Set this field to the directory path that contains the sentinel.hcl (Sentinel) or policies.hcl (OPA) configuration file for the policy set. You can start the path with /, but this is optional. Terraform Cloud assumes that relative paths originate from the root of the repository. If you do not set this field, Terraform Cloud uses the repository root directory.\n(Managed Policies Only) Policies: Select managed policies to add to the policy set. You can only add policies written with the policy framework you selected for the policy set.\nTo edit policy sets in the Terraform Cloud UI:\nTo delete a policy set in the Terraform Cloud UI:"
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202306-1/policy-enforcement/manage-policy-sets",
  "text": "You group individual policies into policy sets and apply those policy sets to one or more workspaces in your organization. For each run in those workspaces, Terraform Cloud checks the Terraform plan against the policy set. Depending on the enforcement level, failed policies can stop the run.\nYou group policies into policy sets. Then you can assign those policy sets to workspaces.\nTo add a policy set in the Terraform Cloud UI:\nConfigure the policy set settings:\nPolicy Framework: Choose the policy framework for the policies you want to add. You can only create a policy set from policies written using the same framework. You cannot change the framework type after creation.\nName: Add a name containing letters, numbers, -, and _. Terraform Cloud displays this name in the UI. The name must be unique to your organization.\nDescription: Describe the policy sets purpose. The description supports Markdown rendering, and Terraform Cloud displays this text in the UI.\nScope of policies: Choose whether Terraform Cloud should automatically enforce the policy set on all workspaces, or only on a specific subset.\n(Optional) Workspaces: A Workspaces section appears on the bottom of the form when you scope the policy set to selected workspaces. Select workspaces where Terraform Cloud should apply the policy set.\n(OPA Only) Overrides: Choose whether users with override policy permissions can let Terraform apply plans that have mandatory policy failures.\n(VCS Only) VCS Branch: Specify the branch within your VCS repository where Terraform Cloud should import new versions of policies. If you do not set this field, Terraform Cloud uses the default branch of the VCS repository you selected.\n(VCS Only) Policies Path: Specify the sub-directory in your VCS repository containing the policy set files. This action lets you maintain multiple policy sets within a single repository. Set this field to the directory path that contains the sentinel.hcl (Sentinel) or policies.hcl (OPA) configuration file for the policy set. You can start the path with /, but this is optional. Terraform Cloud assumes that relative paths originate from the root of the repository. If you do not set this field, Terraform Cloud uses the repository root directory.\n(Managed Policies Only) Policies: Select managed policies to add to the policy set. You can only add policies written with the policy framework you selected for the policy set.\nTo edit policy sets in the Terraform Cloud UI:\nTo delete a policy set in the Terraform Cloud UI:"
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202305-2/policy-enforcement/manage-policy-sets",
  "text": "You group individual policies into policy sets and apply those policy sets to one or more workspaces in your organization. For each run in those workspaces, Terraform Cloud checks the Terraform plan against the policy set. Depending on the enforcement level, failed policies can stop the run.\nYou group policies into policy sets. Then you can assign those policy sets to workspaces.\nTo add a policy set in the Terraform Cloud UI:\nConfigure the policy set settings:\nPolicy Framework: Choose the policy framework for the policies you want to add. You can only create a policy set from policies written using the same framework. You cannot change the framework type after creation.\nName: Add a name containing letters, numbers, -, and _. Terraform Cloud displays this name in the UI. The name must be unique to your organization.\nDescription: Describe the policy sets purpose. The description supports Markdown rendering, and Terraform Cloud displays this text in the UI.\nScope of policies: Choose whether Terraform Cloud should automatically enforce the policy set on all workspaces, or only on a specific subset.\n(Optional) Workspaces: A Workspaces section appears on the bottom of the form when you scope the policy set to selected workspaces. Select workspaces where Terraform Cloud should apply the policy set.\n(OPA Only) Overrides: Choose whether users with override policy permissions can let Terraform apply plans that have mandatory policy failures.\n(VCS Only) VCS Branch: Specify the branch within your VCS repository where Terraform Cloud should import new versions of policies. If you do not set this field, Terraform Cloud uses the default branch of the VCS repository you selected.\n(VCS Only) Policies Path: Specify the sub-directory in your VCS repository containing the policy set files. This action lets you maintain multiple policy sets within a single repository. Set this field to the directory path that contains the sentinel.hcl (Sentinel) or policies.hcl (OPA) configuration file for the policy set. You can start the path with /, but this is optional. Terraform Cloud assumes that relative paths originate from the root of the repository. If you do not set this field, Terraform Cloud uses the repository root directory.\n(Managed Policies Only) Policies: Select managed policies to add to the policy set. You can only add policies written with the policy framework you selected for the policy set.\nTo edit policy sets in the Terraform Cloud UI:\nTo delete a policy set in the Terraform Cloud UI:"
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202305-1/policy-enforcement/manage-policy-sets",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202304-1/policy-enforcement/manage-policy-sets",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202303-1/policy-enforcement/manage-policy-sets",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202405-1/policy-enforcement/policy-results",
  "text": "Policy Results - Terraform Enterprise | Terraform\nWhen you add policy sets to a workspace, HCP Terraform enforces those policy sets on every Terraform run. HCP Terraform displays the policy enforcement results in the UI for each run. Depending on each policys enforcement level, policy failures can also stop the run and prevent Terraform from provisioning infrastructure.\nHCP Terraform only evaluates policies for successful plans. HCP Terraform evaluates Sentinel and OPA policy sets separately and at different points in the run.\nSentinel policy checks occur after Terraform completes the plan and after both run tasks and cost estimation. This order lets you write Sentinel policies to restrict costs based on the data in the cost estimates.\nSentinel policy evaluations occur after Terraform completes the plan and after any run tasks. HCP Terraform evaluates Sentinel policy evaluations immediately before cost estimation.\nOPA policy evaluations occur after Terraform completes the plan and after any run tasks. HCP Terraform evaluates OPA policies immediately before cost estimation.\nRefer to Run States and Stages for more details.\nTo view the policy results for both Sentinel and OPA policies:\nGo to your workspace and navigate to the Runs page.\nClick a run to view its details.\nHCP Terraform displays a timeline of the runs events. For workspaces with both Sentinel and OPA policy sets, the run details page displays two separate run events: OPA policies for OPA policy sets and Policy check for Sentinel policy sets.\nClick a policy evaluation event to view policy results and details about any failed policies.\nNote: For Sentinel, the Terraform CLI also prints policy results for CLI-driven runs. CLI support for policy results is not available for OPA.\nYou need manage policy overrides permissions to override failed Sentinel and OPA policies.\nSentinel and OPA have different policy enforcement levels that determine when you need to override failed policies to allow a run to continue. To override failed policies, go to the run details page and click Override and Continue at the bottom.\nFor Sentinel only, you can also override soft-mandatory policies with the Terraform CLI. Run the terraform apply command and then enter override when prompted. Note: HCP Terraform does not allow policy overrides for no-operation plans containing no infrastructure changes, unless you choose the Allow empty apply option when starting the run. \nPolicies with an advisory enforcement level never stop runs. If they fail, HCP Terraform displays a warning in the policy results and the run continues.\nYou can override soft-mandatory policies to allow the run to continue. Overriding failed policies on a run does not affect policy evaluations on future runs in that workspace.\nYou cannot override hard-mandatory policies, and all of these policies must pass for the run to continue.\nPolicies with an advisory enforcement level never stop runs. If they fail, HCP Terraform displays a warning in the policy results and the run continues.\nWhen running Sentinel policies as policy evaluations, soft-mandatory and hard-mandatory enforcement levels are internally converted to mandatory enforcement level. You can override mandatory policies to allow the run to continue.\nPolicies with an advisory enforcement level never stop runs. If they fail, HCP Terraform displays a warning in the policy results and the run continues.\nYou can override mandatory policies to allow the run to continue."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202407-1/policy-enforcement/policy-results",
  "text": "Policy Results - Terraform Enterprise | Terraform\nWhen you add policy sets to a workspace, HCP Terraform enforces those policy sets on every Terraform run. HCP Terraform displays the policy enforcement results in the UI for each run. Depending on each policys enforcement level, policy failures can also stop the run and prevent Terraform from provisioning infrastructure.\nHCP Terraform only evaluates policies for successful plans. HCP Terraform evaluates Sentinel and OPA policy sets separately and at different points in the run.\nSentinel policy checks occur after Terraform completes the plan and after both run tasks and cost estimation. This order lets you write Sentinel policies to restrict costs based on the data in the cost estimates.\nSentinel policy evaluations occur after Terraform completes the plan and after any run tasks. HCP Terraform evaluates Sentinel policy evaluations immediately before cost estimation.\nOPA policy evaluations occur after Terraform completes the plan and after any run tasks. HCP Terraform evaluates OPA policies immediately before cost estimation.\nRefer to Run States and Stages for more details.\nTo view the policy results for both Sentinel and OPA policies:\nGo to your workspace and navigate to the Runs page.\nClick a run to view its details.\nHCP Terraform displays a timeline of the runs events. For workspaces with both Sentinel and OPA policy sets, the run details page displays two separate run events: OPA policies for OPA policy sets and Policy check for Sentinel policy sets.\nClick a policy evaluation event to view policy results and details about any failed policies.\nNote: For Sentinel, the Terraform CLI also prints policy results for CLI-driven runs. CLI support for policy results is not available for OPA.\nYou need manage policy overrides permissions to override failed Sentinel and OPA policies.\nSentinel and OPA have different policy enforcement levels that determine when you need to override failed policies to allow a run to continue. To override failed policies, go to the run details page and click Override and Continue at the bottom.\nFor Sentinel only, you can also override soft-mandatory policies with the Terraform CLI. Run the terraform apply command and then enter override when prompted.\nNote: HCP Terraform does not allow policy overrides for no-operation plans containing no infrastructure changes, unless you choose the Allow empty apply option when starting the run. \nPolicies with an advisory enforcement level never stop runs. If they fail, HCP Terraform displays a warning in the policy results and the run continues.\nYou can override soft-mandatory policies to allow the run to continue. Overriding failed policies on a run does not affect policy evaluations on future runs in that workspace.\nYou cannot override hard-mandatory policies, and all of these policies must pass for the run to continue.\nWhen running Sentinel policies as policy evaluations, soft-mandatory and hard-mandatory enforcement levels are internally converted to mandatory enforcement level. You can override mandatory policies to allow the run to continue.\nYou can override mandatory policies to allow the run to continue."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202406-1/policy-enforcement/policy-results",
  "text": "Policy Results - Terraform Enterprise | Terraform\nWhen you add policy sets to a workspace, HCP Terraform enforces those policy sets on every Terraform run. HCP Terraform displays the policy enforcement results in the UI for each run. Depending on each policys enforcement level, policy failures can also stop the run and prevent Terraform from provisioning infrastructure.\nHCP Terraform only evaluates policies for successful plans. HCP Terraform evaluates Sentinel and OPA policy sets separately and at different points in the run.\nSentinel policy checks occur after Terraform completes the plan and after both run tasks and cost estimation. This order lets you write Sentinel policies to restrict costs based on the data in the cost estimates.\nSentinel policy evaluations occur after Terraform completes the plan and after any run tasks. HCP Terraform evaluates Sentinel policy evaluations immediately before cost estimation.\nOPA policy evaluations occur after Terraform completes the plan and after any run tasks. HCP Terraform evaluates OPA policies immediately before cost estimation.\nRefer to Run States and Stages for more details.\nTo view the policy results for both Sentinel and OPA policies:\nGo to your workspace and navigate to the Runs page.\nClick a run to view its details.\nHCP Terraform displays a timeline of the runs events. For workspaces with both Sentinel and OPA policy sets, the run details page displays two separate run events: OPA policies for OPA policy sets and Policy check for Sentinel policy sets.\nClick a policy evaluation event to view policy results and details about any failed policies.\nNote: For Sentinel, the Terraform CLI also prints policy results for CLI-driven runs. CLI support for policy results is not available for OPA.\nYou need manage policy overrides permissions to override failed Sentinel and OPA policies.\nSentinel and OPA have different policy enforcement levels that determine when you need to override failed policies to allow a run to continue. To override failed policies, go to the run details page and click Override and Continue at the bottom.\nFor Sentinel only, you can also override soft-mandatory policies with the Terraform CLI. Run the terraform apply command and then enter override when prompted.\nNote: HCP Terraform does not allow policy overrides for no-operation plans containing no infrastructure changes, unless you choose the Allow empty apply option when starting the run. \nYou can override soft-mandatory policies to allow the run to continue. Overriding failed policies on a run does not affect policy evaluations on future runs in that workspace.\nYou cannot override hard-mandatory policies, and all of these policies must pass for the run to continue.\nWhen running Sentinel policies as policy evaluations, soft-mandatory and hard-mandatory enforcement levels are internally converted to mandatory enforcement level. You can override mandatory policies to allow the run to continue.\nYou can override mandatory policies to allow the run to continue."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202404-2/policy-enforcement/policy-results",
  "text": "Policy Results - Terraform Enterprise | Terraform\nWhen you add policy sets to a workspace, HCP Terraform enforces those policy sets on every Terraform run. HCP Terraform displays the policy enforcement results in the UI for each run. Depending on each policys enforcement level, policy failures can also stop the run and prevent Terraform from provisioning infrastructure.\nHCP Terraform only evaluates policies for successful plans. HCP Terraform evaluates Sentinel and OPA policy sets separately and at different points in the run.\nSentinel policy checks occur after Terraform completes the plan and after both run tasks and cost estimation. This order lets you write Sentinel policies to restrict costs based on the data in the cost estimates.\nSentinel policy evaluations occur after Terraform completes the plan and after any run tasks. HCP Terraform evaluates Sentinel policy evaluations immediately before cost estimation.\nOPA policy evaluations occur after Terraform completes the plan and after any run tasks. HCP Terraform evaluates OPA policies immediately before cost estimation.\nRefer to Run States and Stages for more details.\nTo view the policy results for both Sentinel and OPA policies:\nGo to your workspace and navigate to the Runs page.\nClick a run to view its details.\nHCP Terraform displays a timeline of the runs events. For workspaces with both Sentinel and OPA policy sets, the run details page displays two separate run events: OPA policies for OPA policy sets and Policy check for Sentinel policy sets.\nClick a policy evaluation event to view policy results and details about any failed policies.\nNote: For Sentinel, the Terraform CLI also prints policy results for CLI-driven runs. CLI support for policy results is not available for OPA.\nYou need manage policy overrides permissions to override failed Sentinel and OPA policies.\nSentinel and OPA have different policy enforcement levels that determine when you need to override failed policies to allow a run to continue. To override failed policies, go to the run details page and click Override and Continue at the bottom.\nFor Sentinel only, you can also override soft-mandatory policies with the Terraform CLI. Run the terraform apply command and then enter override when prompted.\nNote: HCP Terraform does not allow policy overrides for no-operation plans containing no infrastructure changes, unless you choose the Allow empty apply option when starting the run. \nYou can override soft-mandatory policies to allow the run to continue. Overriding failed policies on a run does not affect policy evaluations on future runs in that workspace.\nYou cannot override hard-mandatory policies, and all of these policies must pass for the run to continue.\nWhen running Sentinel policies as policy evaluations, soft-mandatory and hard-mandatory enforcement levels are internally converted to mandatory enforcement level. You can override mandatory policies to allow the run to continue.\nYou can override mandatory policies to allow the run to continue."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202402-1/policy-enforcement/policy-results",
  "text": "When you add policy sets to a workspace, Terraform Cloud enforces those policy sets on every Terraform run. Terraform Cloud displays the policy enforcement results in the UI for each run. Depending on each policys enforcement level, policy failures can also stop the run and prevent Terraform from provisioning infrastructure.\nTerraform Cloud only evaluates policies for successful plans. Terraform Cloud evaluates Sentinel and OPA policy sets separately and at different points in the run.\nSentinel policy evaluations occur after Terraform completes the plan and after any run tasks. Terraform Cloud evaluates Sentinel policy evaluations immediately before cost estimation.\nOPA policy evaluations occur after Terraform completes the plan and after any run tasks. Terraform Cloud evaluates OPA policies immediately before cost estimation.\nTerraform Cloud displays a timeline of the runs events. For workspaces with both Sentinel and OPA policy sets, the run details page displays two separate run events: OPA policies for OPA policy sets and Policy check for Sentinel policy sets.\nFor Sentinel only, you can also override soft-mandatory policies with the Terraform CLI. Run the terraform apply command and then enter override when prompted. Note: Terraform Cloud does not allow policy overrides for no-operation plans containing no infrastructure changes, unless you choose the Allow empty apply option when starting the run. \nPolicies with an advisory enforcement level never stop runs. If they fail, Terraform Cloud displays a warning in the policy results and the run continues.\nPolicies with an advisory enforcement level never stop runs. If they fail, Terraform Cloud displays a warning in the policy results and the run continues.\nPolicies with an advisory enforcement level never stop runs. If they fail, Terraform Cloud displays a warning in the policy results and the run continues."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202404-1/policy-enforcement/policy-results",
  "text": "For Sentinel only, you can also override soft-mandatory policies with the Terraform CLI. Run the terraform apply command and then enter override when prompted. Note: HCP Terraform does not allow policy overrides for no-operation plans containing no infrastructure changes, unless you choose the Allow empty apply option when starting the run. "
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202402-2/policy-enforcement/policy-results",
  "text": "For Sentinel only, you can also override soft-mandatory policies with the Terraform CLI. Run the terraform apply command and then enter override when prompted. Note: HCP Terraform does not allow policy overrides for no-operation plans containing no infrastructure changes, unless you choose the Allow empty apply option when starting the run. "
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202401-2/policy-enforcement/policy-results",
  "text": "When you add policy sets to a workspace, Terraform Cloud enforces those policy sets on every Terraform run. Terraform Cloud displays the policy enforcement results in the UI for each run. Depending on each policys enforcement level, policy failures can also stop the run and prevent Terraform from provisioning infrastructure.\nTerraform Cloud only evaluates policies for successful plans. Terraform Cloud evaluates Sentinel and OPA policy sets separately and at different points in the run.\nSentinel policy evaluations occur after Terraform completes the plan and after any run tasks. Terraform Cloud evaluates Sentinel policy evaluations immediately before cost estimation.\nOPA policy evaluations occur after Terraform completes the plan and after any run tasks. Terraform Cloud evaluates OPA policies immediately before cost estimation.\nTerraform Cloud displays a timeline of the runs events. For workspaces with both Sentinel and OPA policy sets, the run details page displays two separate run events: OPA policies for OPA policy sets and Policy check for Sentinel policy sets.\nFor Sentinel only, you can also override soft-mandatory policies with the Terraform CLI. Run the terraform apply command and then enter override when prompted. Note: Terraform Cloud does not allow policy overrides for no-operation plans containing no infrastructure changes, unless you choose the Allow empty apply option when starting the run. \nPolicies with an advisory enforcement level never stop runs. If they fail, Terraform Cloud displays a warning in the policy results and the run continues."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202401-1/policy-enforcement/policy-results",
  "text": "When you add policy sets to a workspace, Terraform Cloud enforces those policy sets on every Terraform run. Terraform Cloud displays the policy enforcement results in the UI for each run. Depending on each policys enforcement level, policy failures can also stop the run and prevent Terraform from provisioning infrastructure.\nTerraform Cloud only evaluates policies for successful plans. Terraform Cloud evaluates Sentinel and OPA policy sets separately and at different points in the run.\nSentinel policy evaluations occur after Terraform completes the plan and after any run tasks. Terraform Cloud evaluates Sentinel policy evaluations immediately before cost estimation.\nOPA policy evaluations occur after Terraform completes the plan and after any run tasks. Terraform Cloud evaluates OPA policies immediately before cost estimation.\nTerraform Cloud displays a timeline of the runs events. For workspaces with both Sentinel and OPA policy sets, the run details page displays two separate run events: OPA policies for OPA policy sets and Policy check for Sentinel policy sets.\nFor Sentinel only, you can also override soft-mandatory policies with the Terraform CLI. Run the terraform apply command and then enter override when prompted. Note: Terraform Cloud does not allow policy overrides for no-operation plans containing no infrastructure changes, unless you choose the Allow empty apply option when starting the run. "
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202312-1/policy-enforcement/policy-results",
  "text": "When you add policy sets to a workspace, Terraform Cloud enforces those policy sets on every Terraform run. Terraform Cloud displays the policy enforcement results in the UI for each run. Depending on each policys enforcement level, policy failures can also stop the run and prevent Terraform from provisioning infrastructure.\nTerraform Cloud only evaluates policies for successful plans. Terraform Cloud evaluates Sentinel and OPA policy sets separately and at different points in the run.\nSentinel policy checks occur after Terraform completes the plan and after both run tasks and cost estimation. This order lets you write Sentinel policies to restrict costs based on the data in the cost estimates. Note: Sentinel policy evaluations are currently in beta\nSentinel policy evaluations occur after Terraform completes the plan and after any run tasks. Terraform Cloud evaluates Sentinel policy evaluations immediately before cost estimation.\nOPA policy evaluations occur after Terraform completes the plan and after any run tasks. Terraform Cloud evaluates OPA policies immediately before cost estimation.\nTerraform Cloud displays a timeline of the runs events. For workspaces with both Sentinel and OPA policy sets, the run details page displays two separate run events: OPA policies for OPA policy sets and Policy check for Sentinel policy sets.\nFor Sentinel only, you can also override soft-mandatory policies with the Terraform CLI. Run the terraform apply command and then enter override when prompted. Note: Terraform Cloud does not allow policy overrides for no-operation plans containing no infrastructure changes, unless you choose the Allow empty apply option when starting the run. \nNote: Sentinel policy evaluations are currently in beta"
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202310-1/policy-enforcement/policy-results",
  "text": "When you add policy sets to a workspace, Terraform Cloud enforces those policy sets on every Terraform run. Terraform Cloud displays the results of policy checks in the UI for each run. Depending on each policys enforcement level, policy failures can also stop the run and prevent Terraform from provisioning infrastructure.\nSentinel policy evaluations occur after Terraform completes the plan and after both run tasks and cost estimation. This order lets you write Sentinel policies to restrict costs based on the data in the cost estimates.\nOPA policy evaluations occur after Terraform completes the plan and after any run tasks. Unlike Sentinel policies, Terraform Cloud evaluates OPA policies immediately before cost estimation."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202311-1/policy-enforcement/policy-results",
  "text": "Sentinel policy checks occur after Terraform completes the plan and after both run tasks and cost estimation. This order lets you write Sentinel policies to restrict costs based on the data in the cost estimates. Note: Sentinel policy evaluations are currently in beta\nNote: Sentinel policy evaluations are currently in beta"
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202309-1/policy-enforcement/policy-results",
  "text": "When you add policy sets to a workspace, Terraform Cloud enforces those policy sets on every Terraform run. Terraform Cloud displays the results of policy checks in the UI for each run. Depending on each policys enforcement level, policy failures can also stop the run and prevent Terraform from provisioning infrastructure.\nSentinel policy evaluations occur after Terraform completes the plan and after both run tasks and cost estimation. This order lets you write Sentinel policies to restrict costs based on the data in the cost estimates.\nOPA policy evaluations occur after Terraform completes the plan and after any run tasks. Unlike Sentinel policies, Terraform Cloud evaluates OPA policies immediately before cost estimation."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202308-1/policy-enforcement/policy-results",
  "text": "When you add policy sets to a workspace, Terraform Cloud enforces those policy sets on every Terraform run. Terraform Cloud displays the results of policy checks in the UI for each run. Depending on each policys enforcement level, policy failures can also stop the run and prevent Terraform from provisioning infrastructure.\nSentinel policy evaluations occur after Terraform completes the plan and after both run tasks and cost estimation. This order lets you write Sentinel policies to restrict costs based on the data in the cost estimates.\nOPA policy evaluations occur after Terraform completes the plan and after any run tasks. Unlike Sentinel policies, Terraform Cloud evaluates OPA policies immediately before cost estimation."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202307-1/policy-enforcement/policy-results",
  "text": "When you add policy sets to a workspace, Terraform Cloud enforces those policy sets on every Terraform run. Terraform Cloud displays the results of policy checks in the UI for each run. Depending on each policys enforcement level, policy failures can also stop the run and prevent Terraform from provisioning infrastructure.\nSentinel policy evaluations occur after Terraform completes the plan and after both run tasks and cost estimation. This order lets you write Sentinel policies to restrict costs based on the data in the cost estimates.\nOPA policy evaluations occur after Terraform completes the plan and after any run tasks. Unlike Sentinel policies, Terraform Cloud evaluates OPA policies immediately before cost estimation."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202306-1/policy-enforcement/policy-results",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202305-2/policy-enforcement/policy-results",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202305-1/policy-enforcement/policy-results",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202304-1/policy-enforcement/policy-results",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202303-1/policy-enforcement/policy-results",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202309-1/workspaces/settings/notifications",
  "text": "Notifications - Workspaces - Terraform Enterprise | Terraform\nTerraform Cloud can use webhooks to notify external systems about run progress and other events. Each workspace has its own notification settings and can notify up to 20 destinations.\nNote: Workspaces configured with Local execution mode do not support notifications.\nConfiguring notifications requires admin access to the workspace. Refer to Permissions for details.\nTo add, edit, or delete notifications for a workspace, go to the workspace and click Settings > Notifications. The Notifications page appears, showing existing notification configurations.\nA notification configuration specifies a destination URL, a payload type, and the events that should generate a notification. To create a notification configuration:\nClick Settings > Notifications. The Notifications page appears.\nClick Create a Notification. The Create a Notification form appears.\nConfigure the notifications:\nDestination: Terraform Cloud can deliver either a generic payload or a payload formatted specifically for Slack, Microsoft Teams, or Email. Refer to Notification Payloads for details.\nName: A display name for this notification configuration.\nWebhook URL This URL is only available for generic, Slack, and Microsoft Teams webhooks. The webhook URL is the destination for the webhook payload. This URL must accept HTTP or HTTPS POST requests and should be able to use the chosen payload type. For details, refer to the Slack documentation - Create an Incoming Webhook and the Microsoft Teams documentation - Create an Incoming Webhook.\nToken (Optional) This notification is only available for generic webhooks. A token is an arbitrary secret string that Terraform Cloud will use to sign its notification webhooks. Refer to Notification Authenticity for details. You cannot view the token after you save the notification configuration.\nEmail Recipients This notification is only available for emails. Select users that should receive notifications.\nWorkspace Events: Terraform Cloud can send notifications for all events or only for specific events. The following events are available:\nDrift: Terraform Cloud detected configuration drift. This notification is only available if you enable health assessments for the workspace.\nCheck Failure: Terraform Cloud detected one or more failed continuous validation checks. This notification is only available if you enable health assessments for the workspace.\nHealth Assessment Fail: A health assessment failed. This notification is only available if you enable health assessments for the workspace. Health assessments fail when Terraform Cloud cannot perform drift detection, continuous validation, or both. The notification does not specify the cause of the failure, but you can use the Assessment Result logs to help diagnose the issue.\nRun Events: Terraform Cloud can send notifications for all events or only for specific events. The following events are available:\nCreated: A run begins and enters the Pending stage.\nPlanning: A run acquires the lock and starts to execute.\nNeeds Attention: A plan has changes and Terraform requires user input to continue. This event may include approving the plan or a policy override.\nApplying: A run enters the Apply stage, where Terraform makes the infrastructure changes described in the plan.\nCompleted: A run completed successfully.\nErrored: A run terminated early due to error or cancellation.\nClick Create a notification.\nTo enable or disable a configuration, toggle the Enabled/Disabled switch on its detail page. Terraform Cloud will attempt to verify the configuration for generic and slack webhooks by sending a test message, and will enable the notification configuration if the test succeeds.\nFor a verification to be successful, the destination must respond with a 2xx HTTP code. If verification fails, Terraform Cloud displays the error message and the configuration will remain disabled.\nFor both successful and unsuccessful verifications, click the Last Response box to view more information about the verification results. You can also send additional test messages with the Send a Test link.\nSlack\nNotifications to Slack will contain the following information:\nThe run's workspace (as a link)\nThe Terraform Cloud username and avatar of the person that created the run\nThe run ID (as a link)\nThe reason the run was queued (usually a commit message or a custom message)\nThe time the run was created\nThe event that triggered the notification and the time that event occurred\nMicrosoft Teams\nNotifications to Microsoft Teams contain the following information:\nThe run's workspace (as a link)\nThe Terraform Cloud username and avatar of the person that created the run\nThe run ID\nA link to view the run\nThe reason the run was queued (usually a commit message or a custom message)\nThe time the run was created\nThe event that triggered the notification and the time that event occurred\nEmail\nEmail notifications will contain the following information:\nThe run's workspace (as a link)\nThe run ID (as a link)\nThe event that triggered the notification, and if the run needs to be acted upon or not\nGeneric\nA generic notification will contain information about a run and its state at the time the triggering event occurred. The complete generic notification payload is described in the API documentation.\nSome of the values in the payload can be used to retrieve additional information through the API, such as:\nThe run ID\nThe workspace ID\nThe organization name\nSlack notifications use Slack's own protocols for verifying Terraform Cloud's webhook requests.\nGeneric notifications can include a signature for verifying the request. For notification configurations that include a secret token, Terraform Cloud's webhook requests will include an X-TFE-Notification-Signature header, which contains an HMAC signature computed from the token using the SHA-512 digest algorithm. The receiving service is responsible for validating the signature. More information, as well as an example of how to validate the signature, can be found in the API documentation."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202308-1/workspaces/settings/notifications",
  "text": "Notifications - Workspaces - Terraform Enterprise | Terraform\nTerraform Cloud can use webhooks to notify external systems about run progress and other events. Each workspace has its own notification settings and can notify up to 20 destinations.\nNote: Workspaces configured with Local execution mode do not support notifications.\nConfiguring notifications requires admin access to the workspace. Refer to Permissions for details.\nTo add, edit, or delete notifications for a workspace, go to the workspace and click Settings > Notifications. The Notifications page appears, showing existing notification configurations.\nA notification configuration specifies a destination URL, a payload type, and the events that should generate a notification. To create a notification configuration:\nClick Settings > Notifications. The Notifications page appears.\nClick Create a Notification. The Create a Notification form appears.\nConfigure the notifications:\nDestination: Terraform Cloud can deliver either a generic payload or a payload formatted specifically for Slack, Microsoft Teams, or Email. Refer to Notification Payloads for details.\nName: A display name for this notification configuration.\nWebhook URL This URL is only available for generic, Slack, and Microsoft Teams webhooks. The webhook URL is the destination for the webhook payload. This URL must accept HTTP or HTTPS POST requests and should be able to use the chosen payload type. For details, refer to the Slack documentation - Create an Incoming Webhook and the Microsoft Teams documentation - Create an Incoming Webhook.\nToken (Optional) This notification is only available for generic webhooks. A token is an arbitrary secret string that Terraform Cloud will use to sign its notification webhooks. Refer to Notification Authenticity for details. You cannot view the token after you save the notification configuration.\nEmail Recipients This notification is only available for emails. Select users that should receive notifications.\nWorkspace Events: Terraform Cloud can send notifications for all events or only for specific events. The following events are available:\nDrift: Terraform Cloud detected configuration drift. This notification is only available if you enable health assessments for the workspace.\nCheck Failure: Terraform Cloud detected one or more failed continuous validation checks. This notification is only available if you enable health assessments for the workspace.\nHealth Assessment Fail: A health assessment failed. This notification is only available if you enable health assessments for the workspace. Health assessments fail when Terraform Cloud cannot perform drift detection, continuous validation, or both. The notification does not specify the cause of the failure, but you can use the Assessment Result logs to help diagnose the issue.\nRun Events: Terraform Cloud can send notifications for all events or only for specific events. The following events are available:\nCreated: A run begins and enters the Pending stage.\nPlanning: A run acquires the lock and starts to execute.\nNeeds Attention: A plan has changes and Terraform requires user input to continue. This event may include approving the plan or a policy override.\nApplying: A run enters the Apply stage, where Terraform makes the infrastructure changes described in the plan.\nCompleted: A run completed successfully.\nErrored: A run terminated early due to error or cancellation.\nClick Create a notification.\nTo enable or disable a configuration, toggle the Enabled/Disabled switch on its detail page. Terraform Cloud will attempt to verify the configuration for generic and slack webhooks by sending a test message, and will enable the notification configuration if the test succeeds.\nFor a verification to be successful, the destination must respond with a 2xx HTTP code. If verification fails, Terraform Cloud displays the error message and the configuration will remain disabled.\nFor both successful and unsuccessful verifications, click the Last Response box to view more information about the verification results. You can also send additional test messages with the Send a Test link.\nSlack\nNotifications to Slack will contain the following information:\nThe run's workspace (as a link)\nThe Terraform Cloud username and avatar of the person that created the run\nThe run ID (as a link)\nThe reason the run was queued (usually a commit message or a custom message)\nThe time the run was created\nThe event that triggered the notification and the time that event occurred\nMicrosoft Teams\nNotifications to Microsoft Teams contain the following information:\nThe Terraform Cloud username and avatar of the person that created the run\nThe run ID\nA link to view the run\nThe reason the run was queued (usually a commit message or a custom message)\nThe time the run was created\nThe event that triggered the notification and the time that event occurred\nEmail\nEmail notifications will contain the following information:\nThe run ID (as a link)\nThe event that triggered the notification, and if the run needs to be acted upon or not\nGeneric\nA generic notification will contain information about a run and its state at the time the triggering event occurred. The complete generic notification payload is described in the API documentation.\nSome of the values in the payload can be used to retrieve additional information through the API, such as:\nThe run ID\nThe workspace ID\nThe organization name\nSlack notifications use Slack's own protocols for verifying Terraform Cloud's webhook requests.\nGeneric notifications can include a signature for verifying the request. For notification configurations that include a secret token, Terraform Cloud's webhook requests will include an X-TFE-Notification-Signature header, which contains an HMAC signature computed from the token using the SHA-512 digest algorithm. The receiving service is responsible for validating the signature. More information, as well as an example of how to validate the signature, can be found in the API documentation."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202306-1/workspaces/settings/notifications",
  "text": "Notifications - Workspaces - Terraform Enterprise | Terraform\nTerraform Cloud can use webhooks to notify external systems about run progress and other events. Each workspace has its own notification settings and can notify up to 20 destinations.\nNote: Workspaces configured with Local execution mode do not support notifications.\nConfiguring notifications requires admin access to the workspace. Refer to Permissions for details.\nTo add, edit, or delete notifications for a workspace, go to the workspace and click Settings > Notifications. The Notifications page appears, showing existing notification configurations.\nA notification configuration specifies a destination URL, a payload type, and the events that should generate a notification. To create a notification configuration:\nClick Settings > Notifications. The Notifications page appears.\nClick Create a Notification. The Create a Notification form appears.\nConfigure the notifications:\nDestination: Terraform Cloud can deliver either a generic payload or a payload formatted specifically for Slack, Microsoft Teams, or Email. Refer to Notification Payloads for details.\nName: A display name for this notification configuration.\nWebhook URL This URL is only available for generic, Slack, and Microsoft Teams webhooks. The webhook URL is the destination for the webhook payload. This URL must accept HTTP or HTTPS POST requests and should be able to use the chosen payload type. For details, refer to the Slack documentation - Create an Incoming Webhook and the Microsoft Teams documentation - Create an Incoming Webhook.\nToken (Optional) This notification is only available for generic webhooks. A token is an arbitrary secret string that Terraform Cloud will use to sign its notification webhooks. Refer to Notification Authenticity for details. You cannot view the token after you save the notification configuration.\nEmail Recipients This notification is only available for emails. Select users that should receive notifications.\nWorkspace Events: Terraform Cloud can send notifications for all events or only for specific events. The following events are available:\nDrift: Terraform Cloud detected configuration drift. This notification is only available if you enable health assessments for the workspace.\nCheck Failure: Terraform Cloud detected one or more failed continuous validation checks. This notification is only available if you enable health assessments for the workspace.\nHealth Assessment Fail: A health assessment failed. This notification is only available if you enable health assessments for the workspace. Health assessments fail when Terraform Cloud cannot perform drift detection, continuous validation, or both. The notification does not specify the cause of the failure, but you can use the Assessment Result logs to help diagnose the issue.\nRun Events: Terraform Cloud can send notifications for all events or only for specific events. The following events are available:\nCreated: A run begins and enters the Pending stage.\nPlanning: A run acquires the lock and starts to execute.\nNeeds Attention: A plan has changes and Terraform requires user input to continue. This event may include approving the plan or a policy override.\nApplying: A run enters the Apply stage, where Terraform makes the infrastructure changes described in the plan.\nCompleted: A run completed successfully.\nErrored: A run terminated early due to error or cancellation.\nClick Create a notification.\nTo enable or disable a configuration, toggle the Enabled/Disabled switch on its detail page. Terraform Cloud will attempt to verify the configuration for generic and slack webhooks by sending a test message, and will enable the notification configuration if the test succeeds.\nFor a verification to be successful, the destination must respond with a 2xx HTTP code. If verification fails, Terraform Cloud displays the error message and the configuration will remain disabled.\nFor both successful and unsuccessful verifications, click the Last Response box to view more information about the verification results. You can also send additional test messages with the Send a Test link.\nSlack\nNotifications to Slack will contain the following information:\nMicrosoft Teams\nNotifications to Microsoft Teams contain the following information:\nA link to view the run\nEmail\nEmail notifications will contain the following information:\nThe event that triggered the notification, and if the run needs to be acted upon or not\nGeneric\nA generic notification will contain information about a run and its state at the time the triggering event occurred. The complete generic notification payload is described in the API documentation.\nSome of the values in the payload can be used to retrieve additional information through the API, such as:\nThe workspace ID\nThe organization name\nSlack notifications use Slack's own protocols for verifying Terraform Cloud's webhook requests.\nGeneric notifications can include a signature for verifying the request. For notification configurations that include a secret token, Terraform Cloud's webhook requests will include an X-TFE-Notification-Signature header, which contains an HMAC signature computed from the token using the SHA-512 digest algorithm. The receiving service is responsible for validating the signature. More information, as well as an example of how to validate the signature, can be found in the API documentation."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202305-2/workspaces/settings/notifications",
  "text": "Notifications - Workspaces - Terraform Enterprise | Terraform\nTerraform Cloud can use webhooks to notify external systems about run progress and other events. Each workspace has its own notification settings and can notify up to 20 destinations.\nNote: Workspaces configured with Local execution mode do not support notifications.\nConfiguring notifications requires admin access to the workspace. Refer to Permissions for details.\nTo add, edit, or delete notifications for a workspace, go to the workspace and click Settings > Notifications. The Notifications page appears, showing existing notification configurations.\nA notification configuration specifies a destination URL, a payload type, and the events that should generate a notification. To create a notification configuration:\nClick Settings > Notifications. The Notifications page appears.\nClick Create a Notification. The Create a Notification form appears.\nConfigure the notifications:\nDestination: Terraform Cloud can deliver either a generic payload or a payload formatted specifically for Slack, Microsoft Teams, or Email. Refer to Notification Payloads for details.\nName: A display name for this notification configuration.\nWebhook URL This URL is only available for generic, Slack, and Microsoft Teams webhooks. The webhook URL is the destination for the webhook payload. This URL must accept HTTP or HTTPS POST requests and should be able to use the chosen payload type. For details, refer to the Slack documentation - Create an Incoming Webhook and the Microsoft Teams documentation - Create an Incoming Webhook.\nToken (Optional) This notification is only available for generic webhooks. A token is an arbitrary secret string that Terraform Cloud will use to sign its notification webhooks. Refer to Notification Authenticity for details. You cannot view the token after you save the notification configuration.\nEmail Recipients This notification is only available for emails. Select users that should receive notifications.\nWorkspace Events: Terraform Cloud can send notifications for all events or only for specific events. The following events are available:\nDrift: Terraform Cloud detected configuration drift. This notification is only available if you enable health assessments for the workspace.\nCheck Failure: Terraform Cloud detected one or more failed continuous validation checks. This notification is only available if you enable health assessments for the workspace.\nHealth Assessment Fail: A health assessment failed. This notification is only available if you enable health assessments for the workspace. Health assessments fail when Terraform Cloud cannot perform drift detection, continuous validation, or both. The notification does not specify the cause of the failure, but you can use the Assessment Result logs to help diagnose the issue.\nNote: Health assessments are available in the Terraform Cloud Business tier. Continuous validation is in beta and not available in Terraform Enterprise.\nRun Events: Terraform Cloud can send notifications for all events or only for specific events. The following events are available:\nCreated: A run begins and enters the Pending stage.\nPlanning: A run acquires the lock and starts to execute.\nNeeds Attention: A plan has changes and Terraform requires user input to continue. This event may include approving the plan or a policy override.\nApplying: A run enters the Apply stage, where Terraform makes the infrastructure changes described in the plan.\nCompleted: A run completed successfully.\nErrored: A run terminated early due to error or cancellation.\nClick Create a notification.\nTo enable or disable a configuration, toggle the Enabled/Disabled switch on its detail page. Terraform Cloud will attempt to verify the configuration for generic and slack webhooks by sending a test message, and will enable the notification configuration if the test succeeds.\nFor a verification to be successful, the destination must respond with a 2xx HTTP code. If verification fails, Terraform Cloud displays the error message and the configuration will remain disabled.\nFor both successful and unsuccessful verifications, click the Last Response box to view more information about the verification results. You can also send additional test messages with the Send a Test link.\nSlack\nNotifications to Slack will contain the following information:\nMicrosoft Teams\nNotifications to Microsoft Teams contain the following information:\nA link to view the run\nEmail\nEmail notifications will contain the following information:\nThe event that triggered the notification, and if the run needs to be acted upon or not\nGeneric\nA generic notification will contain information about a run and its state at the time the triggering event occurred. The complete generic notification payload is described in the API documentation.\nSome of the values in the payload can be used to retrieve additional information through the API, such as:\nThe workspace ID\nThe organization name\nSlack notifications use Slack's own protocols for verifying Terraform Cloud's webhook requests.\nGeneric notifications can include a signature for verifying the request. For notification configurations that include a secret token, Terraform Cloud's webhook requests will include an X-TFE-Notification-Signature header, which contains an HMAC signature computed from the token using the SHA-512 digest algorithm. The receiving service is responsible for validating the signature. More information, as well as an example of how to validate the signature, can be found in the API documentation."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202307-1/workspaces/settings/notifications",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202305-1/workspaces/settings/notifications",
  "text": "Note: Health assessments are available in the Terraform Cloud Business tier. Continuous validation is in beta and not available in Terraform Enterprise."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202304-1/workspaces/settings/notifications",
  "text": "Note: Health assessments are available in the Terraform Cloud Business tier. Continuous validation is in beta and not available in Terraform Enterprise."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202303-1/workspaces/settings/notifications",
  "text": "Note: Health assessments are available in the Terraform Cloud Business tier. Continuous validation is in beta and not available in Terraform Enterprise."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202302-1/workspaces/settings/notifications",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202301-2/workspaces/settings/notifications",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202301-1/workspaces/settings/notifications",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202212-1/workspaces/settings/notifications",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202212-2/workspaces/settings/notifications",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202211-1/workspaces/settings/notifications",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202210-1/workspaces/settings/notifications",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202209-2/workspaces/settings/notifications",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202407-1/workspaces/settings/notifications",
  "text": "HCP Terraform can use webhooks to notify external systems about run progress and other events. Each workspace has its own notification settings and can notify up to 20 destinations.\nNote: Speculative plans and workspaces configured with Local execution mode do not support notifications.\nDestination: HCP Terraform can deliver either a generic payload or a payload formatted specifically for Slack, Microsoft Teams, or Email. Refer to Notification Payloads for details.\nToken (Optional) This notification is only available for generic webhooks. A token is an arbitrary secret string that HCP Terraform will use to sign its notification webhooks. Refer to Notification Authenticity for details. You cannot view the token after you save the notification configuration.\nWorkspace Events: HCP Terraform can send notifications for all events or only for specific events. The following events are available:\nDrift: HCP Terraform detected configuration drift. This notification is only available if you enable health assessments for the workspace.\nCheck Failure: HCP Terraform detected one or more failed continuous validation checks. This notification is only available if you enable health assessments for the workspace.\nHealth Assessment Fail: A health assessment failed. This notification is only available if you enable health assessments for the workspace. Health assessments fail when HCP Terraform cannot perform drift detection, continuous validation, or both. The notification does not specify the cause of the failure, but you can use the Assessment Result logs to help diagnose the issue.\nAuto destroy reminder: Sends reminders 12 and 24 hours before a schedule auto destroy run.\nAuto destroy results: HCP Terraform performed an auto destroy run in the workspace. Reports both successful and errored runs.\nRun Events: HCP Terraform can send notifications for all events or only for specific events. The following events are available:\nTo enable or disable a configuration, toggle the Enabled/Disabled switch on its detail page. HCP Terraform will attempt to verify the configuration for generic and slack webhooks by sending a test message, and will enable the notification configuration if the test succeeds.\nFor a verification to be successful, the destination must respond with a 2xx HTTP code. If verification fails, HCP Terraform displays the error message and the configuration will remain disabled.\nThe HCP Terraform username and avatar of the person that created the run\nThe HCP Terraform username and avatar of the person that created the run\nSlack notifications use Slack's own protocols for verifying HCP Terraform's webhook requests.\nGeneric notifications can include a signature for verifying the request. For notification configurations that include a secret token, HCP Terraform's webhook requests will include an X-TFE-Notification-Signature header, which contains an HMAC signature computed from the token using the SHA-512 digest algorithm. The receiving service is responsible for validating the signature. More information, as well as an example of how to validate the signature, can be found in the API documentation."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202406-1/workspaces/settings/notifications",
  "text": "HCP Terraform can use webhooks to notify external systems about run progress and other events. Each workspace has its own notification settings and can notify up to 20 destinations.\nNote: Speculative plans and workspaces configured with Local execution mode do not support notifications.\nDestination: HCP Terraform can deliver either a generic payload or a payload formatted specifically for Slack, Microsoft Teams, or Email. Refer to Notification Payloads for details.\nToken (Optional) This notification is only available for generic webhooks. A token is an arbitrary secret string that HCP Terraform will use to sign its notification webhooks. Refer to Notification Authenticity for details. You cannot view the token after you save the notification configuration.\nWorkspace Events: HCP Terraform can send notifications for all events or only for specific events. The following events are available:\nDrift: HCP Terraform detected configuration drift. This notification is only available if you enable health assessments for the workspace.\nCheck Failure: HCP Terraform detected one or more failed continuous validation checks. This notification is only available if you enable health assessments for the workspace.\nHealth Assessment Fail: A health assessment failed. This notification is only available if you enable health assessments for the workspace. Health assessments fail when HCP Terraform cannot perform drift detection, continuous validation, or both. The notification does not specify the cause of the failure, but you can use the Assessment Result logs to help diagnose the issue.\nAuto destroy reminder: Sends reminders 12 and 24 hours before a schedule auto destroy run.\nAuto destroy results: HCP Terraform performed an auto destroy run in the workspace. Reports both successful and errored runs.\nRun Events: HCP Terraform can send notifications for all events or only for specific events. The following events are available:\nTo enable or disable a configuration, toggle the Enabled/Disabled switch on its detail page. HCP Terraform will attempt to verify the configuration for generic and slack webhooks by sending a test message, and will enable the notification configuration if the test succeeds.\nFor a verification to be successful, the destination must respond with a 2xx HTTP code. If verification fails, HCP Terraform displays the error message and the configuration will remain disabled.\nThe HCP Terraform username and avatar of the person that created the run\nThe HCP Terraform username and avatar of the person that created the run\nSlack notifications use Slack's own protocols for verifying HCP Terraform's webhook requests.\nGeneric notifications can include a signature for verifying the request. For notification configurations that include a secret token, HCP Terraform's webhook requests will include an X-TFE-Notification-Signature header, which contains an HMAC signature computed from the token using the SHA-512 digest algorithm. The receiving service is responsible for validating the signature. More information, as well as an example of how to validate the signature, can be found in the API documentation."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202209-1/workspaces/settings/notifications",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202208-3/workspaces/settings/notifications",
  "text": "Webhook URL This is only available for generic, Slack, and Microsoft Teams webhooks. The webhook URL is the destination for the webhook payload. This URL must accept HTTP or HTTPS POST requests and should be able to use the chosen payload type. For details, refer to the Slack documentation - Create an Incoming Webhook and the Microsoft Teams documentation - Create an Incoming Webhook.\nToken (Optional) This is only available for generic webhooks. A token is an arbitrary secret string that Terraform Cloud will use to sign its notification webhooks. Refer to Notification Authenticity for details. The token is encrypted for storage, so you cannot view it after saving the notification configuration.\nEmail Recipients This is only available for emails. Select users that should receive notifications.\nCreated: A run is created and enters the Pending stage.\nNeeds Attention: A plan has changes and Terraform requires user input to continue. This may include approving the plan or a policy override.\nCompleted: A run has completed successfully.\nErrored: A run has terminated early due to error or cancellation."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202208-2/workspaces/settings/notifications",
  "text": "Webhook URL This is only available for generic, Slack, and Microsoft Teams webhooks. The webhook URL is the destination for the webhook payload. This URL must accept HTTP or HTTPS POST requests and should be able to use the chosen payload type. For details, refer to the Slack documentation - Create an Incoming Webhook and the Microsoft Teams documentation - Create an Incoming Webhook.\nToken (Optional) This is only available for generic webhooks. A token is an arbitrary secret string that Terraform Cloud will use to sign its notification webhooks. Refer to Notification Authenticity for details. The token is encrypted for storage, so you cannot view it after saving the notification configuration.\nEmail Recipients This is only available for emails. Select users that should receive notifications.\nCreated: A run is created and enters the Pending stage.\nNeeds Attention: A plan has changes and Terraform requires user input to continue. This may include approving the plan or a policy override.\nCompleted: A run has completed successfully.\nErrored: A run has terminated early due to error or cancellation."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202405-1/workspaces/settings/notifications",
  "text": "HCP Terraform can use webhooks to notify external systems about run progress and other events. Each workspace has its own notification settings and can notify up to 20 destinations.\nDestination: HCP Terraform can deliver either a generic payload or a payload formatted specifically for Slack, Microsoft Teams, or Email. Refer to Notification Payloads for details.\nToken (Optional) This notification is only available for generic webhooks. A token is an arbitrary secret string that HCP Terraform will use to sign its notification webhooks. Refer to Notification Authenticity for details. You cannot view the token after you save the notification configuration.\nWorkspace Events: HCP Terraform can send notifications for all events or only for specific events. The following events are available:\nDrift: HCP Terraform detected configuration drift. This notification is only available if you enable health assessments for the workspace.\nCheck Failure: HCP Terraform detected one or more failed continuous validation checks. This notification is only available if you enable health assessments for the workspace.\nHealth Assessment Fail: A health assessment failed. This notification is only available if you enable health assessments for the workspace. Health assessments fail when HCP Terraform cannot perform drift detection, continuous validation, or both. The notification does not specify the cause of the failure, but you can use the Assessment Result logs to help diagnose the issue.\nAuto destroy reminder: Sends reminders 12 and 24 hours before a schedule auto destroy run.\nAuto destroy results: HCP Terraform performed an auto destroy run in the workspace. Reports both successful and errored runs.\nRun Events: HCP Terraform can send notifications for all events or only for specific events. The following events are available:\nTo enable or disable a configuration, toggle the Enabled/Disabled switch on its detail page. HCP Terraform will attempt to verify the configuration for generic and slack webhooks by sending a test message, and will enable the notification configuration if the test succeeds.\nFor a verification to be successful, the destination must respond with a 2xx HTTP code. If verification fails, HCP Terraform displays the error message and the configuration will remain disabled.\nSlack notifications use Slack's own protocols for verifying HCP Terraform's webhook requests.\nGeneric notifications can include a signature for verifying the request. For notification configurations that include a secret token, HCP Terraform's webhook requests will include an X-TFE-Notification-Signature header, which contains an HMAC signature computed from the token using the SHA-512 digest algorithm. The receiving service is responsible for validating the signature. More information, as well as an example of how to validate the signature, can be found in the API documentation."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202404-2/workspaces/settings/notifications",
  "text": "HCP Terraform can use webhooks to notify external systems about run progress and other events. Each workspace has its own notification settings and can notify up to 20 destinations.\nDestination: HCP Terraform can deliver either a generic payload or a payload formatted specifically for Slack, Microsoft Teams, or Email. Refer to Notification Payloads for details.\nToken (Optional) This notification is only available for generic webhooks. A token is an arbitrary secret string that HCP Terraform will use to sign its notification webhooks. Refer to Notification Authenticity for details. You cannot view the token after you save the notification configuration.\nWorkspace Events: HCP Terraform can send notifications for all events or only for specific events. The following events are available:\nDrift: HCP Terraform detected configuration drift. This notification is only available if you enable health assessments for the workspace.\nCheck Failure: HCP Terraform detected one or more failed continuous validation checks. This notification is only available if you enable health assessments for the workspace.\nHealth Assessment Fail: A health assessment failed. This notification is only available if you enable health assessments for the workspace. Health assessments fail when HCP Terraform cannot perform drift detection, continuous validation, or both. The notification does not specify the cause of the failure, but you can use the Assessment Result logs to help diagnose the issue.\nAuto destroy reminder: Sends reminders 12 and 24 hours before a schedule auto destroy run.\nAuto destroy results: HCP Terraform performed an auto destroy run in the workspace. Reports both successful and errored runs.\nRun Events: HCP Terraform can send notifications for all events or only for specific events. The following events are available:\nTo enable or disable a configuration, toggle the Enabled/Disabled switch on its detail page. HCP Terraform will attempt to verify the configuration for generic and slack webhooks by sending a test message, and will enable the notification configuration if the test succeeds.\nFor a verification to be successful, the destination must respond with a 2xx HTTP code. If verification fails, HCP Terraform displays the error message and the configuration will remain disabled.\nSlack notifications use Slack's own protocols for verifying HCP Terraform's webhook requests.\nGeneric notifications can include a signature for verifying the request. For notification configurations that include a secret token, HCP Terraform's webhook requests will include an X-TFE-Notification-Signature header, which contains an HMAC signature computed from the token using the SHA-512 digest algorithm. The receiving service is responsible for validating the signature. More information, as well as an example of how to validate the signature, can be found in the API documentation."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202208-1/workspaces/settings/notifications",
  "text": "Webhook URL This is only available for generic, Slack, and Microsoft Teams webhooks. The webhook URL is the destination for the webhook payload. This URL must accept HTTP or HTTPS POST requests and should be able to use the chosen payload type. For details, refer to the Slack documentation - Create an Incoming Webhook and the Microsoft Teams documentation - Create an Incoming Webhook.\nToken (Optional) This is only available for generic webhooks. A token is an arbitrary secret string that Terraform Cloud will use to sign its notification webhooks. Refer to Notification Authenticity for details. The token is encrypted for storage, so you cannot view it after saving the notification configuration.\nEmail Recipients This is only available for emails. Select users that should receive notifications.\nCreated: A run is created and enters the Pending stage.\nNeeds Attention: A plan has changes and Terraform requires user input to continue. This may include approving the plan or a policy override.\nCompleted: A run has completed successfully.\nErrored: A run has terminated early due to error or cancellation."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202207-2/workspaces/settings/notifications",
  "text": "Webhook URL This is only available for generic, Slack, and Microsoft Teams webhooks. The webhook URL is the destination for the webhook payload. This URL must accept HTTP or HTTPS POST requests and should be able to use the chosen payload type. For details, refer to the Slack documentation - Create an Incoming Webhook and the Microsoft Teams documentation - Create an Incoming Webhook.\nToken (Optional) This is only available for generic webhooks. A token is an arbitrary secret string that Terraform Cloud will use to sign its notification webhooks. Refer to Notification Authenticity for details. The token is encrypted for storage, so you cannot view it after saving the notification configuration.\nEmail Recipients This is only available for emails. Select users that should receive notifications.\nCreated: A run is created and enters the Pending stage.\nNeeds Attention: A plan has changes and Terraform requires user input to continue. This may include approving the plan or a policy override.\nCompleted: A run has completed successfully.\nErrored: A run has terminated early due to error or cancellation."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202402-1/workspaces/settings/notifications",
  "text": "Auto destroy results: Terraform Cloud performed an auto destroy run in the workspace. Reports both successful and errored runs."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202404-1/workspaces/settings/notifications",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202402-2/workspaces/settings/notifications",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202207-1/workspaces/settings/notifications",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202401-2/workspaces/settings/notifications",
  "text": "Auto destroy results: Terraform Cloud performed an auto destroy run in the workspace. Reports both successful and errored runs."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202311-1/workspaces/settings/notifications",
  "text": "Auto destroy results: Terraform Cloud performed an auto destroy run in the workspace. Reports both successful and errored runs."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202401-1/workspaces/settings/notifications",
  "text": "Auto destroy results: Terraform Cloud performed an auto destroy run in the workspace. Reports both successful and errored runs."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202407-1/api-docs/admin/opa-versions",
  "text": "OPA Versions - Admin - API Docs - Terraform Enterprise | Terraform\nTerraform Enterprise Only: The admin API is exclusive to Terraform Enterprise, and can only be used by the admins and operators who install and maintain their organization's Terraform Enterprise instance.\nThe OPA Versions Admin API lets site administrators manage which versions of OPA are available to the HCP Terraform users within their enterprise.\nGET /api/v2/admin/opa-versions\nThis endpoint lists all known versions of OPA.\nQuery Parameters\nThis endpoint supports pagination with standard URL query parameters. Remember to percent-encode [ as %5B and ] as %5D if your tooling doesn't automatically encode URLs.\nParameterDescription\nfilter[version]\tOptional. A query string. This will find an exact OPA version matching the version queried. This option takes precedence over search queries.\t\nsearch[version]\tOptional. A search query string. This will search for OPA versions matching the version number queried.\t\npage[number]\tOptional. If omitted, the endpoint will return the first page.\t\npage[size]\tOptional. If omitted, the endpoint will return 20 OPA versions per page.\t\nSample Request\ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ \"https://app.terraform.io/api/v2/admin/opa-versions\" \nSample Response\n{ \"data\": [ { \"id\": \"tool-L4oe7rNwn7J4E5Yr\", \"type\": \"opa-versions\", \"attributes\": { \"version\": \"0.55.0\", \"url\": \"https://github.com/open-policy-agent/opa/releases/download/v0.55.0/opa_linux_arm64_static\", \"sha\": \"d19603df4ab619e98cc515084f62b839464ee5bff61383d1df7724db8a7027a9\", \"deprecated\": false, \"deprecated-reason\": null, \"official\": true, \"enabled\": true, \"beta\": false, \"usage\": 0, \"created-at\": \"2023-08-23T22:34:24.561Z\" } }, { \"id\": \"tool-qcbYn12vuRKPgPpy\", \"type\": \"opa-versions\", \"attributes\": { \"version\": \"0.54.0\", \"url\": \"https://github.com/open-policy-agent/opa/releases/download/v0.54.0/opa_linux_arm64_static\", \"sha\": \"883e22c082508e2f95ba25333559ba8a5c38c9c5ef667314e132c9d8451450d8\", \"deprecated\": false, \"deprecated-reason\": null, \"official\": true, \"enabled\": true, \"beta\": false, \"usage\": 2, \"created-at\": \"2023-08-23T22:34:24.561Z\" } } ], \"links\": { \"self\": \"https://tfe.example.com/api/v2/admin/opa-versions?page%5Bnumber%5D=1&page%5Bsize%5D=20\", \"first\": \"https://tfe.example.com/api/v2/admin/opa-versions?page%5Bnumber%5D=1&page%5Bsize%5D=20\", \"prev\": null, \"next\": \"https://tfe.example.com/api/v2/admin/opa-versions?page%5Bnumber%5D=2&page%5Bsize%5D=20\", \"last\": \"https://tfe.example.com/api/v2/admin/opa-versions?page%5Bnumber%5D=4&page%5Bsize%5D=20\" }, \"meta\": { \"pagination\": { \"current-page\": 1, \"prev-page\": null, \"next-page\": 2, \"total-pages\": 4, \"total-count\": 70 } } } \nPOST /api/v2/admin/opa-versions\nRequest Body\nThis POST endpoint requires a JSON object with the following properties as a request payload.\nProperties without a default value are required.\nKey pathTypeDefaultDescription\ndata.type\tstring\t\tMust be \"opa-versions\".\t\ndata.attributes.version\tstring\t\tA semantic version string in N.N.N or N.N.N-bundleName format (\"0.11.0\" or \"0.12.20-beta1\").\t\ndata.attributes.url\tstring\t\tThe URL where you can download the 64-bit Linux binary of this version.\t\ndata.attributes.sha\tstring\t\tThe SHA-256 checksum of the OPA binary.\t\ndata.attributes.deprecated\tbool\tfalse\tWhether or not this version of OPA is deprecated.\t\ndata.attributes.deprecated-reason\tstring\tnull\tAdditional context about why a version of OPA is deprecated. Field is null unless deprecated is true.\t\ndata.attributes.official\tbool\tfalse\tWhether or not this is an official release of OPA.\t\ndata.attributes.enabled\tbool\ttrue\tWhether or not this version of OPA is enabled for use in HCP Terraform.\t\ndata.attributes.beta\tbool\tfalse\tWhether or not this version of OPA is a beta pre-release.\t\nSample Payload\n{ \"data\": { \"type\": \"opa-versions\", \"attributes\": { \"version\": \"0.11.8\", \"url\": \"https://github.com/open-policy-agent/opa/releases/download/v0.54.0/opa_linux_arm64_static\", \"sha\": \"883e22c082508e2f95ba25333559ba8a5c38c9c5ef667314e132c9d8451450d8\", \"official\": true, \"enabled\": true, \"beta\": false } } } \nSample Request\ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request POST \\ --data @payload.json \\ https://app.terraform.io/api/v2/admin/opa-versions \nSample Response\n{ \"data\": { \"id\": \"tool-L4oe7rNwn7J4E5Yr\", \"type\": \"opa-versions\", \"attributes\": { \"version\": \"0.54.0\", \"url\": \"https://github.com/open-policy-agent/opa/releases/download/v0.54.0/opa_linux_arm64_static\", \"sha\": \"883e22c082508e2f95ba25333559ba8a5c38c9c5ef667314e132c9d8451450d8\", \"official\": true, \"deprecated\": false, \"deprecated-reason\": null, \"enabled\": true, \"beta\": false, \"usage\": 0, \"created-at\": \"2023-08-23T22:34:24.561Z\" } } } \nGET /api/v2/admin/opa-versions/:id\nParameterDescription\n:id\tThe ID of the OPA version to show.\t\nStatusResponseReason\n200\tJSON API document (type: \"opa-versions\")\tThe request was successful, returns the OPA version with the matching ID.\t\n404\tJSON API error object\tThe request could not find a matching OPA version with the specified ID, or the client is not an administrator.\t\nSample Request\ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ https://app.terraform.io/api/v2/admin/opa-versions/tool-L4oe7rNwn7J4E5Yr \nSample Response\n{ \"data\": { \"id\": \"tool-L4oe7rNwn7J4E5Yr\", \"type\": \"opa-versions\", \"attributes\": { \"version\": \"0.54.0\", \"url\": \"https://github.com/open-policy-agent/opa/releases/download/v0.54.0/opa_linux_arm64_static\", \"sha\": \"883e22c082508e2f95ba25333559ba8a5c38c9c5ef667314e132c9d8451450d8\", \"official\": true, \"deprecated\": false, \"deprecated-reason\": null, \"enabled\": true, \"beta\": false, \"usage\": 0, \"created-at\": \"2023-08-23T22:34:24.561Z\" } } } \nPATCH /api/v2/admin/opa-versions/:id\nParameterDescription\n:id\tThe ID of the OPA version to update.\t\nRequest Body\nThis PATCH endpoint requires a JSON object with the following properties as a request payload.\nProperties without a default value are required.\nKey pathTypeDefaultDescription\ndata.type\tstring\t\tMust be \"opa-versions\".\t\ndata.attributes.version\tstring\t(previous value)\tA semantic version string in N.N.N or N.N.N-bundleName format (\"0.11.0\" or \"0.12.20-beta1\").\t\ndata.attributes.url\tstring\t(previous value)\tThe URL where you can download the 64-bit Linux binary of this version.\t\ndata.attributes.sha\tstring\t(previous value)\tThe SHA-256 checksum of the OPA binary.\t\ndata.attributes.official\tbool\t(previous value)\tWhether or not this is an official release of OPA.\t\ndata.attributes.deprecated\tbool\t(previous value)\tWhether or not this version of OPA is deprecated.\t\ndata.attributes.deprecated-reason\tstring\t(previous value)\tAdditional context about why a version of OPA is deprecated.\t\ndata.attributes.enabled\tbool\t(previous value)\tWhether or not this version of OPA is enabled for use in HCP Terraform.\t\ndata.attributes.beta\tbool\t(previous value)\tWhether or not this version of OPA is a beta pre-release.\t\nSample Payload\n{ \"data\": { \"type\": \"opa-versions\", \"attributes\": { \"deprecated\": true, \"deprecated-reason\": \"A bug was discovered in this version of OPA. Please upgrade as soon as possible\" } } } \nSample Request\ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request PATCH \\ --data @payload.json \\ https://app.terraform.io/api/v2/admin/opa-versions/tool-L4oe7rNwn7J4E5Yr \nSample Response\n{ \"data\": { \"id\": \"tool-L4oe7rNwn7J4E5Yr\", \"type\": \"opa-versions\", \"attributes\": { \"version\": \"0.54.0\", \"url\": \"https://github.com/open-policy-agent/opa/releases/download/v0.54.0/opa_linux_arm64_static\", \"sha\": \"883e22c082508e2f95ba25333559ba8a5c38c9c5ef667314e132c9d8451450d8\", \"official\": true, \"deprecated\": true, \"deprecated-reason\": \"A bug was discovered in this version of OPA. Please upgrade as soon as possible\", \"enabled\": true, \"beta\": false, \"usage\": 0, \"created-at\": \"2023-08-23T22:34:24.561Z\" } } } \nDELETE /api/v2/admin/opa-versions/:id\nThis endpoint removes an OPA version from HCP Terraform. You cannot remove officially labeled OPA versions or versions used by a workspace or policy set.\nParameterDescription\n:id\tThe ID of the OPA version to delete.\t\nStatusResponseReason\n204\tEmpty response\tThe OPA version was successfully deleted.\t\n404\tJSON API error object\tThe request could not find a matching OPA version with the specified ID, or the client is not an administrator.\t\n422\tJSON API error object\tThe request could not remove the OPA version because it is an official version or a workspace or policy set uses it.\t\ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request DELETE \\ https://app.terraform.io/api/v2/admin/opa-versions/tool-L4oe7rNwn7J4E5Yr"
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202206-1/workspaces/settings/notifications",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202312-1/workspaces/settings/notifications",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202310-1/workspaces/settings/notifications",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202406-1/api-docs/admin/opa-versions",
  "text": "OPA Versions - Admin - API Docs - Terraform Enterprise | Terraform\nTerraform Enterprise Only: The admin API is exclusive to Terraform Enterprise, and can only be used by the admins and operators who install and maintain their organization's Terraform Enterprise instance.\nThe OPA Versions Admin API lets site administrators manage which versions of OPA are available to the HCP Terraform users within their enterprise.\nGET /api/v2/admin/opa-versions\nThis endpoint lists all known versions of OPA.\nQuery Parameters\nThis endpoint supports pagination with standard URL query parameters. Remember to percent-encode [ as %5B and ] as %5D if your tooling doesn't automatically encode URLs.\nfilter[version]\tOptional. A query string. This will find an exact OPA version matching the version queried. This option takes precedence over search queries.\t\nsearch[version]\tOptional. A search query string. This will search for OPA versions matching the version number queried.\t\npage[number]\tOptional. If omitted, the endpoint will return the first page.\t\npage[size]\tOptional. If omitted, the endpoint will return 20 OPA versions per page.\t\ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ \"https://app.terraform.io/api/v2/admin/opa-versions\" \n{ \"data\": [ { \"id\": \"tool-L4oe7rNwn7J4E5Yr\", \"type\": \"opa-versions\", \"attributes\": { \"version\": \"0.55.0\", \"url\": \"https://github.com/open-policy-agent/opa/releases/download/v0.55.0/opa_linux_arm64_static\", \"sha\": \"d19603df4ab619e98cc515084f62b839464ee5bff61383d1df7724db8a7027a9\", \"deprecated\": false, \"deprecated-reason\": null, \"official\": true, \"enabled\": true, \"beta\": false, \"usage\": 0, \"created-at\": \"2023-08-23T22:34:24.561Z\" } }, { \"id\": \"tool-qcbYn12vuRKPgPpy\", \"type\": \"opa-versions\", \"attributes\": { \"version\": \"0.54.0\", \"url\": \"https://github.com/open-policy-agent/opa/releases/download/v0.54.0/opa_linux_arm64_static\", \"sha\": \"883e22c082508e2f95ba25333559ba8a5c38c9c5ef667314e132c9d8451450d8\", \"deprecated\": false, \"deprecated-reason\": null, \"official\": true, \"enabled\": true, \"beta\": false, \"usage\": 2, \"created-at\": \"2023-08-23T22:34:24.561Z\" } } ], \"links\": { \"self\": \"https://tfe.example.com/api/v2/admin/opa-versions?page%5Bnumber%5D=1&page%5Bsize%5D=20\", \"first\": \"https://tfe.example.com/api/v2/admin/opa-versions?page%5Bnumber%5D=1&page%5Bsize%5D=20\", \"prev\": null, \"next\": \"https://tfe.example.com/api/v2/admin/opa-versions?page%5Bnumber%5D=2&page%5Bsize%5D=20\", \"last\": \"https://tfe.example.com/api/v2/admin/opa-versions?page%5Bnumber%5D=4&page%5Bsize%5D=20\" }, \"meta\": { \"pagination\": { \"current-page\": 1, \"prev-page\": null, \"next-page\": 2, \"total-pages\": 4, \"total-count\": 70 } } } \nPOST /api/v2/admin/opa-versions\nRequest Body\nThis POST endpoint requires a JSON object with the following properties as a request payload.\nProperties without a default value are required.\nKey pathTypeDefaultDescription\ndata.type\tstring\t\tMust be \"opa-versions\".\t\ndata.attributes.version\tstring\t\tA semantic version string in N.N.N or N.N.N-bundleName format (\"0.11.0\" or \"0.12.20-beta1\").\t\ndata.attributes.url\tstring\t\tThe URL where you can download the 64-bit Linux binary of this version.\t\ndata.attributes.sha\tstring\t\tThe SHA-256 checksum of the OPA binary.\t\ndata.attributes.deprecated\tbool\tfalse\tWhether or not this version of OPA is deprecated.\t\ndata.attributes.deprecated-reason\tstring\tnull\tAdditional context about why a version of OPA is deprecated. Field is null unless deprecated is true.\t\ndata.attributes.official\tbool\tfalse\tWhether or not this is an official release of OPA.\t\ndata.attributes.enabled\tbool\ttrue\tWhether or not this version of OPA is enabled for use in HCP Terraform.\t\ndata.attributes.beta\tbool\tfalse\tWhether or not this version of OPA is a beta pre-release.\t\nSample Payload\n{ \"data\": { \"type\": \"opa-versions\", \"attributes\": { \"version\": \"0.11.8\", \"url\": \"https://github.com/open-policy-agent/opa/releases/download/v0.54.0/opa_linux_arm64_static\", \"sha\": \"883e22c082508e2f95ba25333559ba8a5c38c9c5ef667314e132c9d8451450d8\", \"official\": true, \"enabled\": true, \"beta\": false } } } \ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request POST \\ --data @payload.json \\ https://app.terraform.io/api/v2/admin/opa-versions \n{ \"data\": { \"id\": \"tool-L4oe7rNwn7J4E5Yr\", \"type\": \"opa-versions\", \"attributes\": { \"version\": \"0.54.0\", \"url\": \"https://github.com/open-policy-agent/opa/releases/download/v0.54.0/opa_linux_arm64_static\", \"sha\": \"883e22c082508e2f95ba25333559ba8a5c38c9c5ef667314e132c9d8451450d8\", \"official\": true, \"deprecated\": false, \"deprecated-reason\": null, \"enabled\": true, \"beta\": false, \"usage\": 0, \"created-at\": \"2023-08-23T22:34:24.561Z\" } } } \nGET /api/v2/admin/opa-versions/:id\n:id\tThe ID of the OPA version to show.\t\nStatusResponseReason\n200\tJSON API document (type: \"opa-versions\")\tThe request was successful, returns the OPA version with the matching ID.\t\n404\tJSON API error object\tThe request could not find a matching OPA version with the specified ID, or the client is not an administrator.\t\ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ https://app.terraform.io/api/v2/admin/opa-versions/tool-L4oe7rNwn7J4E5Yr \n{ \"data\": { \"id\": \"tool-L4oe7rNwn7J4E5Yr\", \"type\": \"opa-versions\", \"attributes\": { \"version\": \"0.54.0\", \"url\": \"https://github.com/open-policy-agent/opa/releases/download/v0.54.0/opa_linux_arm64_static\", \"sha\": \"883e22c082508e2f95ba25333559ba8a5c38c9c5ef667314e132c9d8451450d8\", \"official\": true, \"deprecated\": false, \"deprecated-reason\": null, \"enabled\": true, \"beta\": false, \"usage\": 0, \"created-at\": \"2023-08-23T22:34:24.561Z\" } } } \nPATCH /api/v2/admin/opa-versions/:id\n:id\tThe ID of the OPA version to update.\t\nRequest Body\nThis PATCH endpoint requires a JSON object with the following properties as a request payload.\nProperties without a default value are required.\nKey pathTypeDefaultDescription\ndata.type\tstring\t\tMust be \"opa-versions\".\t\ndata.attributes.version\tstring\t(previous value)\tA semantic version string in N.N.N or N.N.N-bundleName format (\"0.11.0\" or \"0.12.20-beta1\").\t\ndata.attributes.url\tstring\t(previous value)\tThe URL where you can download the 64-bit Linux binary of this version.\t\ndata.attributes.sha\tstring\t(previous value)\tThe SHA-256 checksum of the OPA binary.\t\ndata.attributes.official\tbool\t(previous value)\tWhether or not this is an official release of OPA.\t\ndata.attributes.deprecated\tbool\t(previous value)\tWhether or not this version of OPA is deprecated.\t\ndata.attributes.deprecated-reason\tstring\t(previous value)\tAdditional context about why a version of OPA is deprecated.\t\ndata.attributes.enabled\tbool\t(previous value)\tWhether or not this version of OPA is enabled for use in HCP Terraform.\t\ndata.attributes.beta\tbool\t(previous value)\tWhether or not this version of OPA is a beta pre-release.\t\nSample Payload\n{ \"data\": { \"type\": \"opa-versions\", \"attributes\": { \"deprecated\": true, \"deprecated-reason\": \"A bug was discovered in this version of OPA. Please upgrade as soon as possible\" } } } \ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request PATCH \\ --data @payload.json \\ https://app.terraform.io/api/v2/admin/opa-versions/tool-L4oe7rNwn7J4E5Yr \n{ \"data\": { \"id\": \"tool-L4oe7rNwn7J4E5Yr\", \"type\": \"opa-versions\", \"attributes\": { \"version\": \"0.54.0\", \"url\": \"https://github.com/open-policy-agent/opa/releases/download/v0.54.0/opa_linux_arm64_static\", \"sha\": \"883e22c082508e2f95ba25333559ba8a5c38c9c5ef667314e132c9d8451450d8\", \"official\": true, \"deprecated\": true, \"deprecated-reason\": \"A bug was discovered in this version of OPA. Please upgrade as soon as possible\", \"enabled\": true, \"beta\": false, \"usage\": 0, \"created-at\": \"2023-08-23T22:34:24.561Z\" } } } \nDELETE /api/v2/admin/opa-versions/:id\nThis endpoint removes an OPA version from HCP Terraform. You cannot remove officially labeled OPA versions or versions used by a workspace or policy set.\n:id\tThe ID of the OPA version to delete.\t\nStatusResponseReason\n204\tEmpty response\tThe OPA version was successfully deleted.\t\n404\tJSON API error object\tThe request could not find a matching OPA version with the specified ID, or the client is not an administrator.\t\n422\tJSON API error object\tThe request could not remove the OPA version because it is an official version or a workspace or policy set uses it.\t\ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request DELETE \\ https://app.terraform.io/api/v2/admin/opa-versions/tool-L4oe7rNwn7J4E5Yr"
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202404-2/api-docs/admin/opa-versions",
  "text": "OPA Versions - Admin - API Docs - Terraform Enterprise | Terraform\nTerraform Enterprise Only: The admin API is exclusive to Terraform Enterprise, and can only be used by the admins and operators who install and maintain their organization's Terraform Enterprise instance.\nThe OPA Versions Admin API lets site administrators manage which versions of OPA are available to the HCP Terraform users within their enterprise.\nGET /api/v2/admin/opa-versions\nThis endpoint lists all known versions of OPA.\nQuery Parameters\nThis endpoint supports pagination with standard URL query parameters. Remember to percent-encode [ as %5B and ] as %5D if your tooling doesn't automatically encode URLs.\nfilter[version]\tOptional. A query string. This will find an exact OPA version matching the version queried. This option takes precedence over search queries.\t\nsearch[version]\tOptional. A search query string. This will search for OPA versions matching the version number queried.\t\npage[number]\tOptional. If omitted, the endpoint will return the first page.\t\npage[size]\tOptional. If omitted, the endpoint will return 20 OPA versions per page.\t\ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ \"https://app.terraform.io/api/v2/admin/opa-versions\" \n{ \"data\": [ { \"id\": \"tool-L4oe7rNwn7J4E5Yr\", \"type\": \"opa-versions\", \"attributes\": { \"version\": \"0.55.0\", \"url\": \"https://github.com/open-policy-agent/opa/releases/download/v0.55.0/opa_linux_arm64_static\", \"sha\": \"d19603df4ab619e98cc515084f62b839464ee5bff61383d1df7724db8a7027a9\", \"deprecated\": false, \"deprecated-reason\": null, \"official\": true, \"enabled\": true, \"beta\": false, \"usage\": 0, \"created-at\": \"2023-08-23T22:34:24.561Z\" } }, { \"id\": \"tool-qcbYn12vuRKPgPpy\", \"type\": \"opa-versions\", \"attributes\": { \"version\": \"0.54.0\", \"url\": \"https://github.com/open-policy-agent/opa/releases/download/v0.54.0/opa_linux_arm64_static\", \"sha\": \"883e22c082508e2f95ba25333559ba8a5c38c9c5ef667314e132c9d8451450d8\", \"deprecated\": false, \"deprecated-reason\": null, \"official\": true, \"enabled\": true, \"beta\": false, \"usage\": 2, \"created-at\": \"2023-08-23T22:34:24.561Z\" } } ], \"links\": { \"self\": \"https://tfe.example.com/api/v2/admin/opa-versions?page%5Bnumber%5D=1&page%5Bsize%5D=20\", \"first\": \"https://tfe.example.com/api/v2/admin/opa-versions?page%5Bnumber%5D=1&page%5Bsize%5D=20\", \"prev\": null, \"next\": \"https://tfe.example.com/api/v2/admin/opa-versions?page%5Bnumber%5D=2&page%5Bsize%5D=20\", \"last\": \"https://tfe.example.com/api/v2/admin/opa-versions?page%5Bnumber%5D=4&page%5Bsize%5D=20\" }, \"meta\": { \"pagination\": { \"current-page\": 1, \"prev-page\": null, \"next-page\": 2, \"total-pages\": 4, \"total-count\": 70 } } } \nPOST /api/v2/admin/opa-versions\nThis POST endpoint requires a JSON object with the following properties as a request payload.\ndata.attributes.version\tstring\t\tA semantic version string in N.N.N or N.N.N-bundleName format (\"0.11.0\" or \"0.12.20-beta1\").\t\ndata.attributes.url\tstring\t\tThe URL where you can download the 64-bit Linux binary of this version.\t\ndata.attributes.sha\tstring\t\tThe SHA-256 checksum of the OPA binary.\t\ndata.attributes.deprecated\tbool\tfalse\tWhether or not this version of OPA is deprecated.\t\ndata.attributes.deprecated-reason\tstring\tnull\tAdditional context about why a version of OPA is deprecated. Field is null unless deprecated is true.\t\ndata.attributes.official\tbool\tfalse\tWhether or not this is an official release of OPA.\t\ndata.attributes.enabled\tbool\ttrue\tWhether or not this version of OPA is enabled for use in HCP Terraform.\t\ndata.attributes.beta\tbool\tfalse\tWhether or not this version of OPA is a beta pre-release.\t\n{ \"data\": { \"type\": \"opa-versions\", \"attributes\": { \"version\": \"0.11.8\", \"url\": \"https://github.com/open-policy-agent/opa/releases/download/v0.54.0/opa_linux_arm64_static\", \"sha\": \"883e22c082508e2f95ba25333559ba8a5c38c9c5ef667314e132c9d8451450d8\", \"official\": true, \"enabled\": true, \"beta\": false } } } \ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request POST \\ --data @payload.json \\ https://app.terraform.io/api/v2/admin/opa-versions \nGET /api/v2/admin/opa-versions/:id\n:id\tThe ID of the OPA version to show.\t\n200\tJSON API document (type: \"opa-versions\")\tThe request was successful, returns the OPA version with the matching ID.\t\ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ https://app.terraform.io/api/v2/admin/opa-versions/tool-L4oe7rNwn7J4E5Yr \nPATCH /api/v2/admin/opa-versions/:id\n:id\tThe ID of the OPA version to update.\t\nThis PATCH endpoint requires a JSON object with the following properties as a request payload.\ndata.attributes.version\tstring\t(previous value)\tA semantic version string in N.N.N or N.N.N-bundleName format (\"0.11.0\" or \"0.12.20-beta1\").\t\ndata.attributes.url\tstring\t(previous value)\tThe URL where you can download the 64-bit Linux binary of this version.\t\ndata.attributes.sha\tstring\t(previous value)\tThe SHA-256 checksum of the OPA binary.\t\ndata.attributes.official\tbool\t(previous value)\tWhether or not this is an official release of OPA.\t\ndata.attributes.deprecated\tbool\t(previous value)\tWhether or not this version of OPA is deprecated.\t\ndata.attributes.deprecated-reason\tstring\t(previous value)\tAdditional context about why a version of OPA is deprecated.\t\ndata.attributes.enabled\tbool\t(previous value)\tWhether or not this version of OPA is enabled for use in HCP Terraform.\t\ndata.attributes.beta\tbool\t(previous value)\tWhether or not this version of OPA is a beta pre-release.\t\n{ \"data\": { \"type\": \"opa-versions\", \"attributes\": { \"deprecated\": true, \"deprecated-reason\": \"A bug was discovered in this version of OPA. Please upgrade as soon as possible\" } } } \ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request PATCH \\ --data @payload.json \\ https://app.terraform.io/api/v2/admin/opa-versions/tool-L4oe7rNwn7J4E5Yr \n{ \"data\": { \"id\": \"tool-L4oe7rNwn7J4E5Yr\", \"type\": \"opa-versions\", \"attributes\": { \"version\": \"0.54.0\", \"url\": \"https://github.com/open-policy-agent/opa/releases/download/v0.54.0/opa_linux_arm64_static\", \"sha\": \"883e22c082508e2f95ba25333559ba8a5c38c9c5ef667314e132c9d8451450d8\", \"official\": true, \"deprecated\": true, \"deprecated-reason\": \"A bug was discovered in this version of OPA. Please upgrade as soon as possible\", \"enabled\": true, \"beta\": false, \"usage\": 0, \"created-at\": \"2023-08-23T22:34:24.561Z\" } } } \nDELETE /api/v2/admin/opa-versions/:id\nThis endpoint removes an OPA version from HCP Terraform. You cannot remove officially labeled OPA versions or versions used by a workspace or policy set.\n:id\tThe ID of the OPA version to delete.\t\n204\tEmpty response\tThe OPA version was successfully deleted.\t\n422\tJSON API error object\tThe request could not remove the OPA version because it is an official version or a workspace or policy set uses it.\t\ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request DELETE \\ https://app.terraform.io/api/v2/admin/opa-versions/tool-L4oe7rNwn7J4E5Yr"
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202405-1/api-docs/admin/opa-versions",
  "text": "OPA Versions - Admin - API Docs - Terraform Enterprise | Terraform\nTerraform Enterprise Only: The admin API is exclusive to Terraform Enterprise, and can only be used by the admins and operators who install and maintain their organization's Terraform Enterprise instance.\nThe OPA Versions Admin API lets site administrators manage which versions of OPA are available to the HCP Terraform users within their enterprise.\nGET /api/v2/admin/opa-versions\nThis endpoint lists all known versions of OPA.\nQuery Parameters\nThis endpoint supports pagination with standard URL query parameters. Remember to percent-encode [ as %5B and ] as %5D if your tooling doesn't automatically encode URLs.\nfilter[version]\tOptional. A query string. This will find an exact OPA version matching the version queried. This option takes precedence over search queries.\t\nsearch[version]\tOptional. A search query string. This will search for OPA versions matching the version number queried.\t\npage[number]\tOptional. If omitted, the endpoint will return the first page.\t\npage[size]\tOptional. If omitted, the endpoint will return 20 OPA versions per page.\t\ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ \"https://app.terraform.io/api/v2/admin/opa-versions\" \n{ \"data\": [ { \"id\": \"tool-L4oe7rNwn7J4E5Yr\", \"type\": \"opa-versions\", \"attributes\": { \"version\": \"0.55.0\", \"url\": \"https://github.com/open-policy-agent/opa/releases/download/v0.55.0/opa_linux_arm64_static\", \"sha\": \"d19603df4ab619e98cc515084f62b839464ee5bff61383d1df7724db8a7027a9\", \"deprecated\": false, \"deprecated-reason\": null, \"official\": true, \"enabled\": true, \"beta\": false, \"usage\": 0, \"created-at\": \"2023-08-23T22:34:24.561Z\" } }, { \"id\": \"tool-qcbYn12vuRKPgPpy\", \"type\": \"opa-versions\", \"attributes\": { \"version\": \"0.54.0\", \"url\": \"https://github.com/open-policy-agent/opa/releases/download/v0.54.0/opa_linux_arm64_static\", \"sha\": \"883e22c082508e2f95ba25333559ba8a5c38c9c5ef667314e132c9d8451450d8\", \"deprecated\": false, \"deprecated-reason\": null, \"official\": true, \"enabled\": true, \"beta\": false, \"usage\": 2, \"created-at\": \"2023-08-23T22:34:24.561Z\" } } ], \"links\": { \"self\": \"https://tfe.example.com/api/v2/admin/opa-versions?page%5Bnumber%5D=1&page%5Bsize%5D=20\", \"first\": \"https://tfe.example.com/api/v2/admin/opa-versions?page%5Bnumber%5D=1&page%5Bsize%5D=20\", \"prev\": null, \"next\": \"https://tfe.example.com/api/v2/admin/opa-versions?page%5Bnumber%5D=2&page%5Bsize%5D=20\", \"last\": \"https://tfe.example.com/api/v2/admin/opa-versions?page%5Bnumber%5D=4&page%5Bsize%5D=20\" }, \"meta\": { \"pagination\": { \"current-page\": 1, \"prev-page\": null, \"next-page\": 2, \"total-pages\": 4, \"total-count\": 70 } } } \nPOST /api/v2/admin/opa-versions\nThis POST endpoint requires a JSON object with the following properties as a request payload.\ndata.attributes.version\tstring\t\tA semantic version string in N.N.N or N.N.N-bundleName format (\"0.11.0\" or \"0.12.20-beta1\").\t\ndata.attributes.url\tstring\t\tThe URL where you can download the 64-bit Linux binary of this version.\t\ndata.attributes.sha\tstring\t\tThe SHA-256 checksum of the OPA binary.\t\ndata.attributes.deprecated\tbool\tfalse\tWhether or not this version of OPA is deprecated.\t\ndata.attributes.deprecated-reason\tstring\tnull\tAdditional context about why a version of OPA is deprecated. Field is null unless deprecated is true.\t\ndata.attributes.official\tbool\tfalse\tWhether or not this is an official release of OPA.\t\ndata.attributes.enabled\tbool\ttrue\tWhether or not this version of OPA is enabled for use in HCP Terraform.\t\ndata.attributes.beta\tbool\tfalse\tWhether or not this version of OPA is a beta pre-release.\t\n{ \"data\": { \"type\": \"opa-versions\", \"attributes\": { \"version\": \"0.11.8\", \"url\": \"https://github.com/open-policy-agent/opa/releases/download/v0.54.0/opa_linux_arm64_static\", \"sha\": \"883e22c082508e2f95ba25333559ba8a5c38c9c5ef667314e132c9d8451450d8\", \"official\": true, \"enabled\": true, \"beta\": false } } } \ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request POST \\ --data @payload.json \\ https://app.terraform.io/api/v2/admin/opa-versions \nGET /api/v2/admin/opa-versions/:id\n:id\tThe ID of the OPA version to show.\t\n200\tJSON API document (type: \"opa-versions\")\tThe request was successful, returns the OPA version with the matching ID.\t\ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ https://app.terraform.io/api/v2/admin/opa-versions/tool-L4oe7rNwn7J4E5Yr \nPATCH /api/v2/admin/opa-versions/:id\n:id\tThe ID of the OPA version to update.\t\nThis PATCH endpoint requires a JSON object with the following properties as a request payload.\ndata.attributes.version\tstring\t(previous value)\tA semantic version string in N.N.N or N.N.N-bundleName format (\"0.11.0\" or \"0.12.20-beta1\").\t\ndata.attributes.url\tstring\t(previous value)\tThe URL where you can download the 64-bit Linux binary of this version.\t\ndata.attributes.sha\tstring\t(previous value)\tThe SHA-256 checksum of the OPA binary.\t\ndata.attributes.official\tbool\t(previous value)\tWhether or not this is an official release of OPA.\t\ndata.attributes.deprecated\tbool\t(previous value)\tWhether or not this version of OPA is deprecated.\t\ndata.attributes.deprecated-reason\tstring\t(previous value)\tAdditional context about why a version of OPA is deprecated.\t\ndata.attributes.enabled\tbool\t(previous value)\tWhether or not this version of OPA is enabled for use in HCP Terraform.\t\ndata.attributes.beta\tbool\t(previous value)\tWhether or not this version of OPA is a beta pre-release.\t\n{ \"data\": { \"type\": \"opa-versions\", \"attributes\": { \"deprecated\": true, \"deprecated-reason\": \"A bug was discovered in this version of OPA. Please upgrade as soon as possible\" } } } \ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request PATCH \\ --data @payload.json \\ https://app.terraform.io/api/v2/admin/opa-versions/tool-L4oe7rNwn7J4E5Yr \n{ \"data\": { \"id\": \"tool-L4oe7rNwn7J4E5Yr\", \"type\": \"opa-versions\", \"attributes\": { \"version\": \"0.54.0\", \"url\": \"https://github.com/open-policy-agent/opa/releases/download/v0.54.0/opa_linux_arm64_static\", \"sha\": \"883e22c082508e2f95ba25333559ba8a5c38c9c5ef667314e132c9d8451450d8\", \"official\": true, \"deprecated\": true, \"deprecated-reason\": \"A bug was discovered in this version of OPA. Please upgrade as soon as possible\", \"enabled\": true, \"beta\": false, \"usage\": 0, \"created-at\": \"2023-08-23T22:34:24.561Z\" } } } \nDELETE /api/v2/admin/opa-versions/:id\nThis endpoint removes an OPA version from HCP Terraform. You cannot remove officially labeled OPA versions or versions used by a workspace or policy set.\n:id\tThe ID of the OPA version to delete.\t\n204\tEmpty response\tThe OPA version was successfully deleted.\t\n422\tJSON API error object\tThe request could not remove the OPA version because it is an official version or a workspace or policy set uses it.\t\ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request DELETE \\ https://app.terraform.io/api/v2/admin/opa-versions/tool-L4oe7rNwn7J4E5Yr"
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202404-1/api-docs/admin/opa-versions",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202402-1/api-docs/admin/opa-versions",
  "text": "The OPA Versions Admin API lets site administrators manage which versions of OPA are available to the Terraform Cloud users within their enterprise.\ndata.attributes.enabled\tbool\ttrue\tWhether or not this version of OPA is enabled for use in Terraform Cloud.\t\ndata.attributes.enabled\tbool\t(previous value)\tWhether or not this version of OPA is enabled for use in Terraform Cloud.\t\nThis endpoint removes an OPA version from Terraform Cloud. You cannot remove officially labeled OPA versions or versions used by a workspace or policy set."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202402-2/api-docs/admin/opa-versions",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202401-2/api-docs/admin/opa-versions",
  "text": "The OPA Versions Admin API lets site administrators manage which versions of OPA are available to the Terraform Cloud users within their enterprise.\ndata.attributes.enabled\tbool\ttrue\tWhether or not this version of OPA is enabled for use in Terraform Cloud.\t\ndata.attributes.enabled\tbool\t(previous value)\tWhether or not this version of OPA is enabled for use in Terraform Cloud.\t\nThis endpoint removes an OPA version from Terraform Cloud. You cannot remove officially labeled OPA versions or versions used by a workspace or policy set."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202401-1/api-docs/admin/opa-versions",
  "text": "The OPA Versions Admin API lets site administrators manage which versions of OPA are available to the Terraform Cloud users within their enterprise.\ndata.attributes.enabled\tbool\ttrue\tWhether or not this version of OPA is enabled for use in Terraform Cloud.\t\ndata.attributes.enabled\tbool\t(previous value)\tWhether or not this version of OPA is enabled for use in Terraform Cloud.\t\nThis endpoint removes an OPA version from Terraform Cloud. You cannot remove officially labeled OPA versions or versions used by a workspace or policy set."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202312-1/api-docs/admin/opa-versions",
  "text": "The OPA Versions Admin API lets site administrators manage which versions of OPA are available to the Terraform Cloud users within their enterprise.\ndata.attributes.enabled\tbool\ttrue\tWhether or not this version of OPA is enabled for use in Terraform Cloud.\t\ndata.attributes.enabled\tbool\t(previous value)\tWhether or not this version of OPA is enabled for use in Terraform Cloud.\t\nThis endpoint removes an OPA version from Terraform Cloud. You cannot remove officially labeled OPA versions or versions used by a workspace or policy set."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202407-1/api-docs/admin/sentinel-versions",
  "text": "Sentinel Versions - Admin - API Docs - Terraform Enterprise | Terraform\nThe Sentinel Versions Admin API lets site administrators manage which versions of Sentinel are available to the HCP Terraform users within their enterprise.\nGET /api/v2/admin/sentinel-versions\nThis endpoint lists all known versions of Sentinel.\nfilter[version]\tOptional. A query string. This will find an exact Sentinel version matching the version queried. This option takes precedence over search queries.\t\nsearch[version]\tOptional. A search query string. This will search for Sentinel versions matching the version number queried.\t\npage[size]\tOptional. If omitted, the endpoint will return 20 Sentinel versions per page.\t\ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ \"https://app.terraform.io/api/v2/admin/sentinel-versions\" \n{ \"data\": [ { \"id\": \"tool-L4oe7rNwn7J4E5Yr\", \"type\": \"sentinel-versions\", \"attributes\": { \"version\": \"0.22.1\", \"url\": \"https://releases.hashicorp.com/sentinel/0.22.1/sentinel_0.22.1_linux_amd64.zip\", \"sha\": \"0a4a2b2baf46bfeb81d5137b2656b159ccc881487df3bebacd350ea48b53e76c\", \"deprecated\": false, \"deprecated-reason\": null, \"official\": true, \"enabled\": true, \"beta\": false, \"usage\": 0, \"created-at\": \"2023-08-23T22:34:24.561Z\" } } ], \"links\": { \"self\": \"https://tfe.example.com/api/v2/admin/sentinel-versions?page%5Bnumber%5D=1&page%5Bsize%5D=20\", \"first\": \"https://tfe.example.com/api/v2/admin/sentinel-versions?page%5Bnumber%5D=1&page%5Bsize%5D=20\", \"prev\": null, \"next\": \"https://tfe.example.com/api/v2/admin/sentinel-versions?page%5Bnumber%5D=2&page%5Bsize%5D=20\", \"last\": \"https://tfe.example.com/api/v2/admin/sentinel-versions?page%5Bnumber%5D=4&page%5Bsize%5D=20\" }, \"meta\": { \"pagination\": { \"current-page\": 1, \"prev-page\": null, \"next-page\": 2, \"total-pages\": 4, \"total-count\": 70 } } } \nPOST /api/v2/admin/sentinel-versions\ndata.type\tstring\t\tMust be \"sentinel-versions\".\t\ndata.attributes.sha\tstring\t\tThe SHA-256 checksum of the compressed Sentinel binary.\t\ndata.attributes.deprecated\tbool\tfalse\tWhether or not this version of Sentinel is deprecated.\t\ndata.attributes.deprecated-reason\tstring\tnull\tAdditional context about why a version of Sentinel is deprecated. Field is null unless deprecated is true.\t\ndata.attributes.official\tbool\tfalse\tWhether or not this is an official release of Sentinel.\t\ndata.attributes.enabled\tbool\ttrue\tWhether or not this version of Sentinel is enabled for use in HCP Terraform.\t\ndata.attributes.beta\tbool\tfalse\tWhether or not this version of Sentinel is a beta pre-release.\t\n{ \"data\": { \"type\": \"sentinel-versions\", \"attributes\": { \"version\": \"0.22.1\", \"url\": \"https://releases.hashicorp.com/sentinel/0.22.1/sentinel_0.22.1_linux_amd64.zip\", \"sha\": \"0a4a2b2baf46bfeb81d5137b2656b159ccc881487df3bebacd350ea48b53e76c\", \"official\": true, \"enabled\": true, \"beta\": false } } } \ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request POST \\ --data @payload.json \\ https://app.terraform.io/api/v2/admin/sentinel-versions \n{ \"data\": { \"id\": \"tool-L4oe7rNwn7J4E5Yr\", \"type\": \"sentinel-versions\", \"attributes\": { \"version\": \"0.22.1\", \"url\": \"https://releases.hashicorp.com/sentinel/0.22.1/sentinel_0.22.1_linux_amd64.zip\", \"sha\": \"0a4a2b2baf46bfeb81d5137b2656b159ccc881487df3bebacd350ea48b53e76c\", \"official\": true, \"deprecated\": false, \"deprecated-reason\": null, \"enabled\": true, \"beta\": false, \"usage\": 0, \"created-at\": \"2023-08-23T22:34:24.561Z\" } } } \nGET /api/v2/admin/sentinel-versions/:id\n:id\tThe ID of the Sentinel version to show.\t\n200\tJSON API document (type: \"sentinel-versions\")\tSuccessfully shows the specified Sentinel version.\t\n404\tJSON API error object\tCould not find the specified Sentinel version, or client is not an administrator.\t\ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ https://app.terraform.io/api/v2/admin/sentinel-versions/tool-L4oe7rNwn7J4E5Yr \n{ \"data\": { \"id\": \"tool-L4oe7rNwn7J4E5Yr\", \"type\": \"sentinel-versions\", \"attributes\": { \"version\": \"0.22.1\", \"url\": \"https://releases.hashicorp.com/sentinel/0.22.1/sentinel_0.22.1_linux_amd64.zip\", \"sha\": \"0a4a2b2baf46bfeb81d5137b2656b159ccc881487df3bebacd350ea48b53e76c\", \"official\": true, \"deprecated\": false, \"deprecated-reason\": null, \"enabled\": true, \"beta\": false, \"usage\": 0, \"created-at\": \"2023-08-23T22:34:24.561Z\" } } } \nPATCH /api/v2/admin/sentinel-versions/:id\n:id\tThe ID of the Sentinel version to update.\t\ndata.type\tstring\t\tMust be \"sentinel-versions\".\t\ndata.attributes.version\tstring\t(previous value)\tA semantic version string in N.N.N or N.N.N-bundleName format (\"0.11.0\", \"0.12.20-beta1\").\t\ndata.attributes.url\tstring\t(previous value)\tThe URL you can download the 64-bit Linux binary of this version.\t\ndata.attributes.sha\tstring\t(previous value)\tThe SHA-256 checksum of the compressed Sentinel binary.\t\ndata.attributes.official\tbool\t(previous value)\tWhether or not this is an official release of Sentinel.\t\ndata.attributes.deprecated\tbool\t(previous value)\tWhether or not this version of Sentinel is deprecated.\t\ndata.attributes.deprecated-reason\tstring\t(previous value)\tAdditional context about why a version of Sentinel is deprecated.\t\ndata.attributes.enabled\tbool\t(previous value)\tWhether or not this version of Sentinel is enabled for use in HCP Terraform.\t\ndata.attributes.beta\tbool\t(previous value)\tWhether or not this version of Sentinel is a beta pre-release.\t\n{ \"data\": { \"type\": \"sentinel-versions\", \"attributes\": { \"deprecated\": true, \"deprecated-reason\": \"A bug was discovered in this version of Sentinel. Please upgrade as soon as possible\" } } } \ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request PATCH \\ --data @payload.json \\ https://app.terraform.io/api/v2/admin/sentinel-versions/tool-L4oe7rNwn7J4E5Yr \n{ \"data\": { \"id\": \"tool-L4oe7rNwn7J4E5Yr\", \"type\": \"sentinel-versions\", \"attributes\": { \"version\": \"0.22.1\", \"url\": \"https://releases.hashicorp.com/sentinel/0.22.1/sentinel_0.22.1_linux_amd64.zip\", \"sha\": \"0a4a2b2baf46bfeb81d5137b2656b159ccc881487df3bebacd350ea48b53e76c\", \"official\": true, \"deprecated\": true, \"deprecated-reason\": \"A bug was discovered in this version of Sentinel. Please upgrade as soon as possible\", \"enabled\": true, \"beta\": false, \"usage\": 0, \"created-at\": \"2023-08-23T22:34:24.561Z\" } } } \nDELETE /api/v2/admin/sentinel-versions/:id\nThis endpoint removes a Sentinel version from HCP Terraform. You cannot remove officially labeled Sentinel versions or any version used by a workspace.\n:id\tThe ID of the Sentinel version to delete.\t\n204\tEmpty response\tThe Sentinel version was successfully deleted.\t\n404\tJSON API error object\tThe request could not find a matching Sentinel version with the specified ID, or the client is not an administrator.\t\n422\tJSON API error object\tThe request could not remove the Sentinel version because it is an official version or a workspace uses it.\t\ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request DELETE \\ https://app.terraform.io/api/v2/admin/sentinel-versions/tool-L4oe7rNwn7J4E5Yr"
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202406-1/api-docs/admin/sentinel-versions",
  "text": "Sentinel Versions - Admin - API Docs - Terraform Enterprise | Terraform\nThe Sentinel Versions Admin API lets site administrators manage which versions of Sentinel are available to the HCP Terraform users within their enterprise.\nGET /api/v2/admin/sentinel-versions\nThis endpoint lists all known versions of Sentinel.\nfilter[version]\tOptional. A query string. This will find an exact Sentinel version matching the version queried. This option takes precedence over search queries.\t\nsearch[version]\tOptional. A search query string. This will search for Sentinel versions matching the version number queried.\t\npage[size]\tOptional. If omitted, the endpoint will return 20 Sentinel versions per page.\t\ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ \"https://app.terraform.io/api/v2/admin/sentinel-versions\" \n{ \"data\": [ { \"id\": \"tool-L4oe7rNwn7J4E5Yr\", \"type\": \"sentinel-versions\", \"attributes\": { \"version\": \"0.22.1\", \"url\": \"https://releases.hashicorp.com/sentinel/0.22.1/sentinel_0.22.1_linux_amd64.zip\", \"sha\": \"0a4a2b2baf46bfeb81d5137b2656b159ccc881487df3bebacd350ea48b53e76c\", \"deprecated\": false, \"deprecated-reason\": null, \"official\": true, \"enabled\": true, \"beta\": false, \"usage\": 0, \"created-at\": \"2023-08-23T22:34:24.561Z\" } } ], \"links\": { \"self\": \"https://tfe.example.com/api/v2/admin/sentinel-versions?page%5Bnumber%5D=1&page%5Bsize%5D=20\", \"first\": \"https://tfe.example.com/api/v2/admin/sentinel-versions?page%5Bnumber%5D=1&page%5Bsize%5D=20\", \"prev\": null, \"next\": \"https://tfe.example.com/api/v2/admin/sentinel-versions?page%5Bnumber%5D=2&page%5Bsize%5D=20\", \"last\": \"https://tfe.example.com/api/v2/admin/sentinel-versions?page%5Bnumber%5D=4&page%5Bsize%5D=20\" }, \"meta\": { \"pagination\": { \"current-page\": 1, \"prev-page\": null, \"next-page\": 2, \"total-pages\": 4, \"total-count\": 70 } } } \nPOST /api/v2/admin/sentinel-versions\ndata.type\tstring\t\tMust be \"sentinel-versions\".\t\ndata.attributes.sha\tstring\t\tThe SHA-256 checksum of the compressed Sentinel binary.\t\ndata.attributes.deprecated\tbool\tfalse\tWhether or not this version of Sentinel is deprecated.\t\ndata.attributes.deprecated-reason\tstring\tnull\tAdditional context about why a version of Sentinel is deprecated. Field is null unless deprecated is true.\t\ndata.attributes.official\tbool\tfalse\tWhether or not this is an official release of Sentinel.\t\ndata.attributes.enabled\tbool\ttrue\tWhether or not this version of Sentinel is enabled for use in HCP Terraform.\t\ndata.attributes.beta\tbool\tfalse\tWhether or not this version of Sentinel is a beta pre-release.\t\n{ \"data\": { \"type\": \"sentinel-versions\", \"attributes\": { \"version\": \"0.22.1\", \"url\": \"https://releases.hashicorp.com/sentinel/0.22.1/sentinel_0.22.1_linux_amd64.zip\", \"sha\": \"0a4a2b2baf46bfeb81d5137b2656b159ccc881487df3bebacd350ea48b53e76c\", \"official\": true, \"enabled\": true, \"beta\": false } } } \ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request POST \\ --data @payload.json \\ https://app.terraform.io/api/v2/admin/sentinel-versions \n{ \"data\": { \"id\": \"tool-L4oe7rNwn7J4E5Yr\", \"type\": \"sentinel-versions\", \"attributes\": { \"version\": \"0.22.1\", \"url\": \"https://releases.hashicorp.com/sentinel/0.22.1/sentinel_0.22.1_linux_amd64.zip\", \"sha\": \"0a4a2b2baf46bfeb81d5137b2656b159ccc881487df3bebacd350ea48b53e76c\", \"official\": true, \"deprecated\": false, \"deprecated-reason\": null, \"enabled\": true, \"beta\": false, \"usage\": 0, \"created-at\": \"2023-08-23T22:34:24.561Z\" } } } \nGET /api/v2/admin/sentinel-versions/:id\n:id\tThe ID of the Sentinel version to show.\t\n200\tJSON API document (type: \"sentinel-versions\")\tSuccessfully shows the specified Sentinel version.\t\n404\tJSON API error object\tCould not find the specified Sentinel version, or client is not an administrator.\t\ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ https://app.terraform.io/api/v2/admin/sentinel-versions/tool-L4oe7rNwn7J4E5Yr \n{ \"data\": { \"id\": \"tool-L4oe7rNwn7J4E5Yr\", \"type\": \"sentinel-versions\", \"attributes\": { \"version\": \"0.22.1\", \"url\": \"https://releases.hashicorp.com/sentinel/0.22.1/sentinel_0.22.1_linux_amd64.zip\", \"sha\": \"0a4a2b2baf46bfeb81d5137b2656b159ccc881487df3bebacd350ea48b53e76c\", \"official\": true, \"deprecated\": false, \"deprecated-reason\": null, \"enabled\": true, \"beta\": false, \"usage\": 0, \"created-at\": \"2023-08-23T22:34:24.561Z\" } } } \nPATCH /api/v2/admin/sentinel-versions/:id\n:id\tThe ID of the Sentinel version to update.\t\ndata.type\tstring\t\tMust be \"sentinel-versions\".\t\ndata.attributes.version\tstring\t(previous value)\tA semantic version string in N.N.N or N.N.N-bundleName format (\"0.11.0\", \"0.12.20-beta1\").\t\ndata.attributes.url\tstring\t(previous value)\tThe URL you can download the 64-bit Linux binary of this version.\t\ndata.attributes.sha\tstring\t(previous value)\tThe SHA-256 checksum of the compressed Sentinel binary.\t\ndata.attributes.official\tbool\t(previous value)\tWhether or not this is an official release of Sentinel.\t\ndata.attributes.deprecated\tbool\t(previous value)\tWhether or not this version of Sentinel is deprecated.\t\ndata.attributes.deprecated-reason\tstring\t(previous value)\tAdditional context about why a version of Sentinel is deprecated.\t\ndata.attributes.enabled\tbool\t(previous value)\tWhether or not this version of Sentinel is enabled for use in HCP Terraform.\t\ndata.attributes.beta\tbool\t(previous value)\tWhether or not this version of Sentinel is a beta pre-release.\t\n{ \"data\": { \"type\": \"sentinel-versions\", \"attributes\": { \"deprecated\": true, \"deprecated-reason\": \"A bug was discovered in this version of Sentinel. Please upgrade as soon as possible\" } } } \ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request PATCH \\ --data @payload.json \\ https://app.terraform.io/api/v2/admin/sentinel-versions/tool-L4oe7rNwn7J4E5Yr \n{ \"data\": { \"id\": \"tool-L4oe7rNwn7J4E5Yr\", \"type\": \"sentinel-versions\", \"attributes\": { \"version\": \"0.22.1\", \"url\": \"https://releases.hashicorp.com/sentinel/0.22.1/sentinel_0.22.1_linux_amd64.zip\", \"sha\": \"0a4a2b2baf46bfeb81d5137b2656b159ccc881487df3bebacd350ea48b53e76c\", \"official\": true, \"deprecated\": true, \"deprecated-reason\": \"A bug was discovered in this version of Sentinel. Please upgrade as soon as possible\", \"enabled\": true, \"beta\": false, \"usage\": 0, \"created-at\": \"2023-08-23T22:34:24.561Z\" } } } \nDELETE /api/v2/admin/sentinel-versions/:id\nThis endpoint removes a Sentinel version from HCP Terraform. You cannot remove officially labeled Sentinel versions or any version used by a workspace.\n:id\tThe ID of the Sentinel version to delete.\t\n204\tEmpty response\tThe Sentinel version was successfully deleted.\t\n404\tJSON API error object\tThe request could not find a matching Sentinel version with the specified ID, or the client is not an administrator.\t\n422\tJSON API error object\tThe request could not remove the Sentinel version because it is an official version or a workspace uses it.\t\ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request DELETE \\ https://app.terraform.io/api/v2/admin/sentinel-versions/tool-L4oe7rNwn7J4E5Yr"
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202311-1/api-docs/admin/opa-versions",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202405-1/api-docs/admin/sentinel-versions",
  "text": "Sentinel Versions - Admin - API Docs - Terraform Enterprise | Terraform\nThe Sentinel Versions Admin API lets site administrators manage which versions of Sentinel are available to the HCP Terraform users within their enterprise.\nGET /api/v2/admin/sentinel-versions\nThis endpoint lists all known versions of Sentinel.\nfilter[version]\tOptional. A query string. This will find an exact Sentinel version matching the version queried. This option takes precedence over search queries.\t\nsearch[version]\tOptional. A search query string. This will search for Sentinel versions matching the version number queried.\t\npage[size]\tOptional. If omitted, the endpoint will return 20 Sentinel versions per page.\t\ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ \"https://app.terraform.io/api/v2/admin/sentinel-versions\" \n{ \"data\": [ { \"id\": \"tool-L4oe7rNwn7J4E5Yr\", \"type\": \"sentinel-versions\", \"attributes\": { \"version\": \"0.22.1\", \"url\": \"https://releases.hashicorp.com/sentinel/0.22.1/sentinel_0.22.1_linux_amd64.zip\", \"sha\": \"0a4a2b2baf46bfeb81d5137b2656b159ccc881487df3bebacd350ea48b53e76c\", \"deprecated\": false, \"deprecated-reason\": null, \"official\": true, \"enabled\": true, \"beta\": false, \"usage\": 0, \"created-at\": \"2023-08-23T22:34:24.561Z\" } } ], \"links\": { \"self\": \"https://tfe.example.com/api/v2/admin/sentinel-versions?page%5Bnumber%5D=1&page%5Bsize%5D=20\", \"first\": \"https://tfe.example.com/api/v2/admin/sentinel-versions?page%5Bnumber%5D=1&page%5Bsize%5D=20\", \"prev\": null, \"next\": \"https://tfe.example.com/api/v2/admin/sentinel-versions?page%5Bnumber%5D=2&page%5Bsize%5D=20\", \"last\": \"https://tfe.example.com/api/v2/admin/sentinel-versions?page%5Bnumber%5D=4&page%5Bsize%5D=20\" }, \"meta\": { \"pagination\": { \"current-page\": 1, \"prev-page\": null, \"next-page\": 2, \"total-pages\": 4, \"total-count\": 70 } } } \nPOST /api/v2/admin/sentinel-versions\ndata.attributes.sha\tstring\t\tThe SHA-256 checksum of the compressed Sentinel binary.\t\ndata.attributes.deprecated\tbool\tfalse\tWhether or not this version of Sentinel is deprecated.\t\ndata.attributes.deprecated-reason\tstring\tnull\tAdditional context about why a version of Sentinel is deprecated. Field is null unless deprecated is true.\t\ndata.attributes.official\tbool\tfalse\tWhether or not this is an official release of Sentinel.\t\ndata.attributes.enabled\tbool\ttrue\tWhether or not this version of Sentinel is enabled for use in HCP Terraform.\t\ndata.attributes.beta\tbool\tfalse\tWhether or not this version of Sentinel is a beta pre-release.\t\n{ \"data\": { \"type\": \"sentinel-versions\", \"attributes\": { \"version\": \"0.22.1\", \"url\": \"https://releases.hashicorp.com/sentinel/0.22.1/sentinel_0.22.1_linux_amd64.zip\", \"sha\": \"0a4a2b2baf46bfeb81d5137b2656b159ccc881487df3bebacd350ea48b53e76c\", \"official\": true, \"enabled\": true, \"beta\": false } } } \ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request POST \\ --data @payload.json \\ https://app.terraform.io/api/v2/admin/sentinel-versions \nGET /api/v2/admin/sentinel-versions/:id\n:id\tThe ID of the Sentinel version to show.\t\n200\tJSON API document (type: \"sentinel-versions\")\tSuccessfully shows the specified Sentinel version.\t\n404\tJSON API error object\tCould not find the specified Sentinel version, or client is not an administrator.\t\ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ https://app.terraform.io/api/v2/admin/sentinel-versions/tool-L4oe7rNwn7J4E5Yr \nPATCH /api/v2/admin/sentinel-versions/:id\n:id\tThe ID of the Sentinel version to update.\t\ndata.attributes.version\tstring\t(previous value)\tA semantic version string in N.N.N or N.N.N-bundleName format (\"0.11.0\", \"0.12.20-beta1\").\t\ndata.attributes.url\tstring\t(previous value)\tThe URL you can download the 64-bit Linux binary of this version.\t\ndata.attributes.sha\tstring\t(previous value)\tThe SHA-256 checksum of the compressed Sentinel binary.\t\ndata.attributes.official\tbool\t(previous value)\tWhether or not this is an official release of Sentinel.\t\ndata.attributes.deprecated\tbool\t(previous value)\tWhether or not this version of Sentinel is deprecated.\t\ndata.attributes.deprecated-reason\tstring\t(previous value)\tAdditional context about why a version of Sentinel is deprecated.\t\ndata.attributes.enabled\tbool\t(previous value)\tWhether or not this version of Sentinel is enabled for use in HCP Terraform.\t\ndata.attributes.beta\tbool\t(previous value)\tWhether or not this version of Sentinel is a beta pre-release.\t\n{ \"data\": { \"type\": \"sentinel-versions\", \"attributes\": { \"deprecated\": true, \"deprecated-reason\": \"A bug was discovered in this version of Sentinel. Please upgrade as soon as possible\" } } } \ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request PATCH \\ --data @payload.json \\ https://app.terraform.io/api/v2/admin/sentinel-versions/tool-L4oe7rNwn7J4E5Yr \n{ \"data\": { \"id\": \"tool-L4oe7rNwn7J4E5Yr\", \"type\": \"sentinel-versions\", \"attributes\": { \"version\": \"0.22.1\", \"url\": \"https://releases.hashicorp.com/sentinel/0.22.1/sentinel_0.22.1_linux_amd64.zip\", \"sha\": \"0a4a2b2baf46bfeb81d5137b2656b159ccc881487df3bebacd350ea48b53e76c\", \"official\": true, \"deprecated\": true, \"deprecated-reason\": \"A bug was discovered in this version of Sentinel. Please upgrade as soon as possible\", \"enabled\": true, \"beta\": false, \"usage\": 0, \"created-at\": \"2023-08-23T22:34:24.561Z\" } } } \nDELETE /api/v2/admin/sentinel-versions/:id\nThis endpoint removes a Sentinel version from HCP Terraform. You cannot remove officially labeled Sentinel versions or any version used by a workspace.\n:id\tThe ID of the Sentinel version to delete.\t\n204\tEmpty response\tThe Sentinel version was successfully deleted.\t\n404\tJSON API error object\tThe request could not find a matching Sentinel version with the specified ID, or the client is not an administrator.\t\n422\tJSON API error object\tThe request could not remove the Sentinel version because it is an official version or a workspace uses it.\t\ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request DELETE \\ https://app.terraform.io/api/v2/admin/sentinel-versions/tool-L4oe7rNwn7J4E5Yr"
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202404-2/api-docs/admin/sentinel-versions",
  "text": "Sentinel Versions - Admin - API Docs - Terraform Enterprise | Terraform\nThe Sentinel Versions Admin API lets site administrators manage which versions of Sentinel are available to the HCP Terraform users within their enterprise.\nGET /api/v2/admin/sentinel-versions\nThis endpoint lists all known versions of Sentinel.\nfilter[version]\tOptional. A query string. This will find an exact Sentinel version matching the version queried. This option takes precedence over search queries.\t\nsearch[version]\tOptional. A search query string. This will search for Sentinel versions matching the version number queried.\t\npage[size]\tOptional. If omitted, the endpoint will return 20 Sentinel versions per page.\t\ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ \"https://app.terraform.io/api/v2/admin/sentinel-versions\" \n{ \"data\": [ { \"id\": \"tool-L4oe7rNwn7J4E5Yr\", \"type\": \"sentinel-versions\", \"attributes\": { \"version\": \"0.22.1\", \"url\": \"https://releases.hashicorp.com/sentinel/0.22.1/sentinel_0.22.1_linux_amd64.zip\", \"sha\": \"0a4a2b2baf46bfeb81d5137b2656b159ccc881487df3bebacd350ea48b53e76c\", \"deprecated\": false, \"deprecated-reason\": null, \"official\": true, \"enabled\": true, \"beta\": false, \"usage\": 0, \"created-at\": \"2023-08-23T22:34:24.561Z\" } } ], \"links\": { \"self\": \"https://tfe.example.com/api/v2/admin/sentinel-versions?page%5Bnumber%5D=1&page%5Bsize%5D=20\", \"first\": \"https://tfe.example.com/api/v2/admin/sentinel-versions?page%5Bnumber%5D=1&page%5Bsize%5D=20\", \"prev\": null, \"next\": \"https://tfe.example.com/api/v2/admin/sentinel-versions?page%5Bnumber%5D=2&page%5Bsize%5D=20\", \"last\": \"https://tfe.example.com/api/v2/admin/sentinel-versions?page%5Bnumber%5D=4&page%5Bsize%5D=20\" }, \"meta\": { \"pagination\": { \"current-page\": 1, \"prev-page\": null, \"next-page\": 2, \"total-pages\": 4, \"total-count\": 70 } } } \nPOST /api/v2/admin/sentinel-versions\ndata.attributes.sha\tstring\t\tThe SHA-256 checksum of the compressed Sentinel binary.\t\ndata.attributes.deprecated\tbool\tfalse\tWhether or not this version of Sentinel is deprecated.\t\ndata.attributes.deprecated-reason\tstring\tnull\tAdditional context about why a version of Sentinel is deprecated. Field is null unless deprecated is true.\t\ndata.attributes.official\tbool\tfalse\tWhether or not this is an official release of Sentinel.\t\ndata.attributes.enabled\tbool\ttrue\tWhether or not this version of Sentinel is enabled for use in HCP Terraform.\t\ndata.attributes.beta\tbool\tfalse\tWhether or not this version of Sentinel is a beta pre-release.\t\n{ \"data\": { \"type\": \"sentinel-versions\", \"attributes\": { \"version\": \"0.22.1\", \"url\": \"https://releases.hashicorp.com/sentinel/0.22.1/sentinel_0.22.1_linux_amd64.zip\", \"sha\": \"0a4a2b2baf46bfeb81d5137b2656b159ccc881487df3bebacd350ea48b53e76c\", \"official\": true, \"enabled\": true, \"beta\": false } } } \ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request POST \\ --data @payload.json \\ https://app.terraform.io/api/v2/admin/sentinel-versions \nGET /api/v2/admin/sentinel-versions/:id\n:id\tThe ID of the Sentinel version to show.\t\n200\tJSON API document (type: \"sentinel-versions\")\tSuccessfully shows the specified Sentinel version.\t\n404\tJSON API error object\tCould not find the specified Sentinel version, or client is not an administrator.\t\ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ https://app.terraform.io/api/v2/admin/sentinel-versions/tool-L4oe7rNwn7J4E5Yr \nPATCH /api/v2/admin/sentinel-versions/:id\n:id\tThe ID of the Sentinel version to update.\t\ndata.attributes.version\tstring\t(previous value)\tA semantic version string in N.N.N or N.N.N-bundleName format (\"0.11.0\", \"0.12.20-beta1\").\t\ndata.attributes.url\tstring\t(previous value)\tThe URL you can download the 64-bit Linux binary of this version.\t\ndata.attributes.sha\tstring\t(previous value)\tThe SHA-256 checksum of the compressed Sentinel binary.\t\ndata.attributes.official\tbool\t(previous value)\tWhether or not this is an official release of Sentinel.\t\ndata.attributes.deprecated\tbool\t(previous value)\tWhether or not this version of Sentinel is deprecated.\t\ndata.attributes.deprecated-reason\tstring\t(previous value)\tAdditional context about why a version of Sentinel is deprecated.\t\ndata.attributes.enabled\tbool\t(previous value)\tWhether or not this version of Sentinel is enabled for use in HCP Terraform.\t\ndata.attributes.beta\tbool\t(previous value)\tWhether or not this version of Sentinel is a beta pre-release.\t\n{ \"data\": { \"type\": \"sentinel-versions\", \"attributes\": { \"deprecated\": true, \"deprecated-reason\": \"A bug was discovered in this version of Sentinel. Please upgrade as soon as possible\" } } } \ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request PATCH \\ --data @payload.json \\ https://app.terraform.io/api/v2/admin/sentinel-versions/tool-L4oe7rNwn7J4E5Yr \n{ \"data\": { \"id\": \"tool-L4oe7rNwn7J4E5Yr\", \"type\": \"sentinel-versions\", \"attributes\": { \"version\": \"0.22.1\", \"url\": \"https://releases.hashicorp.com/sentinel/0.22.1/sentinel_0.22.1_linux_amd64.zip\", \"sha\": \"0a4a2b2baf46bfeb81d5137b2656b159ccc881487df3bebacd350ea48b53e76c\", \"official\": true, \"deprecated\": true, \"deprecated-reason\": \"A bug was discovered in this version of Sentinel. Please upgrade as soon as possible\", \"enabled\": true, \"beta\": false, \"usage\": 0, \"created-at\": \"2023-08-23T22:34:24.561Z\" } } } \nDELETE /api/v2/admin/sentinel-versions/:id\nThis endpoint removes a Sentinel version from HCP Terraform. You cannot remove officially labeled Sentinel versions or any version used by a workspace.\n:id\tThe ID of the Sentinel version to delete.\t\n204\tEmpty response\tThe Sentinel version was successfully deleted.\t\n404\tJSON API error object\tThe request could not find a matching Sentinel version with the specified ID, or the client is not an administrator.\t\n422\tJSON API error object\tThe request could not remove the Sentinel version because it is an official version or a workspace uses it.\t\ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request DELETE \\ https://app.terraform.io/api/v2/admin/sentinel-versions/tool-L4oe7rNwn7J4E5Yr"
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202404-1/api-docs/admin/sentinel-versions",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202402-2/api-docs/admin/sentinel-versions",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202402-1/api-docs/admin/sentinel-versions",
  "text": "The Sentinel Versions Admin API lets site administrators manage which versions of Sentinel are available to the Terraform Cloud users within their enterprise.\ndata.attributes.enabled\tbool\ttrue\tWhether or not this version of Sentinel is enabled for use in Terraform Cloud.\t\ndata.attributes.enabled\tbool\t(previous value)\tWhether or not this version of Sentinel is enabled for use in Terraform Cloud.\t\nThis endpoint removes a Sentinel version from Terraform Cloud. You cannot remove officially labeled Sentinel versions or any version used by a workspace."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202401-2/api-docs/admin/sentinel-versions",
  "text": "The Sentinel Versions Admin API lets site administrators manage which versions of Sentinel are available to the Terraform Cloud users within their enterprise.\ndata.attributes.enabled\tbool\ttrue\tWhether or not this version of Sentinel is enabled for use in Terraform Cloud.\t\ndata.attributes.enabled\tbool\t(previous value)\tWhether or not this version of Sentinel is enabled for use in Terraform Cloud.\t\nThis endpoint removes a Sentinel version from Terraform Cloud. You cannot remove officially labeled Sentinel versions or any version used by a workspace."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202401-1/api-docs/admin/sentinel-versions",
  "text": "The Sentinel Versions Admin API lets site administrators manage which versions of Sentinel are available to the Terraform Cloud users within their enterprise.\ndata.attributes.enabled\tbool\ttrue\tWhether or not this version of Sentinel is enabled for use in Terraform Cloud.\t\ndata.attributes.enabled\tbool\t(previous value)\tWhether or not this version of Sentinel is enabled for use in Terraform Cloud.\t\nThis endpoint removes a Sentinel version from Terraform Cloud. You cannot remove officially labeled Sentinel versions or any version used by a workspace."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202312-1/api-docs/admin/sentinel-versions",
  "text": "The Sentinel Versions Admin API lets site administrators manage which versions of Sentinel are available to the Terraform Cloud users within their enterprise.\ndata.attributes.enabled\tbool\ttrue\tWhether or not this version of Sentinel is enabled for use in Terraform Cloud.\t\ndata.attributes.enabled\tbool\t(previous value)\tWhether or not this version of Sentinel is enabled for use in Terraform Cloud.\t\nThis endpoint removes a Sentinel version from Terraform Cloud. You cannot remove officially labeled Sentinel versions or any version used by a workspace."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202212-2/api-docs/run-tasks/run-tasks-integration",
  "text": "Run Tasks Integration - API Docs - Terraform Enterprise | Terraform\nNote: Run Tasks is a paid feature, available as part of the Team & Governance upgrade package. Refer to Terraform Cloud pricing for details.\nRun tasks allow Terraform Cloud to interact with external systems at specific points in the Terraform Cloud run lifecycle. This page lists the API endpoints used to trigger a run task and the expected response from the integration.\nRefer to run tasks for the API endpoints to create and manage run tasks within Terraform Cloud. You can also access a complete list of all run tasks in the Terraform Registry. \nWhen a run reaches the appropriate phase and a run task is triggered, Terraform Cloud will send a request to the run task's URL. The service receiving the run task request should respond with 200 OK, or Terraform Cloud will retry to trigger the run task.\nPOST :url\n:url\tThe URL configured in the run task to send requests to.\t\n200\tNothing\tSuccessfully submitted a run task\t\nThe POST request submits a JSON object with the following properties as a request payload.\nCommon Properties\nAll request payloads contain the following properties.\nKey pathTypeValuesDescription\npayload_version\tinteger\t1\tSchema version of the payload. Only 1 is supported.\t\naccess_token\tstring\t\tBearer token to use when calling back to Terraform Cloud.\t\nstage\tstring\tpre_plan, post_plan, pre_apply\tThe run stage when Terraform Cloud triggers the run task.\t\nis_speculative\tbool\t\tWhether the task is part of a speculative run.\t\ntask_result_id\tstring\t\tID of task result within Terraform Cloud.\t\ntask_result_enforcement_level\tstring\tmandatory, advisory\tEnforcement level for this task.\t\ntask_result_callback_url\tstring\t\tURL that should called back with the result of this task.\t\nrun_id\tstring\t\tId of the run this task is part of.\t\nrun_app_url\tstring\t\tURL within Terraform Cloud to the run.\t\nrun_message\tstring\t\tMessage that was associated with the run.\t\nrun_created_at\tstring\t\tWhen the run was started.\t\nrun_created_by\tstring\t\tWho created the run.\t\nworkspace_id\tstring\t\tId of the workspace the task is associated with.\t\nworkspace_name\tstring\t\tName of the workspace.\t\nworkspace_app_url\tstring\t\tURL within Terraform Cloud to the workspace.\t\norganization_name\tstring\t\tName of the organization the task is configured within.\t\nvcs_repo_url\tstring\t\tURL to the workspace's VCS repository. This is null if the workspace does not have a VCS repository.\t\nvcs_branch\tstring\t\tRepository branch that the workspace executes from. This is null if the workspace does not have a VCS repository.\t\nvcs_pull_request_url\tstring\t\tURL to the Pull Request/Merge Request that triggered this run. This is null if the run was not triggered.\t\nvcs_commit_url\tstring\t\tURL to the commit that triggered this run. This is null if the workspace does not a VCS repository.\t\nPre-Plan Properties\nRequests with stage set to pre_plan contain the following additional properties.\nPost-Plan Properties\nRequests with stage set to post_plan contain the following additional properties.\nKey pathTypeValuesDescription\nplan_json_api_url\tstring\t\tThe URL to retrieve the JSON Terraform plan for this run.\t\n{ \"payload_version\": 1, \"access_token\": \"4QEuyyxug1f2rw.atlasv1.iDyxqhXGVZ0ykes53YdQyHyYtFOrdAWNBxcVUgWvzb64NFHjcquu8gJMEdUwoSLRu4Q\", \"stage\": \"post_plan\", \"is_speculative\": false, \"task_result_id\": \"taskrs-2nH5dncYoXaMVQmJ\", \"task_result_enforcement_level\": \"mandatory\", \"task_result_callback_url\": \"https://app.terraform.io/api/v2/task-results/5ea8d46c-2ceb-42cd-83f2-82e54697bddd/callback\", \"run_app_url\": \"https://app.terraform.io/app/hashicorp/my-workspace/runs/run-i3Df5to9ELvibKpQ\", \"run_id\": \"run-i3Df5to9ELvibKpQ\", \"run_message\": \"Triggered via UI\", \"run_created_at\": \"2021-09-02T14:47:13.036Z\", \"run_created_by\": \"username\", \"workspace_id\": \"ws-ck4G5bb1Yei5szRh\", \"workspace_name\": \"tfr_github_0\", \"workspace_app_url\": \"https://app.terraform.io/app/hashicorp/my-workspace\", \"organization_name\": \"hashicorp\", \"plan_json_api_url\": \"https://app.terraform.io/api/v2/plans/plan-6AFmRJW1PFJ7qbAh/json-output\", \"vcs_repo_url\": \"https://github.com/hashicorp/terraform-random\", \"vcs_branch\": \"main\", \"vcs_pull_request_url\": null, \"vcs_commit_url\": \"https://github.com/hashicorp/terraform-random/commit/7d8fb2a2d601edebdb7a59ad2088a96673637d22\" } \nRequest Headers\nThe POST request submits the following properties as the request headers.\nNameValueDescription\nContent-Type\tapplication/json\tSpecifies the type of data in the request body\t\nUser-Agent\tTFC/1.0 (+https://app.terraform.io; TFC)\tIdentifies the request is coming from Terraform Cloud\t\nX-TFC-Task-Signature\tstring\tIf the run task is configured with an HMAC Key, this header contains the signed SHA512 sum of the request payload using the configured HMAC key. Otherwise, this is an empty string.\t\nOnce a run task request has been fulfilled by the external integration, the integration must call back into Terraform Cloud with the result. Terraform expects this callback within 10 minutes, or the request will be considered to have errored.\nPATCH :callback_url\n:callback_url\tThe task_result_callback_url specified in the run task request. Typically /task-results/:guid/callback.\t\n200\tNothing\tSuccessfully submitted a run task result\t\n401\tJSON API error object\tNot authorized to perform action\t\n422\tJSON API error object\tInvalid response payload. This could be caused by invalid attributes, or sending a status that is not accepted.\t\nThe PATCH request submits a JSON object with the following properties as a request payload.\nKey pathTypeDescription\ndata.type\tstring\tMust be \"task-results\".\t\ndata.attributes.status\tstring\tThe current status of the task. Only passed, failed or running are allowed.\t\ndata.attributes.message\tstring\t(Optional) A short message describing the status of the task.\t\ndata.attributes.url\tstring\t(Optional) A URL where users can obtain more information about the task.\t\nStatus values other than passed, failed, or running return an error. Both the passed and failed statuses represent a final state for a run task. The running status allows one or more partial updates until the task has reached a final state.\n{ \"data\": { \"type\": \"task-results\", \"attributes\": { \"status\": \"passed\", \"message\": \"4 passed, 0 skipped, 0 failed\", \"url\": \"https://external.service.dev/terraform-plan-checker/run-i3Df5to9ELvibKpQ\" } } } \nRequest Headers\nThe PATCH request must use the token supplied in the originating request (access_token) for authentication."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202212-1/api-docs/run-tasks/run-tasks-integration",
  "text": "Run Tasks Integration - API Docs - Terraform Enterprise | Terraform\nNote: Run Tasks is a paid feature, available as part of the Team & Governance upgrade package. Refer to Terraform Cloud pricing for details.\nRun tasks allow Terraform Cloud to interact with external systems at specific points in the Terraform Cloud run lifecycle. This page lists the API endpoints used to trigger a run task and the expected response from the integration.\nRefer to run tasks for the API endpoints to create and manage run tasks within Terraform Cloud. You can also access a complete list of all run tasks in the Terraform Registry. \nWhen a run reaches the appropriate phase and a run task is triggered, Terraform Cloud will send a request to the run task's URL. The service receiving the run task request should respond with 200 OK, or Terraform Cloud will retry to trigger the run task.\nPOST :url\n:url\tThe URL configured in the run task to send requests to.\t\n200\tNothing\tSuccessfully submitted a run task\t\nThe POST request submits a JSON object with the following properties as a request payload.\nCommon Properties\nAll request payloads contain the following properties.\nKey pathTypeValuesDescription\npayload_version\tinteger\t1\tSchema version of the payload. Only 1 is supported.\t\naccess_token\tstring\t\tBearer token to use when calling back to Terraform Cloud.\t\nstage\tstring\tpre_plan, post_plan, pre_apply\tThe run stage when Terraform Cloud triggers the run task.\t\nis_speculative\tbool\t\tWhether the task is part of a speculative run.\t\ntask_result_id\tstring\t\tID of task result within Terraform Cloud.\t\ntask_result_enforcement_level\tstring\tmandatory, advisory\tEnforcement level for this task.\t\ntask_result_callback_url\tstring\t\tURL that should called back with the result of this task.\t\nrun_id\tstring\t\tId of the run this task is part of.\t\nrun_app_url\tstring\t\tURL within Terraform Cloud to the run.\t\nrun_message\tstring\t\tMessage that was associated with the run.\t\nrun_created_at\tstring\t\tWhen the run was started.\t\nrun_created_by\tstring\t\tWho created the run.\t\nworkspace_id\tstring\t\tId of the workspace the task is associated with.\t\nworkspace_name\tstring\t\tName of the workspace.\t\nworkspace_app_url\tstring\t\tURL within Terraform Cloud to the workspace.\t\norganization_name\tstring\t\tName of the organization the task is configured within.\t\nvcs_repo_url\tstring\t\tURL to the workspace's VCS repository. This is null if the workspace does not have a VCS repository.\t\nvcs_branch\tstring\t\tRepository branch that the workspace executes from. This is null if the workspace does not have a VCS repository.\t\nvcs_pull_request_url\tstring\t\tURL to the Pull Request/Merge Request that triggered this run. This is null if the run was not triggered.\t\nvcs_commit_url\tstring\t\tURL to the commit that triggered this run. This is null if the workspace does not a VCS repository.\t\nPre-Plan Properties\nRequests with stage set to pre_plan contain the following additional properties.\nPost-Plan Properties\nRequests with stage set to post_plan contain the following additional properties.\nKey pathTypeValuesDescription\nplan_json_api_url\tstring\t\tThe URL to retrieve the JSON Terraform plan for this run.\t\n{ \"payload_version\": 1, \"access_token\": \"4QEuyyxug1f2rw.atlasv1.iDyxqhXGVZ0ykes53YdQyHyYtFOrdAWNBxcVUgWvzb64NFHjcquu8gJMEdUwoSLRu4Q\", \"stage\": \"post_plan\", \"is_speculative\": false, \"task_result_id\": \"taskrs-2nH5dncYoXaMVQmJ\", \"task_result_enforcement_level\": \"mandatory\", \"task_result_callback_url\": \"https://app.terraform.io/api/v2/task-results/5ea8d46c-2ceb-42cd-83f2-82e54697bddd/callback\", \"run_app_url\": \"https://app.terraform.io/app/hashicorp/my-workspace/runs/run-i3Df5to9ELvibKpQ\", \"run_id\": \"run-i3Df5to9ELvibKpQ\", \"run_message\": \"Triggered via UI\", \"run_created_at\": \"2021-09-02T14:47:13.036Z\", \"run_created_by\": \"username\", \"workspace_id\": \"ws-ck4G5bb1Yei5szRh\", \"workspace_name\": \"tfr_github_0\", \"workspace_app_url\": \"https://app.terraform.io/app/hashicorp/my-workspace\", \"organization_name\": \"hashicorp\", \"plan_json_api_url\": \"https://app.terraform.io/api/v2/plans/plan-6AFmRJW1PFJ7qbAh/json-output\", \"vcs_repo_url\": \"https://github.com/hashicorp/terraform-random\", \"vcs_branch\": \"main\", \"vcs_pull_request_url\": null, \"vcs_commit_url\": \"https://github.com/hashicorp/terraform-random/commit/7d8fb2a2d601edebdb7a59ad2088a96673637d22\" } \nRequest Headers\nThe POST request submits the following properties as the request headers.\nNameValueDescription\nContent-Type\tapplication/json\tSpecifies the type of data in the request body\t\nUser-Agent\tTFC/1.0 (+https://app.terraform.io; TFC)\tIdentifies the request is coming from Terraform Cloud\t\nX-TFC-Task-Signature\tstring\tIf the run task is configured with an HMAC Key, this header contains the signed SHA512 sum of the request payload using the configured HMAC key. Otherwise, this is an empty string.\t\nOnce a run task request has been fulfilled by the external integration, the integration must call back into Terraform Cloud with the result. Terraform expects this callback within 10 minutes, or the request will be considered to have errored.\nPATCH :callback_url\n:callback_url\tThe task_result_callback_url specified in the run task request. Typically /task-results/:guid/callback.\t\n200\tNothing\tSuccessfully submitted a run task result\t\n401\tJSON API error object\tNot authorized to perform action\t\n422\tJSON API error object\tInvalid response payload. This could be caused by invalid attributes, or sending a status that is not accepted.\t\nThe PATCH request submits a JSON object with the following properties as a request payload.\nKey pathTypeDescription\ndata.type\tstring\tMust be \"task-results\".\t\ndata.attributes.status\tstring\tThe current status of the task. Only passed, failed or running are allowed.\t\ndata.attributes.message\tstring\t(Optional) A short message describing the status of the task.\t\ndata.attributes.url\tstring\t(Optional) A URL where users can obtain more information about the task.\t\nStatus values other than passed, failed, or running return an error. Both the passed and failed statuses represent a final state for a run task. The running status allows one or more partial updates until the task has reached a final state.\n{ \"data\": { \"type\": \"task-results\", \"attributes\": { \"status\": \"passed\", \"message\": \"4 passed, 0 skipped, 0 failed\", \"url\": \"https://external.service.dev/terraform-plan-checker/run-i3Df5to9ELvibKpQ\" } } } \nRequest Headers\nThe PATCH request must use the token supplied in the originating request (access_token) for authentication."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202211-1/api-docs/run-tasks/run-tasks-integration",
  "text": "Run Tasks Integration - API Docs - Terraform Enterprise | Terraform\nNote: Run Tasks is a paid feature, available as part of the Team & Governance upgrade package. Refer to Terraform Cloud pricing for details.\nRun tasks allow Terraform Cloud to interact with external systems at specific points in the Terraform Cloud run lifecycle. This page lists the API endpoints used to trigger a run task and the expected response from the integration.\nRefer to run tasks for the API endpoints to create and manage run tasks within Terraform Cloud.\nWhen a run reaches the appropriate phase and a run task is triggered, Terraform Cloud will send a request to the run task's URL. The service receiving the run task request should respond with 200 OK, or Terraform Cloud will retry to trigger the run task.\nPOST :url\n:url\tThe URL configured in the run task to send requests to.\t\n200\tNothing\tSuccessfully submitted a run task\t\nThe POST request submits a JSON object with the following properties as a request payload.\nCommon Properties\nAll request payloads contain the following properties.\npayload_version\tinteger\t1\tSchema version of the payload. Only 1 is supported.\t\naccess_token\tstring\t\tBearer token to use when calling back to Terraform Cloud.\t\nstage\tstring\tpre_plan, post_plan, pre_apply\tThe run stage when Terraform Cloud triggers the run task. The stage property is in beta.\t\nis_speculative\tbool\t\tWhether the task is part of a speculative run.\t\ntask_result_id\tstring\t\tID of task result within Terraform Cloud.\t\ntask_result_enforcement_level\tstring\tmandatory, advisory\tEnforcement level for this task.\t\ntask_result_callback_url\tstring\t\tURL that should called back with the result of this task.\t\nrun_id\tstring\t\tId of the run this task is part of.\t\nrun_app_url\tstring\t\tURL within Terraform Cloud to the run.\t\nrun_message\tstring\t\tMessage that was associated with the run.\t\nrun_created_at\tstring\t\tWhen the run was started.\t\nrun_created_by\tstring\t\tWho created the run.\t\nworkspace_id\tstring\t\tId of the workspace the task is associated with.\t\nworkspace_name\tstring\t\tName of the workspace.\t\nworkspace_app_url\tstring\t\tURL within Terraform Cloud to the workspace.\t\norganization_name\tstring\t\tName of the organization the task is configured within.\t\nvcs_repo_url\tstring\t\tURL to the workspace's VCS repository. This is null if the workspace does not have a VCS repository.\t\nvcs_branch\tstring\t\tRepository branch that the workspace executes from. This is null if the workspace does not have a VCS repository.\t\nvcs_pull_request_url\tstring\t\tURL to the Pull Request/Merge Request that triggered this run. This is null if the run was not triggered.\t\nvcs_commit_url\tstring\t\tURL to the commit that triggered this run. This is null if the workspace does not a VCS repository.\t\nPre-Plan Properties\nNote: Pre-plan run tasks are in beta.\nRequests with stage set to pre_plan contain the following additional properties.\nPost-Plan Properties\nRequests with stage set to post_plan contain the following additional properties.\nplan_json_api_url\tstring\t\tThe URL to retrieve the JSON Terraform plan for this run.\t\n{ \"payload_version\": 1, \"access_token\": \"4QEuyyxug1f2rw.atlasv1.iDyxqhXGVZ0ykes53YdQyHyYtFOrdAWNBxcVUgWvzb64NFHjcquu8gJMEdUwoSLRu4Q\", \"stage\": \"post_plan\", \"is_speculative\": false, \"task_result_id\": \"taskrs-2nH5dncYoXaMVQmJ\", \"task_result_enforcement_level\": \"mandatory\", \"task_result_callback_url\": \"https://app.terraform.io/api/v2/task-results/5ea8d46c-2ceb-42cd-83f2-82e54697bddd/callback\", \"run_app_url\": \"https://app.terraform.io/app/hashicorp/my-workspace/runs/run-i3Df5to9ELvibKpQ\", \"run_id\": \"run-i3Df5to9ELvibKpQ\", \"run_message\": \"Triggered via UI\", \"run_created_at\": \"2021-09-02T14:47:13.036Z\", \"run_created_by\": \"username\", \"workspace_id\": \"ws-ck4G5bb1Yei5szRh\", \"workspace_name\": \"tfr_github_0\", \"workspace_app_url\": \"https://app.terraform.io/app/hashicorp/my-workspace\", \"organization_name\": \"hashicorp\", \"plan_json_api_url\": \"https://app.terraform.io/api/v2/plans/plan-6AFmRJW1PFJ7qbAh/json-output\", \"vcs_repo_url\": \"https://github.com/hashicorp/terraform-random\", \"vcs_branch\": \"main\", \"vcs_pull_request_url\": null, \"vcs_commit_url\": \"https://github.com/hashicorp/terraform-random/commit/7d8fb2a2d601edebdb7a59ad2088a96673637d22\" } \nThe POST request submits the following properties as the request headers.\nNameValueDescription\nContent-Type\tapplication/json\tSpecifies the type of data in the request body\t\nUser-Agent\tTFC/1.0 (+https://app.terraform.io; TFC)\tIdentifies the request is coming from Terraform Cloud\t\nX-TFC-Task-Signature\tstring\tIf the run task is configured with an HMAC Key, this header contains the signed SHA512 sum of the request payload using the configured HMAC key. Otherwise, this is an empty string.\t\nOnce a run task request has been fulfilled by the external integration, the integration must call back into Terraform Cloud with the result. Terraform expects this callback within 10 minutes, or the request will be considered to have errored.\nPATCH :callback_url\n:callback_url\tThe task_result_callback_url specified in the run task request. Typically /task-results/:guid/callback.\t\n200\tNothing\tSuccessfully submitted a run task result\t\n401\tJSON API error object\tNot authorized to perform action\t\n422\tJSON API error object\tInvalid response payload. This could be caused by invalid attributes, or sending a status that is not accepted.\t\nThe PATCH request submits a JSON object with the following properties as a request payload.\nKey pathTypeDescription\ndata.type\tstring\tMust be \"task-results\".\t\ndata.attributes.status\tstring\tThe current status of the task. Only passed, failed or running are allowed.\t\ndata.attributes.message\tstring\t(Optional) A short message describing the status of the task.\t\ndata.attributes.url\tstring\t(Optional) A URL where users can obtain more information about the task.\t\nStatus values other than passed, failed, or running return an error. Both the passed and failed statuses represent a final state for a run task. The running status allows one or more partial updates until the task has reached a final state.\n{ \"data\": { \"type\": \"task-results\", \"attributes\": { \"status\": \"passed\", \"message\": \"4 passed, 0 skipped, 0 failed\", \"url\": \"https://external.service.dev/terraform-plan-checker/run-i3Df5to9ELvibKpQ\" } } } \nThe PATCH request must use the token supplied in the originating request (access_token) for authentication."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202311-1/api-docs/admin/sentinel-versions",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202210-1/api-docs/run-tasks/run-tasks-integration",
  "text": "Run Tasks Integration - API Docs - Terraform Enterprise | Terraform\nNote: Run Tasks is a paid feature, available as part of the Team & Governance upgrade package. Refer to Terraform Cloud pricing for details.\nRun tasks allow Terraform Cloud to interact with external systems at specific points in the Terraform Cloud run lifecycle. This page lists the API endpoints used to trigger a run task and the expected response from the integration.\nRefer to run tasks for the API endpoints to create and manage run tasks within Terraform Cloud.\nWhen a run reaches the appropriate phase and a run task is triggered, Terraform Cloud will send a request to the run task's URL. The service receiving the run task request should respond with 200 OK, or Terraform Cloud will retry to trigger the run task.\nPOST :url\n:url\tThe URL configured in the run task to send requests to.\t\n200\tNothing\tSuccessfully submitted a run task\t\nThe POST request submits a JSON object with the following properties as a request payload.\nCommon Properties\nAll request payloads contain the following properties.\npayload_version\tinteger\t1\tSchema version of the payload. Only 1 is supported.\t\naccess_token\tstring\t\tBearer token to use when calling back to Terraform Cloud.\t\nstage\tstring\tpre_plan, post_plan, pre_apply\tThe run stage when Terraform Cloud triggers the run task. The stage property is in beta.\t\nis_speculative\tbool\t\tWhether the task is part of a speculative run.\t\ntask_result_id\tstring\t\tID of task result within Terraform Cloud.\t\ntask_result_enforcement_level\tstring\tmandatory, advisory\tEnforcement level for this task.\t\ntask_result_callback_url\tstring\t\tURL that should called back with the result of this task.\t\nrun_id\tstring\t\tId of the run this task is part of.\t\nrun_app_url\tstring\t\tURL within Terraform Cloud to the run.\t\nrun_message\tstring\t\tMessage that was associated with the run.\t\nrun_created_at\tstring\t\tWhen the run was started.\t\nrun_created_by\tstring\t\tWho created the run.\t\nworkspace_id\tstring\t\tId of the workspace the task is associated with.\t\nworkspace_name\tstring\t\tName of the workspace.\t\nworkspace_app_url\tstring\t\tURL within Terraform Cloud to the workspace.\t\norganization_name\tstring\t\tName of the organization the task is configured within.\t\nvcs_repo_url\tstring\t\tURL to the workspace's VCS repository. This is null if the workspace does not have a VCS repository.\t\nvcs_branch\tstring\t\tRepository branch that the workspace executes from. This is null if the workspace does not have a VCS repository.\t\nvcs_pull_request_url\tstring\t\tURL to the Pull Request/Merge Request that triggered this run. This is null if the run was not triggered.\t\nvcs_commit_url\tstring\t\tURL to the commit that triggered this run. This is null if the workspace does not a VCS repository.\t\nPre-Plan Properties\nNote: Pre-plan run tasks are in beta.\nRequests with stage set to pre_plan contain the following additional properties.\nPost-Plan Properties\nRequests with stage set to post_plan contain the following additional properties.\nplan_json_api_url\tstring\t\tThe URL to retrieve the JSON Terraform plan for this run.\t\n{ \"payload_version\": 1, \"access_token\": \"4QEuyyxug1f2rw.atlasv1.iDyxqhXGVZ0ykes53YdQyHyYtFOrdAWNBxcVUgWvzb64NFHjcquu8gJMEdUwoSLRu4Q\", \"stage\": \"post_plan\", \"is_speculative\": false, \"task_result_id\": \"taskrs-2nH5dncYoXaMVQmJ\", \"task_result_enforcement_level\": \"mandatory\", \"task_result_callback_url\": \"https://app.terraform.io/api/v2/task-results/5ea8d46c-2ceb-42cd-83f2-82e54697bddd/callback\", \"run_app_url\": \"https://app.terraform.io/app/hashicorp/my-workspace/runs/run-i3Df5to9ELvibKpQ\", \"run_id\": \"run-i3Df5to9ELvibKpQ\", \"run_message\": \"Triggered via UI\", \"run_created_at\": \"2021-09-02T14:47:13.036Z\", \"run_created_by\": \"username\", \"workspace_id\": \"ws-ck4G5bb1Yei5szRh\", \"workspace_name\": \"tfr_github_0\", \"workspace_app_url\": \"https://app.terraform.io/app/hashicorp/my-workspace\", \"organization_name\": \"hashicorp\", \"plan_json_api_url\": \"https://app.terraform.io/api/v2/plans/plan-6AFmRJW1PFJ7qbAh/json-output\", \"vcs_repo_url\": \"https://github.com/hashicorp/terraform-random\", \"vcs_branch\": \"main\", \"vcs_pull_request_url\": null, \"vcs_commit_url\": \"https://github.com/hashicorp/terraform-random/commit/7d8fb2a2d601edebdb7a59ad2088a96673637d22\" } \nThe POST request submits the following properties as the request headers.\nNameValueDescription\nContent-Type\tapplication/json\tSpecifies the type of data in the request body\t\nUser-Agent\tTFC/1.0 (+https://app.terraform.io; TFC)\tIdentifies the request is coming from Terraform Cloud\t\nX-TFC-Task-Signature\tstring\tIf the run task is configured with an HMAC Key, this header contains the signed SHA512 sum of the request payload using the configured HMAC key. Otherwise, this is an empty string.\t\nOnce a run task request has been fulfilled by the external integration, the integration must call back into Terraform Cloud with the result. Terraform expects this callback within 10 minutes, or the request will be considered to have errored.\nPATCH :callback_url\n:callback_url\tThe task_result_callback_url specified in the run task request. Typically /task-results/:guid/callback.\t\n200\tNothing\tSuccessfully submitted a run task result\t\n401\tJSON API error object\tNot authorized to perform action\t\n422\tJSON API error object\tInvalid response payload. This could be caused by invalid attributes, or sending a status that is not accepted.\t\nThe PATCH request submits a JSON object with the following properties as a request payload.\nKey pathTypeDescription\ndata.type\tstring\tMust be \"task-results\".\t\ndata.attributes.status\tstring\tThe current status of the task. Only passed, failed or running are allowed.\t\ndata.attributes.message\tstring\t(Optional) A short message describing the status of the task.\t\ndata.attributes.url\tstring\t(Optional) A URL where users can obtain more information about the task.\t\nStatus values other than passed, failed, or running return an error. Both the passed and failed statuses represent a final state for a run task. The running status allows one or more partial updates until the task has reached a final state.\n{ \"data\": { \"type\": \"task-results\", \"attributes\": { \"status\": \"passed\", \"message\": \"4 passed, 0 skipped, 0 failed\", \"url\": \"https://external.service.dev/terraform-plan-checker/run-i3Df5to9ELvibKpQ\" } } } \nThe PATCH request must use the token supplied in the originating request (access_token) for authentication."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202209-2/api-docs/run-tasks/run-tasks-integration",
  "text": "Refer to run tasks for the API endpoints to create and manage run tasks within Terraform Cloud.\nstage\tstring\tpre_plan, post_plan, pre_apply\tThe run stage when Terraform Cloud triggers the run task. The stage property is in beta.\t\nNote: Pre-plan run tasks are in beta."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202209-1/api-docs/run-tasks/run-tasks-integration",
  "text": "Refer to run tasks for the API endpoints to create and manage run tasks within Terraform Cloud.\nstage\tstring\tpre_plan, post_plan, pre_apply\tThe run stage when Terraform Cloud triggers the run task. The stage property is in beta.\t\nNote: Pre-plan run tasks are in beta."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202208-3/api-docs/run-tasks/run-tasks-integration",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202208-2/api-docs/run-tasks/run-tasks-integration",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202208-1/api-docs/run-tasks/run-tasks-integration",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202207-2/api-docs/run-tasks/run-tasks-integration",
  "text": "stage\tstring\tpost-plan, pre-apply\tThe stage the task was triggered at.\t\nplan_json_api_url\tstring\t\tURL to retrieve the JSON Terraform plan for this run.\t\n[422][]\tJSON API error object\tInvalid response payload. This could be caused by invalid attributes, or sending a status that is not accepted.\t"
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202207-1/api-docs/run-tasks/run-tasks-integration",
  "text": "stage\tstring\tpost-plan, pre-apply\tThe stage the task was triggered at.\t\nplan_json_api_url\tstring\t\tURL to retrieve the JSON Terraform plan for this run.\t\n[422][]\tJSON API error object\tInvalid response payload. This could be caused by invalid attributes, or sending a status that is not accepted.\t"
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202407-1/api-docs/run-tasks/run-tasks-integration",
  "text": "Run tasks allow HCP Terraform to interact with external systems at specific points in the HCP Terraform run lifecycle. This page lists the API endpoints used to trigger a run task and the expected response from the integration.\nRefer to run tasks for the API endpoints to create and manage run tasks within HCP Terraform. You can also access a complete list of all run tasks in the Terraform Registry.\nWhen a run reaches the appropriate phase and a run task is triggered, HCP Terraform will send a request to the run task's URL. The service receiving the run task request should respond with 200 OK, or HCP Terraform will retry to trigger the run task.\n200\tNo Content\tSuccessfully submitted a run task\t\nstage\tstring\tpre_plan, post_plan, pre_apply, post_apply\tThe run stage when HCP Terraform triggers the run task.\t\naccess_token\tstring\t\tBearer token to use when calling back to HCP Terraform.\t\ncapabilitites\tobject\t\tA map of the capabilities that the caller supports.\t\ncapabilitites.outcomes\tbool\t\tA flag indicating the caller accepts detailed run task outcomes.\t\nconfiguration_version_download_url\tstring\t\tThe URL to download the configuration version. This is null if the configuration version is not available to download.\t\nconfiguration_version_id\tstring\t\tThe ID of the configuration version for the run.\t\nrun_app_url\tstring\t\tURL within HCP Terraform to the run.\t\ntask_result_id\tstring\t\tID of task result within HCP Terraform.\t\nworkspace_app_url\tstring\t\tURL within HCP Terraform to the workspace.\t\nworkspace_working_directory\tstring\t\tThe working directory specified in the run's workspace settings.\t\nPost-Plan, Pre-Apply, and Post-Apply Properties\nRequests with stage set to post_plan, pre_apply or post_apply contain the following additional properties.\n{ \"payload_version\": 1, \"stage\": \"post_plan\", \"access_token\": \"4QEuyyxug1f2rw.atlasv1.iDyxqhXGVZ0ykes53YdQyHyYtFOrdAWNBxcVUgWvzb64NFHjcquu8gJMEdUwoSLRu4Q\", \"capabilities\": { \"outcomes\": true }, \"configuration_version_download_url\": \"https://app.terraform.io/api/v2/configuration-versions/cv-ntv3HbhJqvFzamy7/download\", \"configuration_version_id\": \"cv-ntv3HbhJqvFzamy7\", \"is_speculative\": false, \"organization_name\": \"hashicorp\", \"plan_json_api_url\": \"https://app.terraform.io/api/v2/plans/plan-6AFmRJW1PFJ7qbAh/json-output\", \"run_app_url\": \"https://app.terraform.io/app/hashicorp/my-workspace/runs/run-i3Df5to9ELvibKpQ\", \"run_created_at\": \"2021-09-02T14:47:13.036Z\", \"run_created_by\": \"username\", \"run_id\": \"run-i3Df5to9ELvibKpQ\", \"run_message\": \"Triggered via UI\", \"task_result_callback_url\": \"https://app.terraform.io/api/v2/task-results/5ea8d46c-2ceb-42cd-83f2-82e54697bddd/callback\", \"task_result_enforcement_level\": \"mandatory\", \"task_result_id\": \"taskrs-2nH5dncYoXaMVQmJ\", \"vcs_branch\": \"main\", \"vcs_commit_url\": \"https://github.com/hashicorp/terraform-random/commit/7d8fb2a2d601edebdb7a59ad2088a96673637d22\", \"vcs_pull_request_url\": null, \"vcs_repo_url\": \"https://github.com/hashicorp/terraform-random\", \"workspace_app_url\": \"https://app.terraform.io/app/hashicorp/my-workspace\", \"workspace_id\": \"ws-ck4G5bb1Yei5szRh\", \"workspace_name\": \"tfr_github_0\", \"workspace_working_directory\": \"/terraform\" } \nUser-Agent\tTFC/1.0 (+https://app.terraform.io; TFC)\tIdentifies the request is coming from HCP Terraform\t\nWhile a run task runs, it may send progressive updates to HCP Terraform with a running status. Once an integrator determines that Terraform supports detailed run task outcomes, they can send these outcomes by appending to the run task's callback payload.\nOnce the external integration fulfills the request, that integration must call back into HCP Terraform with the overall result of either passed or failed. Terraform expects this callback within 10 minutes, or the request is considered errored.\nYou can send outcomes with a status of running, passed, or failed, but it is a good practice only to send outcomes when a run task is running.\n200\tNo Content\tSuccessfully submitted a run task result\t\nThe PATCH request submits a JSON object with the following properties as a request payload. This payload is also described in the JSON API schema for run task results.\ndata.attributes.message\tstring\t(Recommended, but optional) A short message describing the status of the task.\t\nrelationships.outcomes.data\tarray\t(Recommended, but optional) A collection of detailed run task outcomes.\t\n{ \"data\": { \"type\": \"task-results\", \"attributes\": { \"status\": \"passed\", \"message\": \"4 passed, 0 skipped, 0 failed\", \"url\": \"https://external.service.dev/terraform-plan-checker/run-i3Df5to9ELvibKpQ\" }, \"relationships\": { \"outcomes\": { \"data\": [...] } } } } \nOutcomes Payload Body\nA run task result may optionally contain one or more detailed outcomes, which improves result visibility and content in the HCP Terraform user interface. The following attributes define the outcome.\noutcome-id\tstring\tA partner supplied identifier for this outcome.\t\ndescription\tstring\tA one-line description of the result.\t\nbody\tstring\t(Optional) A detailed message for the result in Markdown format.\t\nurl\tstring\t(Optional) A URL that a user can navigate to for more information about this result.\t\ntags\tobject\t(Optional) An object containing tag arrays, named by the property key.\t\ntags.key\tstring\tThe two or three word name of the header tag. Special handling is given to severity and status keys.\t\ntags.key[].label\tstring\tThe text value of the tag.\t\ntags.key[].level\tenum string\t(Optional) The error level for the tag. Defaults to none, but accepts none, info, warning, or error. For levels other than none, labels render with a color and icon for that level.\t\nSeverity and Status Tags\nRun task outcomes with tags named \"severity\" or \"status\" are enriched within the outcomes display list in HCP Terraform, enabling an earlier response to issues with severity and status.\n{ \"type\": \"task-result-outcomes\", \"attributes\": { \"outcome-id\": \"PRTNR-CC-TF-127\", \"description\": \"ST-2942: S3 Bucket will not enforce MFA login on delete requests\", \"tags\": { \"Status\": [ { \"label\": \"Denied\", \"level\": \"error\" } ], \"Severity\": [ { \"label\": \"High\", \"level\": \"error\" }, { \"label\": \"Recoverable\", \"level\": \"info\" } ], \"Cost Centre\": [ { \"label\": \"IT-OPS\" } ] }, \"body\": \"# Resolution for issue ST-2942\\n\\n## Impact\\n\\nFollow instructions in the [AWS S3 docs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiFactorAuthenticationDelete.html) to manually configure the MFA setting.\\n-- Payload truncated --\", \"url\": \"https://external.service.dev/result/PRTNR-CC-TF-127\" } } \nComplete Callback Payload Example\nThe example below shows a complete payload explaining the data structure of a callback payload, including all the necessary fields.\n{ \"data\": { \"type\": \"task-results\", \"attributes\": { \"status\": \"failed\", \"message\": \"0 passed, 0 skipped, 1 failed\", \"url\": \"https://external.service.dev/terraform-plan-checker/run-i3Df5to9ELvibKpQ\" }, \"relationships\": { \"outcomes\": { \"data\": [ { \"type\": \"task-result-outcomes\", \"attributes\": { \"outcome-id\": \"PRTNR-CC-TF-127\", \"description\": \"ST-2942: S3 Bucket will not enforce MFA login on delete requests\", \"tags\": { \"Status\": [ { \"label\": \"Denied\", \"level\": \"error\" } ], \"Severity\": [ { \"label\": \"High\", \"level\": \"error\" }, { \"label\": \"Recoverable\", \"level\": \"info\" } ], \"Cost Centre\": [ { \"label\": \"IT-OPS\" } ] }, \"body\": \"# Resolution for issue ST-2942\\n\\n## Impact\\n\\nFollow instructions in the [AWS S3 docs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiFactorAuthenticationDelete.html) to manually configure the MFA setting.\\n-- Payload truncated --\", \"url\": \"https://external.service.dev/result/PRTNR-CC-TF-127\" } } ] } } } } "
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202406-1/api-docs/run-tasks/run-tasks-integration",
  "text": "Run tasks allow HCP Terraform to interact with external systems at specific points in the HCP Terraform run lifecycle. This page lists the API endpoints used to trigger a run task and the expected response from the integration.\nRefer to run tasks for the API endpoints to create and manage run tasks within HCP Terraform. You can also access a complete list of all run tasks in the Terraform Registry.\nWhen a run reaches the appropriate phase and a run task is triggered, HCP Terraform will send a request to the run task's URL. The service receiving the run task request should respond with 200 OK, or HCP Terraform will retry to trigger the run task.\n200\tNo Content\tSuccessfully submitted a run task\t\nstage\tstring\tpre_plan, post_plan, pre_apply, post_apply\tThe run stage when HCP Terraform triggers the run task.\t\naccess_token\tstring\t\tBearer token to use when calling back to HCP Terraform.\t\ncapabilitites\tobject\t\tA map of the capabilities that the caller supports.\t\ncapabilitites.outcomes\tbool\t\tA flag indicating the caller accepts detailed run task outcomes.\t\nconfiguration_version_download_url\tstring\t\tThe URL to download the configuration version. This is null if the configuration version is not available to download.\t\nconfiguration_version_id\tstring\t\tThe ID of the configuration version for the run.\t\nrun_app_url\tstring\t\tURL within HCP Terraform to the run.\t\ntask_result_id\tstring\t\tID of task result within HCP Terraform.\t\nworkspace_app_url\tstring\t\tURL within HCP Terraform to the workspace.\t\nworkspace_working_directory\tstring\t\tThe working directory specified in the run's workspace settings.\t\nPost-Plan, Pre-Apply, and Post-Apply Properties\nRequests with stage set to post_plan, pre_apply or post_apply contain the following additional properties.\n{ \"payload_version\": 1, \"stage\": \"post_plan\", \"access_token\": \"4QEuyyxug1f2rw.atlasv1.iDyxqhXGVZ0ykes53YdQyHyYtFOrdAWNBxcVUgWvzb64NFHjcquu8gJMEdUwoSLRu4Q\", \"capabilities\": { \"outcomes\": true }, \"configuration_version_download_url\": \"https://app.terraform.io/api/v2/configuration-versions/cv-ntv3HbhJqvFzamy7/download\", \"configuration_version_id\": \"cv-ntv3HbhJqvFzamy7\", \"is_speculative\": false, \"organization_name\": \"hashicorp\", \"plan_json_api_url\": \"https://app.terraform.io/api/v2/plans/plan-6AFmRJW1PFJ7qbAh/json-output\", \"run_app_url\": \"https://app.terraform.io/app/hashicorp/my-workspace/runs/run-i3Df5to9ELvibKpQ\", \"run_created_at\": \"2021-09-02T14:47:13.036Z\", \"run_created_by\": \"username\", \"run_id\": \"run-i3Df5to9ELvibKpQ\", \"run_message\": \"Triggered via UI\", \"task_result_callback_url\": \"https://app.terraform.io/api/v2/task-results/5ea8d46c-2ceb-42cd-83f2-82e54697bddd/callback\", \"task_result_enforcement_level\": \"mandatory\", \"task_result_id\": \"taskrs-2nH5dncYoXaMVQmJ\", \"vcs_branch\": \"main\", \"vcs_commit_url\": \"https://github.com/hashicorp/terraform-random/commit/7d8fb2a2d601edebdb7a59ad2088a96673637d22\", \"vcs_pull_request_url\": null, \"vcs_repo_url\": \"https://github.com/hashicorp/terraform-random\", \"workspace_app_url\": \"https://app.terraform.io/app/hashicorp/my-workspace\", \"workspace_id\": \"ws-ck4G5bb1Yei5szRh\", \"workspace_name\": \"tfr_github_0\", \"workspace_working_directory\": \"/terraform\" } \nUser-Agent\tTFC/1.0 (+https://app.terraform.io; TFC)\tIdentifies the request is coming from HCP Terraform\t\nWhile a run task runs, it may send progressive updates to HCP Terraform with a running status. Once an integrator determines that Terraform supports detailed run task outcomes, they can send these outcomes by appending to the run task's callback payload.\nOnce the external integration fulfills the request, that integration must call back into HCP Terraform with the overall result of either passed or failed. Terraform expects this callback within 10 minutes, or the request is considered errored.\nYou can send outcomes with a status of running, passed, or failed, but it is a good practice only to send outcomes when a run task is running.\n200\tNo Content\tSuccessfully submitted a run task result\t\nThe PATCH request submits a JSON object with the following properties as a request payload. This payload is also described in the JSON API schema for run task results.\ndata.attributes.message\tstring\t(Recommended, but optional) A short message describing the status of the task.\t\nrelationships.outcomes.data\tarray\t(Recommended, but optional) A collection of detailed run task outcomes.\t\n{ \"data\": { \"type\": \"task-results\", \"attributes\": { \"status\": \"passed\", \"message\": \"4 passed, 0 skipped, 0 failed\", \"url\": \"https://external.service.dev/terraform-plan-checker/run-i3Df5to9ELvibKpQ\" }, \"relationships\": { \"outcomes\": { \"data\": [...] } } } } \nOutcomes Payload Body\nA run task result may optionally contain one or more detailed outcomes, which improves result visibility and content in the HCP Terraform user interface. The following attributes define the outcome.\noutcome-id\tstring\tA partner supplied identifier for this outcome.\t\ndescription\tstring\tA one-line description of the result.\t\nbody\tstring\t(Optional) A detailed message for the result in Markdown format.\t\nurl\tstring\t(Optional) A URL that a user can navigate to for more information about this result.\t\ntags\tobject\t(Optional) An object containing tag arrays, named by the property key.\t\ntags.key\tstring\tThe two or three word name of the header tag. Special handling is given to severity and status keys.\t\ntags.key[].label\tstring\tThe text value of the tag.\t\ntags.key[].level\tenum string\t(Optional) The error level for the tag. Defaults to none, but accepts none, info, warning, or error. For levels other than none, labels render with a color and icon for that level.\t\nSeverity and Status Tags\nRun task outcomes with tags named \"severity\" or \"status\" are enriched within the outcomes display list in HCP Terraform, enabling an earlier response to issues with severity and status.\n{ \"type\": \"task-result-outcomes\", \"attributes\": { \"outcome-id\": \"PRTNR-CC-TF-127\", \"description\": \"ST-2942: S3 Bucket will not enforce MFA login on delete requests\", \"tags\": { \"Status\": [ { \"label\": \"Denied\", \"level\": \"error\" } ], \"Severity\": [ { \"label\": \"High\", \"level\": \"error\" }, { \"label\": \"Recoverable\", \"level\": \"info\" } ], \"Cost Centre\": [ { \"label\": \"IT-OPS\" } ] }, \"body\": \"# Resolution for issue ST-2942\\n\\n## Impact\\n\\nFollow instructions in the [AWS S3 docs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiFactorAuthenticationDelete.html) to manually configure the MFA setting.\\n-- Payload truncated --\", \"url\": \"https://external.service.dev/result/PRTNR-CC-TF-127\" } } \nComplete Callback Payload Example\nThe example below shows a complete payload explaining the data structure of a callback payload, including all the necessary fields.\n{ \"data\": { \"type\": \"task-results\", \"attributes\": { \"status\": \"failed\", \"message\": \"0 passed, 0 skipped, 1 failed\", \"url\": \"https://external.service.dev/terraform-plan-checker/run-i3Df5to9ELvibKpQ\" }, \"relationships\": { \"outcomes\": { \"data\": [ { \"type\": \"task-result-outcomes\", \"attributes\": { \"outcome-id\": \"PRTNR-CC-TF-127\", \"description\": \"ST-2942: S3 Bucket will not enforce MFA login on delete requests\", \"tags\": { \"Status\": [ { \"label\": \"Denied\", \"level\": \"error\" } ], \"Severity\": [ { \"label\": \"High\", \"level\": \"error\" }, { \"label\": \"Recoverable\", \"level\": \"info\" } ], \"Cost Centre\": [ { \"label\": \"IT-OPS\" } ] }, \"body\": \"# Resolution for issue ST-2942\\n\\n## Impact\\n\\nFollow instructions in the [AWS S3 docs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiFactorAuthenticationDelete.html) to manually configure the MFA setting.\\n-- Payload truncated --\", \"url\": \"https://external.service.dev/result/PRTNR-CC-TF-127\" } } ] } } } } "
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202206-1/api-docs/run-tasks/run-tasks-integration",
  "text": "stage\tstring\tpost-plan, pre-apply\tThe stage the task was triggered at.\t\nplan_json_api_url\tstring\t\tURL to retrieve the JSON Terraform plan for this run.\t\n[422][]\tJSON API error object\tInvalid response payload. This could be caused by invalid attributes, or sending a status that is not accepted.\t"
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202405-1/api-docs/run-tasks/run-tasks-integration",
  "text": "Run tasks allow HCP Terraform to interact with external systems at specific points in the HCP Terraform run lifecycle. This page lists the API endpoints used to trigger a run task and the expected response from the integration.\nRefer to run tasks for the API endpoints to create and manage run tasks within HCP Terraform. You can also access a complete list of all run tasks in the Terraform Registry.\nWhen a run reaches the appropriate phase and a run task is triggered, HCP Terraform will send a request to the run task's URL. The service receiving the run task request should respond with 200 OK, or HCP Terraform will retry to trigger the run task.\n200\tNo Content\tSuccessfully submitted a run task\t\nstage\tstring\tpre_plan, post_plan, pre_apply, post_apply\tThe run stage when Terraform Enterprise triggers the run task.\t\naccess_token\tstring\t\tBearer token to use when calling back to Terraform Enterprise.\t\ncapabilitites\tobject\t\tA map of the capabilities that the caller supports.\t\ncapabilitites.outcomes\tbool\t\tA flag indicating the caller accepts detailed run task outcomes.\t\nconfiguration_version_download_url\tstring\t\tThe URL to download the configuration version. This is null if the configuration version is not available to download.\t\nconfiguration_version_id\tstring\t\tThe ID of the configuration version for the run.\t\nrun_app_url\tstring\t\tURL within\t\nTerraform Enterprise to the run.\t\t\t\t\ntask_result_id\tstring\t\tID of task result within HCP Terraform.\t\nworkspace_app_url\tstring\t\tURL within HCP Terraform to the workspace.\t\nworkspace_working_directory\tstring\t\tThe working directory specified in the run's workspace settings.\t\nPost-Plan, Pre-Apply, and Post-Apply Properties\nRequests with stage set to post_plan, pre_apply or post_apply contain the following additional properties.\n{ \"payload_version\": 1, \"stage\": \"post_plan\", \"access_token\": \"4QEuyyxug1f2rw.atlasv1.iDyxqhXGVZ0ykes53YdQyHyYtFOrdAWNBxcVUgWvzb64NFHjcquu8gJMEdUwoSLRu4Q\", \"capabilities\": { \"outcomes\": true }, \"configuration_version_download_url\": \"https://app.terraform.io/api/v2/configuration-versions/cv-ntv3HbhJqvFzamy7/download\", \"configuration_version_id\": \"cv-ntv3HbhJqvFzamy7\", \"is_speculative\": false, \"organization_name\": \"hashicorp\", \"plan_json_api_url\": \"https://app.terraform.io/api/v2/plans/plan-6AFmRJW1PFJ7qbAh/json-output\", \"run_app_url\": \"https://app.terraform.io/app/hashicorp/my-workspace/runs/run-i3Df5to9ELvibKpQ\", \"run_created_at\": \"2021-09-02T14:47:13.036Z\", \"run_created_by\": \"username\", \"run_id\": \"run-i3Df5to9ELvibKpQ\", \"run_message\": \"Triggered via UI\", \"task_result_callback_url\": \"https://app.terraform.io/api/v2/task-results/5ea8d46c-2ceb-42cd-83f2-82e54697bddd/callback\", \"task_result_enforcement_level\": \"mandatory\", \"task_result_id\": \"taskrs-2nH5dncYoXaMVQmJ\", \"vcs_branch\": \"main\", \"vcs_commit_url\": \"https://github.com/hashicorp/terraform-random/commit/7d8fb2a2d601edebdb7a59ad2088a96673637d22\", \"vcs_pull_request_url\": null, \"vcs_repo_url\": \"https://github.com/hashicorp/terraform-random\", \"workspace_app_url\": \"https://app.terraform.io/app/hashicorp/my-workspace\", \"workspace_id\": \"ws-ck4G5bb1Yei5szRh\", \"workspace_name\": \"tfr_github_0\", \"workspace_working_directory\": \"/terraform\" } \nUser-Agent\tTFC/1.0 (+https://app.terraform.io; TFC)\tIdentifies the request is coming from HCP Terraform\t\nWhile a run task runs, it may send progressive updates to HCP Terraform with a running status. Once an integrator determines that Terraform supports detailed run task outcomes, they can send these outcomes by appending to the run task's callback payload.\nOnce the external integration fulfills the request, that integration must call back into HCP Terraform with the overall result of either passed or failed. Terraform expects this callback within 10 minutes, or the request is considered errored.\nYou can send outcomes with a status of running, passed, or failed, but it is a good practice only to send outcomes when a run task is running.\n200\tNo Content\tSuccessfully submitted a run task result\t\nThe PATCH request submits a JSON object with the following properties as a request payload. This payload is also described in the JSON API schema for run task results.\ndata.attributes.message\tstring\t(Recommended, but optional) A short message describing the status of the task.\t\nrelationships.outcomes.data\tarray\t(Recommended, but optional) A collection of detailed run task outcomes.\t\n{ \"data\": { \"type\": \"task-results\", \"attributes\": { \"status\": \"passed\", \"message\": \"4 passed, 0 skipped, 0 failed\", \"url\": \"https://external.service.dev/terraform-plan-checker/run-i3Df5to9ELvibKpQ\", }, \"relationships\": { \"outcomes\": { \"data\": [...] } } } } \nOutcomes Payload Body\nA run task result may optionally contain one or more detailed outcomes, which improves result visibility and content in the HCP Terraform user interface. The following attributes define the outcome.\noutcome-id\tstring\tA partner supplied identifier for this outcome.\t\ndescription\tstring\tA one-line description of the result.\t\nbody\tstring\t(Optional) A detailed message for the result in Markdown format.\t\nurl\tstring\t(Optional) A URL that a user can navigate to for more information about this result.\t\ntags\tobject\t(Optional) An object containing tag arrays, named by the property key.\t\ntags.key\tstring\tThe two or three word name of the header tag. Special handling is given to severity and status keys.\t\ntags.key[].label\tstring\tThe text value of the tag.\t\ntags.key[].level\tenum string\t(Optional) The error level for the tag. Defaults to none, but accepts none, info, warning, or error. For levels other than none, labels render with a color and icon for that level.\t\nSeverity and Status Tags\nRun task outcomes with tags named \"severity\" or \"status\" are enriched within the outcomes display list in HCP Terraform, enabling an earlier response to issues with severity and status.\n{ \"type\": \"task-result-outcomes\", \"attributes\": { \"outcome-id\": \"PRTNR-CC-TF-127\", \"description\": \"ST-2942: S3 Bucket will not enforce MFA login on delete requests\", \"tags\": { \"Status\": [{\"label\": \"Denied\", \"level\": \"error\" }], \"Severity\": [ {\"label\": \"High\", \"level\": \"error\" }, {\"label\": \"Recoverable\", \"level\": \"info\" }, ], \"Cost Centre\": [{\"label\": \"IT-OPS\"}] }, \"body\": \"# Resolution for issue ST-2942\\n\\n## Impact\\n\\nFollow instructions in the [AWS S3 docs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiFactorAuthenticationDelete.html) to manually configure the MFA setting.\\n-- Payload truncated --\", \"url\": \"https://external.service.dev/result/PRTNR-CC-TF-127\" } } "
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202404-1/api-docs/run-tasks/run-tasks-integration",
  "text": "Run tasks allow HCP Terraform to interact with external systems at specific points in the HCP Terraform run lifecycle. This page lists the API endpoints used to trigger a run task and the expected response from the integration.\nRefer to run tasks for the API endpoints to create and manage run tasks within HCP Terraform. You can also access a complete list of all run tasks in the Terraform Registry.\nWhen a run reaches the appropriate phase and a run task is triggered, HCP Terraform will send a request to the run task's URL. The service receiving the run task request should respond with 200 OK, or HCP Terraform will retry to trigger the run task.\n200\tNo Content\tSuccessfully submitted a run task\t\nstage\tstring\tpre_plan, post_plan, pre_apply, post_apply\tThe run stage when Terraform Enterprise triggers the run task.\t\naccess_token\tstring\t\tBearer token to use when calling back to Terraform Enterprise.\t\ncapabilitites\tobject\t\tA map of the capabilities that the caller supports.\t\ncapabilitites.outcomes\tbool\t\tA flag indicating the caller accepts detailed run task outcomes.\t\nconfiguration_version_download_url\tstring\t\tThe URL to download the configuration version. This is null if the configuration version is not available to download.\t\nconfiguration_version_id\tstring\t\tThe ID of the configuration version for the run.\t\nrun_app_url\tstring\t\tURL within\t\nTerraform Enterprise to the run.\t\t\t\t\ntask_result_id\tstring\t\tID of task result within HCP Terraform.\t\nworkspace_app_url\tstring\t\tURL within HCP Terraform to the workspace.\t\nworkspace_working_directory\tstring\t\tThe working directory specified in the run's workspace settings.\t\nPost-Plan, Pre-Apply, and Post-Apply Properties\nRequests with stage set to post_plan, pre_apply or post_apply contain the following additional properties.\n{ \"payload_version\": 1, \"stage\": \"post_plan\", \"access_token\": \"4QEuyyxug1f2rw.atlasv1.iDyxqhXGVZ0ykes53YdQyHyYtFOrdAWNBxcVUgWvzb64NFHjcquu8gJMEdUwoSLRu4Q\", \"capabilities\": { \"outcomes\": true }, \"configuration_version_download_url\": \"https://app.terraform.io/api/v2/configuration-versions/cv-ntv3HbhJqvFzamy7/download\", \"configuration_version_id\": \"cv-ntv3HbhJqvFzamy7\", \"is_speculative\": false, \"organization_name\": \"hashicorp\", \"plan_json_api_url\": \"https://app.terraform.io/api/v2/plans/plan-6AFmRJW1PFJ7qbAh/json-output\", \"run_app_url\": \"https://app.terraform.io/app/hashicorp/my-workspace/runs/run-i3Df5to9ELvibKpQ\", \"run_created_at\": \"2021-09-02T14:47:13.036Z\", \"run_created_by\": \"username\", \"run_id\": \"run-i3Df5to9ELvibKpQ\", \"run_message\": \"Triggered via UI\", \"task_result_callback_url\": \"https://app.terraform.io/api/v2/task-results/5ea8d46c-2ceb-42cd-83f2-82e54697bddd/callback\", \"task_result_enforcement_level\": \"mandatory\", \"task_result_id\": \"taskrs-2nH5dncYoXaMVQmJ\", \"vcs_branch\": \"main\", \"vcs_commit_url\": \"https://github.com/hashicorp/terraform-random/commit/7d8fb2a2d601edebdb7a59ad2088a96673637d22\", \"vcs_pull_request_url\": null, \"vcs_repo_url\": \"https://github.com/hashicorp/terraform-random\", \"workspace_app_url\": \"https://app.terraform.io/app/hashicorp/my-workspace\", \"workspace_id\": \"ws-ck4G5bb1Yei5szRh\", \"workspace_name\": \"tfr_github_0\", \"workspace_working_directory\": \"/terraform\" } \nUser-Agent\tTFC/1.0 (+https://app.terraform.io; TFC)\tIdentifies the request is coming from HCP Terraform\t\nWhile a run task runs, it may send progressive updates to HCP Terraform with a running status. Once an integrator determines that Terraform supports detailed run task outcomes, they can send these outcomes by appending to the run task's callback payload.\nOnce the external integration fulfills the request, that integration must call back into HCP Terraform with the overall result of either passed or failed. Terraform expects this callback within 10 minutes, or the request is considered errored.\nYou can send outcomes with a status of running, passed, or failed, but it is a good practice only to send outcomes when a run task is running.\n200\tNo Content\tSuccessfully submitted a run task result\t\nThe PATCH request submits a JSON object with the following properties as a request payload. This payload is also described in the JSON API schema for run task results.\ndata.attributes.message\tstring\t(Recommended, but optional) A short message describing the status of the task.\t\nrelationships.outcomes.data\tarray\t(Recommended, but optional) A collection of detailed run task outcomes.\t\n{ \"data\": { \"type\": \"task-results\", \"attributes\": { \"status\": \"passed\", \"message\": \"4 passed, 0 skipped, 0 failed\", \"url\": \"https://external.service.dev/terraform-plan-checker/run-i3Df5to9ELvibKpQ\", }, \"relationships\": { \"outcomes\": { \"data\": [...] } } } } \nOutcomes Payload Body\nA run task result may optionally contain one or more detailed outcomes, which improves result visibility and content in the HCP Terraform user interface. The following attributes define the outcome.\noutcome-id\tstring\tA partner supplied identifier for this outcome.\t\ndescription\tstring\tA one-line description of the result.\t\nbody\tstring\t(Optional) A detailed message for the result in Markdown format.\t\nurl\tstring\t(Optional) A URL that a user can navigate to for more information about this result.\t\ntags\tobject\t(Optional) An object containing tag arrays, named by the property key.\t\ntags.key\tstring\tThe two or three word name of the header tag. Special handling is given to severity and status keys.\t\ntags.key[].label\tstring\tThe text value of the tag.\t\ntags.key[].level\tenum string\t(Optional) The error level for the tag. Defaults to none, but accepts none, info, warning, or error. For levels other than none, labels render with a color and icon for that level.\t\nSeverity and Status Tags\nRun task outcomes with tags named \"severity\" or \"status\" are enriched within the outcomes display list in HCP Terraform, enabling an earlier response to issues with severity and status.\n{ \"type\": \"task-result-outcomes\", \"attributes\": { \"outcome-id\": \"PRTNR-CC-TF-127\", \"description\": \"ST-2942: S3 Bucket will not enforce MFA login on delete requests\", \"tags\": { \"Status\": [{\"label\": \"Denied\", \"level\": \"error\" }], \"Severity\": [ {\"label\": \"High\", \"level\": \"error\" }, {\"label\": \"Recoverable\", \"level\": \"info\" }, ], \"Cost Centre\": [{\"label\": \"IT-OPS\"}] }, \"body\": \"# Resolution for issue ST-2942\\n\\n## Impact\\n\\nFollow instructions in the [AWS S3 docs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiFactorAuthenticationDelete.html) to manually configure the MFA setting.\\n-- Payload truncated --\", \"url\": \"https://external.service.dev/result/PRTNR-CC-TF-127\" } } "
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202402-2/api-docs/run-tasks/run-tasks-integration",
  "text": "stage\tstring\tpre_plan, post_plan, pre_apply, post_apply\tThe run stage when Terraform Enterprise triggers the run task.\t\naccess_token\tstring\t\tBearer token to use when calling back to Terraform Enterprise.\t\nrun_app_url\tstring\t\tURL within\t\nHCP Terraform to the run.\t\t\t\t\ntask_result_id\tstring\t\tID of task result within Terraform Enterprise.\t\nworkspace_app_url\tstring\t\tURL within Terraform Enterprise to the workspace.\t\n{ \"data\": { \"type\": \"task-results\", \"attributes\": { \"status\": \"passed\", \"message\": \"4 passed, 0 skipped, 0 failed\", \"url\": \"https://external.service.dev/terraform-plan-checker/run-i3Df5to9ELvibKpQ\", }, \"relationships\": { \"outcomes\": { \"data\": [...] } } } } \n{ \"type\": \"task-result-outcomes\", \"attributes\": { \"outcome-id\": \"PRTNR-CC-TF-127\", \"description\": \"ST-2942: S3 Bucket will not enforce MFA login on delete requests\", \"tags\": { \"Status\": [{\"label\": \"Denied\", \"level\": \"error\" }], \"Severity\": [ {\"label\": \"High\", \"level\": \"error\" }, {\"label\": \"Recoverable\", \"level\": \"info\" }, ], \"Cost Centre\": [{\"label\": \"IT-OPS\"}] }, \"body\": \"# Resolution for issue ST-2942\\n\\n## Impact\\n\\nFollow instructions in the [AWS S3 docs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiFactorAuthenticationDelete.html) to manually configure the MFA setting.\\n-- Payload truncated --\", \"url\": \"https://external.service.dev/result/PRTNR-CC-TF-127\" } } "
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202402-1/api-docs/run-tasks/run-tasks-integration",
  "text": "Refer to run tasks for the API endpoints to create and manage run tasks within Terraform Cloud. You can also access a complete list of all run tasks in the Terraform Registry.\nstage\tstring\tpre_plan, post_plan, pre_apply, post_apply\tThe run stage when Terraform Enterprise triggers the run task.\t\naccess_token\tstring\t\tBearer token to use when calling back to Terraform Enterprise.\t\ntask_result_id\tstring\t\tID of task result within Terraform Enterprise.\t\nworkspace_app_url\tstring\t\tURL within Terraform Enterprise to the workspace.\t\nWhile a run task runs, it may send progressive updates to Terraform Cloud with a running status. Once an integrator determines that Terraform supports detailed run task outcomes, they can send these outcomes by appending to the run task's callback payload.\nOnce the external integration fulfills the request, that integration must call back into Terraform Cloud with the overall result of either passed or failed. Terraform expects this callback within 10 minutes, or the request is considered errored.\n{ \"data\": { \"type\": \"task-results\", \"attributes\": { \"status\": \"passed\", \"message\": \"4 passed, 0 skipped, 0 failed\", \"url\": \"https://external.service.dev/terraform-plan-checker/run-i3Df5to9ELvibKpQ\", }, \"relationships\": { \"outcomes\": { \"data\": [...] } } } } \nA run task result may optionally contain one or more detailed outcomes, which improves result visibility and content in the Terraform Cloud user interface. The following attributes define the outcome.\nRun task outcomes with tags named \"severity\" or \"status\" are enriched within the outcomes display list in Terraform Cloud, enabling an earlier response to issues with severity and status.\n{ \"type\": \"task-result-outcomes\", \"attributes\": { \"outcome-id\": \"PRTNR-CC-TF-127\", \"description\": \"ST-2942: S3 Bucket will not enforce MFA login on delete requests\", \"tags\": { \"Status\": [{\"label\": \"Denied\", \"level\": \"error\" }], \"Severity\": [ {\"label\": \"High\", \"level\": \"error\" }, {\"label\": \"Recoverable\", \"level\": \"info\" }, ], \"Cost Centre\": [{\"label\": \"IT-OPS\"}] }, \"body\": \"# Resolution for issue ST-2942\\n\\n## Impact\\n\\nFollow instructions in the [AWS S3 docs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiFactorAuthenticationDelete.html) to manually configure the MFA setting.\\n-- Payload truncated --\", \"url\": \"https://external.service.dev/result/PRTNR-CC-TF-127\" } } "
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202404-2/api-docs/run-tasks/run-tasks-integration",
  "text": "stage\tstring\tpre_plan, post_plan, pre_apply, post_apply\tThe run stage when HCP Terraform triggers the run task.\t\naccess_token\tstring\t\tBearer token to use when calling back to HCP Terraform.\t\nrun_app_url\tstring\t\tURL within HCP Terraform to the run.\t"
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202312-1/api-docs/run-tasks/run-tasks-integration",
  "text": "Refer to run tasks for the API endpoints to create and manage run tasks within Terraform Cloud. You can also access a complete list of all run tasks in the Terraform Registry.\nstage\tstring\tpre_plan, post_plan, pre_apply\tThe run stage when Terraform Cloud triggers the run task.\t\nPost-Plan and Pre-Apply Properties\nRequests with stage set to post_plan or pre_apply contain the following additional properties.\nWhile a run task runs, it may send progressive updates to Terraform Cloud with a running status. Once an integrator determines that Terraform supports detailed run task outcomes, they can send these outcomes by appending to the run task's callback payload.\nOnce the external integration fulfills the request, that integration must call back into Terraform Cloud with the overall result of either passed or failed. Terraform expects this callback within 10 minutes, or the request is considered errored.\nA run task result may optionally contain one or more detailed outcomes, which improves result visibility and content in the Terraform Cloud user interface. The following attributes define the outcome.\nRun task outcomes with tags named \"severity\" or \"status\" are enriched within the outcomes display list in Terraform Cloud, enabling an earlier response to issues with severity and status."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202401-1/api-docs/run-tasks/run-tasks-integration",
  "text": "Refer to run tasks for the API endpoints to create and manage run tasks within Terraform Cloud. You can also access a complete list of all run tasks in the Terraform Registry.\nstage\tstring\tpre_plan, post_plan, pre_apply\tThe run stage when Terraform Cloud triggers the run task.\t\nWhile a run task runs, it may send progressive updates to Terraform Cloud with a running status. Once an integrator determines that Terraform supports detailed run task outcomes, they can send these outcomes by appending to the run task's callback payload.\nOnce the external integration fulfills the request, that integration must call back into Terraform Cloud with the overall result of either passed or failed. Terraform expects this callback within 10 minutes, or the request is considered errored.\nA run task result may optionally contain one or more detailed outcomes, which improves result visibility and content in the Terraform Cloud user interface. The following attributes define the outcome.\nRun task outcomes with tags named \"severity\" or \"status\" are enriched within the outcomes display list in Terraform Cloud, enabling an earlier response to issues with severity and status."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202401-2/api-docs/run-tasks/run-tasks-integration",
  "text": "Refer to run tasks for the API endpoints to create and manage run tasks within Terraform Cloud. You can also access a complete list of all run tasks in the Terraform Registry.\nWhile a run task runs, it may send progressive updates to Terraform Cloud with a running status. Once an integrator determines that Terraform supports detailed run task outcomes, they can send these outcomes by appending to the run task's callback payload.\nOnce the external integration fulfills the request, that integration must call back into Terraform Cloud with the overall result of either passed or failed. Terraform expects this callback within 10 minutes, or the request is considered errored.\nA run task result may optionally contain one or more detailed outcomes, which improves result visibility and content in the Terraform Cloud user interface. The following attributes define the outcome.\nRun task outcomes with tags named \"severity\" or \"status\" are enriched within the outcomes display list in Terraform Cloud, enabling an earlier response to issues with severity and status."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202311-1/api-docs/run-tasks/run-tasks-integration",
  "text": "Post-Plan and Pre-Apply Properties\nRequests with stage set to post_plan or pre_apply contain the following additional properties."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202309-1/api-docs/run-tasks/run-tasks-integration",
  "text": "Post-Plan and Pre-Apply Properties\nRequests with stage set to post_plan or pre_apply contain the following additional properties.\n{ \"payload_version\": 1, \"stage\": \"post_plan\", \"access_token\": \"4QEuyyxug1f2rw.atlasv1.iDyxqhXGVZ0ykes53YdQyHyYtFOrdAWNBxcVUgWvzb64NFHjcquu8gJMEdUwoSLRu4Q\", \"configuration_version_download_url\": \"https://app.terraform.io/api/v2/configuration-versions/cv-ntv3HbhJqvFzamy7/download\", \"configuration_version_id\": \"cv-ntv3HbhJqvFzamy7\", \"is_speculative\": false, \"organization_name\": \"hashicorp\", \"plan_json_api_url\": \"https://app.terraform.io/api/v2/plans/plan-6AFmRJW1PFJ7qbAh/json-output\", \"run_app_url\": \"https://app.terraform.io/app/hashicorp/my-workspace/runs/run-i3Df5to9ELvibKpQ\", \"run_created_at\": \"2021-09-02T14:47:13.036Z\", \"run_created_by\": \"username\", \"run_id\": \"run-i3Df5to9ELvibKpQ\", \"run_message\": \"Triggered via UI\", \"task_result_callback_url\": \"https://app.terraform.io/api/v2/task-results/5ea8d46c-2ceb-42cd-83f2-82e54697bddd/callback\", \"task_result_enforcement_level\": \"mandatory\", \"task_result_id\": \"taskrs-2nH5dncYoXaMVQmJ\", \"vcs_branch\": \"main\", \"vcs_commit_url\": \"https://github.com/hashicorp/terraform-random/commit/7d8fb2a2d601edebdb7a59ad2088a96673637d22\", \"vcs_pull_request_url\": null, \"vcs_repo_url\": \"https://github.com/hashicorp/terraform-random\", \"workspace_app_url\": \"https://app.terraform.io/app/hashicorp/my-workspace\", \"workspace_id\": \"ws-ck4G5bb1Yei5szRh\", \"workspace_name\": \"tfr_github_0\", \"workspace_working_directory\": \"/terraform\" } "
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202308-1/api-docs/run-tasks/run-tasks-integration",
  "text": "Post-Plan and Pre-Apply Properties\nRequests with stage set to post_plan or pre_apply contain the following additional properties.\n{ \"payload_version\": 1, \"stage\": \"post_plan\", \"access_token\": \"4QEuyyxug1f2rw.atlasv1.iDyxqhXGVZ0ykes53YdQyHyYtFOrdAWNBxcVUgWvzb64NFHjcquu8gJMEdUwoSLRu4Q\", \"configuration_version_download_url\": \"https://app.terraform.io/api/v2/configuration-versions/cv-ntv3HbhJqvFzamy7/download\", \"configuration_version_id\": \"cv-ntv3HbhJqvFzamy7\", \"is_speculative\": false, \"organization_name\": \"hashicorp\", \"plan_json_api_url\": \"https://app.terraform.io/api/v2/plans/plan-6AFmRJW1PFJ7qbAh/json-output\", \"run_app_url\": \"https://app.terraform.io/app/hashicorp/my-workspace/runs/run-i3Df5to9ELvibKpQ\", \"run_created_at\": \"2021-09-02T14:47:13.036Z\", \"run_created_by\": \"username\", \"run_id\": \"run-i3Df5to9ELvibKpQ\", \"run_message\": \"Triggered via UI\", \"task_result_callback_url\": \"https://app.terraform.io/api/v2/task-results/5ea8d46c-2ceb-42cd-83f2-82e54697bddd/callback\", \"task_result_enforcement_level\": \"mandatory\", \"task_result_id\": \"taskrs-2nH5dncYoXaMVQmJ\", \"vcs_branch\": \"main\", \"vcs_commit_url\": \"https://github.com/hashicorp/terraform-random/commit/7d8fb2a2d601edebdb7a59ad2088a96673637d22\", \"vcs_pull_request_url\": null, \"vcs_repo_url\": \"https://github.com/hashicorp/terraform-random\", \"workspace_app_url\": \"https://app.terraform.io/app/hashicorp/my-workspace\", \"workspace_id\": \"ws-ck4G5bb1Yei5szRh\", \"workspace_name\": \"tfr_github_0\", \"workspace_working_directory\": \"/terraform\" } "
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202307-1/api-docs/run-tasks/run-tasks-integration",
  "text": "{ \"payload_version\": 1, \"stage\": \"post_plan\", \"access_token\": \"4QEuyyxug1f2rw.atlasv1.iDyxqhXGVZ0ykes53YdQyHyYtFOrdAWNBxcVUgWvzb64NFHjcquu8gJMEdUwoSLRu4Q\", \"configuration_version_download_url\": \"https://app.terraform.io/api/v2/configuration-versions/cv-ntv3HbhJqvFzamy7/download\", \"configuration_version_id\": \"cv-ntv3HbhJqvFzamy7\", \"is_speculative\": false, \"organization_name\": \"hashicorp\", \"plan_json_api_url\": \"https://app.terraform.io/api/v2/plans/plan-6AFmRJW1PFJ7qbAh/json-output\", \"run_app_url\": \"https://app.terraform.io/app/hashicorp/my-workspace/runs/run-i3Df5to9ELvibKpQ\", \"run_created_at\": \"2021-09-02T14:47:13.036Z\", \"run_created_by\": \"username\", \"run_id\": \"run-i3Df5to9ELvibKpQ\", \"run_message\": \"Triggered via UI\", \"task_result_callback_url\": \"https://app.terraform.io/api/v2/task-results/5ea8d46c-2ceb-42cd-83f2-82e54697bddd/callback\", \"task_result_enforcement_level\": \"mandatory\", \"task_result_id\": \"taskrs-2nH5dncYoXaMVQmJ\", \"vcs_branch\": \"main\", \"vcs_commit_url\": \"https://github.com/hashicorp/terraform-random/commit/7d8fb2a2d601edebdb7a59ad2088a96673637d22\", \"vcs_pull_request_url\": null, \"vcs_repo_url\": \"https://github.com/hashicorp/terraform-random\", \"workspace_app_url\": \"https://app.terraform.io/app/hashicorp/my-workspace\", \"workspace_id\": \"ws-ck4G5bb1Yei5szRh\", \"workspace_name\": \"tfr_github_0\", \"workspace_working_directory\": \"/terraform\" } "
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202306-1/api-docs/run-tasks/run-tasks-integration",
  "text": "{ \"payload_version\": 1, \"stage\": \"post_plan\", \"access_token\": \"4QEuyyxug1f2rw.atlasv1.iDyxqhXGVZ0ykes53YdQyHyYtFOrdAWNBxcVUgWvzb64NFHjcquu8gJMEdUwoSLRu4Q\", \"configuration_version_download_url\": \"https://app.terraform.io/api/v2/configuration-versions/cv-ntv3HbhJqvFzamy7/download\", \"configuration_version_id\": \"cv-ntv3HbhJqvFzamy7\", \"is_speculative\": false, \"organization_name\": \"hashicorp\", \"plan_json_api_url\": \"https://app.terraform.io/api/v2/plans/plan-6AFmRJW1PFJ7qbAh/json-output\", \"run_app_url\": \"https://app.terraform.io/app/hashicorp/my-workspace/runs/run-i3Df5to9ELvibKpQ\", \"run_created_at\": \"2021-09-02T14:47:13.036Z\", \"run_created_by\": \"username\", \"run_id\": \"run-i3Df5to9ELvibKpQ\", \"run_message\": \"Triggered via UI\", \"task_result_callback_url\": \"https://app.terraform.io/api/v2/task-results/5ea8d46c-2ceb-42cd-83f2-82e54697bddd/callback\", \"task_result_enforcement_level\": \"mandatory\", \"task_result_id\": \"taskrs-2nH5dncYoXaMVQmJ\", \"vcs_branch\": \"main\", \"vcs_commit_url\": \"https://github.com/hashicorp/terraform-random/commit/7d8fb2a2d601edebdb7a59ad2088a96673637d22\", \"vcs_pull_request_url\": null, \"vcs_repo_url\": \"https://github.com/hashicorp/terraform-random\", \"workspace_app_url\": \"https://app.terraform.io/app/hashicorp/my-workspace\", \"workspace_id\": \"ws-ck4G5bb1Yei5szRh\", \"workspace_name\": \"tfr_github_0\", \"workspace_working_directory\": \"/terraform\" } "
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202305-2/api-docs/run-tasks/run-tasks-integration",
  "text": "Refer to run tasks for the API endpoints to create and manage run tasks within Terraform Cloud. You can also access a complete list of all run tasks in the Terraform Registry. "
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202305-1/api-docs/run-tasks/run-tasks-integration",
  "text": "Refer to run tasks for the API endpoints to create and manage run tasks within Terraform Cloud. You can also access a complete list of all run tasks in the Terraform Registry. "
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202310-1/api-docs/run-tasks/run-tasks-integration",
  "text": "A run task result may optionally contain one or more detailed outcomes, which improves result visibility and content in the Terraform Cloud user interface.\n{ \"outcome-id\": \"PRTNR-CC-TF-127\", \"description\": \"ST-2942: S3 Bucket will not enforce MFA login on delete requests\", \"tags\": { \"Status\": [{\"label\": \"Denied\", \"level\": \"error\" }], \"Severity\": [ {\"label\": \"High\", \"level\": \"error\" }, {\"label\": \"Recoverable\", \"level\": \"info\" }, ], \"Cost Centre\": [\"label\": \"IT-OPS\" ] }, \"body\": \"# Resolution for issue ST-2942\\n\\n## Impact\\n\\nFollow instructions in the [AWS S3 docs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiFactorAuthenticationDelete.html) to manually configure the MFA setting.\\n-- Payload truncated --\", \"url\": \"https://external.service.dev/result/PRTNR-CC-TF-127\" } "
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202304-1/api-docs/run-tasks/run-tasks-integration",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202303-1/api-docs/run-tasks/run-tasks-integration",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202302-1/api-docs/run-tasks/run-tasks-integration",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202301-1/api-docs/run-tasks/run-tasks-integration",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202301-2/api-docs/run-tasks/run-tasks-integration",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202211-1/api-docs/admin/module-sharing",
  "text": "Module Sharing - Admin - API Docs - Terraform Enterprise | Terraform\nTip: This Admin Module Sharing API is deprecated and will be removed in a future release. Transition existing integrations with this API to the Admin Organizations Module Consumers API. Global module sharing should still be configured using the Admin Organizations API.\nThere are two ways to configure module sharing via the Admin API:\nThis endpoint, which allows an organization to share modules with a specific list of other organizations.\nThe update an organization endpoint, whose data.attributes.global-module-sharing property allows an organization to share modules with every organization in the instance.\nEnabling either option will disable the other. For more information, see Administration: Module Sharing.\nThis API endpoint is available in Terraform Enterprise as of version 202012-1.\nPATCH /admin/organizations/:name/module-consumers\nThis endpoint sets the list of organizations that can use modules from the sharing organization's private registry. Sharing with specific organizations will automatically turn off global module sharing, which is configured with the update an organization endpoint (via the data.attributes.global-module-sharing property).\n:name\tThe name of the organization whose registry is being shared\t\ndata.type\tstring\t\tMust be \"module-partnerships\"\t\ndata.attributes.module-consuming-organization-ids\tarray[string]\t\tA list of external ids for organizations that will be able to access modules in the producing organization's registry. These should have an org- prefix.\t\n{ \"data\": { \"type\": \"module-partnerships\", \"attributes\": { \"module-consuming-organization-ids\": [ \"org-939hp5K7kecppVmd\" ] } } } \ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request PATCH \\ --data @payload.json \\ https://tfe.example.com/api/v2/admin/organizations/my-organization/module-consumers \n{ \"data\": [ { \"id\": \"mp-tQATArr4gyYDBvkF\", \"type\": \"module-partnerships\", \"attributes\": { \"consuming-organization-id\": \"org-939hp5K7kecppVmd\", \"consuming-organization-name\": \"other-organization\", \"producing-organization-id\": \"org-etdex8r9VLnyHFct\", \"producing-organization-name\": \"my-organization\" } } ] }"
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202210-1/api-docs/admin/module-sharing",
  "text": "Module Sharing - Admin - API Docs - Terraform Enterprise | Terraform\nTip: This Admin Module Sharing API is deprecated and will be removed in a future release. Transition existing integrations with this API to the Admin Organizations Module Consumers API. Global module sharing should still be configured using the Admin Organizations API.\nThere are two ways to configure module sharing via the Admin API:\nThis endpoint, which allows an organization to share modules with a specific list of other organizations.\nThe update an organization endpoint, whose data.attributes.global-module-sharing property allows an organization to share modules with every organization in the instance.\nEnabling either option will disable the other. For more information, see Administration: Module Sharing.\nThis API endpoint is available in Terraform Enterprise as of version 202012-1.\nPATCH /admin/organizations/:name/module-consumers\nThis endpoint sets the list of organizations that can use modules from the sharing organization's private registry. Sharing with specific organizations will automatically turn off global module sharing, which is configured with the update an organization endpoint (via the data.attributes.global-module-sharing property).\n:name\tThe name of the organization whose registry is being shared\t\ndata.type\tstring\t\tMust be \"module-partnerships\"\t\ndata.attributes.module-consuming-organization-ids\tarray[string]\t\tA list of external ids for organizations that will be able to access modules in the producing organization's registry. These should have an org- prefix.\t\n{ \"data\": { \"type\": \"module-partnerships\", \"attributes\": { \"module-consuming-organization-ids\": [ \"org-939hp5K7kecppVmd\" ] } } } \ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request PATCH \\ --data @payload.json \\ https://tfe.example.com/api/v2/admin/organizations/my-organization/module-consumers \n{ \"data\": [ { \"id\": \"mp-tQATArr4gyYDBvkF\", \"type\": \"module-partnerships\", \"attributes\": { \"consuming-organization-id\": \"org-939hp5K7kecppVmd\", \"consuming-organization-name\": \"other-organization\", \"producing-organization-id\": \"org-etdex8r9VLnyHFct\", \"producing-organization-name\": \"my-organization\" } } ] }"
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202209-2/api-docs/admin/module-sharing",
  "text": "Module Sharing - Admin - API Docs - Terraform Enterprise | Terraform\nTip: This Admin Module Sharing API is deprecated and will be removed in a future release. Transition existing integrations with this API to the Admin Organizations Module Consumers API. Global module sharing should still be configured using the Admin Organizations API.\nThere are two ways to configure module sharing via the Admin API:\nThis endpoint, which allows an organization to share modules with a specific list of other organizations.\nThe update an organization endpoint, whose data.attributes.global-module-sharing property allows an organization to share modules with every organization in the instance.\nEnabling either option will disable the other. For more information, see Administration: Module Sharing.\nThis API endpoint is available in Terraform Enterprise as of version 202012-1.\nPATCH /admin/organizations/:name/module-consumers\nThis endpoint sets the list of organizations that can use modules from the sharing organization's private registry. Sharing with specific organizations will automatically turn off global module sharing, which is configured with the update an organization endpoint (via the data.attributes.global-module-sharing property).\n:name\tThe name of the organization whose registry is being shared\t\ndata.type\tstring\t\tMust be \"module-partnerships\"\t\ndata.attributes.module-consuming-organization-ids\tarray[string]\t\tA list of external ids for organizations that will be able to access modules in the producing organization's registry. These should have an org- prefix.\t\n{ \"data\": { \"type\": \"module-partnerships\", \"attributes\": { \"module-consuming-organization-ids\": [ \"org-939hp5K7kecppVmd\" ] } } } \ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request PATCH \\ --data @payload.json \\ https://tfe.example.com/api/v2/admin/organizations/my-organization/module-consumers \n{ \"data\": [ { \"id\": \"mp-tQATArr4gyYDBvkF\", \"type\": \"module-partnerships\", \"attributes\": { \"consuming-organization-id\": \"org-939hp5K7kecppVmd\", \"consuming-organization-name\": \"other-organization\", \"producing-organization-id\": \"org-etdex8r9VLnyHFct\", \"producing-organization-name\": \"my-organization\" } } ] }"
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202208-3/api-docs/admin/module-sharing",
  "text": "Module Sharing - Admin - API Docs - Terraform Enterprise | Terraform\nTip: This Admin Module Sharing API is deprecated and will be removed in a future release. Transition existing integrations with this API to the Admin Organizations Module Consumers API. Global module sharing should still be configured using the Admin Organizations API.\nThere are two ways to configure module sharing via the Admin API:\nThis endpoint, which allows an organization to share modules with a specific list of other organizations.\nThe update an organization endpoint, whose data.attributes.global-module-sharing property allows an organization to share modules with every organization in the instance.\nEnabling either option will disable the other. For more information, see Administration: Module Sharing.\nThis API endpoint is available in Terraform Enterprise as of version 202012-1.\nPATCH /admin/organizations/:name/module-consumers\nThis endpoint sets the list of organizations that can use modules from the sharing organization's private registry. Sharing with specific organizations will automatically turn off global module sharing, which is configured with the update an organization endpoint (via the data.attributes.global-module-sharing property).\n:name\tThe name of the organization whose registry is being shared\t\ndata.type\tstring\t\tMust be \"module-partnerships\"\t\ndata.attributes.module-consuming-organization-ids\tarray[string]\t\tA list of external ids for organizations that will be able to access modules in the producing organization's registry. These should have an org- prefix.\t\n{ \"data\": { \"type\": \"module-partnerships\", \"attributes\": { \"module-consuming-organization-ids\": [ \"org-939hp5K7kecppVmd\" ] } } } \ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request PATCH \\ --data @payload.json \\ https://tfe.example.com/api/v2/admin/organizations/my-organization/module-consumers \n{ \"data\": [ { \"id\": \"mp-tQATArr4gyYDBvkF\", \"type\": \"module-partnerships\", \"attributes\": { \"consuming-organization-id\": \"org-939hp5K7kecppVmd\", \"consuming-organization-name\": \"other-organization\", \"producing-organization-id\": \"org-etdex8r9VLnyHFct\", \"producing-organization-name\": \"my-organization\" } } ] }"
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202209-1/api-docs/admin/module-sharing",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202208-1/api-docs/admin/module-sharing",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202208-2/api-docs/admin/module-sharing",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202207-1/api-docs/admin/module-sharing",
  "text": "Terraform Enterprise feature: The admin API is exclusive to Terraform Enterprise, and can only be used by the admins and operators who install and maintain their organization's Terraform Enterprise instance."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202206-1/api-docs/admin/module-sharing",
  "text": "Terraform Enterprise feature: The admin API is exclusive to Terraform Enterprise, and can only be used by the admins and operators who install and maintain their organization's Terraform Enterprise instance.\n{ \"data\": { \"type\": \"module-partnerships\", \"attributes\": { \"module-consuming-organization-ids\": [\"org-939hp5K7kecppVmd\"] } } } "
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202207-2/api-docs/admin/module-sharing",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202407-1/api-docs/admin/module-sharing",
  "text": "Note:: This Admin Module Sharing API is deprecated and will be removed in a future release. Transition existing integrations with this API to the Registry Sharing API. Continue using the Admin Organizations API to configure global module sharing."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202406-1/api-docs/admin/module-sharing",
  "text": "Note:: This Admin Module Sharing API is deprecated and will be removed in a future release. Transition existing integrations with this API to the Registry Sharing API. Continue using the Admin Organizations API to configure global module sharing."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202405-1/api-docs/admin/module-sharing",
  "text": "Note:: This Admin Module Sharing API is deprecated and will be removed in a future release. Transition existing integrations with this API to the Registry Sharing API. Continue using the Admin Organizations API to configure global module sharing."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202404-2/api-docs/admin/module-sharing",
  "text": "Note:: This Admin Module Sharing API is deprecated and will be removed in a future release. Transition existing integrations with this API to the Registry Sharing API. Continue using the Admin Organizations API to configure global module sharing."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202404-1/api-docs/admin/module-sharing",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202402-1/api-docs/admin/module-sharing",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202402-2/api-docs/admin/module-sharing",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202212-1/api-docs/admin/module-sharing",
  "text": "Module Sharing - Admin - API Docs - Terraform Enterprise | Terraform\nTerraform Enterprise Only: The admin API is exclusive to Terraform Enterprise, and can only be used by the admins and operators who install and maintain their organization's Terraform Enterprise instance.\nTip: This Admin Module Sharing API is deprecated and will be removed in a future release. Transition existing integrations with this API to the Admin Organizations Module Consumers API. Global module sharing should still be configured using the Admin Organizations API.\nThere are two ways to configure module sharing via the Admin API:\nThis endpoint, which allows an organization to share modules with a specific list of other organizations.\nThe update an organization endpoint, whose data.attributes.global-module-sharing property allows an organization to share modules with every organization in the instance.\nEnabling either option will disable the other. For more information, see Administration: Module Sharing.\nThis API endpoint is available in Terraform Enterprise as of version 202012-1.\nPATCH /admin/organizations/:name/module-consumers\nThis endpoint sets the list of organizations that can use modules from the sharing organization's private registry. Sharing with specific organizations will automatically turn off global module sharing, which is configured with the update an organization endpoint (via the data.attributes.global-module-sharing property).\nParameterDescription\n:name\tThe name of the organization whose registry is being shared\t\nRequest Body\nThis PATCH endpoint requires a JSON object with the following properties as a request payload.\nKey pathTypeDefaultDescription\ndata.type\tstring\t\tMust be \"module-partnerships\"\t\ndata.attributes.module-consuming-organization-ids\tarray[string]\t\tA list of external ids for organizations that will be able to access modules in the producing organization's registry. These should have an org- prefix.\t\nSample Payload\n{ \"data\": { \"type\": \"module-partnerships\", \"attributes\": { \"module-consuming-organization-ids\": [ \"org-939hp5K7kecppVmd\" ] } } } \nSample Request\ncurl \\ --header \"Authorization: Bearer $TOKEN\" \\ --header \"Content-Type: application/vnd.api+json\" \\ --request PATCH \\ --data @payload.json \\ https://tfe.example.com/api/v2/admin/organizations/my-organization/module-consumers \nSample Response\n{ \"data\": [ { \"id\": \"mp-tQATArr4gyYDBvkF\", \"type\": \"module-partnerships\", \"attributes\": { \"consuming-organization-id\": \"org-939hp5K7kecppVmd\", \"consuming-organization-name\": \"other-organization\", \"producing-organization-id\": \"org-etdex8r9VLnyHFct\", \"producing-organization-name\": \"my-organization\" } } ] }"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/v1.1.x/upgrade-guides/0-13",
  "text": "HashiCorp Developer\nThis page does not exist for version v1.7.x.\nPlease select either the most recent version or a valid version that includes the page you are looking for."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/v1.1.x/upgrade-guides/0-12",
  "text": "HashiCorp Developer\nThis page does not exist for version v1.7.x.\nPlease select either the most recent version or a valid version that includes the page you are looking for."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/v1.1.x/upgrade-guides/0-13",
  "text": "HashiCorp Developer\nThis page does not exist for version v1.8.x.\nPlease select either the most recent version or a valid version that includes the page you are looking for."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/pow",
  "text": "pow - Functions - Configuration Language | Terraform\npow Function\npow calculates an exponent, by raising its first argument to the power of the second argument.\n> pow(3, 2) 9 > pow(4, 0) 1"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/v1.1.x/upgrade-guides/0-12",
  "text": "HashiCorp Developer\nThis page does not exist for version v1.8.x.\nPlease select either the most recent version or a valid version that includes the page you are looking for."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/nonsensitive",
  "text": "nonsensitive - Functions - Configuration Language | Terraform\nNote: This function is only available in Terraform v0.15 and later.\nnonsensitive takes a sensitive value and returns a copy of that value with the sensitive marking removed, thereby exposing the sensitive value.\nWarning: Using this function indiscriminately will cause values that Terraform would normally have considered as sensitive to be treated as normal values and shown clearly in Terraform's output. Use this function only when you've derived a new value from a sensitive value in a way that eliminates the sensitive portions of the value.\nNormally Terraform tracks when you use expressions to derive a new value from a value that is marked as sensitive, so that the result can also be marked as sensitive.\nHowever, you may wish to write expressions that derive non-sensitive results from sensitive values. For example, if you know based on details of your particular system and its threat model that a SHA256 hash of a particular sensitive value is safe to include clearly in Terraform output, you could use the nonsensitive function to indicate that, overriding Terraform's normal conservative behavior:\noutput \"sensitive_example_hash\" { value = nonsensitive(sha256(var.sensitive_example)) } \nAnother example might be if the original value is only partially sensitive and you've written expressions to separate the sensitive and non-sensitive parts:\nvariable \"mixed_content_json\" { description = \"A JSON string containing a mixture of sensitive and non-sensitive values.\" type = string sensitive = true } locals { # mixed_content is derived from var.mixed_content_json, so it # is also considered to be sensitive. mixed_content = jsondecode(var.mixed_content_json) # password_from_json is derived from mixed_content, so it's # also considered to be sensitive. password_from_json = local.mixed_content[\"password\"] # username_from_json would normally be considered to be # sensitive too, but system-specific knowledge tells us # that the username is a non-sensitive fragment of the # original document, and so we can override Terraform's # determination. username_from_json = nonsensitive(local.mixed_content[\"username\"]) } \nWhen you use this function, it's your responsibility to ensure that the expression passed as its argument will remove all sensitive content from the sensitive value it depends on. By passing a value to nonsensitive you are declaring to Terraform that you have done all that is necessary to ensure that the resulting value has no sensitive content, even though it was derived from sensitive content. If a sensitive value appears in Terraform's output due to an inappropriate call to nonsensitive in your module, that's a bug in your module and not a bug in Terraform itself. Use this function sparingly and only with due care.\nnonsensitive will make no changes to values that aren't marked as sensitive, even though such a call may be redundant and potentially confusing. Use nonsensitive only after careful consideration and with definite intent.\nConsider including a comment adjacent to your call to explain to future maintainers what makes the usage safe and thus what invariants they must take care to preserve under future modifications.\nThe following examples are from terraform console when running in the context of the example above with variable \"mixed_content_json\" and the local value mixed_content, with a valid JSON string assigned to var.mixed_content_json.\n> var.mixed_content_json (sensitive value) > local.mixed_content (sensitive value) > local.mixed_content[\"password\"] (sensitive value) > nonsensitive(local.mixed_content[\"username\"]) \"zqb\" > nonsensitive(\"clear\") Error: Invalid function argument Invalid value for \"value\" parameter: the given value is not sensitive, so this call is redundant. \nNote though that it's always your responsibility to use nonsensitive only when it's safe to do so. If you use nonsensitive with content that ought to be considered sensitive then that content will be disclosed:\n> nonsensitive(var.mixed_content_json) <<EOT { \"username\": \"zqb\", \"password\": \"p4ssw0rd\" } EOT > nonsensitive(local.mixed_content) { \"password\" = \"p4ssw0rd\" \"username\" = \"zqb\" } > nonsensitive(local.mixed_content[\"password\"]) \"p4ssw0rd\""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/timestamp",
  "text": "timestamp - Functions - Configuration Language | Terraform\ntimestamp returns a UTC timestamp string in RFC 3339 format.\nIn the Terraform language, timestamps are conventionally represented as strings using RFC 3339 \"Date and Time format\" syntax, and so timestamp returns a string in this format.\nThe result of this function will change every second, so using this function directly with resource attributes will cause a diff to be detected on every Terraform run. We do not recommend using this function in resource attributes, but in rare cases it can be used in conjunction with the ignore_changes lifecycle meta-argument to take the timestamp only on initial creation of the resource. For more stable time handling, see the Time Provider.\nDue to the constantly changing return value, the result of this function cannot be predicted during Terraform's planning phase, and so the timestamp will be taken only once the plan is being applied.\n> timestamp() 2018-05-13T07:44:12Z \nformatdate can convert the resulting timestamp to other date and time formats.\nplantimestamp will return a consistent timestamp representing the date and time during the plan."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/issensitive",
  "text": "issensitive - Functions - Configuration Language | Terraform\nNote: This function is only available in Terraform v1.8 and later.\nissensitive takes any value and returns true if Terraform treats it as sensitive, with the same meaning and behavior as for sensitive input variables.\nIf a value not marked as sensitive is passed the function returns false.\nSee sensitive, nonsensitive, and sensitive input variables for more information on sensitive values.\n> issensitive(sensitive(\"secret\")) true > issensitive(\"hello\") false > sensitive(var.my-var-with-sensitive-set-to-true) true"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/sensitive",
  "text": "sensitive - Functions - Configuration Language | Terraform\nNote: This function is only available in Terraform v0.15 and later.\nsensitive takes any value and returns a copy of it marked so that Terraform will treat it as sensitive, with the same meaning and behavior as for sensitive input variables.\nWherever possible we recommend marking your input variable and/or output value declarations as sensitive directly, instead of using this function, because in that case you can be sure that there is no way to refer to those values without Terraform automatically considering them as sensitive.\nThe sensitive function might be useful in some less-common situations where a sensitive value arises from a definition within your module, such as if you've loaded sensitive data from a file on disk as part of your configuration:\nlocals { sensitive_content = sensitive(file(\"${path.module}/sensitive.txt\")) } \nHowever, we generally don't recommend writing sensitive values directly within your module any of the files you distribute statically as part of that module, because they may be exposed in other ways outside of Terraform's control.\n> sensitive(1) (sensitive value) > sensitive(\"hello\") (sensitive value) > sensitive([]) (sensitive value)"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/tobool",
  "text": "tobool - Functions - Configuration Language | Terraform\ntobool converts its argument to a boolean value.\nExplicit type conversions are rarely necessary in Terraform because it will convert types automatically where required. Use the explicit type conversion functions only to normalize types returned in module outputs.\nOnly boolean values, null, and the exact strings \"true\" and \"false\" can be converted to boolean. All other values will produce an error.\n> tobool(true) true > tobool(\"true\") true > tobool(null) null > tobool(\"no\") Error: Invalid function argument Invalid value for \"v\" parameter: cannot convert \"no\" to bool: only the strings \"true\" or \"false\" are allowed. > tobool(1) Error: Invalid function argument Invalid value for \"v\" parameter: cannot convert number to bool."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/tolist",
  "text": "tolist - Functions - Configuration Language | Terraform\ntolist Function\ntolist converts its argument to a list value.\nExplicit type conversions are rarely necessary in Terraform because it will convert types automatically where required. Use the explicit type conversion functions only to normalize types returned in module outputs.\nPass a set value to tolist to convert it to a list. Since set elements are not ordered, the resulting list will have an undefined order that will be consistent within a particular run of Terraform.\n> tolist([\"a\", \"b\", \"c\"]) [ \"a\", \"b\", \"c\", ] \nSince Terraform's concept of a list requires all of the elements to be of the same type, mixed-typed elements will be converted to the most general type:\n> tolist([\"a\", \"b\", 3]) [ \"a\", \"b\", \"3\", ]"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/tomap",
  "text": "tomap - Functions - Configuration Language | Terraform\ntomap Function\ntomap converts its argument to a map value.\nExplicit type conversions are rarely necessary in Terraform because it will convert types automatically where required. Use the explicit type conversion functions only to normalize types returned in module outputs.\n> tomap({\"a\" = 1, \"b\" = 2}) { \"a\" = 1 \"b\" = 2 } \nSince Terraform's concept of a map requires all of the elements to be of the same type, mixed-typed elements will be converted to the most general type:\n> tomap({\"a\" = \"foo\", \"b\" = true}) { \"a\" = \"foo\" \"b\" = \"true\" }"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/try",
  "text": "try - Functions - Configuration Language | Terraform\ntry evaluates all of its argument expressions in turn and returns the result of the first one that does not produce any errors.\nThis is a special function that is able to catch errors produced when evaluating its arguments, which is particularly useful when working with complex data structures whose shape is not well-known at implementation time.\nFor example, if some data is retrieved from an external system in JSON or YAML format and then decoded, the result may have attributes that are not guaranteed to be set. We can use try to produce a normalized data structure which has a predictable type that can therefore be used more conveniently elsewhere in the configuration:\nlocals { raw_value = yamldecode(file(\"${path.module}/example.yaml\")) normalized_value = { name = tostring(try(local.raw_value.name, null)) groups = try(local.raw_value.groups, []) } } \nWith the above local value expressions, configuration elsewhere in the module can refer to local.normalized_value attributes without the need to repeatedly check for and handle absent attributes that would otherwise produce errors.\nWe can also use try to deal with situations where a value might be provided in two different forms, allowing us to normalize to the most general form:\nvariable \"example\" { type = any } locals { example = try( [tostring(var.example)], tolist(var.example), ) } \nThe above permits var.example to be either a list or a single string. If it's a single string then it'll be normalized to a single-element list containing that string, again allowing expressions elsewhere in the configuration to just assume that local.example is always a list.\nThis second example contains two expressions that can both potentially fail. For example, if var.example were set to {} then it could be converted to neither a string nor a list. If try exhausts all of the given expressions without any succeeding, it will return an error describing all of the problems it encountered.\nWe strongly suggest using try only in special local values whose expressions perform normalization, so that the error handling is confined to a single location in the module and the rest of the module can just use straightforward references to the normalized structure and thus be more readable for future maintainers.\nThe try function can only catch and handle dynamic errors resulting from access to data that isn't known until runtime. It will not catch errors relating to expressions that can be proven to be invalid for any input, such as a malformed resource reference.\nWarning: The try function is intended only for concise testing of the presence of and types of object attributes. Although it can technically accept any sort of expression, we recommend using it only with simple attribute references and type conversion functions as shown in the examples above. Overuse of try to suppress errors will lead to a configuration that is hard to understand and maintain.\n> local.foo { \"bar\" = \"baz\" } > try(local.foo.bar, \"fallback\") baz > try(local.foo.boop, \"fallback\") fallback \nThe try function will not catch errors relating to constructs that are provably invalid even before dynamic expression evaluation, such as a malformed reference or a reference to a top-level object that has not been declared:\n> try(local.nonexist, \"fallback\") Error: Reference to undeclared local value A local value with the name \"nonexist\" has not been declared. \ncan, which tries evaluating an expression and returns a boolean value indicating whether it succeeded."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/tonumber",
  "text": "tonumber - Functions - Configuration Language | Terraform\ntonumber converts its argument to a number value.\nExplicit type conversions are rarely necessary in Terraform because it will convert types automatically where required. Use the explicit type conversion functions only to normalize types returned in module outputs.\nOnly numbers, null, and strings containing decimal representations of numbers can be converted to number. All other values will produce an error.\n> tonumber(1) 1 > tonumber(\"1\") 1 > tonumber(null) null > tonumber(\"no\") Error: Invalid function argument Invalid value for \"v\" parameter: cannot convert \"no\" to number: string must be a decimal representation of a number."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/type",
  "text": "type - Functions - Configuration Language | Terraform\nNote: This function is available only in Terraform 1.0 and later.\ntype returns the type of a given value.\nSometimes a Terraform configuration can result in confusing errors regarding inconsistent types. This function displays terraform's evaluation of a given value's type, which is useful in understanding this error message.\nThis is a special function which is only available in the terraform console command. It can only be used to examine the type of a given value, and should not be used in more complex expressions.\nHere we have a conditional output which prints either the value of var.list or a local named default_list:\nvariable \"list\" { default = [] } locals { default_list = [ { foo = \"bar\" map = { bleep = \"bloop\" } }, { beep = \"boop\" }, ] } output \"list\" { value = var.list != [] ? var.list : local.default_list } \nApplying this configuration results in the following error:\nError: Inconsistent conditional result types on main.tf line 18, in output \"list\": 18: value = var.list != [] ? var.list : local.default_list |---------------- | local.default_list is tuple with 2 elements | var.list is empty tuple The true and false result expressions must have consistent types. The given expressions are tuple and tuple, respectively. \nWhile this error message does include some type information, it can be helpful to inspect the exact type that Terraform has determined for each given input. Examining both var.list and local.default_list using the type function provides more context for the error message:\n> type(var.list) tuple > type(local.default_list) tuple([ object({ foo: string, map: object({ bleep: string, }), }), object({ beep: string, }), ])"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/v1.1.x/upgrade-guides/0-13",
  "text": "This page does not exist for version v1.6.x."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/v1.1.x/upgrade-guides/0-12",
  "text": "This page does not exist for version v1.6.x."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/pow",
  "text": "pow - Functions - Configuration Language | Terraform\npow Function\npow calculates an exponent, by raising its first argument to the power of the second argument.\n> pow(3, 2) 9 > pow(4, 0) 1"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/timestamp",
  "text": "timestamp - Functions - Configuration Language | Terraform\ntimestamp returns a UTC timestamp string in RFC 3339 format.\nIn the Terraform language, timestamps are conventionally represented as strings using RFC 3339 \"Date and Time format\" syntax, and so timestamp returns a string in this format.\nThe result of this function will change every second, so using this function directly with resource attributes will cause a diff to be detected on every Terraform run. We do not recommend using this function in resource attributes, but in rare cases it can be used in conjunction with the ignore_changes lifecycle meta-argument to take the timestamp only on initial creation of the resource. For more stable time handling, see the Time Provider.\nDue to the constantly changing return value, the result of this function cannot be predicted during Terraform's planning phase, and so the timestamp will be taken only once the plan is being applied.\n> timestamp() 2018-05-13T07:44:12Z \nformatdate can convert the resulting timestamp to other date and time formats.\nplantimestamp will return a consistent timestamp representing the date and time during the plan."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/nonsensitive",
  "text": "nonsensitive - Functions - Configuration Language | Terraform\nNote: This function is only available in Terraform v0.15 and later.\nnonsensitive takes a sensitive value and returns a copy of that value with the sensitive marking removed, thereby exposing the sensitive value.\nWarning: Using this function indiscriminately will cause values that Terraform would normally have considered as sensitive to be treated as normal values and shown clearly in Terraform's output. Use this function only when you've derived a new value from a sensitive value in a way that eliminates the sensitive portions of the value.\nNormally Terraform tracks when you use expressions to derive a new value from a value that is marked as sensitive, so that the result can also be marked as sensitive.\nHowever, you may wish to write expressions that derive non-sensitive results from sensitive values. For example, if you know based on details of your particular system and its threat model that a SHA256 hash of a particular sensitive value is safe to include clearly in Terraform output, you could use the nonsensitive function to indicate that, overriding Terraform's normal conservative behavior:\noutput \"sensitive_example_hash\" { value = nonsensitive(sha256(var.sensitive_example)) } \nAnother example might be if the original value is only partially sensitive and you've written expressions to separate the sensitive and non-sensitive parts:\nvariable \"mixed_content_json\" { description = \"A JSON string containing a mixture of sensitive and non-sensitive values.\" type = string sensitive = true } locals { # mixed_content is derived from var.mixed_content_json, so it # is also considered to be sensitive. mixed_content = jsondecode(var.mixed_content_json) # password_from_json is derived from mixed_content, so it's # also considered to be sensitive. password_from_json = local.mixed_content[\"password\"] # username_from_json would normally be considered to be # sensitive too, but system-specific knowledge tells us # that the username is a non-sensitive fragment of the # original document, and so we can override Terraform's # determination. username_from_json = nonsensitive(local.mixed_content[\"username\"]) } \nWhen you use this function, it's your responsibility to ensure that the expression passed as its argument will remove all sensitive content from the sensitive value it depends on. By passing a value to nonsensitive you are declaring to Terraform that you have done all that is necessary to ensure that the resulting value has no sensitive content, even though it was derived from sensitive content. If a sensitive value appears in Terraform's output due to an inappropriate call to nonsensitive in your module, that's a bug in your module and not a bug in Terraform itself. Use this function sparingly and only with due care.\nnonsensitive will return an error if you pass a value that isn't marked as sensitive, because such a call would be redundant and potentially confusing or misleading to a future maintainer of your module. Use nonsensitive only after careful consideration and with definite intent.\nConsider including a comment adjacent to your call to explain to future maintainers what makes the usage safe and thus what invariants they must take care to preserve under future modifications.\nThe following examples are from terraform console when running in the context of the example above with variable \"mixed_content_json\" and the local value mixed_content, with a valid JSON string assigned to var.mixed_content_json.\n> var.mixed_content_json (sensitive value) > local.mixed_content (sensitive value) > local.mixed_content[\"password\"] (sensitive value) > nonsensitive(local.mixed_content[\"username\"]) \"zqb\" > nonsensitive(\"clear\") Error: Invalid function argument Invalid value for \"value\" parameter: the given value is not sensitive, so this call is redundant. \nNote though that it's always your responsibility to use nonsensitive only when it's safe to do so. If you use nonsensitive with content that ought to be considered sensitive then that content will be disclosed:\n> nonsensitive(var.mixed_content_json) <<EOT { \"username\": \"zqb\", \"password\": \"p4ssw0rd\" } EOT > nonsensitive(local.mixed_content) { \"password\" = \"p4ssw0rd\" \"username\" = \"zqb\" } > nonsensitive(local.mixed_content[\"password\"]) \"p4ssw0rd\""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/sensitive",
  "text": "sensitive - Functions - Configuration Language | Terraform\nNote: This function is only available in Terraform v0.15 and later.\nsensitive takes any value and returns a copy of it marked so that Terraform will treat it as sensitive, with the same meaning and behavior as for sensitive input variables.\nWherever possible we recommend marking your input variable and/or output value declarations as sensitive directly, instead of using this function, because in that case you can be sure that there is no way to refer to those values without Terraform automatically considering them as sensitive.\nThe sensitive function might be useful in some less-common situations where a sensitive value arises from a definition within your module, such as if you've loaded sensitive data from a file on disk as part of your configuration:\nlocals { sensitive_content = sensitive(file(\"${path.module}/sensitive.txt\")) } \nHowever, we generally don't recommend writing sensitive values directly within your module any of the files you distribute statically as part of that module, because they may be exposed in other ways outside of Terraform's control.\n> sensitive(1) (sensitive value) > sensitive(\"hello\") (sensitive value) > sensitive([]) (sensitive value)"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/tolist",
  "text": "tolist - Functions - Configuration Language | Terraform\ntolist Function\ntolist converts its argument to a list value.\nPass a set value to tolist to convert it to a list. Since set elements are not ordered, the resulting list will have an undefined order that will be consistent within a particular run of Terraform.\n> tolist([\"a\", \"b\", \"c\"]) [ \"a\", \"b\", \"c\", ] \nSince Terraform's concept of a list requires all of the elements to be of the same type, mixed-typed elements will be converted to the most general type:\n> tolist([\"a\", \"b\", 3]) [ \"a\", \"b\", \"3\", ]"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/tomap",
  "text": "tomap - Functions - Configuration Language | Terraform\ntomap Function\ntomap converts its argument to a map value.\n> tomap({\"a\" = 1, \"b\" = 2}) { \"a\" = 1 \"b\" = 2 } \nSince Terraform's concept of a map requires all of the elements to be of the same type, mixed-typed elements will be converted to the most general type:\n> tomap({\"a\" = \"foo\", \"b\" = true}) { \"a\" = \"foo\" \"b\" = \"true\" }"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/tonumber",
  "text": "tonumber - Functions - Configuration Language | Terraform\ntonumber converts its argument to a number value.\nOnly numbers, null, and strings containing decimal representations of numbers can be converted to number. All other values will produce an error.\n> tonumber(1) 1 > tonumber(\"1\") 1 > tonumber(null) null > tonumber(\"no\") Error: Invalid function argument Invalid value for \"v\" parameter: cannot convert \"no\" to number: string must be a decimal representation of a number."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/try",
  "text": "try - Functions - Configuration Language | Terraform\ntry evaluates all of its argument expressions in turn and returns the result of the first one that does not produce any errors.\nThis is a special function that is able to catch errors produced when evaluating its arguments, which is particularly useful when working with complex data structures whose shape is not well-known at implementation time.\nFor example, if some data is retrieved from an external system in JSON or YAML format and then decoded, the result may have attributes that are not guaranteed to be set. We can use try to produce a normalized data structure which has a predictable type that can therefore be used more conveniently elsewhere in the configuration:\nlocals { raw_value = yamldecode(file(\"${path.module}/example.yaml\")) normalized_value = { name = tostring(try(local.raw_value.name, null)) groups = try(local.raw_value.groups, []) } } \nWith the above local value expressions, configuration elsewhere in the module can refer to local.normalized_value attributes without the need to repeatedly check for and handle absent attributes that would otherwise produce errors.\nWe can also use try to deal with situations where a value might be provided in two different forms, allowing us to normalize to the most general form:\nvariable \"example\" { type = any } locals { example = try( [tostring(var.example)], tolist(var.example), ) } \nThe above permits var.example to be either a list or a single string. If it's a single string then it'll be normalized to a single-element list containing that string, again allowing expressions elsewhere in the configuration to just assume that local.example is always a list.\nThis second example contains two expressions that can both potentially fail. For example, if var.example were set to {} then it could be converted to neither a string nor a list. If try exhausts all of the given expressions without any succeeding, it will return an error describing all of the problems it encountered.\nWe strongly suggest using try only in special local values whose expressions perform normalization, so that the error handling is confined to a single location in the module and the rest of the module can just use straightforward references to the normalized structure and thus be more readable for future maintainers.\nThe try function can only catch and handle dynamic errors resulting from access to data that isn't known until runtime. It will not catch errors relating to expressions that can be proven to be invalid for any input, such as a malformed resource reference.\nWarning: The try function is intended only for concise testing of the presence of and types of object attributes. Although it can technically accept any sort of expression, we recommend using it only with simple attribute references and type conversion functions as shown in the examples above. Overuse of try to suppress errors will lead to a configuration that is hard to understand and maintain.\n> local.foo { \"bar\" = \"baz\" } > try(local.foo.bar, \"fallback\") baz > try(local.foo.boop, \"fallback\") fallback \nThe try function will not catch errors relating to constructs that are provably invalid even before dynamic expression evaluation, such as a malformed reference or a reference to a top-level object that has not been declared:\n> try(local.nonexist, \"fallback\") Error: Reference to undeclared local value A local value with the name \"nonexist\" has not been declared. \ncan, which tries evaluating an expression and returns a boolean value indicating whether it succeeded."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/type",
  "text": "type - Functions - Configuration Language | Terraform\nNote: This function is available only in Terraform 1.0 and later.\ntype returns the type of a given value.\nSometimes a Terraform configuration can result in confusing errors regarding inconsistent types. This function displays terraform's evaluation of a given value's type, which is useful in understanding this error message.\nThis is a special function which is only available in the terraform console command. It can only be used to examine the type of a given value, and should not be used in more complex expressions.\nHere we have a conditional output which prints either the value of var.list or a local named default_list:\nvariable \"list\" { default = [] } locals { default_list = [ { foo = \"bar\" map = { bleep = \"bloop\" } }, { beep = \"boop\" }, ] } output \"list\" { value = var.list != [] ? var.list : local.default_list } \nApplying this configuration results in the following error:\nError: Inconsistent conditional result types on main.tf line 18, in output \"list\": 18: value = var.list != [] ? var.list : local.default_list |---------------- | local.default_list is tuple with 2 elements | var.list is empty tuple The true and false result expressions must have consistent types. The given expressions are tuple and tuple, respectively. \nWhile this error message does include some type information, it can be helpful to inspect the exact type that Terraform has determined for each given input. Examining both var.list and local.default_list using the type function provides more context for the error message:\n> type(var.list) tuple > type(local.default_list) tuple([ object({ foo: string, map: object({ bleep: string, }), }), object({ beep: string, }), ])"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/tobool",
  "text": "tobool - Functions - Configuration Language | Terraform\ntobool converts its argument to a boolean value.\nOnly boolean values, null, and the exact strings \"true\" and \"false\" can be converted to boolean. All other values will produce an error.\n> tobool(true) true > tobool(\"true\") true > tobool(null) null > tobool(\"no\") Error: Invalid function argument Invalid value for \"v\" parameter: cannot convert \"no\" to bool: only the strings \"true\" or \"false\" are allowed. > tobool(1) Error: Invalid function argument Invalid value for \"v\" parameter: cannot convert number to bool."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/v1.1.x/upgrade-guides/0-13",
  "text": "This page does not exist for version v1.4.x."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/timestamp",
  "text": "timestamp - Functions - Configuration Language | Terraform\ntimestamp returns a UTC timestamp string in RFC 3339 format.\nIn the Terraform language, timestamps are conventionally represented as strings using RFC 3339 \"Date and Time format\" syntax, and so timestamp returns a string in this format.\nThe result of this function will change every second, so using this function directly with resource attributes will cause a diff to be detected on every Terraform run. We do not recommend using this function in resource attributes, but in rare cases it can be used in conjunction with the ignore_changes lifecycle meta-argument to take the timestamp only on initial creation of the resource. For more stable time handling, see the Time Provider.\nDue to the constantly changing return value, the result of this function cannot be predicted during Terraform's planning phase, and so the timestamp will be taken only once the plan is being applied.\n> timestamp() 2018-05-13T07:44:12Z \nformatdate can convert the resulting timestamp to other date and time formats."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/nonsensitive",
  "text": "nonsensitive - Functions - Configuration Language | Terraform\nnonsensitive takes a sensitive value and returns a copy of that value with the sensitive marking removed, thereby exposing the sensitive value.\nWarning: Using this function indiscriminately will cause values that Terraform would normally have considered as sensitive to be treated as normal values and shown clearly in Terraform's output. Use this function only when you've derived a new value from a sensitive value in a way that eliminates the sensitive portions of the value.\nNormally Terraform tracks when you use expressions to derive a new value from a value that is marked as sensitive, so that the result can also be marked as sensitive.\nHowever, you may wish to write expressions that derive non-sensitive results from sensitive values. For example, if you know based on details of your particular system and its threat model that a SHA256 hash of a particular sensitive value is safe to include clearly in Terraform output, you could use the nonsensitive function to indicate that, overriding Terraform's normal conservative behavior:\noutput \"sensitive_example_hash\" { value = nonsensitive(sha256(var.sensitive_example)) } \nAnother example might be if the original value is only partially sensitive and you've written expressions to separate the sensitive and non-sensitive parts:\nvariable \"mixed_content_json\" { description = \"A JSON string containing a mixture of sensitive and non-sensitive values.\" type = string sensitive = true } locals { # mixed_content is derived from var.mixed_content_json, so it # is also considered to be sensitive. mixed_content = jsondecode(var.mixed_content_json) # password_from_json is derived from mixed_content, so it's # also considered to be sensitive. password_from_json = local.mixed_content[\"password\"] # username_from_json would normally be considered to be # sensitive too, but system-specific knowledge tells us # that the username is a non-sensitive fragment of the # original document, and so we can override Terraform's # determination. username_from_json = nonsensitive(local.mixed_content[\"username\"]) } \nWhen you use this function, it's your responsibility to ensure that the expression passed as its argument will remove all sensitive content from the sensitive value it depends on. By passing a value to nonsensitive you are declaring to Terraform that you have done all that is necessary to ensure that the resulting value has no sensitive content, even though it was derived from sensitive content. If a sensitive value appears in Terraform's output due to an inappropriate call to nonsensitive in your module, that's a bug in your module and not a bug in Terraform itself. Use this function sparingly and only with due care.\nnonsensitive will return an error if you pass a value that isn't marked as sensitive, because such a call would be redundant and potentially confusing or misleading to a future maintainer of your module. Use nonsensitive only after careful consideration and with definite intent.\nConsider including a comment adjacent to your call to explain to future maintainers what makes the usage safe and thus what invariants they must take care to preserve under future modifications.\nThe following examples are from terraform console when running in the context of the example above with variable \"mixed_content_json\" and the local value mixed_content, with a valid JSON string assigned to var.mixed_content_json.\n> var.mixed_content_json (sensitive value) > local.mixed_content (sensitive value) > local.mixed_content[\"password\"] (sensitive value) > nonsensitive(local.mixed_content[\"username\"]) \"zqb\" > nonsensitive(\"clear\") Error: Invalid function argument Invalid value for \"value\" parameter: the given value is not sensitive, so this call is redundant. \nNote though that it's always your responsibility to use nonsensitive only when it's safe to do so. If you use nonsensitive with content that ought to be considered sensitive then that content will be disclosed:\n> nonsensitive(var.mixed_content_json) <<EOT { \"username\": \"zqb\", \"password\": \"p4ssw0rd\" } EOT > nonsensitive(local.mixed_content) { \"password\" = \"p4ssw0rd\" \"username\" = \"zqb\" } > nonsensitive(local.mixed_content[\"password\"]) \"p4ssw0rd\""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/v1.1.x/upgrade-guides/0-12",
  "text": "This page does not exist for version v1.4.x."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/sensitive",
  "text": "sensitive - Functions - Configuration Language | Terraform\nsensitive takes any value and returns a copy of it marked so that Terraform will treat it as sensitive, with the same meaning and behavior as for sensitive input variables.\nWherever possible we recommend marking your input variable and/or output value declarations as sensitive directly, instead of using this function, because in that case you can be sure that there is no way to refer to those values without Terraform automatically considering them as sensitive.\nThe sensitive function might be useful in some less-common situations where a sensitive value arises from a definition within your module, such as if you've loaded sensitive data from a file on disk as part of your configuration:\nlocals { sensitive_content = sensitive(file(\"${path.module}/sensitive.txt\")) } \nHowever, we generally don't recommend writing sensitive values directly within your module any of the files you distribute statically as part of that module, because they may be exposed in other ways outside of Terraform's control.\n> sensitive(1) (sensitive value) > sensitive(\"hello\") (sensitive value) > sensitive([]) (sensitive value)"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/tobool",
  "text": "tobool - Functions - Configuration Language | Terraform\ntobool converts its argument to a boolean value.\nOnly boolean values, null, and the exact strings \"true\" and \"false\" can be converted to boolean. All other values will produce an error.\n> tobool(true) true > tobool(\"true\") true > tobool(null) null > tobool(\"no\") Error: Invalid function argument Invalid value for \"v\" parameter: cannot convert \"no\" to bool: only the strings \"true\" or \"false\" are allowed. > tobool(1) Error: Invalid function argument Invalid value for \"v\" parameter: cannot convert number to bool."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/tolist",
  "text": "tolist - Functions - Configuration Language | Terraform\ntolist Function\ntolist converts its argument to a list value.\nPass a set value to tolist to convert it to a list. Since set elements are not ordered, the resulting list will have an undefined order that will be consistent within a particular run of Terraform.\n> tolist([\"a\", \"b\", \"c\"]) [ \"a\", \"b\", \"c\", ] \nSince Terraform's concept of a list requires all of the elements to be of the same type, mixed-typed elements will be converted to the most general type:\n> tolist([\"a\", \"b\", 3]) [ \"a\", \"b\", \"3\", ]"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/tonumber",
  "text": "tonumber - Functions - Configuration Language | Terraform\ntonumber converts its argument to a number value.\nOnly numbers, null, and strings containing decimal representations of numbers can be converted to number. All other values will produce an error.\n> tonumber(1) 1 > tonumber(\"1\") 1 > tonumber(null) null > tonumber(\"no\") Error: Invalid function argument Invalid value for \"v\" parameter: cannot convert \"no\" to number: string must be a decimal representation of a number."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/tomap",
  "text": "tomap - Functions - Configuration Language | Terraform\ntomap Function\ntomap converts its argument to a map value.\n> tomap({\"a\" = 1, \"b\" = 2}) { \"a\" = 1 \"b\" = 2 } \nSince Terraform's concept of a map requires all of the elements to be of the same type, mixed-typed elements will be converted to the most general type:\n> tomap({\"a\" = \"foo\", \"b\" = true}) { \"a\" = \"foo\" \"b\" = \"true\" }"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/try",
  "text": "try - Functions - Configuration Language | Terraform\ntry evaluates all of its argument expressions in turn and returns the result of the first one that does not produce any errors.\nThis is a special function that is able to catch errors produced when evaluating its arguments, which is particularly useful when working with complex data structures whose shape is not well-known at implementation time.\nFor example, if some data is retrieved from an external system in JSON or YAML format and then decoded, the result may have attributes that are not guaranteed to be set. We can use try to produce a normalized data structure which has a predictable type that can therefore be used more conveniently elsewhere in the configuration:\nlocals { raw_value = yamldecode(file(\"${path.module}/example.yaml\")) normalized_value = { name = tostring(try(local.raw_value.name, null)) groups = try(local.raw_value.groups, []) } } \nWith the above local value expressions, configuration elsewhere in the module can refer to local.normalized_value attributes without the need to repeatedly check for and handle absent attributes that would otherwise produce errors.\nWe can also use try to deal with situations where a value might be provided in two different forms, allowing us to normalize to the most general form:\nvariable \"example\" { type = any } locals { example = try( [tostring(var.example)], tolist(var.example), ) } \nThe above permits var.example to be either a list or a single string. If it's a single string then it'll be normalized to a single-element list containing that string, again allowing expressions elsewhere in the configuration to just assume that local.example is always a list.\nThis second example contains two expressions that can both potentially fail. For example, if var.example were set to {} then it could be converted to neither a string nor a list. If try exhausts all of the given expressions without any succeeding, it will return an error describing all of the problems it encountered.\nWe strongly suggest using try only in special local values whose expressions perform normalization, so that the error handling is confined to a single location in the module and the rest of the module can just use straightforward references to the normalized structure and thus be more readable for future maintainers.\nThe try function can only catch and handle dynamic errors resulting from access to data that isn't known until runtime. It will not catch errors relating to expressions that can be proven to be invalid for any input, such as a malformed resource reference.\nWarning: The try function is intended only for concise testing of the presence of and types of object attributes. Although it can technically accept any sort of expression, we recommend using it only with simple attribute references and type conversion functions as shown in the examples above. Overuse of try to suppress errors will lead to a configuration that is hard to understand and maintain.\n> local.foo { \"bar\" = \"baz\" } > try(local.foo.bar, \"fallback\") baz > try(local.foo.boop, \"fallback\") fallback \nThe try function will not catch errors relating to constructs that are provably invalid even before dynamic expression evaluation, such as a malformed reference or a reference to a top-level object that has not been declared:\n> try(local.nonexist, \"fallback\") Error: Reference to undeclared local value A local value with the name \"nonexist\" has not been declared. \ncan, which tries evaluating an expression and returns a boolean value indicating whether it succeeded."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/pow",
  "text": "pow - Functions - Configuration Language | Terraform\npow Function\npow calculates an exponent, by raising its first argument to the power of the second argument.\n> pow(3, 2) 9 > pow(4, 0) 1"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/configuration-0-11/providers",
  "text": "We couldn't find the page you're looking for.\nPlease check the url you entered for typos, go back to the page you came from, or go to one of the pages below.\nHashiCorp Cloud Platform\nTerraform\nPacker\nConsul\nVault\nBoundary\nNomad\nWaypoint\nVagrant"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.1.x/configuration-0-11/resources",
  "text": "Resources - 0.11 Configuration Language | Terraform\nNote: This page is about Terraform 0.11 and earlier. For Terraform 0.12 and later, see Configuration Language: Resources.\nThe most important thing you'll configure with Terraform are resources. Resources are a component of your infrastructure. It might be some low level component such as a physical server, virtual machine, or container. Or it can be a higher level component such as an email provider, DNS record, or database provider.\nThis page assumes you're familiar with the configuration syntax already.\nA resource configuration looks like the following:\nresource \"aws_instance\" \"web\" { ami = \"ami-408c7f28\" instance_type = \"t1.micro\" } \nThe resource block creates a resource of the given TYPE (first parameter) and NAME (second parameter). The combination of the type and name must be unique.\nWithin the block (the { }) is configuration for the resource. The configuration is dependent on the type. Consult the provider's documentation for details.\ndetails.\nMeta-parameters\nThere are meta-parameters available to all resources:\ncount (int) - The number of identical resources to create. This doesn't apply to all resources. For details on using variables in conjunction with count, see Using Variables with count below.\nModules don't currently support the count parameter.\ndepends_on (list of strings) - Explicit dependencies that this resource has. These dependencies will be created before this resource. For syntax and other details, see the section below on explicit dependencies.\nprovider (string) - The name of a specific provider to use for this resource. The name is in the format of TYPE.ALIAS, for example, aws.west. Where west is set using the alias attribute in a provider. See multiple provider instances.\nlifecycle (configuration block) - Customizes the lifecycle behavior of the resource. The specific options are documented below.\nThe lifecycle block allows the following keys to be set:\ncreate_before_destroy (bool) - This flag is used to ensure the replacement of a resource is created before the original instance is destroyed. As an example, this can be used to create an new DNS record before removing an old record.\nprevent_destroy (bool) - This flag provides extra protection against the destruction of a given resource. When this is set to true, any plan that includes a destroy of this resource will return an error message.\nignore_changes (list of strings) - Customizes how diffs are evaluated for resources, allowing individual attributes to be ignored through changes. As an example, this can be used to ignore dynamic changes to the resource from external resources. Other meta-parameters cannot be ignored.\n~> Ignored attribute names can be matched by their name, not state ID. For example, if an `aws_route_table` has two routes defined and the `ignore_changes` list contains \"route\", both routes will be ignored. Additionally you can also use a single entry with a wildcard (e.g. `\"*\"`) which will match all attribute names. Using a partial string together with a wildcard (e.g. `\"rout*\"`) is **not** supported. \nInterpolations are not currently supported in the lifecycle configuration block (see issue #3116)\nTimeouts\nIndividual Resources may provide a timeouts block to enable users to configure the amount of time a specific operation is allowed to take before being considered an error. For example, the aws_db_instance resource provides configurable timeouts for the create, update, and delete operations. Any Resource that provides Timeouts will document the default values for that operation, and users can overwrite them in their configuration.\nExample overwriting the create and delete timeouts:\nresource \"aws_db_instance\" \"timeout_example\" { allocated_storage = 10 engine = \"mysql\" engine_version = \"5.6.17\" instance_class = \"db.t1.micro\" name = \"mydb\" # ... timeouts { create = \"60m\" delete = \"2h\" } } \nIndividual Resources must opt-in to providing configurable Timeouts, and attempting to configure the timeout for a Resource that does not support Timeouts, or overwriting a specific action that the Resource does not specify as an option, will result in an error. Valid units of time are s, m, h.\nExplicit Dependencies\nTerraform ensures that dependencies are successfully created before a resource is created. During a destroy operation, Terraform ensures that this resource is destroyed before its dependencies.\nA resource automatically depends on anything it references via interpolations. The automatically determined dependencies are all that is needed most of the time. You can also use the depends_on parameter to explicitly define a list of additional dependencies.\nThe primary use case of explicit depends_on is to depend on a side effect of another operation. For example: if a provisioner creates a file, and your resource reads that file, then there is no interpolation reference for Terraform to automatically connect the two resources. However, there is a causal ordering that needs to be represented. This is an ideal case for depends_on. In most cases, however, depends_on should be avoided and Terraform should be allowed to determine dependencies automatically.\nThe syntax of depends_on is a list of resources and modules:\nResources are TYPE.NAME, such as aws_instance.web.\nModules are module.NAME, such as module.foo.\nWhen a resource depends on a module, everything in that module must be created before the resource is created.\nAn example of a resource depending on both a module and resource is shown below. Note that depends_on can contain any number of dependencies:\nresource \"aws_instance\" \"web\" { depends_on = [\"aws_instance.leader\", \"module.vpc\"] } \nUse sparingly! depends_on is rarely necessary. In almost every case, Terraform's automatic dependency system is the best-case scenario by having your resources depend only on what they explicitly use. Please think carefully before you use depends_on to determine if Terraform could automatically do this a better way.\nConnection block\nWithin a resource, you can optionally have a connection block. Connection blocks describe to Terraform how to connect to the resource for provisioning. This block doesn't need to be present if you're using only local provisioners, or if you're not provisioning at all.\nResources provide some data on their own, such as an IP address, but other data must be specified by the user.\nThe full list of settings that can be specified are listed on the provisioner connection page.\nProvisioners\nWithin a resource, you can specify zero or more provisioner blocks. Provisioner blocks configure provisioners.\nWithin the provisioner block is provisioner-specific configuration, much like resource-specific configuration.\nProvisioner blocks can also contain a connection block (documented above). This connection block can be used to provide more specific connection info for a specific provisioner. An example use case might be to use a different user to log in for a single provisioner.\nWhen declaring multiple instances of a resource using count, it is common to want each instance to have a different value for a given attribute.\nYou can use the ${count.index} interpolation along with a map variable to accomplish this.\nFor example, here's how you could create three AWS Instances each with their own static IP address:\nvariable \"instance_ips\" { default = { \"0\" = \"10.11.12.100\" \"1\" = \"10.11.12.101\" \"2\" = \"10.11.12.102\" } } resource \"aws_instance\" \"app\" { count = \"3\" private_ip = \"${lookup(var.instance_ips, count.index)}\" # ... } \nTo reference a particular instance of a resource you can use resource.foo.*.id[#] where # is the index number of the instance.\nFor example, to create a list of all AWS subnet ids vs referencing a specific subnet in the list you can use this syntax:\nresource \"aws_vpc\" \"foo\" { cidr_block = \"198.18.0.0/16\" } resource \"aws_subnet\" \"bar\" { count = 2 vpc_id = \"${aws_vpc.foo.id}\" cidr_block = \"${cidrsubnet(aws_vpc.foo.cidr_block, 8, count.index)}\" } output \"vpc_id\" { value = \"${aws_vpc.foo.id}\" } output \"all_subnet_ids\" { value = \"${aws_subnet.bar.*.id}\" } output \"subnet_id_0\" { value = \"${aws_subnet.bar.*.id[0]}\" } output \"subnet_id_1\" { value = \"${aws_subnet.bar.*.id[1]}\" } \nBy default, a resource targets the provider based on its type. For example an aws_instance resource will target the \"aws\" provider. As of Terraform 0.5.0, a resource can target any provider by name.\nThe primary use case for this is to target a specific configuration of a provider that is configured multiple times to support multiple regions, etc.\nTo target another provider, set the provider field:\nresource \"aws_instance\" \"foo\" { provider = \"aws.west\" # ... } \nThe value of the field should be TYPE or TYPE.ALIAS. The ALIAS value comes from the alias field value when configuring the provider.\nprovider \"aws\" { alias = \"west\" # ... } \nIf no provider field is specified, the default provider is used.\nThe full syntax is:\nresource TYPE NAME { CONFIG ... [count = COUNT] [depends_on = [NAME, ...]] [provider = PROVIDER] [LIFECYCLE] [CONNECTION] [PROVISIONER ...] } \nwhere CONFIG is:\nKEY = VALUE KEY { CONFIG } \nwhere LIFECYCLE is:\nlifecycle { [create_before_destroy = true|false] [prevent_destroy = true|false] [ignore_changes = [ATTRIBUTE NAME, ...]] } \nwhere CONNECTION is:\nconnection { KEY = VALUE ... } \nwhere PROVISIONER is:\nprovisioner NAME { CONFIG ... [when = \"create\"|\"destroy\"] [on_failure = \"continue\"|\"fail\"] [CONNECTION] }"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.1.x/configuration-0-11/syntax",
  "text": "Syntax - 0.11 Configuration Language | Terraform\nNote: This page is about Terraform 0.11 and earlier. For Terraform 0.12 and later, see Configuration Language: Syntax.\nThe syntax of Terraform configurations is called HashiCorp Configuration Language (HCL). It is meant to strike a balance between human readable and editable as well as being machine-friendly. For machine-friendliness, Terraform can also read JSON configurations. For general Terraform configurations, however, we recommend using the HCL Terraform syntax.\nHere is an example of Terraform's HCL syntax:\n# An AMI variable \"ami\" { description = \"the AMI to use\" } /* A multi line comment. */ resource \"aws_instance\" \"web\" { ami = \"${var.ami}\" count = 2 source_dest_check = false connection { user = \"root\" } } \nBasic bullet point reference:\nSingle line comments start with #\nMulti-line comments are wrapped with /* and */\nValues are assigned with the syntax of key = value (whitespace doesn't matter). The value can be any primitive (string, number, boolean), a list, or a map.\nStrings are in double-quotes.\nStrings can interpolate other values using syntax wrapped in ${}, such as ${var.foo}. The full syntax for interpolation is documented here.\nMultiline strings can use shell-style \"here doc\" syntax, with the string starting with a marker like <<EOF and then the string ending with EOF on a line of its own. The lines of the string and the end marker must not be indented.\nNumbers are assumed to be base 10. If you prefix a number with 0x, it is treated as a hexadecimal number.\nBoolean values: true, false.\nLists of primitive types can be made with square brackets ([]). Example: [\"foo\", \"bar\", \"baz\"].\nMaps can be made with braces ({}) and colons (:): { \"foo\": \"bar\", \"bar\": \"baz\" }. Quotes may be omitted on keys, unless the key starts with a number, in which case quotes are required. Commas are required between key/value pairs for single line maps. A newline between key/value pairs is sufficient in multi-line maps.\nIn addition to the basics, the syntax supports hierarchies of sections, such as the \"resource\" and \"variable\" in the example above. These sections are similar to maps, but visually look better. For example, these are nearly equivalent:\nvariable \"ami\" { description = \"the AMI to use\" } \nis equal to:\nvariable = [{ \"ami\": { \"description\": \"the AMI to use\", } }] \nNotice how the top stanza visually looks a lot better? By repeating multiple variable sections, it builds up the variable list. When possible, use sections since they're visually clearer and more readable.\nTerraform also supports reading JSON formatted configuration files. The above example converted to JSON:\n{ \"variable\": { \"ami\": { \"description\": \"the AMI to use\" } }, \"resource\": { \"aws_instance\": { \"web\": { \"ami\": \"${var.ami}\", \"count\": 2, \"source_dest_check\": false, \"connection\": { \"user\": \"root\" } } } } } \nThe conversion should be pretty straightforward and self-documented.\nThe downsides of JSON are less human readability and the lack of comments. Otherwise, the two are completely interoperable."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/type",
  "text": "type - Functions - Configuration Language | Terraform\nNote: This function is available only in Terraform 1.0 and later.\ntype returns the type of a given value.\nSometimes a Terraform configuration can result in confusing errors regarding inconsistent types. This function displays terraform's evaluation of a given value's type, which is useful in understanding this error message.\nThis is a special function which is only available in the terraform console command. It can only be used to examine the type of a given value, and should not be used in more complex expressions.\nHere we have a conditional output which prints either the value of var.list or a local named default_list:\nvariable \"list\" { default = [] } locals { default_list = [ { foo = \"bar\" map = { bleep = \"bloop\" } }, { beep = \"boop\" }, ] } output \"list\" { value = var.list != [] ? var.list : local.default_list } \nApplying this configuration results in the following error:\nError: Inconsistent conditional result types on main.tf line 18, in output \"list\": 18: value = var.list != [] ? var.list : local.default_list |---------------- | local.default_list is tuple with 2 elements | var.list is empty tuple The true and false result expressions must have consistent types. The given expressions are tuple and tuple, respectively. \nWhile this error message does include some type information, it can be helpful to inspect the exact type that Terraform has determined for each given input. Examining both var.list and local.default_list using the type function provides more context for the error message:\n> type(var.list) tuple > type(local.default_list) tuple([ object({ foo: string, map: object({ bleep: string, }), }), object({ beep: string, }), ])"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.1.x/configuration-0-11/data-sources",
  "text": "Data Sources - 0.11 Configuration Language | Terraform\nNote: This page is about Terraform 0.11 and earlier. For Terraform 0.12 and later, see Configuration Language: Configuring Data Sources.\nData sources allow data to be fetched or computed for use elsewhere in Terraform configuration. Use of data sources allows a Terraform configuration to build on information defined outside of Terraform, or defined by another separate Terraform configuration.\nProviders are responsible in Terraform for defining and implementing data sources. Whereas a resource causes Terraform to create and manage a new infrastructure component, data sources present read-only views into pre-existing data, or they compute new values on the fly within Terraform itself.\nFor example, a data source may retrieve remote state data from a Terraform Cloud workspace, configuration information from Consul, or look up a pre-existing AWS resource by filtering on its attributes and tags.\nEvery data source in Terraform is mapped to a provider based on longest-prefix matching. For example the aws_ami data source would map to the aws provider (if that exists).\nThis page assumes you're familiar with the configuration syntax already.\nA data source configuration looks like the following:\n# Find the latest available AMI that is tagged with Component = web data \"aws_ami\" \"web\" { filter { name = \"state\" values = [\"available\"] } filter { name = \"tag:Component\" values = [\"web\"] } most_recent = true } \nThe data block creates a data instance of the given TYPE (first parameter) and NAME (second parameter). The combination of the type and name must be unique.\nWithin the block (the { }) is configuration for the data instance. The configuration is dependent on the type; consult the provider's documentation for details.\nEach data instance will export one or more attributes, which can be interpolated into other resources using variables of the form data.TYPE.NAME.ATTR. For example:\nresource \"aws_instance\" \"web\" { ami = \"${data.aws_ami.web.id}\" instance_type = \"t1.micro\" } \nMeta-parameters\nAs data sources are essentially a read only subset of resources they also support the same meta-parameters of resources except for the lifecycle configuration block.\nSimilarly to resources, the provider meta-parameter can be used where a configuration has multiple aliased instances of the same provider:\ndata \"aws_ami\" \"web\" { provider = \"aws.west\" # ... } \nSee the \"Multiple Provider Instances\" documentation for resources for more information.\nIf the arguments of a data instance contain no references to computed values, such as attributes of resources that have not yet been created, then the data instance will be read and its state updated during Terraform's \"refresh\" phase, which by default runs prior to creating a plan. This ensures that the retrieved data is available for use during planning and the diff will show the real values obtained.\nData instance arguments may refer to computed values, in which case the attributes of the instance itself cannot be resolved until all of its arguments are defined. In this case, refreshing the data instance will be deferred until the \"apply\" phase, and all interpolations of the data instance attributes will show as \"computed\" in the plan since the values are not yet known."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.1.x/configuration-0-11/locals",
  "text": "Local Values - 0.11 Configuration Language | Terraform\nNote: This page is about Terraform 0.11 and earlier. For Terraform 0.12 and later, see Configuration Language: Configuring Local Values.\nLocal values assign a name to an expression, that can then be used multiple times within a module.\nComparing modules to functions in a traditional programming language, if variables are analogous to function arguments and outputs are analogous to function return values then local values are comparable to a function's local variables.\nThis page assumes you're already familiar with the configuration syntax.\nLocal values are defined in locals blocks:\n# Ids for multiple sets of EC2 instances, merged together locals { instance_ids = \"${concat(aws_instance.blue.*.id, aws_instance.green.*.id)}\" } # A computed default name prefix locals { default_name_prefix = \"${var.project_name}-web\" name_prefix = \"${var.name_prefix != \"\" ? var.name_prefix : local.default_name_prefix}\" } # Local values can be interpolated elsewhere using the \"local.\" prefix. resource \"aws_s3_bucket\" \"files\" { bucket = \"${local.name_prefix}-files\" # ... } \nNamed local maps can be merged with local maps to implement common or default values:\n# Define the common tags for all resources locals { common_tags = { Component = \"awesome-app\" Environment = \"production\" } } # Create a resource that blends the common tags with instance-specific tags. resource \"aws_instance\" \"server\" { ami = \"ami-123456\" instance_type = \"t2.micro\" tags = \"${merge( local.common_tags, map( \"Name\", \"awesome-app-server\", \"Role\", \"server\" ) )}\" } \nThe locals block defines one or more local variables within a module. Each locals block can have as many locals as needed, and there can be any number of locals blocks within a module.\nThe names given for the items in the locals block must be unique throughout a module. The given value can be any expression that is valid within the current module.\nThe expression of a local value can refer to other locals, but as usual reference cycles are not allowed. That is, a local cannot refer to itself or to a variable that refers (directly or indirectly) back to it.\nIt's recommended to group together logically-related local values into a single block, particularly if they depend on each other. This will help the reader understand the relationships between variables. Conversely, prefer to define unrelated local values in separate blocks, and consider annotating each block with a comment describing any context common to all of the enclosed locals."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.1.x/configuration-0-11/interpolation",
  "text": "Interpolation Syntax - 0.11 Configuration Language | Terraform\nNote: This page is about Terraform 0.11 and earlier. For Terraform 0.12 and later, see Configuration Language: Expressions and Configuration Language: Functions.\nEmbedded within strings in Terraform, whether you're using the Terraform syntax or JSON syntax, you can interpolate other values. These interpolations are wrapped in ${}, such as ${var.foo}.\nThe interpolation syntax is powerful and allows you to reference variables, attributes of resources, call functions, etc.\nYou can perform simple math in interpolations, allowing you to write expressions such as ${count.index + 1}. And you can also use conditionals to determine a value based on some logic.\nYou can escape interpolation with double dollar signs: $${foo} will be rendered as a literal ${foo}.\nThere are a variety of available variable references you can use.\nUser string variables\nUse the var. prefix followed by the variable name. For example, ${var.foo} will interpolate the foo variable value.\nUser map variables\nThe syntax is var.<MAP>[\"<KEY>\"]. For example, ${var.amis[\"us-east-1\"]} would get the value of the us-east-1 key within the amis map variable.\nUser list variables\nThe syntax is \"${var.<LIST>}\". For example, \"${var.subnets}\" would get the value of the subnets list, as a list. You can also return list elements by index: ${var.subnets[idx]}.\nAttributes of your own resource\nThe syntax is self.<ATTRIBUTE>. For example ${self.private_ip} will interpolate that resource's private IP address.\nNote: The self.<ATTRIBUTE> syntax is only allowed and valid within provisioners.\nAttributes of other resources\nThe syntax is <TYPE>.<NAME>.<ATTRIBUTE>. For example, ${aws_instance.web.id} will interpolate the ID attribute from the aws_instance resource named web. If the resource has a count attribute set, you can access individual attributes with a zero-based index, such as ${aws_instance.web.0.id}. You can also use the splat syntax to get a list of all the attributes: ${aws_instance.web.*.id}.\nAttributes of a data source\nThe syntax is data.<TYPE>.<NAME>.<ATTRIBUTE>. For example. ${data.aws_ami.ubuntu.id} will interpolate the id attribute from the aws_ami data source named ubuntu. If the data source has a count attribute set, you can access individual attributes with a zero-based index, such as ${data.aws_subnet.example.0.cidr_block}. You can also use the splat syntax to get a list of all the attributes: ${data.aws_subnet.example.*.cidr_block}.\nOutputs from a module\nThe syntax is module.<NAME>.<OUTPUT>. For example ${module.foo.bar} will interpolate the bar output from the foo module.\nCount information\nThe syntax is count.index. For example, ${count.index} will interpolate the current index in a multi-count resource. For more information on count, see the resource configuration page.\nPath information\nThe syntax is path.<TYPE>. TYPE can be cwd, module, or root. cwd will interpolate the current working directory. module will interpolate the path to the current module. root will interpolate the path of the root module. In general, you probably want the path.module variable.\nTerraform meta information\nThe syntax is terraform.<FIELD>. This variable type contains metadata about the currently executing Terraform run. FIELD can currently only be env to reference the currently active workspace.\nInterpolations may contain conditionals to branch on the final value.\nresource \"aws_instance\" \"web\" { subnet = \"${var.env == \"production\" ? var.prod_subnet : var.dev_subnet}\" } \nThe conditional syntax is the well-known ternary operation:\nCONDITION ? TRUEVAL : FALSEVAL \nThe condition can be any valid interpolation syntax, such as variable access, a function call, or even another conditional. The true and false value can also be any valid interpolation syntax. The returned types by the true and false side must be the same.\nThe supported operators are:\nEquality: == and !=\nNumerical comparison: >, <, >=, <=\nBoolean logic: &&, ||, unary !\nA common use case for conditionals is to enable/disable a resource by conditionally setting the count:\nresource \"aws_instance\" \"vpn\" { count = \"${var.something ? 1 : 0}\" } \nIn the example above, the \"vpn\" resource will only be included if \"var.something\" evaluates to true. Otherwise, the VPN resource will not be created at all.\nTerraform ships with built-in functions. Functions are called with the syntax name(arg, arg2, ...). For example, to read a file: ${file(\"path.txt\")}.\nNote: Proper escaping is required for JSON field values containing quotes (\") such as environment values. If directly setting the JSON, they should be escaped as \\\" in the JSON, e.g. \"value\": \"I \\\"love\\\" escaped quotes\". If using a Terraform variable value, they should be escaped as \\\\\\\" in the variable, e.g. value = \"I \\\\\\\"love\\\\\\\" escaped quotes\" in the variable and \"value\": \"${var.myvariable}\" in the JSON.\nSupported built-in functions\nThe supported built-in functions are:\nabs(float) - Returns the absolute value of a given float. Example: abs(1) returns 1, and abs(-1) would also return 1, whereas abs(-3.14) would return 3.14. See also the signum function.\nbasename(path) - Returns the last element of a path.\nbase64decode(string) - Given a base64-encoded string, decodes it and returns the original string.\nbase64encode(string) - Returns a base64-encoded representation of the given string.\nbase64gzip(string) - Compresses the given string with gzip and then encodes the result to base64. This can be used with certain resource arguments that allow binary data to be passed with base64 encoding, since Terraform strings are required to be valid UTF-8.\nbase64sha256(string) - Returns a base64-encoded representation of raw SHA-256 sum of the given string. This is not equivalent of base64encode(sha256(string)) since sha256() returns hexadecimal representation.\nbase64sha512(string) - Returns a base64-encoded representation of raw SHA-512 sum of the given string. This is not equivalent of base64encode(sha512(string)) since sha512() returns hexadecimal representation.\nbcrypt(password, cost) - Returns the Blowfish encrypted hash of the string at the given cost. A default cost of 10 will be used if not provided.\nceil(float) - Returns the least integer value greater than or equal to the argument.\nchomp(string) - Removes trailing newlines from the given string.\nchunklist(list, size) - Returns the list items chunked by size. Examples:\nchunklist(aws_subnet.foo.*.id, 1): will outputs [[\"id1\"], [\"id2\"], [\"id3\"]]\nchunklist(var.list_of_strings, 2): will outputs [[\"id1\", \"id2\"], [\"id3\", \"id4\"], [\"id5\"]]\ncidrhost(iprange, hostnum) - Takes an IP address range in CIDR notation and creates an IP address with the given host number. If given host number is negative, the count starts from the end of the range. For example, cidrhost(\"10.0.0.0/8\", 2) returns 10.0.0.2 and cidrhost(\"10.0.0.0/8\", -2) returns 10.255.255.254.\ncidrnetmask(iprange) - Takes an IP address range in CIDR notation and returns the address-formatted subnet mask format that some systems expect for IPv4 interfaces. For example, cidrnetmask(\"10.0.0.0/8\") returns 255.0.0.0. Not applicable to IPv6 networks since CIDR notation is the only valid notation for IPv6.\ncidrsubnet(iprange, newbits, netnum) - Takes an IP address range in CIDR notation (like 10.0.0.0/8) and extends its prefix to include an additional subnet number. For example, cidrsubnet(\"10.0.0.0/8\", 8, 2) returns 10.2.0.0/16; cidrsubnet(\"2607:f298:6051:516c::/64\", 8, 2) returns 2607:f298:6051:516c:200::/72.\ncoalesce(string1, string2, ...) - Returns the first non-empty value from the given arguments. At least two arguments must be provided.\ncoalescelist(list1, list2, ...) - Returns the first non-empty list from the given arguments. At least two arguments must be provided.\ncompact(list) - Removes empty string elements from a list. This can be useful in some cases, for example when passing joined lists as module variables or when parsing module outputs. Example: compact(module.my_asg.load_balancer_names)\nconcat(list1, list2, ...) - Combines two or more lists into a single list. Example: concat(aws_instance.db.*.tags.Name, aws_instance.web.*.tags.Name)\ncontains(list, element) - Returns true if a list contains the given element and returns false otherwise. Examples: contains(var.list_of_strings, \"an_element\")\ndirname(path) - Returns all but the last element of path, typically the path's directory.\ndistinct(list) - Removes duplicate items from a list. Keeps the first occurrence of each element, and removes subsequent occurrences. This function is only valid for flat lists. Example: distinct(var.usernames)\nelement(list, index) - Returns a single element from a list at the given index. If the index is greater than the number of elements, this function will wrap using a standard mod algorithm. This function only works on flat lists. Examples:\nelement(aws_subnet.foo.*.id, count.index)\nelement(var.list_of_strings, 2)\nfile(path) - Reads the contents of a file into the string. Variables in this file are not interpolated. The contents of the file are read as-is. The path is interpreted relative to the working directory. Path variables can be used to reference paths relative to other base locations. For example, when using file() from inside a module, you generally want to make the path relative to the module base, like this: file(\"${path.module}/file\").\nfloor(float) - Returns the greatest integer value less than or equal to the argument.\nflatten(list of lists) - Flattens lists of lists down to a flat list of primitive values, eliminating any nested lists recursively. Examples:\nflatten(data.github_user.user.*.gpg_keys)\nformat(format, args, ...) - Formats a string according to the given format. The syntax for the format is standard sprintf syntax. Good documentation for the syntax can be found here. Example to zero-prefix a count, used commonly for naming servers: format(\"web-%03d\", count.index + 1).\nformatlist(format, args, ...) - Formats each element of a list according to the given format, similarly to format, and returns a list. Non-list arguments are repeated for each list element. For example, to convert a list of DNS addresses to a list of URLs, you might use: formatlist(\"https://%s:%s/\", aws_instance.foo.*.public_dns, var.port). If multiple args are lists, and they have the same number of elements, then the formatting is applied to the elements of the lists in parallel. Example: formatlist(\"instance %v has private ip %v\", aws_instance.foo.*.id, aws_instance.foo.*.private_ip). Passing lists with different lengths to formatlist results in an error.\nindent(numspaces, string) - Prepends the specified number of spaces to all but the first line of the given multi-line string. May be useful when inserting a multi-line string into an already-indented context. The first line is not indented, to allow for the indented string to be placed after some sort of already-indented preamble. Example: \" \\\"items\\\": ${ indent(4, \"[\\n \\\"item1\\\"\\n]\") },\"\nindex(list, elem) - Finds the index of a given element in a list. This function only works on flat lists. Example: index(aws_instance.foo.*.tags.Name, \"foo-test\")\njoin(delim, list) - Joins the list with the delimiter for a resultant string. This function works only on flat lists. Examples:\njoin(\",\", aws_instance.foo.*.id)\njoin(\",\", var.ami_list)\njsonencode(value) - Returns a JSON-encoded representation of the given value, which can contain arbitrarily-nested lists and maps. Note that if the value is a string then its value will be placed in quotes.\nkeys(map) - Returns a lexically sorted list of the map keys.\nlength(list) - Returns the number of members in a given list or map, or the number of characters in a given string.\n${length(split(\",\", \"a,b,c\"))} = 3\n${length(\"a,b,c\")} = 5\n${length(map(\"key\", \"val\"))} = 1\nlist(items, ...) - Returns a list consisting of the arguments to the function. This function provides a way of representing list literals in interpolation.\n${list(\"a\", \"b\", \"c\")} returns a list of \"a\", \"b\", \"c\".\n${list()} returns an empty list.\nlog(x, base) - Returns the logarithm of x.\nlookup(map, key, [default]) - Performs a dynamic lookup into a map variable. The map parameter should be another variable, such as var.amis. If key does not exist in map, the interpolation will fail unless you specify a third argument, default, which should be a string value to return if no key is found in map. This function only works on flat maps and will return an error for maps that include nested lists or maps.\nlower(string) - Returns a copy of the string with all Unicode letters mapped to their lower case.\nmap(key, value, ...) - Returns a map consisting of the key/value pairs specified as arguments. Every odd argument must be a string key, and every even argument must have the same type as the other values specified. Duplicate keys are not allowed. Examples:\nmap(\"hello\", \"world\")\nmap(\"us-east\", list(\"a\", \"b\", \"c\"), \"us-west\", list(\"b\", \"c\", \"d\"))\nmatchkeys(values, keys, searchset) - For two lists values and keys of equal length, returns all elements from values where the corresponding element from keys exists in the searchset list. E.g. matchkeys(aws_instance.example.*.id, aws_instance.example.*.availability_zone, list(\"us-west-2a\")) will return a list of the instance IDs of the aws_instance.example instances in \"us-west-2a\". No match will result in empty list. Items of keys are processed sequentially, so the order of returned values is preserved.\nmax(float1, float2, ...) - Returns the largest of the floats.\nmerge(map1, map2, ...) - Returns the union of 2 or more maps. The maps are consumed in the order provided, and duplicate keys overwrite previous entries.\n${merge(map(\"a\", \"b\"), map(\"c\", \"d\"))} returns {\"a\": \"b\", \"c\": \"d\"}\nmin(float1, float2, ...) - Returns the smallest of the floats.\nmd5(string) - Returns a (conventional) hexadecimal representation of the MD5 hash of the given string.\npathexpand(string) - Returns a filepath string with ~ expanded to the home directory. Note: This will create a plan diff between two different hosts, unless the filepaths are the same.\npow(x, y) - Returns the base x of exponential y as a float.\nExample:\n${pow(3,2)} = 9\n${pow(4,0)} = 1\nreplace(string, search, replace) - Does a search and replace on the given string. All instances of search are replaced with the value of replace. If search is wrapped in forward slashes, it is treated as a regular expression. If using a regular expression, replace can reference subcaptures in the regular expression by using $n where n is the index or name of the subcapture. If using a regular expression, the syntax conforms to the re2 regular expression syntax.\nrsadecrypt(string, key) - Decrypts string using RSA. The padding scheme PKCS #1 v1.5 is used. The string must be base64-encoded. key must be an RSA private key in PEM format. You may use file() to load it from a file.\nsha1(string) - Returns a (conventional) hexadecimal representation of the SHA-1 hash of the given string. Example: \"${sha1(\"${aws_vpc.default.tags.customer}-s3-bucket\")}\"\nsha256(string) - Returns a (conventional) hexadecimal representation of the SHA-256 hash of the given string. Example: \"${sha256(\"${aws_vpc.default.tags.customer}-s3-bucket\")}\"\nsha512(string) - Returns a (conventional) hexadecimal representation of the SHA-512 hash of the given string. Example: \"${sha512(\"${aws_vpc.default.tags.customer}-s3-bucket\")}\"\nsignum(integer) - Returns -1 for negative numbers, 0 for 0 and 1 for positive numbers. This function is useful when you need to set a value for the first resource and a different value for the rest of the resources. Example: element(split(\",\", var.r53_failover_policy), signum(count.index)) where the 0th index points to PRIMARY and 1st to FAILOVER\nslice(list, from, to) - Returns the portion of list between from (inclusive) and to (exclusive). Example: slice(var.list_of_strings, 0, length(var.list_of_strings) - 1)\nsort(list) - Returns a lexicographically sorted list of the strings contained in the list passed as an argument. Sort may only be used with lists which contain only strings. Examples: sort(aws_instance.foo.*.id), sort(var.list_of_strings)\nsplit(delim, string) - Returns a list by splitting the string based on the delimiter. This is useful for pushing lists through module outputs since they currently only support string values. Depending on the use, the string this is being performed within may need to be wrapped in brackets to indicate that the output is actually a list, e.g. a_resource_param = [\"${split(\",\", var.CSV_STRING)}\"]. Example: split(\",\", module.amod.server_ids)\nsubstr(string, offset, length) - Extracts a substring from the input string. A negative offset is interpreted as being equivalent to a positive offset measured backwards from the end of the string. A length of -1 is interpreted as meaning \"until the end of the string\".\ntimestamp() - Returns a UTC timestamp string in RFC 3339 format. This string will change with every invocation of the function, so in order to prevent diffs on every plan & apply, it must be used with the ignore_changes lifecycle attribute.\ntimeadd(time, duration) - Returns a UTC timestamp string corresponding to adding a given duration to time in RFC 3339 format. For example, timeadd(\"2017-11-22T00:00:00Z\", \"10m\") produces a value \"2017-11-22T00:10:00Z\".\ntitle(string) - Returns a copy of the string with the first characters of all the words capitalized.\ntranspose(map) - Swaps the keys and list values in a map of lists of strings. For example, transpose(map(\"a\", list(\"1\", \"2\"), \"b\", list(\"2\", \"3\")) produces a value equivalent to map(\"1\", list(\"a\"), \"2\", list(\"a\", \"b\"), \"3\", list(\"b\")).\ntrimspace(string) - Returns a copy of the string with all leading and trailing white spaces removed.\nupper(string) - Returns a copy of the string with all Unicode letters mapped to their upper case.\nurlencode(string) - Returns an URL-safe copy of the string.\nuuid() - Returns a random UUID string. This string will change with every invocation of the function, so in order to prevent diffs on every plan & apply, it must be used with the ignore_changes lifecycle attribute.\nvalues(map) - Returns a list of the map values, in the order of the keys returned by the keys function. This function only works on flat maps and will return an error for maps that include nested lists or maps.\nzipmap(list, list) - Creates a map from a list of keys and a list of values. The keys must all be of type string, and the length of the lists must be the same. For example, to output a mapping of AWS IAM user names to the fingerprint of the key used to encrypt their initial password, you might use: zipmap(aws_iam_user.users.*.name, aws_iam_user_login_profile.users.*.key_fingerprint).\nThe hashing functions base64sha256, base64sha512, md5, sha1, sha256, and sha512 all have variants with a file prefix, like filesha1, which interpret their first argument as a path to a file on disk rather than as a literal string. This allows safely creating hashes of binary files that might otherwise be corrupted in memory if loaded into Terraform strings (which are assumed to be UTF-8). filesha1(filename) is equivalent to sha1(file(filename)) in Terraform 0.11 and earlier, but the latter will fail for binary files in Terraform 0.12 and later.\nLong strings can be managed using templates. Templates are data-sources defined by a string with interpolation tokens (usually loaded from a file) and some variables to use during interpolation. They have a computed rendered attribute containing the result.\nA template data source looks like:\n# templates/greeting.tpl ${hello} ${world}! \ndata \"template_file\" \"example\" { template = \"${file(\"templates/greeting.tpl\")}\" vars { hello = \"goodnight\" world = \"moon\" } } output \"rendered\" { value = \"${data.template_file.example.rendered}\" } \nThen the rendered value would be goodnight moon!.\nNote: If you specify the template as a literal string instead of loading a file, the inline template must use double dollar signs (like $${hello}) to prevent Terraform from interpolating values from the configuration into the string. This is because template_file creates its own instance of the interpolation system, with values provided by its nested vars block instead of by the surrounding scope of the configuration.\nYou may use any of the built-in functions in your template. For more details on template usage, please see the template_file documentation.\nUsing Templates with Count\nHere is an example that combines the capabilities of templates with the interpolation from count to give us a parameterized template, unique to each resource instance:\nvariable \"hostnames\" { default = { \"0\" = \"example1.org\" \"1\" = \"example2.net\" } } data \"template_file\" \"web_init\" { # Render the template once for each instance count = \"${length(var.hostnames)}\" template = \"${file(\"templates/web_init.tpl\")}\" vars { # count.index tells us the index of the instance we are rendering hostname = \"${var.hostnames[count.index]}\" } } resource \"aws_instance\" \"web\" { # Create one instance for each hostname count = \"${length(var.hostnames)}\" # Pass each instance its corresponding template_file user_data = \"${data.template_file.web_init.*.rendered[count.index]}\" } \nWith this, we will build a list of template_file.web_init data resources which we can use in combination with our list of aws_instance.web resources.\nSimple math can be performed in interpolations:\nvariable \"count\" { default = 2 } resource \"aws_instance\" \"web\" { # ... count = \"${var.count}\" # Tag the instance with a counter starting at 1, ie. web-001 tags { Name = \"${format(\"web-%03d\", count.index + 1)}\" } } \nThe supported operations are:\nAdd (+), Subtract (-), Multiply (*), and Divide (/) for float types\nAdd (+), Subtract (-), Multiply (*), Divide (/), and Modulo (%) for integer types\nOperator precedences is the standard mathematical order of operations: Multiply (*), Divide (/), and Modulo (%) have precedence over Add (+) and Subtract (-). Parenthesis can be used to force ordering.\n\"${2 * 4 + 3 * 3}\" # computes to 17 \"${3 * 3 + 2 * 4}\" # computes to 17 \"${2 * (4 + 3) * 3}\" # computes to 42 \nYou can use the terraform console command to try the math operations.\nNote: Since Terraform allows hyphens in resource and variable names, it's best to use spaces between math operators to prevent confusion or unexpected behavior. For example, ${var.instance-count - 1} will subtract 1 from the instance-count variable value, while ${var.instance-count-1} will interpolate the instance-count-1 variable value."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.1.x/configuration-0-11/variables",
  "text": "Input Variables - 0.11 Configuration Language | Terraform\nNote: This page is about Terraform 0.11 and earlier. For Terraform 0.12 and later, see Configuration Language: Input Variables.\nInput variables serve as parameters for a Terraform module.\nWhen used in the root module of a configuration, variables can be set from CLI arguments and environment variables. For child modules, they allow values to pass from parent to child.\nThis page assumes you're familiar with the configuration syntax already.\nInput variables can be defined as follows:\nvariable \"key\" { type = \"string\" } variable \"images\" { type = \"map\" default = { us-east-1 = \"image-1234\" us-west-2 = \"image-4567\" } } variable \"zones\" { type = \"list\" default = [\"us-east-1a\", \"us-east-1b\"] } \nThe variable block configures a single input variable for a Terraform module. Each block declares a single variable.\nThe name given in the block header is used to assign a value to the variable via the CLI and to reference the variable elsewhere in the configuration.\nWithin the block body (between { }) is configuration for the variable, which accepts the following arguments:\ntype (Optional) - If set this defines the type of the variable. Valid values are string, list, and map. If this field is omitted, the variable type will be inferred based on default. If no default is provided, the type is assumed to be string.\ndefault (Optional) - This sets a default value for the variable. If no default is provided, Terraform will raise an error if a value is not provided by the caller. The default value can be of any of the supported data types, as described below. If type is also set, the given value must be of the specified type.\ndescription (Optional) - A human-friendly description for the variable. This is primarily for documentation for users using your Terraform configuration. When a module is published in Terraform Registry, the given description is shown as part of the documentation.\nThe name of a variable can be any valid identifier. However, due to the interpretation of module configuration blocks, the names source, version and providers are reserved for Terraform's own use and are thus not recommended for any module intended to be used as a child module.\nThe default value of an input variable must be a literal value, containing no interpolation expressions. To assign a name to an expression so that it may be re-used within a module, use Local Values instead.\nStrings\nString values are simple and represent a basic key to value mapping where the key is the variable name. An example is:\nvariable \"key\" { type = \"string\" default = \"value\" } \nA multi-line string value can be provided using heredoc syntax.\nvariable \"long_key\" { type = \"string\" default = <<EOF This is a long key. Running over several lines. EOF } \nTerraform performs automatic conversion from string values to numeric and boolean values based on context, so in practice string variables may be used to set arguments of any primitive type. For boolean values in particular there are some caveats, described under Booleans below.\nMaps\nA map value is a lookup table from string keys to string values. This is useful for selecting a value based on some other provided value.\nA common use of maps is to create a table of machine images per region, as follows:\nvariable \"images\" { type = \"map\" default = { \"us-east-1\" = \"image-1234\" \"us-west-2\" = \"image-4567\" } } \nLists\nA list value is an ordered sequence of strings indexed by integers starting with zero. For example:\nvariable \"users\" { type = \"list\" default = [\"admin\", \"ubuntu\"] } \nBooleans\nAlthough Terraform can automatically convert between boolean and string values, there are some subtle implications of these conversions that should be completely understood when using boolean values with input variables.\nIt is recommended for now to specify boolean values for variables as the strings \"true\" and \"false\", to avoid some caveats in the conversion process. A future version of Terraform will properly support boolean values and so relying on the current behavior could result in backwards-incompatibilities at that time.\nFor a configuration such as the following:\nvariable \"active\" { default = false } \nThe false is converted to a string \"0\" when running Terraform.\nThen, depending on where you specify overrides, the behavior can differ:\nVariables with boolean values in a tfvars file will likewise be converted to \"0\" and \"1\" values.\nVariables specified via the -var command line flag will be literal strings \"true\" and \"false\", so care should be taken to explicitly use \"0\" or \"1\".\nVariables specified with the TF_VAR_ environment variables will be literal string values, just like -var.\nA future version of Terraform will fully support first-class boolean types which will make the behavior of booleans consistent as you would expect. This may break some of the above behavior.\nWhen passing boolean-like variables as parameters to resource configurations that expect boolean values, they are converted consistently:\n\"1\" and \"true\" become true\n\"0\" and \"false\" become false\nThe behavior of conversion in this direction (string to boolean) will not change in future Terraform versions. Therefore, using these string values rather than literal booleans is recommended when using input variables.\nEnvironment variables can be used to set the value of an input variable in the root module. The name of the environment variable must be TF_VAR_ followed by the variable name, and the value is the value of the variable.\nFor example, given the configuration below:\nThe variable can be set via an environment variable:\n$ TF_VAR_image=foo terraform apply \nMaps and lists can be specified using environment variables as well using HCL syntax in the value.\nFor a list variable like so:\nvariable \"somelist\" { type = \"list\" } \nThe variable could be set like so:\n$ TF_VAR_somelist='[\"ami-abc123\", \"ami-bcd234\"]' terraform plan \nSimilarly, for a map declared like:\nvariable \"somemap\" { type = \"map\" } \nThe value can be set like this:\n$ TF_VAR_somemap='{foo = \"bar\", baz = \"qux\"}' terraform plan \nValues for the input variables of a root module can be gathered in variable definition files and passed together using the -var-file=FILE option.\nFor all files which match terraform.tfvars or *.auto.tfvars present in the current directory, Terraform automatically loads them to populate variables. If the file is located somewhere else, you can pass the path to the file using the -var-file flag. It is recommended to name such files with names ending in .tfvars.\nVariables files use HCL or JSON syntax to define variable values. Strings, lists or maps may be set in the same manner as the default value in a variable block in Terraform configuration. For example:\nfoo = \"bar\" xyz = \"abc\" somelist = [ \"one\", \"two\", ] somemap = { foo = \"bar\" bax = \"qux\" } \nThe -var-file flag can be used multiple times per command invocation:\n$ terraform apply -var-file=foo.tfvars -var-file=bar.tfvars \nNote: Variable files are evaluated in the order in which they are specified on the command line. If a particular variable is defined in more than one variable file, the last value specified is effective.\nVariable Merging\nWhen multiple values are provided for the same input variable, map values are merged while all other values are overridden by the last definition.\nFor example, if you define a variable twice on the command line:\n$ terraform apply -var foo=bar -var foo=baz \nThen the value of foo will be baz, since it was the last definition seen.\nHowever, for maps, the values are merged:\n$ terraform apply -var 'foo={quux=\"bar\"}' -var 'foo={bar=\"baz\"}' \nThe resulting value of foo will be:\n{ quux = \"bar\" bar = \"baz\" } \nThere is no way currently to unset map values in Terraform. Whenever a map is modified either via variable input or being passed into a module, the values are always merged.\nVariable Precedence\nBoth these files have the variable baz defined:\nfoo.tfvars\nbar.tfvars\nWhen they are passed in the following order:\n$ terraform apply -var-file=foo.tfvars -var-file=bar.tfvars \nThe result will be that baz will contain the value bar because bar.tfvars has the last definition loaded.\nDefinition files passed using the -var-file flag will always be evaluated after those in the working directory.\nValues passed within definition files or with -var will take precedence over TF_VAR_ environment variables, as environment variables are considered defaults."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/v1.1.x/upgrade-guides/0-12",
  "text": "This page does not exist for version v1.3.x."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/pow",
  "text": "pow - Functions - Configuration Language | Terraform\npow Function\npow calculates an exponent, by raising its first argument to the power of the second argument.\n> pow(3, 2) 9 > pow(4, 0) 1"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/timestamp",
  "text": "timestamp - Functions - Configuration Language | Terraform\ntimestamp returns a UTC timestamp string in RFC 3339 format.\nIn the Terraform language, timestamps are conventionally represented as strings using RFC 3339 \"Date and Time format\" syntax, and so timestamp returns a string in this format.\nThe result of this function will change every second, so using this function directly with resource attributes will cause a diff to be detected on every Terraform run. We do not recommend using this function in resource attributes, but in rare cases it can be used in conjunction with the ignore_changes lifecycle meta-argument to take the timestamp only on initial creation of the resource. For more stable time handling, see the Time Provider.\nDue to the constantly changing return value, the result of this function cannot be predicted during Terraform's planning phase, and so the timestamp will be taken only once the plan is being applied.\n> timestamp() 2018-05-13T07:44:12Z \nformatdate can convert the resulting timestamp to other date and time formats."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/nonsensitive",
  "text": "nonsensitive - Functions - Configuration Language | Terraform\nnonsensitive takes a sensitive value and returns a copy of that value with the sensitive marking removed, thereby exposing the sensitive value.\nWarning: Using this function indiscriminately will cause values that Terraform would normally have considered as sensitive to be treated as normal values and shown clearly in Terraform's output. Use this function only when you've derived a new value from a sensitive value in a way that eliminates the sensitive portions of the value.\nNormally Terraform tracks when you use expressions to derive a new value from a value that is marked as sensitive, so that the result can also be marked as sensitive.\nHowever, you may wish to write expressions that derive non-sensitive results from sensitive values. For example, if you know based on details of your particular system and its threat model that a SHA256 hash of a particular sensitive value is safe to include clearly in Terraform output, you could use the nonsensitive function to indicate that, overriding Terraform's normal conservative behavior:\noutput \"sensitive_example_hash\" { value = nonsensitive(sha256(var.sensitive_example)) } \nAnother example might be if the original value is only partially sensitive and you've written expressions to separate the sensitive and non-sensitive parts:\nvariable \"mixed_content_json\" { description = \"A JSON string containing a mixture of sensitive and non-sensitive values.\" type = string sensitive = true } locals { # mixed_content is derived from var.mixed_content_json, so it # is also considered to be sensitive. mixed_content = jsondecode(var.mixed_content_json) # password_from_json is derived from mixed_content, so it's # also considered to be sensitive. password_from_json = local.mixed_content[\"password\"] # username_from_json would normally be considered to be # sensitive too, but system-specific knowledge tells us # that the username is a non-sensitive fragment of the # original document, and so we can override Terraform's # determination. username_from_json = nonsensitive(local.mixed_content[\"username\"]) } \nWhen you use this function, it's your responsibility to ensure that the expression passed as its argument will remove all sensitive content from the sensitive value it depends on. By passing a value to nonsensitive you are declaring to Terraform that you have done all that is necessary to ensure that the resulting value has no sensitive content, even though it was derived from sensitive content. If a sensitive value appears in Terraform's output due to an inappropriate call to nonsensitive in your module, that's a bug in your module and not a bug in Terraform itself. Use this function sparingly and only with due care.\nnonsensitive will return an error if you pass a value that isn't marked as sensitive, because such a call would be redundant and potentially confusing or misleading to a future maintainer of your module. Use nonsensitive only after careful consideration and with definite intent.\nConsider including a comment adjacent to your call to explain to future maintainers what makes the usage safe and thus what invariants they must take care to preserve under future modifications.\nThe following examples are from terraform console when running in the context of the example above with variable \"mixed_content_json\" and the local value mixed_content, with a valid JSON string assigned to var.mixed_content_json.\n> var.mixed_content_json (sensitive value) > local.mixed_content (sensitive value) > local.mixed_content[\"password\"] (sensitive value) > nonsensitive(local.mixed_content[\"username\"]) \"zqb\" > nonsensitive(\"clear\") Error: Invalid function argument Invalid value for \"value\" parameter: the given value is not sensitive, so this call is redundant. \nNote though that it's always your responsibility to use nonsensitive only when it's safe to do so. If you use nonsensitive with content that ought to be considered sensitive then that content will be disclosed:\n> nonsensitive(var.mixed_content_json) <<EOT { \"username\": \"zqb\", \"password\": \"p4ssw0rd\" } EOT > nonsensitive(local.mixed_content) { \"password\" = \"p4ssw0rd\" \"username\" = \"zqb\" } > nonsensitive(local.mixed_content[\"password\"]) \"p4ssw0rd\""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/sensitive",
  "text": "sensitive - Functions - Configuration Language | Terraform\nsensitive takes any value and returns a copy of it marked so that Terraform will treat it as sensitive, with the same meaning and behavior as for sensitive input variables.\nWherever possible we recommend marking your input variable and/or output value declarations as sensitive directly, instead of using this function, because in that case you can be sure that there is no way to refer to those values without Terraform automatically considering them as sensitive.\nThe sensitive function might be useful in some less-common situations where a sensitive value arises from a definition within your module, such as if you've loaded sensitive data from a file on disk as part of your configuration:\nlocals { sensitive_content = sensitive(file(\"${path.module}/sensitive.txt\")) } \nHowever, we generally don't recommend writing sensitive values directly within your module any of the files you distribute statically as part of that module, because they may be exposed in other ways outside of Terraform's control.\n> sensitive(1) (sensitive value) > sensitive(\"hello\") (sensitive value) > sensitive([]) (sensitive value)"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/v1.1.x/upgrade-guides/0-13",
  "text": "This page does not exist for version v1.3.x."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/tobool",
  "text": "tobool - Functions - Configuration Language | Terraform\ntobool converts its argument to a boolean value.\nOnly boolean values, null, and the exact strings \"true\" and \"false\" can be converted to boolean. All other values will produce an error.\n> tobool(true) true > tobool(\"true\") true > tobool(null) null > tobool(\"no\") Error: Invalid function argument Invalid value for \"v\" parameter: cannot convert \"no\" to bool: only the strings \"true\" or \"false\" are allowed. > tobool(1) Error: Invalid function argument Invalid value for \"v\" parameter: cannot convert number to bool."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/tolist",
  "text": "tolist - Functions - Configuration Language | Terraform\ntolist Function\ntolist converts its argument to a list value.\nPass a set value to tolist to convert it to a list. Since set elements are not ordered, the resulting list will have an undefined order that will be consistent within a particular run of Terraform.\n> tolist([\"a\", \"b\", \"c\"]) [ \"a\", \"b\", \"c\", ] \nSince Terraform's concept of a list requires all of the elements to be of the same type, mixed-typed elements will be converted to the most general type:\n> tolist([\"a\", \"b\", 3]) [ \"a\", \"b\", \"3\", ]"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/tonumber",
  "text": "tonumber - Functions - Configuration Language | Terraform\ntonumber converts its argument to a number value.\nOnly numbers, null, and strings containing decimal representations of numbers can be converted to number. All other values will produce an error.\n> tonumber(1) 1 > tonumber(\"1\") 1 > tonumber(null) null > tonumber(\"no\") Error: Invalid function argument Invalid value for \"v\" parameter: cannot convert \"no\" to number: string must be a decimal representation of a number."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/try",
  "text": "try - Functions - Configuration Language | Terraform\ntry evaluates all of its argument expressions in turn and returns the result of the first one that does not produce any errors.\nThis is a special function that is able to catch errors produced when evaluating its arguments, which is particularly useful when working with complex data structures whose shape is not well-known at implementation time.\nFor example, if some data is retrieved from an external system in JSON or YAML format and then decoded, the result may have attributes that are not guaranteed to be set. We can use try to produce a normalized data structure which has a predictable type that can therefore be used more conveniently elsewhere in the configuration:\nlocals { raw_value = yamldecode(file(\"${path.module}/example.yaml\")) normalized_value = { name = tostring(try(local.raw_value.name, null)) groups = try(local.raw_value.groups, []) } } \nWith the above local value expressions, configuration elsewhere in the module can refer to local.normalized_value attributes without the need to repeatedly check for and handle absent attributes that would otherwise produce errors.\nWe can also use try to deal with situations where a value might be provided in two different forms, allowing us to normalize to the most general form:\nvariable \"example\" { type = any } locals { example = try( [tostring(var.example)], tolist(var.example), ) } \nThe above permits var.example to be either a list or a single string. If it's a single string then it'll be normalized to a single-element list containing that string, again allowing expressions elsewhere in the configuration to just assume that local.example is always a list.\nThis second example contains two expressions that can both potentially fail. For example, if var.example were set to {} then it could be converted to neither a string nor a list. If try exhausts all of the given expressions without any succeeding, it will return an error describing all of the problems it encountered.\nWe strongly suggest using try only in special local values whose expressions perform normalization, so that the error handling is confined to a single location in the module and the rest of the module can just use straightforward references to the normalized structure and thus be more readable for future maintainers.\nThe try function can only catch and handle dynamic errors resulting from access to data that isn't known until runtime. It will not catch errors relating to expressions that can be proven to be invalid for any input, such as a malformed resource reference.\nWarning: The try function is intended only for concise testing of the presence of and types of object attributes. Although it can technically accept any sort of expression, we recommend using it only with simple attribute references and type conversion functions as shown in the examples above. Overuse of try to suppress errors will lead to a configuration that is hard to understand and maintain.\n> local.foo { \"bar\" = \"baz\" } > try(local.foo.bar, \"fallback\") baz > try(local.foo.boop, \"fallback\") fallback \nThe try function will not catch errors relating to constructs that are provably invalid even before dynamic expression evaluation, such as a malformed reference or a reference to a top-level object that has not been declared:\n> try(local.nonexist, \"fallback\") Error: Reference to undeclared local value A local value with the name \"nonexist\" has not been declared. \ncan, which tries evaluating an expression and returns a boolean value indicating whether it succeeded."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/type",
  "text": "type - Functions - Configuration Language | Terraform\nNote: This function is available only in Terraform 1.0 and later.\ntype returns the type of a given value.\nSometimes a Terraform configuration can result in confusing errors regarding inconsistent types. This function displays terraform's evaluation of a given value's type, which is useful in understanding this error message.\nThis is a special function which is only available in the terraform console command. It can only be used to examine the type of a given value, and should not be used in more complex expressions.\nHere we have a conditional output which prints either the value of var.list or a local named default_list:\nvariable \"list\" { default = [] } locals { default_list = [ { foo = \"bar\" map = { bleep = \"bloop\" } }, { beep = \"boop\" }, ] } output \"list\" { value = var.list != [] ? var.list : local.default_list } \nApplying this configuration results in the following error:\nError: Inconsistent conditional result types on main.tf line 18, in output \"list\": 18: value = var.list != [] ? var.list : local.default_list |---------------- | local.default_list is tuple with 2 elements | var.list is empty tuple The true and false result expressions must have consistent types. The given expressions are tuple and tuple, respectively. \nWhile this error message does include some type information, it can be helpful to inspect the exact type that Terraform has determined for each given input. Examining both var.list and local.default_list using the type function provides more context for the error message:\n> type(var.list) tuple > type(local.default_list) tuple([ object({ foo: string, map: object({ bleep: string, }), }), object({ beep: string, }), ])"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/v1.1.x/upgrade-guides/0-13",
  "text": "This page does not exist for version v1.10.x."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/v1.1.x/upgrade-guides/0-13",
  "text": "This page does not exist for version v1.5.x."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/upgrade-guides/0-13",
  "text": "This page does not exist for version v1.2.x."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/v1.1.x/upgrade-guides/0-12",
  "text": "This page does not exist for version v1.10.x."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/tomap",
  "text": "tomap - Functions - Configuration Language | Terraform\ntomap Function\ntomap converts its argument to a map value.\n> tomap({\"a\" = 1, \"b\" = 2}) { \"a\" = 1 \"b\" = 2 } \nSince Terraform's concept of a map requires all of the elements to be of the same type, mixed-typed elements will be converted to the most general type:\n> tomap({\"a\" = \"foo\", \"b\" = true}) { \"a\" = \"foo\" \"b\" = \"true\" }"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/upgrade-guides/0-12",
  "text": "This page does not exist for version v1.2.x."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/defaults",
  "text": "defaults - Functions - Configuration Language | Terraform\nNote: This function is available only in Terraform 0.15 and later.\nExperimental: This function is part of the optional attributes experiment and is only available in modules where the module_variable_optional_attrs experiment is explicitly enabled.\nThe defaults function is a specialized function intended for use with input variables whose type constraints are object types or collections of object types that include optional attributes.\nWhen you define an attribute as optional and the caller doesn't provide an explicit value for it, Terraform will set the attribute to null to represent that it was omitted. If you want to use a placeholder value other than null when an attribute isn't set, you can use the defaults function to concisely assign default values only where an attribute value was set to null.\ndefaults(input_value, defaults) \nThe defaults function expects that the input_value argument will be the value of an input variable with an exact type constraint (not containing any). The function will then visit every attribute in the data structure, including attributes of nested objects, and apply the default values given in the defaults object.\nThe interpretation of attributes in the defaults argument depends on what type an attribute has in the input_value:\nPrimitive types (string, number, bool): if a default value is given then it will be used only if the input_value's attribute of the same name has the value null. The default value's type must match the input value's type.\nStructural types (object and tuple types): Terraform will recursively visit all of the attributes or elements of the nested value and repeat the same defaults-merging logic one level deeper. The default value's type must be of the same kind as the input value's type, and a default value for an object type must only contain attribute names that appear in the input value's type.\nCollection types (list, map, and set types): Terraform will visit each of the collection elements in turn and apply defaults to them. In this case the default value is only a single value to be applied to all elements of the collection, so it must have a type compatible with the collection's element type rather than with the collection type itself.\nThe above rules may be easier to follow with an example. Consider the following Terraform configuration:\nterraform { # Optional attributes and the defaults function are # both experimental, so we must opt in to the experiment. experiments = [module_variable_optional_attrs] } variable \"storage\" { type = object({ name = string enabled = optional(bool) website = object({ index_document = optional(string) error_document = optional(string) }) documents = map( object({ source_file = string content_type = optional(string) }) ) }) } locals { storage = defaults(var.storage, { # If \"enabled\" isn't set then it will default # to true. enabled = true # The \"website\" attribute is required, but # it's here to provide defaults for the # optional attributes inside. website = { index_document = \"index.html\" error_document = \"error.html\" } # The \"documents\" attribute has a map type, # so the default value represents defaults # to be applied to all of the elements in # the map, not for the map itself. Therefore # it's a single object matching the map # element type, not a map itself. documents = { # If _any_ of the map elements omit # content_type then this default will be # used instead. content_type = \"application/octet-stream\" } }) } output \"storage\" { value = local.storage } \nTo test this out, we can create a file terraform.tfvars to provide an example value for var.storage:\nstorage = { name = \"example\" website = { error_document = \"error.txt\" } documents = { \"index.html\" = { source_file = \"index.html.tmpl\" content_type = \"text/html\" } \"error.txt\" = { source_file = \"error.txt.tmpl\" content_type = \"text/plain\" } \"terraform.exe\" = { source_file = \"terraform.exe\" } } } \nThe above value conforms to the variable's type constraint because it only omits attributes that are declared as optional. Terraform will automatically populate those attributes with the value null before evaluating anything else, and then the defaults function in local.storage will substitute default values for each of them.\nThe result of this defaults call would therefore be the following object:\nstorage = { \"documents\" = tomap({ \"error.txt\" = { \"content_type\" = \"text/plain\" \"source_file\" = \"error.txt.tmpl\" } \"index.html\" = { \"content_type\" = \"text/html\" \"source_file\" = \"index.html.tmpl\" } \"terraform.exe\" = { \"content_type\" = \"application/octet-stream\" \"source_file\" = \"terraform.exe\" } }) \"enabled\" = true \"name\" = \"example\" \"website\" = { \"error_document\" = \"error.txt\" \"index_document\" = \"index.html\" } } \nNotice that enabled and website.index_document were both populated directly from the defaults. Notice also that the \"terraform.exe\" element of documents had its content_type attribute populated from the documents default, but the default value didn't need to predict that there would be an element key \"terraform.exe\" because the default values apply equally to all elements of the map where the optional attributes are null.\nThe design of the defaults function depends on input values having well-specified type constraints, so it can reliably recognize the difference between similar types: maps vs. objects, lists vs. tuples. The type constraint causes Terraform to convert the caller's value to conform to the constraint and thus defaults can rely on the input to conform.\nElsewhere in the Terraform language it's typical to be less precise about types, for example using the object construction syntax { ... } to construct values that will be used as if they are maps. Because defaults uses the type information of input_value, an input_value that doesn't originate in an input variable will tend not to have an appropriate value type and will thus not be interpreted as expected by defaults.\nWe recommend using defaults only with fully-constrained input variable values in the first argument, so you can use the variable's type constraint to explicitly distinguish between collection and structural types."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/v1.1.x/upgrade-guides/0-12",
  "text": "This page does not exist for version v1.5.x."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/nonsensitive",
  "text": "nonsensitive will return an error if you pass a value that isn't marked as sensitive, because such a call would be redundant and potentially confusing or misleading to a future maintainer of your module. Use nonsensitive only after careful consideration and with definite intent.\n> var.mixed_content_json (sensitive) > local.mixed_content (sensitive) > local.mixed_content[\"password\"] (sensitive) > nonsensitive(local.mixed_content[\"username\"]) \"zqb\" > nonsensitive(\"clear\") Error: Invalid function argument Invalid value for \"value\" parameter: the given value is not sensitive, so this call is redundant. "
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/sensitive",
  "text": "> sensitive(1) (sensitive) > sensitive(\"hello\") (sensitive) > sensitive([]) (sensitive)"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/timestamp",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/pow",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/tolist",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/tomap",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/tobool",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/try",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/tonumber",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/pow",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/type",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/pow",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/pow",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/timestamp",
  "text": "plantimestamp will return a consistent timestamp representing the date and time during the plan."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202310-1/replicated/architecture/system-overview/capacity",
  "text": "Capacity and Performance - System Overview - Terraform Enterprise | Terraform\nThe maximum capacity and performance of Terraform Enterprise is dependent entirely on the resources provided by the Linux instance it is installed on. There are a few settings that allow Terraform Enterprise's capacity to be adjusted to suit the instance.\nThe amount of memory to allocate to a Terraform run and the number of concurrent runs are the primary elements in understanding capacity above the base services.\nBy default, Terraform Enterprise allocates 512 MB of memory to each Terraform run, with a default concurrency of 10 parallel runs. Therefore, by default Terraform Enterprise requires 5.2 GB of memory reserved for runs.\nAfter factoring in the memory needed to run the base services that make up the application, the default memory footprint of Terraform Enterprise is approximately 4 GB.\nSettings\nThe settings for per-run memory and concurrency are available in the dashboard on port 8800, on the Settings page, under the Capacity section. They can also be set via the application settings JSON file when using the automated install procedure.\nTo increase the number of concurrent runs, adjust the capacity_concurrency setting. This setting is not limited by system checks; it depends on the operator to provide enough memory to the system to accommodate the requested concurrent capacity. For example, if capacity_concurrency is set to 100 and the worker memory is set to 512, the instance would require a minimum of 52 GB of memory just for Terraform runs. The rest of the TFE application requires a minimum of 4 GB of memory in addition to the Operating System requirements.\nThe default memory limit of 512 MB per Terraform run is also configurable. Note that this setting is not limited by system checks; it depends on the operator to provide enough memory to the system to accommodate the requested limits. If the memory limit is adjusted to 1024 MB with the default capacity of 10, the instance would require, at a minimum, 10 GB of memory reserved for Terraform runs.\nDownward Adjustment\nWe do not recommend adjusting the memory limit below 512 MB. Memory is Terraform's primary resource and it becomes easy for it to go above smaller limits and be terminated mid-run by the Linux kernel.\nThe required CPU resources for an individual Terraform run vary considerably, but in general they are a much more minor factor than memory due to Terraform mostly waiting on IO from APIs to return.\nOur rule of thumb is 10 Terraform runs per CPU core, with 2 CPU cores allocated for the base Terraform Enterprise services. So a 4-core instance with 16 GB of memory could comfortably run 20 Terraform runs, if the runs are allocated the default 512 MB each.\nAs of the v202109-1 TFE release, you can use the capacity_cpus Replicated configuration option to set the maximum number of CPU cores that can be allocated to a Terraform run. When capacity_cpus is set, the configuration places a hard quota on the number of cores that a Terraform operation and underlying provider plugin logic can consume. This can be an effective tool to prevent one expensive workspace from monopolizing the CPU resources of the host.\nThe amount of disk storage available to a system plays a small role in the capacity of an instance. A root volume with 200 GB of storage can sustain a capacity well over 100 concurrent runs.\nBecause of the amount of churn caused by container creation as well as Terraform state management, highly concurrent setups will begin pushing hard on disk I/O. In cloud environments like AWS that limit disk I/O to IOPS that are credited per disk, it's important to provision a minimum number to prevent I/O related stalls. Low disk I/O can create significant performance issues.\nThis resource is harder to predict than memory or CPU usage because it varies per Terraform module, but we generally recommend a minimum of 50 IOPS per concurrent Terraform run. So if an instance is configured for 10 concurrent runs, the disk should have 500 IOPS allocated. For reference, on AWS, an EBS volume with an allocated size of 250 GB comes with a steady state of 750 IOPS.\nWe recommend using a disk with a minimum of 500 IOPS, but high load systems should consider increasing this significantly. For example, a production instance with a consistently high level of utilization and a concurrency of 10 should ideally have a disk with about 3,000 IOPS. Internal testing has shown performance increases with additional IOPs up to 8,000. Scaling the disk beyond 8,000 IOPs does not significantly improve performance."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202311-1/replicated/architecture/system-overview/capacity",
  "text": "Capacity and Performance - System Overview - Terraform Enterprise | Terraform\nThe maximum capacity and performance of Terraform Enterprise is dependent entirely on the resources provided by the Linux instance it is installed on. There are a few settings that allow Terraform Enterprise's capacity to be adjusted to suit the instance.\nThe amount of memory to allocate to a Terraform run and the number of concurrent runs are the primary elements in understanding capacity above the base services.\nBy default, Terraform Enterprise allocates 512 MB of memory to each Terraform run, with a default concurrency of 10 parallel runs. Therefore, by default Terraform Enterprise requires 5.2 GB of memory reserved for runs.\nAfter factoring in the memory needed to run the base services that make up the application, the default memory footprint of Terraform Enterprise is approximately 4 GB.\nSettings\nThe settings for per-run memory and concurrency are available in the dashboard on port 8800, on the Settings page, under the Capacity section. They can also be set via the application settings JSON file when using the automated install procedure.\nTo increase the number of concurrent runs, adjust the capacity_concurrency setting. This setting is not limited by system checks; it depends on the operator to provide enough memory to the system to accommodate the requested concurrent capacity. For example, if capacity_concurrency is set to 100 and the worker memory is set to 512, the instance would require a minimum of 52 GB of memory just for Terraform runs. The rest of the TFE application requires a minimum of 4 GB of memory in addition to the Operating System requirements.\nThe default memory limit of 512 MB per Terraform run is also configurable. Note that this setting is not limited by system checks; it depends on the operator to provide enough memory to the system to accommodate the requested limits. If the memory limit is adjusted to 1024 MB with the default capacity of 10, the instance would require, at a minimum, 10 GB of memory reserved for Terraform runs.\nDownward Adjustment\nWe do not recommend adjusting the memory limit below 512 MB. Memory is Terraform's primary resource and it becomes easy for it to go above smaller limits and be terminated mid-run by the Linux kernel.\nThe required CPU resources for an individual Terraform run vary considerably, but in general they are a much more minor factor than memory due to Terraform mostly waiting on IO from APIs to return.\nOur rule of thumb is 10 Terraform runs per CPU core, with 2 CPU cores allocated for the base Terraform Enterprise services. So a 4-core instance with 16 GB of memory could comfortably run 20 Terraform runs, if the runs are allocated the default 512 MB each.\nAs of the v202109-1 TFE release, you can use the capacity_cpus Replicated configuration option to set the maximum number of CPU cores that can be allocated to a Terraform run. When capacity_cpus is set, the configuration places a hard quota on the number of cores that a Terraform operation and underlying provider plugin logic can consume. This can be an effective tool to prevent one expensive workspace from monopolizing the CPU resources of the host.\nThe amount of disk storage available to a system plays a small role in the capacity of an instance. A root volume with 200 GB of storage can sustain a capacity well over 100 concurrent runs.\nBecause of the amount of churn caused by container creation as well as Terraform state management, highly concurrent setups will begin pushing hard on disk I/O. In cloud environments like AWS that limit disk I/O to IOPS that are credited per disk, it's important to provision a minimum number to prevent I/O related stalls. Low disk I/O can create significant performance issues.\nThis resource is harder to predict than memory or CPU usage because it varies per Terraform module, but we generally recommend a minimum of 50 IOPS per concurrent Terraform run. So if an instance is configured for 10 concurrent runs, the disk should have 500 IOPS allocated. For reference, on AWS, an EBS volume with an allocated size of 250 GB comes with a steady state of 750 IOPS.\nWe recommend using a disk with a minimum of 500 IOPS, but high load systems should consider increasing this significantly. For example, a production instance with a consistently high level of utilization and a concurrency of 10 should ideally have a disk with about 3,000 IOPS. Internal testing has shown performance increases with additional IOPs up to 8,000. Scaling the disk beyond 8,000 IOPs does not significantly improve performance."
},
{
  "url": "https://developer.hashicorp.com/terraform/enterprise/v202309-1/replicated/architecture/system-overview/capacity",
  "text": "Capacity and Performance - System Overview - Terraform Enterprise | Terraform\nThe maximum capacity and performance of Terraform Enterprise is dependent entirely on the resources provided by the Linux instance it is installed on. There are a few settings that allow Terraform Enterprise's capacity to be adjusted to suit the instance.\nThe amount of memory to allocate to a Terraform run and the number of concurrent runs are the primary elements in understanding capacity above the base services.\nBy default, Terraform Enterprise allocates 512 MB of memory to each Terraform run, with a default concurrency of 10 parallel runs. Therefore, by default Terraform Enterprise requires 5.2 GB of memory reserved for runs.\nAfter factoring in the memory needed to run the base services that make up the application, the default memory footprint of Terraform Enterprise is approximately 4 GB.\nSettings\nThe settings for per-run memory and concurrency are available in the dashboard on port 8800, on the Settings page, under the Capacity section. They can also be set via the application settings JSON file when using the automated install procedure.\nTo increase the number of concurrent runs, adjust the capacity_concurrency setting. This setting is not limited by system checks; it depends on the operator to provide enough memory to the system to accommodate the requested concurrent capacity. For example, if capacity_concurrency is set to 100 and the worker memory is set to 512, the instance would require a minimum of 52 GB of memory just for Terraform runs. The rest of the TFE application requires a minimum of 4 GB of memory in addition to the Operating System requirements.\nThe default memory limit of 512 MB per Terraform run is also configurable. Note that this setting is not limited by system checks; it depends on the operator to provide enough memory to the system to accommodate the requested limits. If the memory limit is adjusted to 1024 MB with the default capacity of 10, the instance would require, at a minimum, 10 GB of memory reserved for Terraform runs.\nDownward Adjustment\nWe do not recommend adjusting the memory limit below 512 MB. Memory is Terraform's primary resource and it becomes easy for it to go above smaller limits and be terminated mid-run by the Linux kernel.\nThe required CPU resources for an individual Terraform run vary considerably, but in general they are a much more minor factor than memory due to Terraform mostly waiting on IO from APIs to return.\nOur rule of thumb is 10 Terraform runs per CPU core, with 2 CPU cores allocated for the base Terraform Enterprise services. So a 4-core instance with 16 GB of memory could comfortably run 20 Terraform runs, if the runs are allocated the default 512 MB each.\nAs of the v202109-1 TFE release, you can use the capacity_cpus Replicated configuration option to set the maximum number of CPU cores that can be allocated to a Terraform run. When capacity_cpus is set, the configuration places a hard quota on the number of cores that a Terraform operation and underlying provider plugin logic can consume. This can be an effective tool to prevent one expensive workspace from monopolizing the CPU resources of the host.\nThe amount of disk storage available to a system plays a small role in the capacity of an instance. A root volume with 200 GB of storage can sustain a capacity well over 100 concurrent runs.\nBecause of the amount of churn caused by container creation as well as Terraform state management, highly concurrent setups will begin pushing hard on disk I/O. In cloud environments like AWS that limit disk I/O to IOPS that are credited per disk, it's important to provision a minimum number to prevent I/O related stalls. Low disk I/O can create significant performance issues.\nThis resource is harder to predict than memory or CPU usage because it varies per Terraform module, but we generally recommend a minimum of 50 IOPS per concurrent Terraform run. So if an instance is configured for 10 concurrent runs, the disk should have 500 IOPS allocated. For reference, on AWS, an EBS volume with an allocated size of 250 GB comes with a steady state of 750 IOPS.\nWe recommend using a disk with a minimum of 500 IOPS, but high load systems should consider increasing this significantly. For example, a production instance with a consistently high level of utilization and a concurrency of 10 should ideally have a disk with about 3,000 IOPS. Internal testing has shown performance increases with additional IOPs up to 8,000. Scaling the disk beyond 8,000 IOPs does not significantly improve performance."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/timestamp",
  "text": "plantimestamp will return a consistent timestamp representing the date and time during the plan."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/functions/plantimestamp",
  "text": "plantimestamp - Functions - Configuration Language | Terraform\nNote: This function is only available in Terraform v1.5 and later.\nplantimestamp returns a UTC timestamp string in RFC 3339 format.\nIn the Terraform language, timestamps are conventionally represented as strings using RFC 3339 \"Date and Time format\" syntax, and so plantimestamp returns a string in this format.\nThe result of this function will change for every plan operation. It is intended for use within Custom Conditions as a way to validate time sensitive resources such as TLS certificates.\nThere are circumstances, such as during a Terraform Refresh-only plan, where the value for this function will be recomputed but not propagated to resources defined within the configuration. As such, it is recommended that this function only be used to compare against timestamps exported by providers and not against timestamps generated in the configuration.\nThe plantimestamp function is not available within the Terraform console.\n> plantimestamp() 2018-05-13T07:44:12Z \ncheck \"terraform_io_certificate\" { data \"tls_certificate\" \"terraform_io\" { url = \"https://www.terraform.io/\" } assert { condition = timecmp(plantimestamp(), data.tls_certificate.terraform_io.certificates[0].not_after) < 0 error_message = \"terraform.io certificate has expired\" } } \ntimestamp returns the current timestamp when it is evaluated during the apply step."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/functions/timeadd",
  "text": "timeadd function reference - Functions - Configuration Language | Terraform\nThis topic provices reference information about the timeadd function. timeadd adds a duration to a timestamp, returning a new timestamp.\nThe Terraform language represents timestamps as strings using RFC 3339's Date and Time format. timeadd requires that the timestamp argument is a string conforming to the Date and Time syntax.\nUse the timeadd function with the following syntax:\ntimeadd(timestamp, duration) \ntimestamp is a string representation of a date in RFC 3339 format. Refer to the external RFC 3339's Internet Date/Time Format section for how to construct a timestamp string.\nduration is a string representation of a time difference. This string consists of sequences of number and unit pairs, such as \"1.5h\" or \"1h30m\". You may use the following units:\nns: nanosecond\nus or s: microsecond\nms: millisecond\ns: second\nm: minute\nh: hour\nTo indicate a negative duration, make the first number negative, such as \"-2h5m\".\nThe timeadd result is a string, also in RFC 3339 format, representing the result of adding the given duration to the given timestamp.\nThis example adds ten minutes.\n> timeadd(\"2024-08-16T12:45:05Z\", \"10m\") \"2024-08-16T12:55:05Z\" \nThis example subtracts ten minutes by using a negative duration.\n> timeadd(\"2024-08-16T12:45:05Z\", \"-10m\") \"2024-08-16T12:35:05Z\" \ntimecmp determines an ordering for two timestamps."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/timestamp",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/functions/timecmp",
  "text": "timecmp - Functions - Configuration Language | Terraform\ntimecmp compares two timestamps and returns a number that represents the ordering of the instants those timestamps represent.\ntimecmp(timestamp_a, timestamp_b) \nConditionReturn Value\ntimestamp_a is before timestamp_b\t-1\t\ntimestamp_a is the same instant as timestamp_b\t0\t\ntimestamp_a is after timestamp_b\t1\t\nWhen comparing the timestamps, timecmp takes into account the UTC offsets given in each timestamp. For example, 06:00:00+0200 and 04:00:00Z are the same instant after taking into account the +0200 offset on the first timestamp.\nIn the Terraform language, timestamps are conventionally represented as strings using RFC 3339 \"Date and Time format\" syntax. timecmp requires the its two arguments to both be strings conforming to this syntax.\n> timecmp(\"2017-11-22T00:00:00Z\", \"2017-11-22T00:00:00Z\") 0 > timecmp(\"2017-11-22T00:00:00Z\", \"2017-11-22T01:00:00Z\") -1 > timecmp(\"2017-11-22T01:00:00Z\", \"2017-11-22T00:00:00Z\") 1 > timecmp(\"2017-11-22T01:00:00Z\", \"2017-11-22T00:00:00-01:00\") 0 \ntimecmp can be particularly useful in defining custom condition checks that involve a specified timestamp being within a particular range. For example, the following resource postcondition would raise an error if a TLS certificate (or other expiring object) expires sooner than 30 days from the time of the \"apply\" step:\nlifecycle { postcondition { condition = timecmp(timestamp(), timeadd(self.expiration_timestamp, \"-720h\")) < 0 error_message = \"Certificate will expire in less than 30 days.\" } } \ntimestamp returns the current timestamp when it is evaluated during the apply step.\ntimeadd can perform arithmetic on timestamps by adding or removing a specified duration."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/expressions/operators",
  "text": "Operators - Configuration Language | Terraform\nArithmetic and Logical Operators\nAn operator is a type of expression that transforms or combines one or more other expressions. Operators either combine two values in some way to produce a third result value, or transform a single given value to produce a single result.\nOperators that work on two values place an operator symbol between the two values, similar to mathematical notation: 1 + 2. Operators that work on only one value place an operator symbol before that value, like !true.\nThe Terraform language has a set of operators for both arithmetic and logic, which are similar to operators in programming languages such as JavaScript or Ruby.\nWhen multiple operators are used together in an expression, they are evaluated in the following order of operations:\n!, - (multiplication by -1)\n*, /, %\n+, - (subtraction)\n>, >=, <, <=\n==, !=\n&&\n||\nUse parentheses to override the default order of operations. Without parentheses, higher levels will be evaluated first, so Terraform will interpret 1 + 2 * 3 as 1 + (2 * 3) and not as (1 + 2) * 3.\nThe different operators can be gathered into a few different groups with similar behavior, as described below. Each group of operators expects its given values to be of a particular type. Terraform will attempt to convert values to the required type automatically, or will produce an error message if automatic conversion is impossible.\nThe arithmetic operators all expect number values and produce number values as results:\na + b returns the result of adding a and b together.\na - b returns the result of subtracting b from a.\na * b returns the result of multiplying a and b.\na / b returns the result of dividing a by b.\na % b returns the remainder of dividing a by b. This operator is generally useful only when used with whole numbers.\n-a returns the result of multiplying a by -1.\nTerraform supports some other less-common numeric operations as functions. For example, you can calculate exponents using the pow function.\nThe equality operators both take two values of any type and produce boolean values as results.\na == b returns true if a and b both have the same type and the same value, or false otherwise.\na != b is the opposite of a == b.\nBecause the equality operators require both arguments to be of exactly the same type in order to decide equality, we recommend using these operators only with values of primitive types or using explicit type conversion functions to indicate which type you are intending to use for comparison.\nComparisons between structural types may produce surprising results if you are not sure about the types of each of the arguments. For example, var.list == [] may seem like it would return true if var.list were an empty list, but [] actually builds a value of type tuple([]) and so the two values can never match. In this situation it's often clearer to write length(var.list) == 0 instead.\nThe comparison operators all expect number values and produce boolean values as results.\na < b returns true if a is less than b, or false otherwise.\na <= b returns true if a is less than or equal to b, or false otherwise.\na > b returns true if a is greater than b, or false otherwise.\na >= b returns true if a is greater than or equal to b, or false otherwise.\nThe logical operators all expect bool values and produce bool values as results.\na || b returns true if either a or b is true, or false if both are false.\na && b returns true if both a and b are true, or false if either one is false.\n!a returns true if a is false, and false if a is true.\nTerraform does not have an operator for the \"exclusive OR\" operation. If you know that both operators are boolean values then exclusive OR is equivalent to the != (\"not equal\") operator.\nThe logical operators in Terraform do not short-circuit, meaning var.foo || var.foo.bar will produce an error message if var.foo is null because both var.foo and var.foo.bar are evaluated."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/expressions/operators",
  "text": "Operators - Configuration Language | Terraform\nArithmetic and Logical Operators\nAn operator is a type of expression that transforms or combines one or more other expressions. Operators either combine two values in some way to produce a third result value, or transform a single given value to produce a single result.\nOperators that work on two values place an operator symbol between the two values, similar to mathematical notation: 1 + 2. Operators that work on only one value place an operator symbol before that value, like !true.\nThe Terraform language has a set of operators for both arithmetic and logic, which are similar to operators in programming languages such as JavaScript or Ruby.\nWhen multiple operators are used together in an expression, they are evaluated in the following order of operations:\n!, - (multiplication by -1)\n*, /, %\n+, - (subtraction)\n>, >=, <, <=\n==, !=\n&&\n||\nUse parentheses to override the default order of operations. Without parentheses, higher levels will be evaluated first, so Terraform will interpret 1 + 2 * 3 as 1 + (2 * 3) and not as (1 + 2) * 3.\nThe different operators can be gathered into a few different groups with similar behavior, as described below. Each group of operators expects its given values to be of a particular type. Terraform will attempt to convert values to the required type automatically, or will produce an error message if automatic conversion is impossible.\nThe arithmetic operators all expect number values and produce number values as results:\na + b returns the result of adding a and b together.\na - b returns the result of subtracting b from a.\na * b returns the result of multiplying a and b.\na / b returns the result of dividing a by b.\na % b returns the remainder of dividing a by b. This operator is generally useful only when used with whole numbers.\n-a returns the result of multiplying a by -1.\nTerraform supports some other less-common numeric operations as functions. For example, you can calculate exponents using the pow function.\nThe equality operators both take two values of any type and produce boolean values as results.\na == b returns true if a and b both have the same type and the same value, or false otherwise.\na != b is the opposite of a == b.\nBecause the equality operators require both arguments to be of exactly the same type in order to decide equality, we recommend using these operators only with values of primitive types or using explicit type conversion functions to indicate which type you are intending to use for comparison.\nComparisons between structural types may produce surprising results if you are not sure about the types of each of the arguments. For example, var.list == [] may seem like it would return true if var.list were an empty list, but [] actually builds a value of type tuple([]) and so the two values can never match. In this situation it's often clearer to write length(var.list) == 0 instead.\nThe comparison operators all expect number values and produce boolean values as results.\na < b returns true if a is less than b, or false otherwise.\na <= b returns true if a is less than or equal to b, or false otherwise.\na > b returns true if a is greater than b, or false otherwise.\na >= b returns true if a is greater than or equal to b, or false otherwise.\nThe logical operators all expect bool values and produce bool values as results.\na || b returns true if either a or b is true, or false if both are false.\na && b returns true if both a and b are true, or false if either one is false.\n!a returns true if a is false, and false if a is true.\nTerraform does not have an operator for the \"exclusive OR\" operation. If you know that both operators are boolean values then exclusive OR is equivalent to the != (\"not equal\") operator.\nThe logical operators in Terraform do not short-circuit, meaning var.foo || var.foo.bar will produce an error message if var.foo is null because both var.foo and var.foo.bar are evaluated."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/expressions/function-calls",
  "text": "Function Calls - Configuration Language | Terraform\nHands-on: Try the Perform Dynamic Operations with Functions tutorial.\nThe Terraform language has a number of built-in functions that can be used in expressions to transform and combine values. These are similar to the operators but all follow a common syntax:\n<FUNCTION NAME>(<ARGUMENT 1>, <ARGUMENT 2>) \nThe function name specifies which function to call. Each defined function expects a specific number of arguments with specific value types, and returns a specific value type as a result.\nSome functions take an arbitrary number of arguments. For example, the min function takes any amount of number arguments and returns the one that is numerically smallest:\nA function call expression evaluates to the function's return value.\nFor a full list of available functions, see the function reference.\nIf the arguments to pass to a function are available in a list or tuple value, that value can be expanded into separate arguments. Provide the list value as an argument and follow it with the ... symbol:\nThe expansion symbol is three periods (...), not a Unicode ellipsis character (). Expansion is a special syntax that is only available in function calls.\nWhen using sensitive data, such as an input variable or an output defined as sensitive as function arguments, the result of the function call will be marked as sensitive.\nThis is a conservative behavior that is true irrespective of the function being called. For example, passing an object containing a sensitive input variable to the keys() function will result in a list that is sensitive:\n> local.baz { \"a\" = (sensitive value) \"b\" = \"dog\" } > keys(local.baz) (sensitive value) \nMost of Terraform's built-in functions are, in programming language terms, pure functions. This means that their result is based only on their arguments and so it doesn't make any practical difference when Terraform would call them.\nHowever, a small subset of functions interact with outside state and so for those it can be helpful to know when Terraform will call them in relation to other events that occur in a Terraform run.\nThe small set of special functions includes file, templatefile, timestamp, and uuid. If you are not working with these functions then you don't need to read this section, although the information here may still be interesting background information.\nThe file and templatefile functions are intended for reading files that are included as a static part of the configuration and so Terraform will execute these functions as part of initial configuration validation, before taking any other actions with the configuration. That means you cannot use either function to read files that your configuration might generate dynamically on disk as part of the plan or apply steps.\nThe timestamp function returns a representation of the current system time at the point when Terraform calls it, and the uuid function returns a random result which differs on each call. Without any special behavior, these would both cause the final configuration during the apply step not to match the actions shown in the plan, which violates the Terraform execution model.\nFor that reason, Terraform arranges for both of those functions to produce unknown value results during the plan step, with the real result being decided only during the apply step. For timestamp in particular, this means that the recorded time will be the instant when Terraform began applying the change, rather than when Terraform planned the change.\nFor more details on the behavior of these functions, refer to their own documentation pages."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/v1.3.x/upgrade-guides",
  "text": "This page does not exist for version v1.10.x."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/v1.2.x/settings/backends/configuration",
  "text": "This page does not exist for version v1.10.x."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/expressions/types",
  "text": "Types and Values - Configuration Language | Terraform\nThe result of an expression is a value. All values have a type, which dictates where that value can be used and what transformations can be applied to it.\nThe Terraform language uses the following types for its values:\nstring: a sequence of Unicode characters representing some text, like \"hello\".\nnumber: a numeric value. The number type can represent both whole numbers like 15 and fractional values like 6.283185.\nbool: a boolean value, either true or false. bool values can be used in conditional logic.\nlist (or tuple): a sequence of values, like [\"us-west-1a\", \"us-west-1c\"]. Identify elements in a list with consecutive whole numbers, starting with zero.\nset: a collection of unique values that do not have any secondary identifiers or ordering.\nmap (or object): a group of values identified by named labels, like {name = \"Mabel\", age = 52}.\nStrings, numbers, and bools are sometimes called primitive types. Lists/tuples and maps/objects are sometimes called complex types, structural types, or collection types. See Type Constraints for a more detailed description of complex types.\nFinally, there is one special value that has no type:\nnull: a value that represents absence or omission. If you set an argument of a resource to null, Terraform behaves as though you had completely omitted it  it will use the argument's default value if it has one, or raise an error if the argument is mandatory. null is most useful in conditional expressions, so you can dynamically omit an argument if a condition isn't met.\nA literal expression is an expression that directly represents a particular constant value. Terraform has a literal expression syntax for each of the value types described above.\nStrings\nStrings are usually represented by a double-quoted sequence of Unicode characters, \"like this\". There is also a \"heredoc\" syntax for more complex strings.\nString literals are the most complex kind of literal expression in Terraform, and have their own page of documentation. See Strings for information about escape sequences, the heredoc syntax, interpolation, and template directives.\nNumbers\nNumbers are represented by unquoted sequences of digits with or without a decimal point, like 15 or 6.283185.\nBools\nBools are represented by the unquoted symbols true and false.\nNull\nThe null value is represented by the unquoted symbol null.\nLists/Tuples\nLists/tuples are represented by a pair of square brackets containing a comma-separated sequence of values, like [\"a\", 15, true].\nList literals can be split into multiple lines for readability, but always require a comma between values. A comma after the final value is allowed, but not required. Values in a list can be arbitrary expressions.\nMaps/Objects\nMaps/objects are represented by a pair of curly braces containing a series of <KEY> = <VALUE> pairs:\n{ name = \"John\" age = 52 } \nKey/value pairs can be separated by either a comma or a line break.\nThe values in a map can be arbitrary expressions.\nThe keys in a map must be strings; they can be left unquoted if they are a valid identifier, but must be quoted otherwise. You can use a non-literal string expression as a key by wrapping it in parentheses, like (var.business_unit_tag_name) = \"SRE\".\nElements of list/tuple and map/object values can be accessed using the square-bracket index notation, like local.list[3]. The expression within the brackets must be a whole number for list and tuple values or a string for map and object values.\nMap/object attributes with names that are valid identifiers can also be accessed using the dot-separated attribute notation, like local.object.attrname. In cases where a map might contain arbitrary user-specified keys, we recommend using only the square-bracket index notation (local.map[\"keyname\"]).\nIn most situations, lists and tuples behave identically, as do maps and objects. Whenever the distinction isn't relevant, the Terraform documentation uses each pair of terms interchangeably (with a historical preference for \"list\" and \"map\").\nHowever, module authors and provider developers should understand the differences between these similar types (and the related set type), since they offer different ways to restrict the allowed values for input variables and resource arguments.\nFor complete details about these types (and an explanation of why the difference usually doesn't matter), see Type Constraints.\nExpressions are most often used to set values for the arguments of resources and child modules. In these cases, the argument has an expected type and the given expression must produce a value of that type.\nWhere possible, Terraform automatically converts values from one type to another in order to produce the expected type. If this isn't possible, Terraform will produce a type mismatch error and you must update the configuration with a more suitable expression.\nTerraform automatically converts number and bool values to strings when needed. It also converts strings to numbers or bools, as long as the string contains a valid representation of a number or bool value.\ntrue converts to \"true\", and vice-versa\nfalse converts to \"false\", and vice-versa\n15 converts to \"15\", and vice-versa"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/functions/pow",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/issensitive",
  "text": "issensitive - Functions - Configuration Language | Terraform\nNote: This function is only available in Terraform v1.8 and later.\nissensitive takes any value and returns true if Terraform treats it as sensitive, with the same meaning and behavior as for sensitive input variables.\nIf a value not marked as sensitive is passed the function returns false.\nSee sensitive, nonsensitive, and sensitive input variables for more information on sensitive values.\n> issensitive(sensitive(\"secret\")) true > issensitive(\"hello\") false > sensitive(var.my-var-with-sensitive-set-to-true) true"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/nonsensitive",
  "text": "nonsensitive will make no changes to values that aren't marked as sensitive, even though such a call may be redundant and potentially confusing. Use nonsensitive only after careful consideration and with definite intent."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/tobool",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/sensitive",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/tolist",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/tomap",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/tonumber",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/try",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/type",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/nonsensitive",
  "text": "nonsensitive will make no changes to values that aren't marked as sensitive, even though such a call may be redundant and potentially confusing. Use nonsensitive only after careful consideration and with definite intent."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/sensitive",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/tobool",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/tolist",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/tomap",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/tonumber",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/try",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/type",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/nonsensitive",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/sensitive",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/tobool",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/tolist",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/tomap",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/try",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/tonumber",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/type",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.18.x/connect/proxies/envoy-extensions/usage/ext-authz",
  "text": "Delegate authorization to an external service | Consul\nThis topic describes how to use the external authorization Envoy extension to delegate data plane authorization requests to external systems. \nComplete the following steps to use the external authorization extension:\nConfigure an EnvoyExtensions block in a service defaults or proxy defaults configuration entry. \nApply the configuration entry.\nAdd Envoy extension configurations to a proxy defaults or service defaults configuration entry. Place the extension configuration in an EnvoyExtensions block in the configuration entry.\nWhen you configure Envoy extensions on proxy defaults, they apply to every service.\nWhen you configure Envoy extensions on service defaults, they apply to a specific service.\nConsul applies Envoy extensions configured in proxy defaults before it applies extensions in service defaults. As a result, the Envoy extension configuration in service defaults may override configurations in proxy defaults.\nThe following example shows a service defaults configuration entry for the api service that directs the Envoy proxy to make gRPC authorization requests to the authz service:\napi-auth-service-defaults.hcl\nKind = \"service-defaults\" Name = \"api\" EnvoyExtensions = [ { Name = \"builtin/ext-authz\" Arguments = { ProxyType = \"connect-proxy\" Config = { GrpcService = { Target = { Service = { Name = \"authz\" } } } } } } ] \nRefer to the external authorization extension configuration reference for details on how to configure the extension. \nRefer to the proxy defaults configuration entry reference and service defaults configuration entry reference for details on how to define the configuration entries. \nWarning: Adding Envoy extensions default proxy configurations may have unintended consequences. We recommend configuring EnvoyExtensions in service defaults configuration entries in most cases.\nUnsupported Envoy configuration fields\nThe following Envoy configurations are not supported:\nApply the configuration entry\nIf your network is deployed to virtual machines, use the consul config write command and specify the proxy defaults or service defaults configuration entry to apply the configuration. For Kubernetes-orchestrated networks, use the kubectl apply command. The following example applies the extension in a proxy defaults configuration entry.\n$ consul config write api-auth-service-defaults.hcl"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.18.x/connect/proxies/envoy-extensions/usage/lambda",
  "text": "Lambda Envoy Extension | Consul\nInvoke Lambda functions in Envoy proxy\nThe Lambda Envoy extension configures outbound traffic on upstream dependencies allowing mesh services to properly invoke AWS Lambda functions. Lambda functions appear in the catalog as any other Consul service.\nYou can only enable the Lambda extension through service-defaults. This is because the Consul uses the service-defaults configuration entry name as the catalog name for the Lambda functions.\nThe Lambda Envoy extension has the following arguments:\nArgumentsDescription\nARN\tSpecifies the AWS ARN for the service's Lambda.\t\nInvocationMode\tDetermines if Consul configures the Lambda to be invoked using the synchronous or asynchronous invocation mode.\t\nPayloadPassthrough\tDetermines if the body Envoy receives is converted to JSON or directly passed to Lambda.\t\nBe aware that unlike manual lambda registration, region is inferred from the ARN when specified through an Envoy extension.\nThere are two steps to configure the Lambda Envoy extension:\nConfigure EnvoyExtensions through service-defaults. \nApply the configuration entry.\nConfigure EnvoyExtensions\nTo use the Lambda Envoy extension, you must configure and apply a service-defaults configuration entry. Consul uses the name of the entry as the Consul service name for the Lambdas in the catalog. Downstream services also use the name to invoke the Lambda.\nThe following example configures the Lambda Envoy extension to create a service named lambda in the mesh that can invoke the associated Lambda function.\nlambda-envoy-extension.hcl\nKind = \"service-defaults\" Name = \"lambdaInvokingApp\" Protocol = \"http\" EnvoyExtensions { Name = \"builtin/aws/lambda\" Arguments = { ARN = \"arn:aws:lambda:us-west-2:111111111111:function:lambda-1234\" } } \nFor a full list of parameters for EnvoyExtensions, refer to the service-defaults and proxy-defaults configuration entries reference documentation. \nNote: You can only enable the Lambda extension through service-defaults.\nRefer to Configuration specification section to find a full list of arguments for the Lambda Envoy extension.\nApply the configuration entry\nApply the service-defaults configuration entry.\n$ consul config write lambda-envoy-extension.hcl \nIn the following example, the Lambda Envoy extension adds a single Lambda function running in two regions into the mesh. Then, you can use the lambda service name to invoke it, as if it was any other service in the mesh.\nlambda-envoy-extension.json\nKind = \"service-defaults\" Name = \"lambda\" Protocol = \"http\" EnvoyExtensions { Name = \"builtin/aws/lambda\" Arguments = { payloadPassthrough: false arn: arn:aws:lambda:us-west-2:111111111111:function:lambda-1234 } } EnvoyExtensions { Name = \"builtin/aws/lambda\" Arguments = { payloadPassthrough: false arn: arn:aws:lambda:us-east-1:111111111111:function:lambda-1234 } }"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.18.x/connect/proxies/envoy-extensions/usage/otel-access-logging",
  "text": "Send access logs to OpenTelemetry collector service | Consul\nThis topic describes how to use the OpenTelemetry Access Logging Envoy extension to send access logs to OpenTelemetry collector service.\nComplete the following steps to use the OpenTelemetry Access Logging extension:\nConfigure an EnvoyExtensions block in a service defaults or proxy defaults configuration entry.\nApply the configuration entry.\nAdd Envoy extension configurations to a proxy defaults or service defaults configuration entry. Place the extension configuration in an EnvoyExtensions block in the configuration entry.\nWhen you configure Envoy extensions on proxy defaults, they apply to every service.\nWhen you configure Envoy extensions on service defaults, they apply to a specific service.\nConsul applies Envoy extensions configured in proxy defaults before it applies extensions in service defaults. As a result, the Envoy extension configuration in service defaults may override configurations in proxy defaults.\nThe following example shows a service defaults configuration entry for the api service that directs the Envoy proxy to make gRPC OpenTelemetry Access Logging requests to the otel-collector service:\napi-otel-collector-service-defaults.hcl\nKind = \"service-defaults\" Name = \"api\" EnvoyExtensions = [ { Name = \"builtin/otel-access-logging\" Arguments = { ProxyType = \"connect-proxy\" Config = { GrpcService = { Target = { Service = { Name = \"otel-collector\" } } } } } } ] \nRefer to the OpenTelemetry Access Logging extension configuration reference for details on how to configure the extension.\nRefer to the proxy defaults configuration entry reference and service defaults configuration entry reference for details on how to define the configuration entries.\nWarning: Adding Envoy extensions default proxy configurations may have unintended consequences. We recommend configuring EnvoyExtensions in service defaults configuration entries in most cases.\nUnsupported Envoy configuration fields\nThe following Envoy configurations are not supported:\nConfigurationWorkaround\ntransport_api_version\tConsul only supports v3 of the transport API. As a result, there is no workaround for implementing the behavior of this field.\t\nApply the configuration entry\nIf your network is deployed to virtual machines, use the consul config write command and specify the proxy defaults or service defaults configuration entry to apply the configuration. For Kubernetes-orchestrated networks, use the kubectl apply command. The following example applies the extension in a proxy defaults configuration entry.\n$ consul config write api-otel-collector-service-defaults.hcl"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.18.x/connect/proxies/envoy-extensions/usage/lua",
  "text": "Lua Envoy Extension | Consul\nRun Lua scripts in Envoy proxy\nThe Lua Envoy extension enables the HTTP Lua filter in your Consul Envoy proxies, letting you run Lua scripts when requests and responses pass through Consul-generated Envoy resources.\nEnvoy filters support setting and getting dynamic metadata, allowing a filter to share state information with subsequent filters. To set dynamic metadata, configure the HTTP Lua filter. Users can call streamInfo:dynamicMetadata() from Lua scripts to get the request's dynamic metadata. \nTo use the Lua Envoy extension, configure the following arguments in the EnvoyExtensions block:\nProxyType: string | connect-proxy - Determines the proxy type the extension applies to. The only supported value is connect-proxy.\nListenerType: string | required - Specifies if the extension is applied to the inbound or outbound listener.\nScript: string | required - The Lua script that is configured to run by the HTTP Lua filter.\nThere are two steps to configure the Lua Envoy extension:\nConfigure EnvoyExtensions through service-defaults or proxy-defaults. \nApply the configuration entry.\nConfigure EnvoyExtensions\nTo use Envoy extensions, you must configure and apply a proxy-defaults or service-defaults configuration entry with the Envoy extension.\nWhen you configure Envoy extensions on proxy-defaults, they apply to every service.\nWhen you configure Envoy extensions on service-defaults, they apply to a specific service.\nConsul applies Envoy extensions configured in proxy-defaults before it applies extensions in service-defaults. As a result, the Envoy extension configuration in service-defaults may override configurations in proxy-defaults.\nThe following example configures the Lua Envoy extension on every service by using the proxy-defaults.\nlua-envoy-extension-proxy-defaults.hcl\nKind = \"proxy-defaults\" Name = \"global\" Config { protocol = \"http\" } EnvoyExtensions { Name = \"builtin/lua\" Arguments = { ProxyType = \"connect-proxy\" Listener = \"inbound\" Script = <<-EOF function envoy_on_request(request_handle) meta = request_handle:streamInfo():dynamicMetadata() m = meta:get(\"consul\") request_handle:headers():add(\"x-consul-service\", m[\"service\"]) request_handle:headers():add(\"x-consul-namespace\", m[\"namespace\"]) request_handle:headers():add(\"x-consul-datacenter\", m[\"datacenter\"]) request_handle:headers():add(\"x-consul-trust-domain\", m[\"trust-domain\"]) end EOF } } \nFor a full list of parameters for EnvoyExtensions, refer to the service-defaults and proxy-defaults configuration entries reference documentation.\nWarning: Applying EnvoyExtensions to ProxyDefaults may produce unintended consequences. We recommend enabling EnvoyExtensions with ServiceDefaults in most cases.\nRefer to Configuration specification section to find a full list of arguments for the Lua Envoy extension.\nApply the configuration entry\nApply the proxy-defaults or service-defaults configuration entry.\n$ consul config write lua-envoy-extension-proxy-defaults.hcl \nIn the following example, the service-defaults configure the Lua Envoy extension to insert the HTTP Lua filter for service myservice and add the Consul service name to thex-consul-service header for all inbound requests. The ListenerType makes it so that the extension applies only on the inbound listener of the service's connect proxy.\nlua-envoy-extension.hcl\nKind = \"service-defaults\" Name = \"myservice\" EnvoyExtensions = [ { Name = \"builtin/lua\" Arguments = { ProxyType = \"connect-proxy\" Listener = \"inbound\" Script = <<EOF function envoy_on_request(request_handle) local service = request_handle:streamInfo():dynamicMetadata():get(\"consul\")[\"service\"] request_handle:headers():add(\"x-consul-service\", service) end EOF } } ] \nAlternatively, you can apply the same extension configuration to proxy-defaults configuration entries.\nYou can also specify multiple Lua filters through the Envoy extensions. They will not override each other.\nlua-envoy-extension.hcl\nKind = \"service-defaults\" Name = \"myservice\" EnvoyExtensions = [ { Name = \"builtin/lua\", Arguments = { ProxyType = \"connect-proxy\" Listener = \"inbound\" Script = <<-EOF function envoy_on_request(request_handle) meta = request_handle:streamInfo():dynamicMetadata() m = meta:get(\"consul\") request_handle:headers():add(\"x-consul-datacenter\", m[\"datacenter1\"]) end EOF } }, { Name = \"builtin/lua\", Arguments = { ProxyType = \"connect-proxy\" Listener = \"inbound\" Script = <<-EOF function envoy_on_request(request_handle) meta = request_handle:streamInfo():dynamicMetadata() m = meta:get(\"consul\") request_handle:headers():add(\"x-consul-datacenter\", m[\"datacenter2\"]) end EOF } } ]"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.18.x/connect/proxies/envoy-extensions/usage/property-override",
  "text": "Configure Envoy proxy properties | Consul\nThis topic describes how to use the property-override extension to set and remove individual properties for the Envoy resources Consul generates. The extension uses the protoreflect, which enables Consul to dynamically manipulate messages.\nThe extension currently supports setting scalar and enum fields, removing individual fields addressable by Path, and initializing unset intermediate message fields indicated in Path.\nIt currently does not support the following use cases:\nAdding, updating, or removing repeated field members\nAdding or updating protobuf map fields\nAdding or updating protobuf Any fields\nComplete the following steps to use the property-override extension:\nConfigure an EnvoyExtensions block in a service defaults or proxy defaults configuration entry. \nSecurity warning: The property override extension is an advanced feature capable of introducing unintended consequences or reducing cluster security if used incorrectly. Consul does not enforce TLS retention, intentions, or other security-critical components of the Envoy configuration. Additionally, Consul does not verify that the configuration does not contain errors that affect service traffic.\nAdd Envoy extension configurations to a proxy defaults or service defaults configuration entry. Place the extension configuration in an EnvoyExtensions block in the configuration entry.\nWhen you configure Envoy extensions on proxy defaults, they apply to every service.\nWhen you configure Envoy extensions on service defaults, they apply to a specific service.\nConsul applies Envoy extensions configured in proxy defaults before it applies extensions in service defaults. As a result, the Envoy extension configuration in service defaults may override configurations in proxy defaults.\nIn the following proxy defaults configuration entry example, Consul sets the /respect_dns_ttl field on the api service proxy's cluster configuration for the other-svc upstream service:\nproperty-override-extension-service-defaults.hcl\nKind = \"service-defaults\" Name = \"api\" Protocol = \"http\" EnvoyExtensions = [ { Name = \"builtin/property-override\" Arguments = { ProxyType = \"connect-proxy\" Patches = [ { ResourceFilter = { ResourceType = \"cluster\" TrafficDirection = \"outbound\" Services = [{ Name = \"other-svc\" }] } Op = \"add\" Path = \"/respect_dns_ttl\" Value = true } ] } } ] \nRefer to the property override configuration reference for details on how to configure the extension. \nRefer to the proxy defaults configuration entry reference and service defaults configuration entry reference for details on how to define the configuration entries. \nWarning: Adding Envoy extensions default proxy configurations may have unintended consequences. We recommend configuring EnvoyExtensions in service defaults configuration entries in most cases.\nConstructing paths\nTo target the properties for an Envoy resource type, you must specify the path where the properties exist in the Path field of the property override extension configuration. Set the Path field to an empty or partially invalid string when saving the configuration entry and Consul returns an error with a list of supported fields for the first unrecognized segment of the path. By default, Consul only returns the first ten fields, but you can set the Debug field to true to direct Consul to output all possible fields. \nIn the following example, Consul outputs the top-level fields available for the Envoy cluster resource:\nKind = \"service-defaults\" Name = \"api\" EnvoyExtensions = [ { Name = \"builtin/property-override\" Arguments = { Debug = true ProxyType = \"connect-proxy\" Patches = [ { ResourceFilter = { ResourceType = \"cluster\" TrafficDirection = \"outbound\" } Op = \"add\" Path = \"\" Value = 5 } ] } } ] \nAfter applying the configuration entry, Consul prints a message that includes the possible fields for the resource:\n$ consul config write api.hcl non-empty, non-root Path is required; available envoy.config.cluster.v3.Cluster fields: transport_socket_matches name alt_stat_name type cluster_type eds_cluster_config connect_timeout ... \nYou can use the output to help you construct the appropriate value for the Path field. For example:\n$ consul config write api.hcl 2>&1 | grep round_robin round_robin_lb_config \nIf your network is deployed to virtual machines, use the consul config write command and specify the proxy defaults or service defaults configuration entry to apply the configuration. For Kubernetes-orchestrated networks, use the kubectl apply command. The following example applies the extension in a proxy defaults configuration entry.\n$ consul config write property-override-extension-service-defaults.hcl"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.17.x/connect/proxies/envoy-extensions/usage/ext-authz",
  "text": "Delegate authorization to an external service | Consul\nThis topic describes how to use the external authorization Envoy extension to delegate data plane authorization requests to external systems. \nComplete the following steps to use the external authorization extension:\nConfigure an EnvoyExtensions block in a service defaults or proxy defaults configuration entry. \nAdd Envoy extension configurations to a proxy defaults or service defaults configuration entry. Place the extension configuration in an EnvoyExtensions block in the configuration entry.\nWhen you configure Envoy extensions on proxy defaults, they apply to every service.\nWhen you configure Envoy extensions on service defaults, they apply to a specific service.\nConsul applies Envoy extensions configured in proxy defaults before it applies extensions in service defaults. As a result, the Envoy extension configuration in service defaults may override configurations in proxy defaults.\nThe following example shows a service defaults configuration entry for the api service that directs the Envoy proxy to make gRPC authorization requests to the authz service:\napi-auth-service-defaults.hcl\nKind = \"service-defaults\" Name = \"api\" EnvoyExtensions = [ { Name = \"builtin/ext-authz\" Arguments = { ProxyType = \"connect-proxy\" Config = { GrpcService = { Target = { Service = { Name = \"authz\" } } } } } } ] \nRefer to the external authorization extension configuration reference for details on how to configure the extension. \nRefer to the proxy defaults configuration entry reference and service defaults configuration entry reference for details on how to define the configuration entries. \nWarning: Adding Envoy extensions default proxy configurations may have unintended consequences. We recommend configuring EnvoyExtensions in service defaults configuration entries in most cases.\nUnsupported Envoy configuration fields\nThe following Envoy configurations are not supported:\nIf your network is deployed to virtual machines, use the consul config write command and specify the proxy defaults or service defaults configuration entry to apply the configuration. For Kubernetes-orchestrated networks, use the kubectl apply command. The following example applies the extension in a proxy defaults configuration entry.\n$ consul config write api-auth-service-defaults.hcl"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.18.x/connect/proxies/envoy-extensions/usage/wasm",
  "text": "Run WebAssembly plug-ins in Envoy proxy | Consul\nThis topic describes how to use the wasm extension, which directs Consul to run your WebAssembly (Wasm) plug-ins for Envoy proxies. \nYou can create Wasm plugins for Envoy and integrate them using the wasm extension. Wasm is a binary instruction format for stack-based virtual machines that has the potential to run anywhere after it has been compiled. Wasm plug-ins run as filters in a service mesh application's sidecar proxy.\nThe following steps describe the process of integrating Wasm plugins:\nCreate your Wasm plugin. You must ensure that your plugin functions as expected. Refer to the WebAssembly website for information and links to documentation.\nConfigure an EnvoyExtensions block in a service defaults or proxy defaults configuration entry. \nAdd Envoy extension configuration to a proxy defaults or service defaults configuration entry. Place the extension configuration in an EnvoyExtensions block in the configuration entry.\nIn the following example, the extension uses an upstream service named file-server to serve a Wasm-based web application firewall (WAF). \nwasm-extension-serve-waf.hcl\nKind = \"service-defaults\" Name = \"api\" Protocol = \"http\" EnvoyExtensions = [ { Name = \"builtin/wasm\" Arguments = { Protocol = \"http\" ListenerType = \"inbound\" PluginConfig = { VmConfig = { Code = { Remote = { HttpURI = { Service = { Name = \"file-server\" } URI = \"https://file-server/waf.wasm\" } SHA256 = \"c9ef17f48dcf0738b912111646de6d30575718ce16c0cbde3e38b21bb1771807\" } } } Configuration = <<EOF { \"rules\": [ \"Include @demo-conf\", \"Include @crs-setup-demo-conf\", \"SecDebugLogLevel 9\", \"SecRuleEngine On\", \"Include @owasp_crs/*.conf\" ] } EOF } } } ] \nRefer to the Wasm extension configuration reference for details on how to configure the extension. \nRefer to the proxy defaults configuration entry reference and service defaults configuration entry reference for details on how to define the configuration entries. \n$ consul config write wasm-extension-serve-waf.hcl"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.17.x/connect/proxies/envoy-extensions/usage/lambda",
  "text": "Lambda Envoy Extension | Consul\nInvoke Lambda functions in Envoy proxy\nThe Lambda Envoy extension configures outbound traffic on upstream dependencies allowing mesh services to properly invoke AWS Lambda functions. Lambda functions appear in the catalog as any other Consul service.\nYou can only enable the Lambda extension through service-defaults. This is because the Consul uses the service-defaults configuration entry name as the catalog name for the Lambda functions.\nThe Lambda Envoy extension has the following arguments:\nArgumentsDescription\nARN\tSpecifies the AWS ARN for the service's Lambda.\t\nInvocationMode\tDetermines if Consul configures the Lambda to be invoked using the synchronous or asynchronous invocation mode.\t\nPayloadPassthrough\tDetermines if the body Envoy receives is converted to JSON or directly passed to Lambda.\t\nBe aware that unlike manual lambda registration, region is inferred from the ARN when specified through an Envoy extension.\nThere are two steps to configure the Lambda Envoy extension:\nConfigure EnvoyExtensions through service-defaults. \nConfigure EnvoyExtensions\nTo use the Lambda Envoy extension, you must configure and apply a service-defaults configuration entry. Consul uses the name of the entry as the Consul service name for the Lambdas in the catalog. Downstream services also use the name to invoke the Lambda.\nThe following example configures the Lambda Envoy extension to create a service named lambda in the mesh that can invoke the associated Lambda function.\nlambda-envoy-extension.hcl\nKind = \"service-defaults\" Name = \"lambdaInvokingApp\" Protocol = \"http\" EnvoyExtensions { Name = \"builtin/aws/lambda\" Arguments = { ARN = \"arn:aws:lambda:us-west-2:111111111111:function:lambda-1234\" } } \nFor a full list of parameters for EnvoyExtensions, refer to the service-defaults and proxy-defaults configuration entries reference documentation. \nNote: You can only enable the Lambda extension through service-defaults.\nRefer to Configuration specification section to find a full list of arguments for the Lambda Envoy extension.\nApply the service-defaults configuration entry.\n$ consul config write lambda-envoy-extension.hcl \nIn the following example, the Lambda Envoy extension adds a single Lambda function running in two regions into the mesh. Then, you can use the lambda service name to invoke it, as if it was any other service in the mesh.\nlambda-envoy-extension.json\nKind = \"service-defaults\" Name = \"lambda\" Protocol = \"http\" EnvoyExtensions { Name = \"builtin/aws/lambda\" Arguments = { payloadPassthrough: false arn: arn:aws:lambda:us-west-2:111111111111:function:lambda-1234 } } EnvoyExtensions { Name = \"builtin/aws/lambda\" Arguments = { payloadPassthrough: false arn: arn:aws:lambda:us-east-1:111111111111:function:lambda-1234 } }"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.17.x/connect/proxies/envoy-extensions/usage/lua",
  "text": "Lua Envoy Extension | Consul\nRun Lua scripts in Envoy proxy\nThe Lua Envoy extension enables the HTTP Lua filter in your Consul Envoy proxies, letting you run Lua scripts when requests and responses pass through Consul-generated Envoy resources.\nEnvoy filters support setting and getting dynamic metadata, allowing a filter to share state information with subsequent filters. To set dynamic metadata, configure the HTTP Lua filter. Users can call streamInfo:dynamicMetadata() from Lua scripts to get the request's dynamic metadata. \nTo use the Lua Envoy extension, configure the following arguments in the EnvoyExtensions block:\nProxyType: string | connect-proxy - Determines the proxy type the extension applies to. The only supported value is connect-proxy.\nListenerType: string | required - Specifies if the extension is applied to the inbound or outbound listener.\nScript: string | required - The Lua script that is configured to run by the HTTP Lua filter.\nThere are two steps to configure the Lua Envoy extension:\nConfigure EnvoyExtensions through service-defaults or proxy-defaults. \nConfigure EnvoyExtensions\nTo use Envoy extensions, you must configure and apply a proxy-defaults or service-defaults configuration entry with the Envoy extension.\nWhen you configure Envoy extensions on proxy-defaults, they apply to every service.\nWhen you configure Envoy extensions on service-defaults, they apply to a specific service.\nConsul applies Envoy extensions configured in proxy-defaults before it applies extensions in service-defaults. As a result, the Envoy extension configuration in service-defaults may override configurations in proxy-defaults.\nThe following example configures the Lua Envoy extension on every service by using the proxy-defaults.\nlua-envoy-extension-proxy-defaults.hcl\nKind = \"proxy-defaults\" Name = \"global\" Config { protocol = \"http\" } EnvoyExtensions { Name = \"builtin/lua\" Arguments = { ProxyType = \"connect-proxy\" Listener = \"inbound\" Script = <<-EOF function envoy_on_request(request_handle) meta = request_handle:streamInfo():dynamicMetadata() m = meta:get(\"consul\") request_handle:headers():add(\"x-consul-service\", m[\"service\"]) request_handle:headers():add(\"x-consul-namespace\", m[\"namespace\"]) request_handle:headers():add(\"x-consul-datacenter\", m[\"datacenter\"]) request_handle:headers():add(\"x-consul-trust-domain\", m[\"trust-domain\"]) end EOF } } \nFor a full list of parameters for EnvoyExtensions, refer to the service-defaults and proxy-defaults configuration entries reference documentation.\nWarning: Applying EnvoyExtensions to ProxyDefaults may produce unintended consequences. We recommend enabling EnvoyExtensions with ServiceDefaults in most cases.\nRefer to Configuration specification section to find a full list of arguments for the Lua Envoy extension.\nApply the proxy-defaults or service-defaults configuration entry.\n$ consul config write lua-envoy-extension-proxy-defaults.hcl \nIn the following example, the service-defaults configure the Lua Envoy extension to insert the HTTP Lua filter for service myservice and add the Consul service name to thex-consul-service header for all inbound requests. The ListenerType makes it so that the extension applies only on the inbound listener of the service's connect proxy.\nlua-envoy-extension.hcl\nKind = \"service-defaults\" Name = \"myservice\" EnvoyExtensions = [ { Name = \"builtin/lua\" Arguments = { ProxyType = \"connect-proxy\" Listener = \"inbound\" Script = <<EOF function envoy_on_request(request_handle) local service = request_handle:streamInfo():dynamicMetadata():get(\"consul\")[\"service\"] request_handle:headers():add(\"x-consul-service\", service) end EOF } } ] \nAlternatively, you can apply the same extension configuration to proxy-defaults configuration entries.\nYou can also specify multiple Lua filters through the Envoy extensions. They will not override each other.\nlua-envoy-extension.hcl\nKind = \"service-defaults\" Name = \"myservice\" EnvoyExtensions = [ { Name = \"builtin/lua\", Arguments = { ProxyType = \"connect-proxy\" Listener = \"inbound\" Script = <<-EOF function envoy_on_request(request_handle) meta = request_handle:streamInfo():dynamicMetadata() m = meta:get(\"consul\") request_handle:headers():add(\"x-consul-datacenter\", m[\"datacenter1\"]) end EOF } }, { Name = \"builtin/lua\", Arguments = { ProxyType = \"connect-proxy\" Listener = \"inbound\" Script = <<-EOF function envoy_on_request(request_handle) meta = request_handle:streamInfo():dynamicMetadata() m = meta:get(\"consul\") request_handle:headers():add(\"x-consul-datacenter\", m[\"datacenter2\"]) end EOF } } ]"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.17.x/connect/proxies/envoy-extensions/usage/otel-access-logging",
  "text": "Send access logs to OpenTelemetry collector service | Consul\nThis topic describes how to use the OpenTelemetry Access Logging Envoy extension to send access logs to OpenTelemetry collector service.\nComplete the following steps to use the OpenTelemetry Access Logging extension:\nConfigure an EnvoyExtensions block in a service defaults or proxy defaults configuration entry.\nThe following example shows a service defaults configuration entry for the api service that directs the Envoy proxy to make gRPC OpenTelemetry Access Logging requests to the otel-collector service:\napi-otel-collector-service-defaults.hcl\nKind = \"service-defaults\" Name = \"api\" EnvoyExtensions = [ { Name = \"builtin/otel-access-logging\" Arguments = { ProxyType = \"connect-proxy\" Config = { GrpcService = { Target = { Service = { Name = \"otel-collector\" } } } } } } ] \nRefer to the OpenTelemetry Access Logging extension configuration reference for details on how to configure the extension.\nRefer to the proxy defaults configuration entry reference and service defaults configuration entry reference for details on how to define the configuration entries.\nUnsupported Envoy configuration fields\nThe following Envoy configurations are not supported:\nConfigurationWorkaround\ntransport_api_version\tConsul only supports v3 of the transport API. As a result, there is no workaround for implementing the behavior of this field.\t\n$ consul config write api-otel-collector-service-defaults.hcl"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.17.x/connect/proxies/envoy-extensions/usage/property-override",
  "text": "Configure Envoy proxy properties | Consul\nThis topic describes how to use the property-override extension to set and remove individual properties for the Envoy resources Consul generates. The extension uses the protoreflect, which enables Consul to dynamically manipulate messages.\nThe extension currently supports setting scalar and enum fields, removing individual fields addressable by Path, and initializing unset intermediate message fields indicated in Path.\nIt currently does not support the following use cases:\nAdding, updating, or removing repeated field members\nAdding or updating protobuf map fields\nAdding or updating protobuf Any fields\nComplete the following steps to use the property-override extension:\nSecurity warning: The property override extension is an advanced feature capable of introducing unintended consequences or reducing cluster security if used incorrectly. Consul does not enforce TLS retention, intentions, or other security-critical components of the Envoy configuration. Additionally, Consul does not verify that the configuration does not contain errors that affect service traffic.\nIn the following proxy defaults configuration entry example, Consul sets the /respect_dns_ttl field on the api service proxy's cluster configuration for the other-svc upstream service:\nproperty-override-extension-service-defaults.hcl\nKind = \"service-defaults\" Name = \"api\" Protocol = \"http\" EnvoyExtensions = [ { Name = \"builtin/property-override\" Arguments = { ProxyType = \"connect-proxy\" Patches = [ { ResourceFilter = { ResourceType = \"cluster\" TrafficDirection = \"outbound\" Services = [{ Name = \"other-svc\" }] } Op = \"add\" Path = \"/respect_dns_ttl\" Value = true } ] } } ] \nRefer to the property override configuration reference for details on how to configure the extension. \nConstructing paths\nTo target the properties for an Envoy resource type, you must specify the path where the properties exist in the Path field of the property override extension configuration. Set the Path field to an empty or partially invalid string when saving the configuration entry and Consul returns an error with a list of supported fields for the first unrecognized segment of the path. By default, Consul only returns the first ten fields, but you can set the Debug field to true to direct Consul to output all possible fields. \nIn the following example, Consul outputs the top-level fields available for the Envoy cluster resource:\nKind = \"service-defaults\" Name = \"api\" EnvoyExtensions = [ { Name = \"builtin/property-override\" Arguments = { Debug = true ProxyType = \"connect-proxy\" Patches = [ { ResourceFilter = { ResourceType = \"cluster\" TrafficDirection = \"outbound\" } Op = \"add\" Path = \"\" Value = 5 } ] } } ] \nAfter applying the configuration entry, Consul prints a message that includes the possible fields for the resource:\n$ consul config write api.hcl non-empty, non-root Path is required; available envoy.config.cluster.v3.Cluster fields: transport_socket_matches name alt_stat_name type cluster_type eds_cluster_config connect_timeout ... \nYou can use the output to help you construct the appropriate value for the Path field. For example:\n$ consul config write api.hcl 2>&1 | grep round_robin round_robin_lb_config \n$ consul config write property-override-extension-service-defaults.hcl"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.17.x/connect/proxies/envoy-extensions/usage/wasm",
  "text": "Run WebAssembly plug-ins in Envoy proxy | Consul\nThis topic describes how to use the wasm extension, which directs Consul to run your WebAssembly (Wasm) plug-ins for Envoy proxies. \nYou can create Wasm plugins for Envoy and integrate them using the wasm extension. Wasm is a binary instruction format for stack-based virtual machines that has the potential to run anywhere after it has been compiled. Wasm plug-ins run as filters in a service mesh application's sidecar proxy.\nThe following steps describe the process of integrating Wasm plugins:\nCreate your Wasm plugin. You must ensure that your plugin functions as expected. Refer to the WebAssembly website for information and links to documentation.\nAdd Envoy extension configuration to a proxy defaults or service defaults configuration entry. Place the extension configuration in an EnvoyExtensions block in the configuration entry.\nIn the following example, the extension uses an upstream service named file-server to serve a Wasm-based web application firewall (WAF). \nwasm-extension-serve-waf.hcl\nKind = \"service-defaults\" Name = \"api\" Protocol = \"http\" EnvoyExtensions = [ { Name = \"builtin/wasm\" Arguments = { Protocol = \"http\" ListenerType = \"inbound\" PluginConfig = { VmConfig = { Code = { Remote = { HttpURI = { Service = { Name = \"file-server\" } URI = \"https://file-server/waf.wasm\" } SHA256 = \"c9ef17f48dcf0738b912111646de6d30575718ce16c0cbde3e38b21bb1771807\" } } } Configuration = <<EOF { \"rules\": [ \"Include @demo-conf\", \"Include @crs-setup-demo-conf\", \"SecDebugLogLevel 9\", \"SecRuleEngine On\", \"Include @owasp_crs/*.conf\" ] } EOF } } } ] \nRefer to the Wasm extension configuration reference for details on how to configure the extension. \n$ consul config write wasm-extension-serve-waf.hcl"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.16.x/connect/proxies/envoy-extensions/usage/ext-authz",
  "text": "Delegate authorization to an external service | Consul\nThis topic describes how to use the external authorization Envoy extension to delegate data plane authorization requests to external systems. \nComplete the following steps to use the external authorization extension:\nThe following example shows a service defaults configuration entry for the api service that directs the Envoy proxy to make gRPC authorization requests to the authz service:\napi-auth-service-defaults.hcl\nKind = \"service-defaults\" Name = \"api\" EnvoyExtensions = [ { Name = \"builtin/ext-authz\" Arguments = { ProxyType = \"connect-proxy\" Config = { GrpcService = { Target = { Service = { Name = \"authz\" } } } } } } ] \nRefer to the external authorization extension configuration reference for details on how to configure the extension. \n$ consul config write api-auth-service-defaults.hcl"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.16.x/connect/proxies/envoy-extensions/usage/lambda",
  "text": "Lambda Envoy Extension | Consul\nInvoke Lambda functions in Envoy proxy\nThe Lambda Envoy extension configures outbound traffic on upstream dependencies allowing mesh services to properly invoke AWS Lambda functions. Lambda functions appear in the catalog as any other Consul service.\nYou can only enable the Lambda extension through service-defaults. This is because the Consul uses the service-defaults configuration entry name as the catalog name for the Lambda functions.\nThe Lambda Envoy extension has the following arguments:\nArgumentsDescription\nARN\tSpecifies the AWS ARN for the service's Lambda.\t\nInvocationMode\tDetermines if Consul configures the Lambda to be invoked using the synchronous or asynchronous invocation mode.\t\nPayloadPassthrough\tDetermines if the body Envoy receives is converted to JSON or directly passed to Lambda.\t\nBe aware that unlike manual lambda registration, region is inferred from the ARN when specified through an Envoy extension.\nThere are two steps to configure the Lambda Envoy extension:\nConfigure EnvoyExtensions through service-defaults. \nTo use the Lambda Envoy extension, you must configure and apply a service-defaults configuration entry. Consul uses the name of the entry as the Consul service name for the Lambdas in the catalog. Downstream services also use the name to invoke the Lambda.\nThe following example configures the Lambda Envoy extension to create a service named lambda in the mesh that can invoke the associated Lambda function.\nlambda-envoy-extension.hcl\nKind = \"service-defaults\" Name = \"lambdaInvokingApp\" Protocol = \"http\" EnvoyExtensions { Name = \"builtin/aws/lambda\" Arguments = { ARN = \"arn:aws:lambda:us-west-2:111111111111:function:lambda-1234\" } } \nFor a full list of parameters for EnvoyExtensions, refer to the service-defaults and proxy-defaults configuration entries reference documentation. \nNote: You can only enable the Lambda extension through service-defaults.\nRefer to Configuration specification section to find a full list of arguments for the Lambda Envoy extension.\nApply the service-defaults configuration entry.\n$ consul config write lambda-envoy-extension.hcl \nIn the following example, the Lambda Envoy extension adds a single Lambda function running in two regions into the mesh. Then, you can use the lambda service name to invoke it, as if it was any other service in the mesh.\nlambda-envoy-extension.json\nKind = \"service-defaults\" Name = \"lambda\" Protocol = \"http\" EnvoyExtensions { Name = \"builtin/aws/lambda\" Arguments = { payloadPassthrough: false arn: arn:aws:lambda:us-west-2:111111111111:function:lambda-1234 } } EnvoyExtensions { Name = \"builtin/aws/lambda\" Arguments = { payloadPassthrough: false arn: arn:aws:lambda:us-east-1:111111111111:function:lambda-1234 } }"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.16.x/connect/proxies/envoy-extensions/usage/lua",
  "text": "Lua Envoy Extension | Consul\nRun Lua scripts in Envoy proxy\nThe Lua Envoy extension enables the HTTP Lua filter in your Consul Envoy proxies, letting you run Lua scripts when requests and responses pass through Consul-generated Envoy resources.\nEnvoy filters support setting and getting dynamic metadata, allowing a filter to share state information with subsequent filters. To set dynamic metadata, configure the HTTP Lua filter. Users can call streamInfo:dynamicMetadata() from Lua scripts to get the request's dynamic metadata. \nTo use the Lua Envoy extension, configure the following arguments in the EnvoyExtensions block:\nProxyType: string | connect-proxy - Determines the proxy type the extension applies to. The only supported value is connect-proxy.\nListenerType: string | required - Specifies if the extension is applied to the inbound or outbound listener.\nScript: string | required - The Lua script that is configured to run by the HTTP Lua filter.\nThere are two steps to configure the Lua Envoy extension:\nConfigure EnvoyExtensions through service-defaults or proxy-defaults. \nTo use Envoy extensions, you must configure and apply a proxy-defaults or service-defaults configuration entry with the Envoy extension.\nWhen you configure Envoy extensions on proxy-defaults, they apply to every service.\nWhen you configure Envoy extensions on service-defaults, they apply to a specific service.\nConsul applies Envoy extensions configured in proxy-defaults before it applies extensions in service-defaults. As a result, the Envoy extension configuration in service-defaults may override configurations in proxy-defaults.\nThe following example configures the Lua Envoy extension on every service by using the proxy-defaults.\nlua-envoy-extension-proxy-defaults.hcl\nKind = \"proxy-defaults\" Name = \"global\" Protocol = \"http\" EnvoyExtensions { Name = \"builtin/lua\" Arguments = { ProxyType = \"connect-proxy\" Listener = \"inbound\" Script = <<-EOS function envoy_on_request(request_handle) meta = request_handle:streamInfo():dynamicMetadata() m = meta:get(\"consul\") request_handle:headers():add(\"x-consul-service\", m[\"service\"]) request_handle:headers():add(\"x-consul-namespace\", m[\"namespace\"]) request_handle:headers():add(\"x-consul-datacenter\", m[\"datacenter\"]) request_handle:headers():add(\"x-consul-trust-domain\", m[\"trust-domain\"]) end EOS } } \nFor a full list of parameters for EnvoyExtensions, refer to the service-defaults and proxy-defaults configuration entries reference documentation.\nWarning: Applying EnvoyExtensions to ProxyDefaults may produce unintended consequences. We recommend enabling EnvoyExtensions with ServiceDefaults in most cases.\nRefer to Configuration specification section to find a full list of arguments for the Lua Envoy extension.\nApply the proxy-defaults or service-defaults configuration entry.\n$ consul config write lua-envoy-extension-proxy-defaults.hcl \nIn the following example, the service-defaults configure the Lua Envoy extension to insert the HTTP Lua filter for service myservice and add the Consul service name to thex-consul-service header for all inbound requests. The ListenerType makes it so that the extension applies only on the inbound listener of the service's connect proxy.\nKind = \"service-defaults\" Name = \"myservice\" EnvoyExtensions = [ { Name = \"builtin/lua\" Arguments = { ProxyType = \"connect-proxy\" Listener = \"inbound\" Script = <<EOF function envoy_on_request(request_handle) local service = request_handle:streamInfo():dynamicMetadata():get(\"consul\")[\"service\"] request_handle:headers():add(\"x-consul-service\", service) end EOF } } ] \nAlternatively, you can apply the same extension configuration to proxy-defaults configuration entries.\nYou can also specify multiple Lua filters through the Envoy extensions. They will not override each other.\nKind = \"service-defaults\" Name = \"myservice\" EnvoyExtensions = [ { Name = \"builtin/lua\", Arguments = { ProxyType = \"connect-proxy\" Listener = \"inbound\" Script = <<-EOF function envoy_on_request(request_handle) meta = request_handle:streamInfo():dynamicMetadata() m = meta:get(\"consul\") request_handle:headers():add(\"x-consul-datacenter\", m[\"datacenter1\"]) end EOF } }, { Name = \"builtin/lua\", Arguments = { ProxyType = \"connect-proxy\" Listener = \"inbound\" Script = <<-EOF function envoy_on_request(request_handle) meta = request_handle:streamInfo():dynamicMetadata() m = meta:get(\"consul\") request_handle:headers():add(\"x-consul-datacenter\", m[\"datacenter2\"]) end EOF } } ]"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.16.x/connect/proxies/envoy-extensions/usage/property-override",
  "text": "Configure Envoy proxy properties | Consul\nThis topic describes how to use the property-override extension to set and remove individual properties for the Envoy resources Consul generates. The extension uses the protoreflect, which enables Consul to dynamically manipulate messages.\nThe extension currently supports setting scalar and enum fields, removing individual fields addressable by Path, and initializing unset intermediate message fields indicated in Path.\nIt currently does not support the following use cases:\nAdding, updating, or removing repeated field members\nAdding or updating protobuf map fields\nAdding or updating protobuf Any fields\nComplete the following steps to use the property-override extension:\nSecurity warning: The property override extension is an advanced feature capable of introducing unintended consequences or reducing cluster security if used incorrectly. Consul does not enforce TLS retention, intentions, or other security-critical components of the Envoy configuration. Additionally, Consul does not verify that the configuration does not contain errors that affect service traffic.\nIn the following proxy defaults configuration entry example, Consul sets the /respect_dns_ttl field on the api service proxy's cluster configuration for the other-svc upstream service:\nproperty-override-extension-service-defaults.hcl\nKind = \"service-defaults\" Name = \"api\" Protocol = \"http\" EnvoyExtensions = [ { Name = \"builtin/property-override\" Arguments = { ProxyType = \"connect-proxy\" Patches = [ { ResourceFilter = { ResourceType = \"cluster\" TrafficDirection = \"outbound\" Services = [{ Name = \"other-svc\" }] } Op = \"add\" Path = \"/respect_dns_ttl\" Value = true } ] } } ] \nRefer to the property override configuration reference for details on how to configure the extension. \nConstructing paths\nTo target the properties for an Envoy resource type, you must specify the path where the properties exist in the Path field of the property override extension configuration. Set the Path field to an empty or partially invalid string when saving the configuration entry and Consul returns an error with a list of supported fields for the first unrecognized segment of the path. By default, Consul only returns the first ten fields, but you can set the Debug field to true to direct Consul to output all possible fields. \nIn the following example, Consul outputs the top-level fields available for the Envoy cluster resource:\nKind = \"service-defaults\" Name = \"api\" EnvoyExtensions = [ { Name = \"builtin/property-override\" Arguments = { Debug = true ProxyType = \"connect-proxy\" Patches = [ { ResourceFilter = { ResourceType = \"cluster\" TrafficDirection = \"outbound\" } Op = \"add\" Path = \"\" Value = 5 } ] } } ] \nAfter applying the configuration entry, Consul prints a message that includes the possible fields for the resource:\n$ consul config write api.hcl non-empty, non-root Path is required; available envoy.config.cluster.v3.Cluster fields: transport_socket_matches name alt_stat_name type cluster_type eds_cluster_config connect_timeout ... \nYou can use the output to help you construct the appropriate value for the Path field. For example:\n$ consul config write api.hcl 2>&1 | grep round_robin round_robin_lb_config \n$ consul config write property-override-extension-service-defaults.hcl"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/connect/proxies/envoy-extensions/usage/apigee-ext-authz",
  "text": "Delegate authorization to Apigee | Consul\nThis topic describes how to use the external authorization Envoy extension to delegate data plane authorization requests to Apigee.\nFor more detailed guidance, refer to the learn-consul-apigee-external-authz repo on GitHub.\nComplete the following steps to use the external authorization extension with Apigee:\nDeploy the Apigee Adapter for Envoy and register the service in Consul.\nConfigure the EnvoyExtensions block in a service defaults or proxy defaults configuration entry.\nThe Apigee Adapter for Envoy is an Apigee-managed API gateway that uses Envoy to proxy API traffic.\nTo download and install Apigee Adapter for Envoy, refer to the getting started documentation or follow along with the learn-consul-apigee-external-authz repo on GitHub.\nAfter you deploy the service in your desired runtime, create a service defaults configuration entry for the service's gRPC protocol.\napigee-remote-service-envoy.hcl\nKind = \"service-defaults\" Name = \"apigee-remote-service-envoy\" Protocol = \"grpc\" \nWhen you configure Envoy extensions on service defaults, they apply to all instances of a service with that name.\nWarning\nAdding Envoy extensions default proxy configurations may have unintended consequences. We recommend configuring `EnvoyExtensions` in service defaults configuration entries in most cases.\nThe following example configures the default behavior for all services named api so that the Envoy proxies running as sidecars for those service instances target the apigee-remote-service-envoy service for gRPC authorization requests:\napi-auth-service-defaults.hcl\nKind = \"service-defaults\" Name = \"api\" EnvoyExtensions = [ { Name = \"builtin/ext-authz\" Arguments = { ProxyType = \"connect-proxy\" Config = { GrpcService = { Target = { Service = { Name = \"apigee-remote-service-envoy\" } } } } } } ] \nRefer to the external authorization extension configuration reference for details on how to configure the extension. \nOn the CLI, you can use the consul config write command and specify the names of the configuration entries to apply them to Consul. For Kubernetes-orchestrated networks, use the kubectl apply command to update the relevant CRD.\n$ consul config write apigee-remote-service-envoy.hcl $ consul config write api-auth-service-defaults.hcl"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.16.x/connect/proxies/envoy-extensions/usage/wasm",
  "text": "Run WebAssembly plug-ins in Envoy proxy | Consul\nThis topic describes how to use the wasm extension, which directs Consul to run your WebAssembly (Wasm) plug-ins for Envoy proxies. \nYou can create Wasm plugins for Envoy and integrate them using the wasm extension. Wasm is a binary instruction format for stack-based virtual machines that has the potential to run anywhere after it has been compiled. Wasm plug-ins run as filters in a service mesh application's sidecar proxy.\nThe following steps describe the process of integrating Wasm plugins:\nCreate your Wasm plugin. You must ensure that your plugin functions as expected. Refer to the WebAssembly website for information and links to documentation.\nAdd Envoy extension configuration to a proxy defaults or service defaults configuration entry. Place the extension configuration in an EnvoyExtensions block in the configuration entry.\nIn the following example, the extension uses an upstream service named file-server to serve a Wasm-based web application firewall (WAF). \nwasm-extension-serve-waf.hcl\nKind = \"service-defaults\" Name = \"api\" Protocol = \"http\" EnvoyExtensions = [ { Name = \"builtin/wasm\" Arguments = { Protocol = \"http\" ListenerType = \"inbound\" PluginConfig = { VmConfig = { Code = { Remote = { HttpURI = { Service = { Name = \"file-server\" } URI = \"https://file-server/waf.wasm\" } SHA256 = \"c9ef17f48dcf0738b912111646de6d30575718ce16c0cbde3e38b21bb1771807\" } } } Configuration = <<EOF { \"rules\": [ \"Include @demo-conf\", \"Include @crs-setup-demo-conf\", \"SecDebugLogLevel 9\", \"SecRuleEngine On\", \"Include @owasp_crs/*.conf\" ] } EOF } } } ] \nRefer to the Wasm extension configuration reference for details on how to configure the extension. \n$ consul config write wasm-extension-serve-waf.hcl"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/connect/proxies/envoy-extensions/configuration/ext-authz",
  "text": "External authorization extension configuration reference | Consul\nThis topic describes how to configure the external authorization Envoy extension, which configures Envoy proxies to request authorization from an external service. Refer to Delegate authorization to an external service for usage information.\nThe following list outlines the field hierarchy, data types, and requirements for the external authorization configuration. Place the configuration inside the EnvoyExtension.Arguments field in the proxy defaults or service defaults configuration entry. Refer to the following documentation for additional information:\nEnvoyExtensions in proxy defaults\nEnvoyExtensions in service defaults\nEnvoy External Authorization documentation\nClick on a property name to view additional details, including default values.\nName: string | required | must be set to builtin/ext-authz \nArguments: map | required\nProxyType: string | required | connect-proxy\nListenerType: string | required | inbound\nInsertOptions: map\nLocation: string\nFilterName: string\nConfig: map | required\nBootstrapMetadataLabelsKey: string\nClearRouteCache: boolean | false | HTTP only\nGrpcService: map\nTarget: map | required\nService: map\nName: string\nNamespace: string | Enterprise\nPartition: string | Enterprise\nURI: string \nTimeout: string | 1s\nAuthority: string \nInitialMetadata: list\nKey: string \nValue: string \nHttpService: map\nTarget: map | required\nService: string\nName: string\nNamespace: string | Enterprise\nPartition: string | Enterprise \nURI: string \nTimeout: string | 1s \nPathPrefix: string \nAuthorizationRequest: map \nAllowedHeaders: list\nContains: string \nExact: string \nIgnoreCase: boolean \nPrefix: string \nSafeRegex: string \nHeadersToAdd: list\nKey: string \nValue: string \nAuthorizationResponse: map\nAllowedUpstreamHeaders: list\nContains: string \nExact: string \nIgnoreCase: boolean \nPrefix: string \nSafeRegex: string \nSuffix: string \nAllowedUpstreamHeadersToAppend: list\nContains: string \nExact: string \nIgnoreCase: boolean \nPrefix: string \nSafeRegex: string \nSuffix: string \nAllowedClientHeaders: list\nContains: string \nExact: string \nIgnoreCase: boolean \nPrefix: string \nSafeRegex: string \nSuffix: string \nAllowedClientHeadersOnSuccess: list\nSuffix: string \nDynamicMetadataFromHeaders: list\nIncludePeerCertificate: boolean | false\nMetadataContextNamespaces: list of strings | HTTP only\nStatusOnError: number | 403 | HTTP only\nStatPrefix: string | response\nWithRequestBody: map | HTTP only\nMaxRequestBytes: number\nAllowPartialMessage: boolean | false\nPackAsBytes: boolean | false\nWhen each field is defined, an ext-authz configuration has the following form:\nName = \"builtin/ext-authz\" Arguments = { ProxyType = \"connect-proxy\" InsertOptions = { Location = \"<location in the filter chain>\" FilterName = \"<filter relative to the location>\" } Config = { BootstrapMetadataLabelsKey = \"<key from bootstrap metadata>\" ClearRouteCache = false // HTTP only GrpcService = { Target = { Service = { Name = \"<upstream service to send gRPC authorization requests to>\" Namespace = \"<namespace containing the upstream service>\" Partition = \"<partition containing the upstream service>\" URI = \"<URI of the upstream service>\" Timeout = \"1s\" Authority = \"<authority header to send in the gRPC request>\" InitialMetadata = [ \"<Key>\" : \"<value>\" HttpService = { Target = { Service = { Name = \"<upstream service to send gRPC authorization requests to>\" Namespace = \"<namespace containing the upstream service>\" Partition = \"<partition containing the upstream service>\" URI = \"<URI of the upstream service>\" Timeout = \"1s\" } } PathPrefix = \"/<authorization-request-header-prefix>/\" AuthorizationRequest = { AllowedHeaders = [ Contains = \"<client request headers must contain this value>\", Exact = \"<client request headers can only be this value>\", IgnoreCase = false, Prefix = \"<client request headers must begin with this value>\", SafeRegex = \"<client request headers can match this regex pattern>\" ] HeadersToAdd = [ \"<header key>\" = \"<header value>\" ] } AuthorizationResponse = { AllowedUpstreamHeaders = [ Contains = \"<authorization response headers must contain this value>\", Exact = \"<authorization response headers can only be this value>\", IgnoreCase = false, Prefix = \"<authorization response headers must begin with this value>\", SafeRegex = \"<authorization response headers can match this regex pattern>\" Suffix = \"<authorization response headers must end with this value>\" ] AllowedUpstreamHeadersToAppend = [ Contains = \"<authorization response headers must contain this value>\", Exact = \"<authorization response headers can only be this value>\", IgnoreCase = false, Prefix = \"<authorization response headers must begin with this value>\", SafeRegex = \"<authorization response headers can match this regex pattern>\" Suffix = \"<authorization response headers must end with this value>\" ] AllowedClientHeaders = [ Contains = \"<client response headers must contain this value>\", Exact = \"<client response headers can only be this value>\", IgnoreCase = false, Prefix = \"<client response headers must begin with this value>\", SafeRegex = \"<client response headers can match this regex pattern>\" Suffix = \"<client response headers must end with the value>\" ] AllowedClientHeadersOnSuccess = [ Contains = \"<client response headers must contain this value>\", Exact = \"<client response headers can only be this value>\", IgnoreCase = false, Prefix = \"<client response headers must begin with this value>\", SafeRegex = \"<client response headers can match this regex pattern>\" Suffix = \"<client response headers must end with the value>\" DynamicMetadataFromHeaders = [ Contains = \"<authorization response headers must contain this value>\", Exact = \"<authorization response headers can only be this value>\", IgnoreCase = false, Prefix = \"<authorization response headers must begin with this value>\", SafeRegex = \"<authorization response headers can match this regex pattern>\" Suffix = \"<authorization response headers must end with the value>\" ] IncludePeerCertificate = false MetadataContextNamespaces = [ \"<metadata namespace>\" ] StatusOnError = 403 // HTTP only StatPrefix = \"response\" WithRequestBody = { //HTTP only MaxRequestBytes = <uint32 value specifying the max size of the message body> AllowPartialMessage = false PackAsBytes = false \nThis section provides details about the fields you can configure for the external authorization extension.\nName\nSpecifies the name of the extension. Must be set to builtin/ext-authz.\nValues\nDefault: None\nThis field is required.\nData type: String value set to builtin/ext-authz.\nArguments\nContains the global configuration for the extension.\nValues\nDefault: None\nThis field is required.\nData type: Map\nArguments.ProxyType\nSpecifies the type of Envoy proxy that this extension applies to. The extension only applies to proxies that match this type and is ignored for all other proxy types. The only supported value is connect-proxy.\nValues\nDefault: connect-proxy\nThis field is required.\nData type: String\nArguments.ListenerType\nSpecifies the type of listener the extension applies to. The listener type is either inbound or outbound. If the listener type is set to inbound, Consul applies the extension so the external authorization is enabled when other services in the mesh send messages to the service attached to the proxy. If the listener type is set to outbound, Consul applies the extension so the external authorization is enabled when the attached proxy sends messages to other services in the mesh.\nValues\nDefault: inbound\nThis field is required.\nData type is one of the following string values:\ninbound\noutbound\nArguments.InsertOptions\nSpecifies options for defining the insertion point for the external authorization filter in the Envoy filter chain. By default, the external authorization filter is inserted as the first filter in the filter chain per the default setting for the Location field. \nDefault: None\nData type: Map\nArguments.InsertOptions.Location\nSpecifies the insertion point for the external authorization filter in the Envoy filter chain. You can specify one of the following string values:\nFirst: Inserts the filter as the first filter in the filter chain, regardless of the filter specified in the FilterName field.\nBeforeLast: Inserts the filter before the last filter in the chain, regardless of the filter specified in the FilterName field. This allows the filter to be inserted after all other filters and immediately before the terminal filter. \nAfterFirstMatch: Inserts the filter after the first filter in the chain that has a name matching the value of the FilterName field.\nAfterLastMatch: Inserts the filter after the last filter in the chain that has a name matching the value of the FilterName field.\nBeforeFirstMatch: Inserts the filter before the first filter in the chain that has a name matching the value of the FilterName field.\nBeforeLastMatch: Inserts the filter before the last filter in the chain that has a name matching the value of the FilterName field. \nDefault: BeforeFirstMatch\nData type: String\nArguments.InsertOptions.FilterName\nSpecifies the name of an existing filter in the chain to match when inserting the external authorization filter. Specifying a filter name enables you to configure an insertion point relative to the position of another filter in the chain. \nDefault: envoy.filters.network.tcp_proxy for TCP services. envoy.filters.http.router for HTTP services.\nData type: String\nArguments.Config\nContains the configuration settings for the extension. \nDefault: None\nData type: Map\nArguments.Config.BootstrapMetadataLabelsKey\nSpecifies a key from the Envoy bootstrap metadata. Envoy adds labels associated with the key to the authorization request context. \nData type: String\nArguments.Config.ClearRouteCache\nDirects Envoy to clear the route cache so that the external authorization service correctly affects routing decisions. If set to true, the filter clears all cached routes. \nEnvoy also clears cached routes if the status returned from the authorization service is 200 for HTTP responses or 0 for gRPC responses. Envoy also clears cached routes if at least one authorization response header is added to the client request or is used for altering another client request header.\nDefault: false\nData type: Boolean\nArguments.Config.GrpcService\nSpecifies the external authorization configuration for gRPC requests. Configure the GrpcService or the HttpService settings, but not both.\nEither the GrpcService or the HttpService configuration is required.\nData type: Map\nArguments.Config.GrpcService.Target\nConfiguration for specifying the service to send gRPC authorization requests to. The Target field may contain the following fields:\nService or Uri\nTimeout\nArguments{}.Config{}.GrpcService{}.Target{}.Service{}\nSpecifies the upstream external authorization service. Configure this field when authorization requests are sent to an upstream service within the service mesh. The service must be configured as an upstream of the service that the filter is applied to.\nConfigure either the Service field or the Uri field, but not both. \nThis field or Uri is required.\nThe following table describes how to configure parameters for the Service field:\nParameterDescriptionData typeDefault\nName\tSpecifies the name of the upstream service.\tString\tNone\t\nNamespace\tEnterprise Specifies the Consul namespace that the upstream service belongs to.\tString\tdefault\t\nPartition\tEnterprise Specifies the Consul admin partition that the upstream service belongs to.\tString\tdefault\t\nArguments.Config.GrpcService.Target.Uri\nSpecifies the URI of the external authorization service. Configure this field when you must provide an explicit URI to the external authorization service, such as cases in which the authorization service is running on the same host or pod. If set, the value of this field must be one of localhost:<port>, 127.0.0.1:<port>, or ::1:<port>.\nConfigure either the Uri field or the Service field, but not both. \nThis field or Service is required.\nArguments.Config.GrpcService.Target.Timeout\nSpecifies the maximum duration that a response can take to arrive upon request.\nDefault: 1s\nArguments.Config.GrpcService.Authority\nSpecifies the authority header to send in the gRPC request. If this field is not set, the authority field is set to the cluster name. This field does not override the SNI that Envoy sends to the external authorization service.\nDefault: Cluster name\nArguments.Config.GrpcService.InitialMetadata[]\nSpecifies additional metadata to include in streams initiated to the GrpcService. You can specify metadata for injecting additional ad-hoc authorization headers, for example, x-foo-bar: baz-key. For more information, including details on header value syntax, refer to the Envoy documentation on custom request headers.\nData type: List of one or more key-value pairs:\nKEY: String\nVALUE: String\nArguments{}.Config{}.HttpService{}\nContains the configuration for raw HTTP communication between the filter and the external authorization service. Configure the HttpService or the GrpcService settings, but not both.\nEither the HttpService or the GrpcService configuration is required.\nArguments{}.Config{}.HttpService{}.Target{}\nConfiguration for specifying the service to send HTTP authorization requests to. The Target field may contain the following fields:\nService or Uri\nTimeout\nArguments{}.Config{}.HttpService{}.Target{}.Service{}\nSpecifies the upstream external authorization service. Configure this field when HTTP authorization requests are sent to an upstream service within the service mesh. The service must be configured as an upstream of the service that the filter is applied to.\nConfigure either the Service field or the Uri field, but not both. \nThis field or Uri is required.\nThe following table describes how to configure parameters for the Service field:\nParameterDescriptionData typeDefault\nName\tSpecifies the name of the upstream service.\tString\tNone\t\nNamespace\tEnterprise Specifies the Consul namespace that the upstream service belongs to.\tString\tdefault\t\nPartition\tEnterprise Specifies the Consul admin partition that the upstream service belongs to.\tString\tdefault\t\nArguments{}.Config{}.HttpService{}.Target{}.Uri\nSpecifies the URI of the external authorization service. Configure this field when you must provide an explicit URI to the external authorization service, such as cases in which the authorization service is running on the same host or pod. If set, the value of this field must be one of localhost:<port>, 127.0.0.1:<port>, or ::1:<port>.\nConfigure either the Uri field or the Service field, but not both. \nThis field or Service is required.\nArguments{}.Config{}.HttpService{}.Target{}.Timeout\nSpecifies the maximum duration that a response can take to arrive upon request.\nDefault: 1s\nArguments{}.Config{}.HttpService{}.PathPrefix\nSpecifies a prefix for the value of the authorization request header Path. You must include the preceding forward slash (/).\nArguments{}.Config{}.HttpService{}.AuthorizationRequest{}\nHTTP-only configuration that controls the HTTP authorization request metadata. The AuthorizationRequest field may contain the following parameters:\nAllowHeaders\nHeadersToAdd\nArguments{}.Config{}.HttpService{}.AuthorizationRequest{}.AllowHeaders[]\nSpecifies a set of rules for matching client request headers. The request to the external authorization service includes any client request headers that satisfy any of the rules. Refer to the Envoy documentation for a detailed explanation.\nData type: List of key-value pairs\nThe following table describes the matching rules you can configure in the AllowHeaders field:\nRuleDescriptionData typeDefault\nContains\tSpecifies a string that the input string must contain.\tString\tN/A\t\nExact\tSpecifies a string that the input string must match exactly.\tString\tN/A\t\nIgnoreCase\tDirects Envoy to ignore capitalization. If set to true, the other matching rules in the configuration are not case sensitive. This rule does not affect SafeRegex.\tBoolean\tfalse\t\nPrefix\tSpecifies a string that the input string must begin with.\tString\tN/A\t\nSafeRegex\tSpecifies a regular expression for matching the input string programmatically. Envoy supports Google's RE2 regex engine.\tString\tN/A\t\nSuffix\tSpecifies a string that the input string must end with.\tString\tN/A\t\nArguments{}.Config{}.HttpService{}.AuthorizationRequest{}.HeadersToAdd[]\nSpecifies a list of headers to include in the request to the authorization service. Note that Envoy overwrites client request headers with the same key.\nData type: List of one or more key-value pairs:\nKEY: String\nVALUE: String\nArguments{}.Config{}.HttpService{}.AuthorizationResponse{}\nHTTP-only configuration that controls HTTP authorization response metadata. The AuthorizationResponse field may contain the following parameters:\nAllowedUpstreamHeaders\nAllowedUpstreamHeadersToAppend\nAllowedClientHeaders\nAllowedClientHeadersOnSuccess\nDynamicMetadataFromHeaders\nArguments{}.Config{}.HttpService{}.AuthorizationResponse{}.AllowedUpstreamHeaders[]\nSpecifies a set of rules for matching authorization response headers. Envoy adds any headers from the external authorization service to the client response that satisfy the rules. Envoy overwrites existing headers.\nThe following table describes the matching rules you can configure in the AllowedUpstreamHeaders field:\nRuleDescriptionData typeDefault\nContains\tSpecifies a string that the input string must contain.\tString\tN/A\t\nExact\tSpecifies a string that the input string must match exactly.\tString\tN/A\t\nIgnoreCase\tDirects Envoy to ignore capitalization. If set to true, the other matching rules in the configuration are not case sensitive. This rule does not affect SafeRegex.\tBoolean\tfalse\t\nPrefix\tSpecifies a string that the input string must begin with.\tString\tN/A\t\nSafeRegex\tSpecifies a regular expression for matching the input string programmatically. Envoy supports Google's RE2 regex engine.\tString\tN/A\t\nSuffix\tSpecifies a string that the input string must end with.\tString\tN/A\t\nArguments{}.Config{}.HttpService{}.AuthorizationResponse{}.AllowedUpstreamHeadersToAppend[]\nSpecifies a set of rules for matching authorization response headers. Envoy appends any headers from the external authorization service to the client response that satisfy the rules. Envoy appends existing headers.\nThe following table describes the matching rules you can configure in the AllowedUpstreamHeadersToAppend field:\nRuleDescriptionData typeDefault\nContains\tSpecifies a string that the input string must contain.\tString\tN/A\t\nExact\tSpecifies a string that the input string must match exactly.\tString\tN/A\t\nIgnoreCase\tDirects Envoy to ignore capitalization. If set to true, the other matching rules in the configuration are not case sensitive. This rule does not affect SafeRegex.\tBoolean\tfalse\t\nPrefix\tSpecifies a string that the input string must begin with.\tString\tN/A\t\nSafeRegex\tSpecifies a regular expression for matching the input string programmatically. Envoy supports Google's RE2 regex engine.\tString\tN/A\t\nSuffix\tSpecifies a string that the input string must end with.\tString\tN/A\t\nArguments{}.Config{}.HttpService{}.AuthorizationResponse{}.AllowedClientHeaders[]\nSpecifies a set of rules for matching client response headers. Envoy adds any headers from the external authorization service to the client response that satisfy the rules. When the list is not set, Envoy includes all authorization response headers except Authority (Host). When a header is included in this list, Envoy automatically adds the following headers: \nPath\nStatus\nContent-Length\nWWWAuthenticate\nLocation\nThe following table describes the matching rules you can configure in the AllowedClientHeaders field:\nRuleDescriptionData typeDefault\nContains\tSpecifies a string that the input string must contain.\tString\tN/A\t\nExact\tSpecifies a string that the input string must match exactly.\tString\tN/A\t\nIgnoreCase\tDirects Envoy to ignore capitalization. If set to true, the other matching rules in the configuration are not case sensitive. This rule does not affect SafeRegex.\tBoolean\tfalse\t\nPrefix\tSpecifies a string that the input string must begin with.\tString\tN/A\t\nSafeRegex\tSpecifies a regular expression for matching the input string programmatically. Envoy supports Google's RE2 regex engine.\tString\tN/A\t\nSuffix\tSpecifies a string that the input string must end with.\tString\tN/A\t\nArguments{}.Config{}.HttpService{}.AuthorizationResponse{}.AllowedClientHeadersOnSuccess[]\nSpecifies a set of rules for matching client response headers. Envoy adds headers from the external authorization service to the client response when the headers satisfy the rules and the authorization is successful. If the headers match the rules but the authorization fails or is denied, the headers are not added. If this field is not set, Envoy does not add any additional headers to the client's response on success. \nThe following table describes the matching rules you can configure in the AllowedClientHeadersOnSuccess field:\nArguments{}.Config{}.HttpService{}.AuthorizationResponse{}.DynamicMetadataFromHeaders[]\nSpecifies a set of rules for matching authorization response headers. Envoy emits headers from the external authorization service as dynamic metadata that the next filter in the chain can consume. \nThe following table describes the matching rules you can configure in the DynamicMetadataFromHeaders field:\nArguments{}.Config{}.IncludePeerCertificate\nIf set to true, Envoy includes the peer X.509 certificate in the authorization request if the certificate is available. \nDefault: false\nData type: Boolean\nArguments{}.Config{}.MetadataContextNamespace[]\nHTTP only field that specifies a list of metadata namespaces. The values of the namespaces are included in the authorization request context. The consul namespace is always included in addition to the namespaces you configure.\nDefault: [\"consul\"]\nData type: List of string values\nArguments{}.Config{}.StatusOnError\nHTTP only field that specifies a return code status to respond with on error. Refer to the Envoy documentation for additional information.\nDefault: 403\nData type: Integer\nArguments{}.Config{}.StatPrefix\nSpecifies a prefix to add when writing statistics.\nDefault: response\nArguments{}.Config{}.WithRequestBody{}\nHTTP only field that configures Envoy to buffer the client request body and send it with the authorization request. If unset, the request body is not sent with the authorization request.\nThe following table describes the parameters that you can include in the WithRequestBody field:\nParameterDescriptionData typeDefault\nMaxRequestBytes\tSpecifies the maximum size of the message body that the filter holds in memory. Envoy returns HTTP 403 and does not initiate the authorization process when the buffer reaches the number set in this field unless AllowPartialMessage is set to true.\tuint32\tNone\t\nAllowPartialMessage\tIf set to true, Envoy buffers the request body until the value of MaxRequestBytes is reached. The authorization request is dispatched with a partial body and no 413 HTTP error returns by the filter.\tBoolean\tfalse\t\nPackAsBytes\tIf set to true, Envoy sends the request body to the external authorization as raw bytes. Otherwise, Envoy sends the request body as a UTF-8 encoded string.\tBoolean\tfalse\t\nThe following examples demonstrate common configuration patterns for specific use cases.\nAuthorize gRPC requests to a URI\nIn the following example, a service defaults configuration entry contains an ext-authz configuration. The configuration allows the api service to make gRPC authorization requests to a service at localhost:9191:\nKind = \"service-defaults\" Name = \"api\" EnvoyExtensions = [ { Name = \"builtin/ext-authz\" Arguments = { ProxyType = \"connect-proxy\" Config = { GrpcService = { Target = { URI = \"127.0.0.1:9191\" } } } } } ] \nUpstream authorization\nIn the following example, a service defaults configuration entry contains an ext-authz configuration. The configuration allows the api service to make gRPC authorization requests to a service named authz:\nKind = \"service-defaults\" Name = \"api\" EnvoyExtensions = [ { Name = \"builtin/ext-authz\" Arguments = { ProxyType = \"connect-proxy\" Config = { GrpcService = { Target = { Service = { Name = \"authz\" } } } } } } ] \nAuthorization requests after service intentions for Consul Enterprise\nIn the following example for Consul Enterprise, the api service is configured to make an HTTP authorization requests to a service named authz in the foo namespace and bar partition. Envoy also inserts the external authorization filter after the envoy.filters.http.rbac filter:\nKind = \"service-defaults\" Name = \"api\" Protocol = \"http\" EnvoyExtensions = [ { Name = \"builtin/ext-authz\" Arguments = { ProxyType = \"connect-proxy\" InsertOptions = { Location = \"AfterLastMatch\" FilterName = \"envoy.filters.http.rbac\" } Config = { HttpService = { Target = { Service = { Name = \"authz\" Namespace = \"foo\" Partition = \"bar\" } } } } } } ]"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/connect/proxies/envoy-extensions/configuration/otel-access-logging",
  "text": "OpenTelemetry Access Logging extension configuration reference | Consul\nThis topic describes how to configure the OpenTelemetry access logging Envoy extension, which configures Envoy proxies to send access logs to OpenTelemetry collector service. Refer to Send access logs to OpenTelemetry collector service for usage information.\nThe following list outlines the field hierarchy, data types, and requirements for the OpenTelemetry access logging configuration. Place the configuration inside the EnvoyExtension.Arguments field in the proxy defaults or service defaults configuration entry. Refer to the following documentation for additional information:\nEnvoyExtensions in proxy defaults\nEnvoyExtensions in service defaults\nEnvoy OpenTelemetry Access Logging Configuration documentation\nClick on a property name to view additional details, including default values.\nName: string | required | must be set to builtin/otel-access-logging \nArguments: map | required\nProxyType: string | required | connect-proxy\nListenerType: string | required | inbound\nConfig: map | required\nLogName: string\nGrpcService: map\nTarget: map | required\nService: map\nName: string\nNamespace: string | Enterprise\nPartition: string | Enterprise\nURI: string\nTimeout: string | 1s\nAuthority: string\nInitialMetadata: list\nKey: string\nValue: string\nBufferFlushInterval: string\nBufferSizeBytes: number\nFilterStateObjectsToLog: list of strings\nRetryPolicy: map\nRetryBackOff: map\nBaseInterval: string | 1s\nMaxInterval: string | 30s\nNumRetries: number\nBody: string, number, boolean or list of bytes\nAttributes: map of string to string, number, boolean or list of bytes\nResourceAttributes: map of string to string, number, boolean or list of bytes\nWhen each field is defined, an otel-access-logging configuration has the following form:\nName = \"builtin/otel-access-logging\" Arguments = { ProxyType = \"connect-proxy\" ListenerType = \"<inbound or outbound>\" Config = { LogName = \"<user-readable name of the access log>\" GrpcService = { Target = { Service = { Name = \"<upstream service to send gRPC authorization requests to>\" Namespace = \"<namespace containing the upstream service>\" Partition = \"<partition containing the upstream service>\" } URI = \"<URI of the upstream service>\" Timeout = \"1s\" } Authority = \"<authority header to send in the gRPC request>\" InitialMetadata = [ \"<Key>\" : \"<value>\" ] } BufferFlushInterval = \"1s\" BufferSizeBytes = 16384 FilterStateObjectsToLog = [ \"Additional filter state objects to log in filter_state_objects\" ] RetryPolicy = { RetryBackOff = { BaseInterval = \"1s\" MaxInterval = \"30s\" } NumRetries = <uint32 value specifying the max number of retries> } Body = \"Log Request Body\" Attributes = { \"<Key>\" : \"<value>\" } ResourceAttributes = { \"<Key>\" : \"<value>\" } \nThis section provides details about the fields you can configure for the OpenTelemetry Access Logging extension.\nName\nSpecifies the name of the extension. Must be set to builtin/otel-access-logging.\nData type: String value set to builtin/otel-access-logging.\nArguments\nContains the global configuration for the extension.\nArguments.ProxyType\nSpecifies the type of Envoy proxy that this extension applies to. The extension only applies to proxies that match this type and is ignored for all other proxy types. The only supported value is connect-proxy.\nDefault: connect-proxy\nArguments.ListenerType\nSpecifies the type of listener the extension applies to. The listener type is either inbound or outbound. If the listener type is set to inbound, Consul applies the extension so the access logging is enabled when other services in the mesh send messages to the service attached to the proxy. If the listener type is set to outbound, Consul applies the extension so the access logging is enabled when the attached proxy sends messages to other services in the mesh.\nDefault: inbound\nData type is one of the following string values:\ninbound\noutbound\nArguments.Config\nContains the configuration settings for the extension.\nArguments.Config.LogName\nSpecifies the user-readable name of the access log to be returned in StreamAccessLogsMessage.Identifier. This allows the access log server to differentiate between different access logs coming from the same Envoy. If you leave it empty, it inherits the value from ListenerType.\nArguments.Config.GrpcService\nSpecifies the OpenTelemetry Access Logging configuration for gRPC requests.\nArguments.Config.GrpcService.Target\nConfiguration for specifying the service to send gRPC access logging requests to. The Target field may contain the following fields:\nService or Uri\nTimeout\nArguments.Config.GrpcService.Target.Service\nSpecifies the upstream OpenTelemetry collector service. Configure this field when access logging requests are sent to an upstream service within the service mesh. The service must be configured as an upstream of the service that the filter is applied to.\nConfigure either the Service field or the Uri field, but not both. \nThis field or Uri is required.\nThe following table describes how to configure parameters for the Service field:\nParameterDescriptionData typeDefault\nName\tSpecifies the name of the upstream service.\tString\tNone\t\nNamespace\tEnterprise Specifies the Consul namespace that the upstream service belongs to.\tString\tdefault\t\nPartition\tEnterprise Specifies the Consul admin partition that the upstream service belongs to.\tString\tdefault\t\nArguments.Config.GrpcService.Target.Uri\nSpecifies the URI of the OpenTelemetry collector service. Configure this field when you must provide an explicit URI to the OpenTelemetry collector service, such as cases in which the access logging service is running on the same host or pod. If set, the value of this field must be one of localhost:<port>, 127.0.0.1:<port>, or ::1:<port>.\nConfigure either the Uri field or the Service field, but not both. \nThis field or Service is required.\nArguments.Config.GrpcService.Target.Timeout\nSpecifies the maximum duration that a response can take to arrive upon request.\nDefault: 1s\nSpecifies the authority header to send in the gRPC request. If this field is not set, the authority field is set to the cluster name. This field does not override the SNI that Envoy sends to the OpenTelemetry collector service.\nDefault: Cluster name\nArguments.Config.GrpcService.InitialMetadata\nSpecifies additional metadata to include in streams initiated to the GrpcService. You can specify metadata for injecting additional ad-hoc authorization headers, for example, x-foo-bar: baz-key. For more information, including details on header value syntax, refer to the Envoy documentation on custom request headers.\nData type: List of one or more key-value pairs:\nKEY: String\nVALUE: String\nArguments.Config.BufferFlushInterval\nSpecifies an interval for flushing access logs to the gRPC stream. The logger flushes requests at the end of every interval or when the log reaches the batch size limit, whichever comes first.\nDefault: 1s\nArguments.Config.BufferSizeBytes\nSpecifies the soft size limit in bytes for the access log entries buffer. The logger buffers requests until it reaches this limit or every time the flush interval elapses, whichever comes first. Set this field to 0 to disable batching.\nDefault: 16384\nData type: Integer\nArguments.Config.FilterStateObjectsToLog\nSpecifies additional filter state objects to log in filter_state_objects. The logger calls FilterState::Object::serializeAsProto to serialize the filter state object.\nData type: List of String\nArguments.Config.RetryPolicy\nDefines a policy for retrying requests to the upstream service when fetching the plugin data. The RetryPolicy field is a map containing the following parameters:\nRetryBackoff\nNumRetries\nArguments.Config.RetryPolicy.RetryBackOff\nSpecifies parameters that control retry backoff strategy. \nThe following table describes the fields you can specify in the RetryBackOff map:\nBaseInterval\tSpecifies the base interval for determining the next backoff computation. Set a value greater than 0 and less than or equal to the MaxInterval value.\tString\t1s\t\nMaxInterval\tSpecifies the maximum interval between retries. Set the value greater than or equal to the BaseInterval value.\tString\t10s\t\nArguments.Config.RetryPolicy.NumRetries\nSpecifies the number of times Envoy retries to fetch plugin data if the initial attempt is unsuccessful.\nDefault: 1\nData type: Integer\nArguments.Config.Body\nSpecifies OpenTelemetry LogResource fields, following Envoy access logging formatting. See body in the LogResource proto for more details.\nArguments.Config.Attributes\nSpecifies attributes in the LogResource. Refer to attributes in the LogResource proto for more details.\nArguments.Config.ResourceAttributes\nSpecifies OpenTelemetry Resource attributes are filled with Envoy node information.\nThe following examples demonstrate common configuration patterns for specific use cases.\nOpenTelemetry Access Logging requests to URI\nIn the following example, a service defaults configuration entry contains an otel-access-logging configuration. The configuration allows the api service to make gRPC OpenTelemetry Access Logging requests to a service at localhost:9191:\nKind = \"service-defaults\" Name = \"api\" EnvoyExtensions = [ { Name = \"builtin/otel-access-logging\" Arguments = { ProxyType = \"connect-proxy\" Config = { GrpcService = { Target = { URI = \"127.0.0.1:9191\" } } } } } ] \nUpstream OpenTelemetry Access Logging\nIn the following example, a service defaults configuration entry contains an otel-access-logging configuration. The configuration allows the api service to make gRPC OpenTelemetry Access Logging requests to a service named otel-collector:\nKind = \"service-defaults\" Name = \"api\" EnvoyExtensions = [ { Name = \"builtin/otel-access-logging\" Arguments = { ProxyType = \"connect-proxy\" Config = { GrpcService = { Target = { Service = { Name = \"otel-collector\" } } } } } } ]"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.15.x/connect/proxies/envoy-extensions/usage/lambda",
  "text": "Lambda Envoy Extension | Consul\nInvoke Lambda functions in Envoy proxy\nThe Lambda Envoy extension configures outbound traffic on upstream dependencies allowing mesh services to properly invoke AWS Lambda functions. Lambda functions appear in the catalog as any other Consul service.\nYou can only enable the Lambda extension through service-defaults. This is because the Consul uses the service-defaults configuration entry name as the catalog name for the Lambda functions.\nThe Lambda Envoy extension has the following arguments:\nArgumentsDescription\nARN\tSpecifies the AWS ARN for the service's Lambda.\t\nInvocationMode\tDetermines if Consul configures the Lambda to be invoked using the synchronous or asynchronous invocation mode.\t\nPayloadPassthrough\tDetermines if the body Envoy receives is converted to JSON or directly passed to Lambda.\t\nBe aware that unlike manual lambda registration, region is inferred from the ARN when specified through an Envoy extension.\nThere are two steps to configure the Lambda Envoy extension:\nConfigure EnvoyExtensions through service-defaults. \nTo use the Lambda Envoy extension, you must configure and apply a service-defaults configuration entry. Consul uses the name of the entry as the Consul service name for the Lambdas in the catalog. Downstream services also use the name to invoke the Lambda.\nThe following example configures the Lambda Envoy extension to create a service named lambda in the mesh that can invoke the associated Lambda function.\nlambda-envoy-extension.hcl\nKind = \"service-defaults\" Name = \"lambdaInvokingApp\" Protocol = \"http\" EnvoyExtensions { Name = \"builtin/aws/lambda\" Arguments = { ARN = \"arn:aws:lambda:us-west-2:111111111111:function:lambda-1234\" } } \nFor a full list of parameters for EnvoyExtensions, refer to the service-defaults and proxy-defaults configuration entries reference documentation. \nNote: You can only enable the Lambda extension through service-defaults.\nRefer to Configuration specification section to find a full list of arguments for the Lambda Envoy extension.\nApply the service-defaults configuration entry.\n$ consul config write lambda-envoy-extension.hcl \nIn the following example, the Lambda Envoy extension adds a single Lambda function running in two regions into the mesh. Then, you can use the lambda service name to invoke it, as if it was any other service in the mesh.\nlambda-envoy-extension.json\nKind = \"service-defaults\" Name = \"lambda\" Protocol = \"http\" EnvoyExtensions { Name = \"builtin/aws/lambda\" Arguments = { payloadPassthrough: false arn: arn:aws:lambda:us-west-2:111111111111:function:lambda-1234 } } EnvoyExtensions { Name = \"builtin/aws/lambda\" Arguments = { payloadPassthrough: false arn: arn:aws:lambda:us-east-1:111111111111:function:lambda-1234 } }"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/connect/proxies/envoy-extensions/configuration/wasm",
  "text": "WebAssembly extension configuration reference | Consul\nThis topic describes how to configure the wasm extension, which directs Consul to run WebAssembly (Wasm) plugins in Envoy proxies. Refer to Run WebAssembly plug-ins in Envoy proxy for usage information.\nThe following list outlines the field hierarchy, data types, and requirements for the wasm configuration. Place the configuration inside the EnvoyExtension.Arguments field in the proxy defaults or service defaults configuration entry. Refer the following documentation for additional information:\nEnvoyExtensions in proxy defaults\nEnvoyExtensions in service defaults\nClick on a property name to view additional details, including default values.\nProtocol: string\nListenerType: string | required\nProxyType: string | connect-proxy\nPluginConfig: map | required\nName: string\nRootID: string | required\nVmConfig: map\nVmID: string\nRuntime: string | v8\nCode: map\nLocal: map\nFilename: string\nRemote: map\nHttpURI: map\nService: map\nNamespace: string\nPartition: string\nURI: string\nTimeout: string\nSHA256: string\nRetryPolicy: map\nRetryBackOff: map\nBaseInterval: string\nMaxInterval: string\nNumRetries: number | -1\nConfiguration: string\nEnvironmentVariables: map\nHostEnvKeys: list of strings\nKeyValues: map\nConfiguration: string\nCapabilityRestrictionConfiguration: map\nAllowedCapabilities: map of strings\nWhen all parameters are set for the extension, the configuration has the following form:\nProtocol = \"<tcp or http>\" ListenerType = \"<inbound or outbound>\" ProxyType = \"connect-proxy\" PluginConfig = { Name = \"<name for the filter>\" RootID = \"<ID for the set of filters on a VM>\" VmConfig = { VmID = \"<ID of the VM>\" Runtime = \"v8\" Code = { Local = { # Set either `Local` or `Remote', not both Filename = \"</path/to/plugin>\" } Remote = { # Set either `Local` or `Remote', not both HttpURI = { Service = { Name = \"<name of the upstream service>\" Namespace = \"<Consul namespace containing the upstream service>\" Partition = \"<Consul partition containing the upstream service>\" } URI = \"<URI of the plugin data>\" Timeout = \"1s\" SHA256 = \"<SHA256 for verifying the remote data>\" RetryPolicy = { RetryBackOff = { BaseInterval = \"1s\" MaxInterval = \"10s\" } NumRetries = -1 } } Configuration = \"<configuration passed to plugin on VM startup>\" EnvironmentVariables = { HostEnvKeys = [ <\"keys\"> ] KeyValues = { [ <\"key = value\"> ] } } Configuration = \"<configuration passed to plugin on plugin startup>\" CapabilityRestrictionConfiguration = { AllowedCapabilities = { \"fd_read\" = {} \"fd_seek\" = {} \"environ_get\" = {} \"clock_get_time\" = {} } } } \nThis section provides details about the fields you can configure for the wasm extension.\nProtocol\nSpecifies the type of Wasm filter to apply. You can set either tcp or http. Set the Protocol to the protocol that the Wasm plugin implements when loaded by the filter. For Consul to apply the filter, the protocol must match the service's protocol.\nData type is one of the following string values:\ntcp\nhttp\nListenerType\nSpecifies the type of listener the extension applies to. The listener type is either inbound or outbound. If the listener type is set to inbound, Consul applies the extension so the Wasm plugin is run when other services in the mesh send messages to the service attached to the proxy. If the listener type is set to outbound, Consul applies the extension so the Wasm plugin is run when the attached proxy sends messages to other services in the mesh.\nData type is one of the following string values:\ninbound\noutbound\nProxyType\nSpecifies the type of Envoy proxy that the extension applies to. The only supported value is connect-proxy.\nDefault: connect-proxy\nPluginConfig{}\nMap containing the following configuration parameters for your Wasm plugin:\nName\nRootID\nVmConfig\nConfiguration\nCapabilitiesRestrictionConfiguration\nPluginConfig{}.Name\nSpecifies a unique name for a filter in a VM. Envoy uses the name to identify specific filters if multiple filters are processed on a VM with the same VmID and RootID. The name also appears in logs for debugging purposes.\nPluginConfig{}.RootID\nSpecifies a unique ID for a set of filters in a VM that share a RootContext and Contexts, such as a Wasm HttpFilter and a Wasm AccessLog, if applicable. All filters with the same RootID and VmID share Contexts.\nPluginConfig{}.VmConfig{}\nMap containing the following configuration parameters for the VM that runs your Wasm plugin:\nVmID\nRuntime\nCode\nConfiguration\nEnvironmentVariables\nPluginConfig{}.VmConfig{}.VmID\nSpecifies an ID that Envoy uses with a hash of the Wasm code to determine which VM runs the plugin. All plugins with the same VmID and Code use the same VM. If unspecified, all plugins with the same code run in the same VM. Sharing a VM between plugins may have security implications, but can reduce memory utilization and can make data sharing easier.\nPluginConfig{}.VmConfig{}.Runtime\nSpecifies the type of Wasm runtime.\nDefault: v8\nv8\nwastime\nwamr\nwavm\nPluginConfig{}.VmConfig{}.Code{}\nMap containing one of the following configuration parameters:\nLocal\nRemote\nYou can configure either Local or Remote, but not both. The Code block instructs Consul how to find the Wasm plugin code for Envoy to execute.\nData type is a map containing one of the following configurations:\nLocal\nRemote\nPluginConfig{}.VmConfig{}.Code{}.Local{}\nInstructs Envoy to load the plugin code from a local volume. Do not configure the Local parameter if the plugin code is on a remote server.\nThe Local field is a map that contains a Filename parameter. The Filename parameter takes a string value that specifies the path to the plugin on the local file system.\nLocal plug-ins are not supported in Kubernetes-orchestrated environments.\nData type is a map containing the Filename parameter. The Filename parameter takes a string value that specifies the path to the plugin on the local file system.\nPluginConfig{}.VmConfig{}.Code{}.Remote{}\nInstructs Envoy to load the plugin code from a remote server. Do not configure the Remote parameter if the plugin code is on the local VM.\nThe Remote field is a map containing the following parameters:\nHttpURI\nSHA256\nRetryPolicy\nPluginConfig{}.VmConfig{}.Code{}.Remote{}.HttpURI{}\nSpecifies the configuration for fetching the remote data. The HttpURI field is a map containing the following parameters:\nService\nURI\nTimeout\nPluginConfig{}.VmConfig{}.Code{}.Remote{}.HttpURI{}.Service\nSpecifies the upstream service to fetch the remote plugin from.\nThe following table describes the fields you can specify in the Service map:\nName\tSpecifies the name of the upstream service.\tString\tNone\t\nNamespace\tEnterprise Specifies the Consul namespace that the upstream service belongs to.\tString\tdefault\t\nPartition\tEnterprise Specifies the Consul admin partition that the upstream service belongs to.\tString\tdefault\t\nPluginConfig{}.VmConfig{}.Code{}.Remote{}.HttpURI{}.URI\nSpecifies the URI Envoy uses to fetch the plugin file from the upstream. This field is required for Envoy to retrieve plugin code from a remote location. You must specify the fully-qualified domain name (FQDN) of the remote URI, which includes the protocol, host, and path.\nData type: String value that specifies a FQDN\nPluginConfig{}.VmConfig{}.Code{}.Remote{}.HttpURI{}.Timeout\nSpecifies the maximum duration that a response can take to complete the request for the plugin data.\nPluginConfig{}.VmConfig{}.Code{}.Remote{}.SHA256\nSpecifies the required SHA256 string for verifying the remote data.\nPluginConfig{}.VmConfig{}.Code{}.Remote{}.RetryPolicy{}\nDefines a policy for retrying requests to the upstream service when fetching the plugin data. The RetryPolicy field is a map containing the following parameters:\nRetryBackoff\nNumRetries\nPluginConfig{}.VmConfig{}.Code{}.Remote{}.RetryPolicy{}.RetryBackOff{}\nSpecifies parameters that control retry backoff strategy.\nThe following table describes the fields you can specify in the RetryBackOff map:\nBaseInterval\tSpecifies the base interval for determining the next backoff computation. Set a value greater than 0 and less than or equal to the MaxInterval value.\tString\t1s\t\nMaxInterval\tSpecifies the maximum interval between retries. Set the value greater than or equal to the BaseInterval value.\tString\t10s\t\nPluginConfig{}.VmConfig{}.Code{}.Remote{}.RetryPolicy{}.NumRetries\nSpecifies the number of times Envoy retries to fetch plugin data if the initial attempt is unsuccessful.\nDefault: 1\nData type: Integer\nPluginConfig{}.VmConfig{}.Configuration\nSpecifies the configuration Envoy encodes as bytes and passes to the plugin during VM startup. Refer to proxy_on_vm_start in the Proxy Wasm ABI documentation for additional information.\nPluginConfig{}.VmConfig{}.EnvironmentVariables{}\nSpecifies environment variables for Envoy to inject into this VM so that they are available through WASI's environ_get and environ_get_sizes system calls.\nIn most cases, WASI calls the functions implicitly in your language's standard library. As a result, you do not need to call them directly. You can also access environment variables as you would on native platforms.\nEnvoy rejects the configuration if there is a key space conflict.\nThe EnvironmentVariables field is a map containing parameters for setting the keys and values.\nThe following table describes the parameters contained in the EnvironmentVariables map:\nHostEnvKeys\tSpecifies a list of Envoy environment variable keys to expose to the VM. If a key exists in Envoy's environment variables, then the key-value pair is injected. Envoy ignores HostEnvKeys that do not exist in its environment variables.\tList\tNone\t\nKeyValues\tSpecifies a map of explicit key-value pairs to inject into the VM.\tMap of string keys and values\tNone\t\nPluginConfig{}.Configuration\nSpecifies the configuration Consul encodes as bytes and passes to the plugin during plugin startup. Refer to proxy_on_configure in the Envoy documentation for additional information.\nPluginConfig{}.CapabilityRestrictionConfiguration{}\nSpecifies a configuration for restricting the proxy-Wasm capabilities that are available to the module.\nThe CapabilityRestrictionConfiguration field is a map that contains a AllowedCapabilities parameter. The AllowedCapabilities parameter takes a map of string values that correspond to Envoy capability names. Refer to the Envoy documentation for additional information.\nSecurity warning: Consul ignores the value that each capability maps to. You can leave the AllowedCapabilities empty to allow all capabilities, but doing so gives the configured plugin full unrestricted access to the runtime API provided by the Wasm VM. You must set this to a non-empty map if you want to restrict access to specific capabilities provided by the Wasm runtime API.\nDefault: \"\"\nData type is a map containing the AllowedCapabilities parameter. The AllowedCapabilities parameter takes a map of string values that correspond to Envoy capability names. Refer to the Envoy documentation for additional information.\nThe following examples demonstrate patterns that you may be able to model your configurations on.\nRun a Wasm plugin from a local file\nIn the following example, Consul figures the Envoy proxy for the db service with an inbound TCP Wasm filter that uses the plugin code from the local /consul/extensions/sqli.wasm file.\nKind = \"service-defaults\" Name = \"db\" Protocol = \"tcp\" EnvoyExtensions = [ { Name = \"builtin/wasm\" Required = true Arguments = { Protocol = \"tcp\" ListenerType = \"inbound\" PluginConfig = { VmConfig = { Code = { Local = { Filename = \"file:///consul/extensions/sqli.wasm\" } } } Configuration = <<EOF { \"key\": \"value\" } EOF } } } ] \nRun a Wasm plugin from a remote file\nIn the following example, Consul configures the Envoy proxy for all HTTP services with an HTTP Wasm filter. The filter uses the plugin code from a remote https://extension-server/waf.wasm file. The Envoy proxy for each service fetches the remote file and verify the SHA256 checksum. The proxy times if Consul cannot fetch the remote plugin after three seconds.\nKind = \"proxy-defaults\" Name = \"global\" EnvoyExtensions = [ { Name = \"builtin/wasm\" Arguments = { Protocol = \"http\" ListenerType = \"inbound\" PluginConfig = { VmConfig = { Code = { Remote = { HttpURI = { URI = \"https://extension-server/waf.wasm\" Timeout = \"3s\" } SHA256 = \"ef57657e...\" } } } Configuration = \"...\" } } } ]"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/connect/proxies/envoy-extensions/configuration/property-override",
  "text": "Property override configuration reference | Consul\nThis topic describes how to configure the property-override extension so that you can set and remove individual properties on the Envoy resources Consul generates. Refer to Configure Envoy proxy properties for usage information.\nThe following list outlines the field hierarchy, data types, and requirements for the property-override configuration. Place the configuration inside the EnvoyExtension.Arguments field in the proxy defaults or service defaults configuration entry. Refer the following documentation for additional information:\nEnvoyExtensions in proxy defaults\nEnvoyExtensions in service defaults\nClick on a property name to view additional details, including default values.\nProxyType: string | connect-proxy\nDebug: bool | false \nPatches: list | required\nResourceFilter: map \nResourceType: string | required\nTrafficDirection: string | required\nServices: list\nNamespace: string | default | Enterprise\nPartition: string | default | Enterprise\nOp: string | required\nPath: string | required\nValue: map, number, boolean, or string\nWhen each field is defined, a property-override configuration has the following form:\nProxyType = \"connect-proxy\" Debug = false Patches = [ { ResourceFilter = { ResourceType = \"<type of resource>\" TrafficDirection = \"<inbound or outbound>\" Services = [ { Name = \"<name of service to filter for>\" Namespace = \"<Consul namespace containing the service>\" Partition = \"<Consul partition containing the service>\" } ] } Op = \"<add or remove>\" Path = \"</path/to/field>\" Value = \"<values or map of field-values>\" } ] \nThis section provides details about the fields you can configure for the property-override extension.\nProxyType\nSpecifies the type of Envoy proxy that the extension applies to. The only supported value is connect-proxy.\nDefault: connect-proxy\nData type: String \nDebug\nEnables full debug mode. When Debug is set to true, all possible fields for the given ResourceType and first unmatched segment of Path are returned on error. When set to false, the error message only includes the first ten possible fields. \nDefault: false\nData type: Boolean\nPatches[]\nSpecifies a list of one or more JSON Patches that map to the Envoy proxy configurations you want to modify. Refer to IETF RFC 6902 for information about the JSON Patch specification. \nThe Patches parameter is a list of configurations in JSON Patch format. Each patch can contain the following fields:\nResourceFilter\nOp\nPath\nValue\nPatches[].ResourceFilter{}\nSpecifies the filter for targeting specific Envoy resources. The ResourceFilter configuration is not part of the JSON Patch specification. \nThe following table describes how to configure a ResourceFilter:\nParameterDescriptionType\nProxyType\tSpecifies the proxy type that the extension applies to. The only supported value is connect-proxy.\tString\t\nResourceType\tSpecifies the Envoy resource type that the extension applies to. You can specify one of the following values for each ResourceFilter: \ncluster\ncluster-load-assignment\nroute\nlistener\n\tString\t\nTrafficDirection\tSpecifies the type of traffic that the extension applies to relative to the current proxy. You can specify one of the following values for each ResourceFilter: \ninbound: Targets resources for the proxy's inbound traffic.\noutbound: Targets resources for the proxy's upstream services.\n\tString\t\nServices\tSpecifies a list of services to target. Each member of the list has the following fields:\nName: Specifies the service associated with the traffic.\nNamespace: Specifies the Consul Enterprise namespace the service is in.\nPartition: Specifies the Consul Enterprise admin partition the service is in.\nIf TrafficDirection is set to outbound, upstream services in this field correspond to local Envoy resources that Consul patches at runtime. \nDo not configure the Services field if TrafficDirection is set to inbound.\nIf this field is not set, Envoy targets all applicable resources. When patching outbound listeners, the patch includes the outbound transparent proxy listener only if Services is unset and if the local service is in transparent proxy mode.\tList of maps\t\nPatches[].Op\nSpecifies the JSON Patch operation to perform when the ResourceFilter matches a local Envoy proxy configuration. You can specify one of the following values for each patch:\nadd: Replaces a property or message specified by Path with the given value. The JSON Patch add operation does not merge objects. To emulate merges, you must configure discrete add operations for each changed field. Consul returns an error if the target field does not exist in the corresponding schema.\nremove: Unsets the value of the field specified by Path. If the field is not set, no changes are made. Consul returns an error if the target field does not exist in the corresponding schema.\nadd\nremove\nPatches[].Path\nSpecifies where the extension performs the associated operation on the specified resource type. Refer to ResourceType for information about specifying a resource type to target. Refer to Op for information about setting an operation to perform on the resources. \nThe Path field does not support addressing array elements or protobuf map field entries. Refer to Constructing paths for information about how to construct paths. \nWhen setting fields, the extension sets any unset intermediate fields to their default values. A single operation on a nested field can set multiple intermediate fields. Because Consul sets the intermediate fields to their default values, you may need to configure subsequent patches to satisfy Envoy or Consul validation. \nPatches[].Value{}\nDefines a value to set at the specified path if the operation is set to add. You can specify either a scalar or enum value, an array of scalar or enum values (for repeated fields), or define a map that contains string keys and values corresponding to scalar or enum child fields. Single and repeated scalar and enum values are supported. Refer to the example configurations for additional guidance and to the Envoy API documentation for additional information about Envoy proxy interfaces.\nIf Envoy specifies a wrapper as the target field type, the extension automatically coerces simple values to the wrapped type when patching. For example, the value 32768 is allowed when targeting a cluster's per_connection_buffer_limit_bytes, which is a UInt32Value field. Refer to the protobuf documentation for additional information about wrappers.\nThis field is required if Op is set to add, otherwise you must omit the field.\nThis field takes one of the following data types:\nscalar\nenum\nmap \nThe following examples demonstrate patterns that you may be able to model your configurations on.\nEnable respect_dns_ttl in a cluster\nIn the following example, the add operation patches the outbound cluster corresponding to the other-svc upstream service to enable respect_dns_ttl. The Path specifies the Cluster /respect_dns_ttl top-level field and Value specifies a value of true:\nKind = \"service-defaults\" Name = \"my-svc\" Protocol = \"http\" EnvoyExtensions = [ { Name = \"builtin/property-override\", Arguments = { ProxyType = \"connect-proxy\", Patches = [ { ResourceFilter = { ResourceType = \"cluster\" TrafficDirection = \"outbound\" Service = { Name = \"other-svc\" } } Op = \"add\" Path = \"/respect_dns_ttl\" Value = true } ] } } ] \nUpdate multiple values in a message field\nIn the following example, both ResourceFilter blocks target the cluster corresponding to the other-svc upstream service and modify Cluster /outlier_detection properties:\nKind = \"service-defaults\" Name = \"my-svc\" Protocol = \"http\" EnvoyExtensions = [ { Name = \"builtin/property-override\", Arguments = { ProxyType = \"connect-proxy\", Patches = [ { ResourceFilter = { ResourceType = \"cluster\" TrafficDirection = \"outbound\" Services = [{ Name = \"other-svc\" }] } Op = \"add\" Path = \"/outlier_detection/max_ejection_time/seconds\" Value = 120 }, { ResourceFilter = { ResourceType = \"cluster\" TrafficDirection = \"outbound\" Services = [{ Name = \"other-svc\" }] } Op = \"add\" Path = \"/outlier_detection/max_ejection_time_jitter/seconds\" Value = 1 } ] } } ] \nThe use of /seconds in these examples corresponds to the same field in the google.protobuf.Duration proto definition, since the extension does not support JSON serialized string forms of common protobuf types (e.g. 120s).\nNote: Using separate patches per field preserves any existing configuration of other fields in outlier_detection that may be directly set by Consul, such as enforcing_consecutive_5xx.\nReplace a message field\nIn the following example, a ResourceFilter targets the cluster corresponding to the other-svc upstream service and replaces the entire map of properties located at /outlier_detection, including explicitly set enforcing_success_rate and success_rate_minimum_hosts properties:\nKind = \"service-defaults\" Name = \"my-svc\" Protocol = \"http\" EnvoyExtensions = [ { Name = \"builtin/property-override\" Arguments = { ProxyType = \"connect-proxy\" Patches = [ { ResourceFilter = { ResourceType = \"cluster\" TrafficDirection = \"outbound\" Services = [{ Name = \"other-svc\" }] } Op = \"add\" Path = \"/outlier_detection\" Value = { \"enforcing_success_rate\" = 80 \"success_rate_minimum_hosts\" = 2 } } ] } } ] \nUnlike the previous example, other /outlier_detection values set by Consul will not be retained unless they match Envoy's defaults, because the entire value of /outlier_detection will be replaced."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.15.x/connect/proxies/envoy-extensions/usage/lua",
  "text": "Lua Envoy Extension | Consul\nRun Lua scripts in Envoy proxy\nThe Lua Envoy extension enables the HTTP Lua filter in your Consul Envoy proxies, letting you run Lua scripts when requests and responses pass through Consul-generated Envoy resources.\nEnvoy filters support setting and getting dynamic metadata, allowing a filter to share state information with subsequent filters. To set dynamic metadata, configure the HTTP Lua filter. Users can call streamInfo:dynamicMetadata() from Lua scripts to get the request's dynamic metadata. \nTo use the Lua Envoy extension, configure the following arguments in the EnvoyExtensions block:\nProxyType\tDetermines the proxy type the extension applies to. The only supported value is connect-proxy.\t\nListenerType\tSpecifies if the extension is applied to the inbound or outbound listener.\t\nScript\tThe Lua script that is configured to run by the HTTP Lua filter.\t\nThere are two steps to configure the Lua Envoy extension:\nConfigure EnvoyExtensions through service-defaults or proxy-defaults. \nTo use Envoy extensions, you must configure and apply a proxy-defaults or service-defaults configuration entry with the Envoy extension.\nWhen you configure Envoy extensions on proxy-defaults, they apply to every service.\nWhen you configure Envoy extensions on service-defaults, they apply to a specific service.\nConsul applies Envoy extensions configured in proxy-defaults before it applies extensions in service-defaults. As a result, the Envoy extension configuration in service-defaults may override configurations in proxy-defaults.\nThe following example configures the Lua Envoy extension on every service by using the proxy-defaults.\nlua-envoy-extension-proxy-defaults.hcl\nKind = \"proxy-defaults\" Name = \"global\" Protocol = \"http\" EnvoyExtensions { Name = \"builtin/lua\" Arguments = { ProxyType = \"connect-proxy\" Listener = \"inbound\" Script = <<-EOS function envoy_on_request(request_handle) meta = request_handle:streamInfo():dynamicMetadata() m = meta:get(\"consul\") request_handle:headers():add(\"x-consul-service\", m[\"service\"]) request_handle:headers():add(\"x-consul-namespace\", m[\"namespace\"]) request_handle:headers():add(\"x-consul-datacenter\", m[\"datacenter\"]) request_handle:headers():add(\"x-consul-trust-domain\", m[\"trust-domain\"]) end EOS } } \nFor a full list of parameters for EnvoyExtensions, refer to the service-defaults and proxy-defaults configuration entries reference documentation.\nWarning: Applying EnvoyExtensions to ProxyDefaults may produce unintended consequences. We recommend enabling EnvoyExtensions with ServiceDefaults in most cases.\nRefer to Configuration specification section to find a full list of arguments for the Lua Envoy extension.\nApply the proxy-defaults or service-defaults configuration entry.\n$ consul config write lua-envoy-extension-proxy-defaults.hcl \nIn the following example, the service-defaults configure the Lua Envoy extension to insert the HTTP Lua filter for service myservice and add the Consul service name to thex-consul-service header for all inbound requests. The ListenerType makes it so that the extension applies only on the inbound listener of the service's connect proxy.\nlua-envoy-extension.json\nKind = \"service-defaults\" Name = \"myservice\" EnvoyExtensions = [ { Name = \"builtin/lua\" Arguments = { ProxyType = \"connect-proxy\" Listener = \"inbound\" Script = <<EOF function envoy_on_request(request_handle) local service = request_handle:streamInfo():dynamicMetadata():get(\"consul\")[\"service\"] request_handle:headers():add(\"x-consul-service\", service) end EOF } } ] \nAlternatively, you can apply the same extension configuration to proxy-defaults configuration entries.\nYou can also specify multiple Lua filters through the Envoy extensions. They will not override each other.\nlua-envoy-extension.json\nKind = \"service-defaults\" Name = \"myservice\" EnvoyExtensions = [ { Name = \"builtin/lua\", Arguments = { ProxyType = \"connect-proxy\" Listener = \"inbound\" Script = <<-EOF function envoy_on_request(request_handle) meta = request_handle:streamInfo():dynamicMetadata() m = meta:get(\"consul\") request_handle:headers():add(\"x-consul-datacenter\", m[\"datacenter1\"]) end EOF } }, { Name = \"builtin/lua\", Arguments = { ProxyType = \"connect-proxy\" Listener = \"inbound\" Script = <<-EOF function envoy_on_request(request_handle) meta = request_handle:streamInfo():dynamicMetadata() m = meta:get(\"consul\") request_handle:headers():add(\"x-consul-datacenter\", m[\"datacenter2\"]) end EOF } } ]"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.17.x/connect/traffic-management/failover",
  "text": "Consul | HashiCorp Developer\nThis page does not exist for version v1.17.x."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.18.x/connect/observability/service",
  "text": "Monitoring service-to-service communication with Envoy | Consul\nWhen running a service mesh with Envoy as the proxy, there are a wide array of possible metrics produced from traffic flowing through the data plane. This document covers a set of scenarios and key baseline metrics and potential alerts that will help you maintain the overall health and resilience of the mesh for HTTP services. In addition, it provides examples of using these metrics in specific ways to generate a Grafana dashboard using a Prometheus backend to better understand how the metrics behave.\nWhen collecting metrics, it is important to establish a baseline. This baseline ensures your Consul deployment is healthy, and serves as a reference point when troubleshooting abnormal Cluster behavior. Once you have established a baseline for your metrics, use them and the following recommendations to configure reasonable alerts for your Consul agent. \nNote\nThe following examples assume that the operator adds the cluster name (i.e. datacenter) using the label cluster and the node name (i.e. machine or pod) using the label node to all scrape targets.\nIs Envoy's configuration growing stale?\nWhen Envoy connects to the Consul control plane over xDS, it will rapidly converge to the current configuration that the control plane expects it to have. If the xDS stream terminates and does not reconnect for an extended period, then the xDS configuration currently in the Envoy instances will fail static and slowly grow out of date.\nMetric\nenvoy_control_plane_connected_state\nAlerting\nIf the value for a given node/pod/machine was 0 for an extended period of time.\nExample dashboard (table)\ngroup(last_over_time(envoy_control_plane_connected_state{cluster=\"$cluster\"}[1m] ) == 0) by (node) \nIs this service being sent requests?\nWithin a mesh, a request travels from one service to another. You may choose to measure many relevant metrics from the calling-side, the serving-side, or both.\nIt is useful to track the perceived request rate of requests from the calling-side as that would include all requests, even those that fail to arrive at the serving-side due to any failures.\nAny measurement of the request rate is also generally useful for capacity planning purposes as increased traffic typically correlates with a need for a scale-up event in the near future.\nMetric\nenvoy_cluster_upstream_rq_total\nAlerting\nIf the value has a significant change, check if services are properly interacting with each other and if you need to increase your Consul agent resource requirements.\nExample dashboard (plot; rate)\nsum(irate(envoy_cluster_upstream_rq_total{consul_destination_datacenter=\"$cluster\", consul_destination_service=\"$service\"}[1m])) by (cluster, local_cluster) \nAre requests sent to this service mostly successful?\nA service mesh is about communication between services, so it is important to track the perceived success rate of requests witnessed by the calling services.\nMetric\nenvoy_cluster_upstream_rq_xx\nAlerting\nIf the value crosses a user defined baseline.\nExample dashboard (plot; %)\nsum(irate(envoy_cluster_upstream_rq_xx{envoy_response_code_class!=\"5\",consul_destination_datacenter=\"$cluster\",consul_destination_service=\"$service\"}[1m])) by (cluster, local_cluster) / sum(irate(envoy_cluster_upstream_rq_xx{consul_destination_datacenter=\"$cluster\",consul_destination_service=\"$service\"}[1m])) by (cluster, local_cluster) \nAre requests sent to this service handled in a timely manner?\nIf you undersize your infrastructure from a resource perspective, then you may expect a decline in response speed over time. You can track this by plotting the 95th percentile of the latency as experienced by the clients.\nMetric\nenvoy_cluster_upstream_rq_time_bucket\nAlerting\nIf the value crosses a user defined baseline.\nExample dashboard (plot; value)\nhistogram_quantile(0.95, sum(rate(envoy_cluster_upstream_rq_time_bucket{consul_destination_datacenter=\"$cluster\",consul_destination_service=\"$service\",local_cluster!=\"\"}[1m])) by (le, cluster, local_cluster)) \nIs this service responding to requests that it receives?\nUnlike the perceived request rate, which is measured from the calling side, this is the real request rate measured on the serving-side. This is a serving-side parallel metric that can help clarify underlying causes of problems in the calling-side equivalent metric. Ideally this metric should roughly track the calling side values in a 1-1 manner.\nenvoy_http_downstream_rq_total\nIf the value crosses a user defined baseline.\nExample dashboard (plot; rate)\nsum(irate(envoy_http_downstream_rq_total{cluster=\"$cluster\",local_cluster=\"$service\",envoy_http_conn_manager_prefix=\"public_listener\"}[1m])) \nAre responses from this service mostly successful?\nUnlike the perceived success rate of requests, which is measured from the calling side, this is the real success rate of requests measured on the serving-side. This is a serving-side parallel metric that can help clarify underlying causes of problems in the calling-side equivalent metric. Ideally this metric should roughly track the calling side values in a 1-1 manner.\nMetrics\nenvoy_http_downstream_rq_total\nenvoy_http_downstream_rq_xx\nIf the value crosses a user defined baseline.\nExample dashboard (plot; %)\nTotal\nsum(increase(envoy_http_downstream_rq_total{cluster=\"$cluster\",local_cluster=\"$service\",envoy_http_conn_manager_prefix=\"public_listener\"}[1m])) \nBY STATUS CODE:\nsum(increase(envoy_http_downstream_rq_xx{cluster=\"$cluster\",local_cluster=\"$service\",envoy_http_conn_manager_prefix=\"public_listener\"}[1m])) by (envoy_response_code_class) \nIs this service sending traffic to its upstreams?\nSimilar to the real request rate for requests arriving at a service, it may be helpful to view the perceived request rate departing from a service through its upstreams.\nenvoy_cluster_upstream_rq_total\nIf the value crosses a user defined success threshold.\nExample dashboard (plot; rate)\nsum(irate(envoy_cluster_upstream_rq_total{cluster=\"$cluster\", local_cluster=\"$service\", consul_destination_target!=\"\"}[1m])) by (consul_destination_target) \nAre requests from this service to its upstreams mostly successful?\nSimilar to the real success rate of requests arriving at a service, it is also important to track the perceived success rate of requests departing from a service through its upstreams.\nenvoy_cluster_upstream_rq_xx\nIf the value crosses a user defined success threshold.\nExample dashboard (plot; value)\nsum(irate(envoy_cluster_upstream_rq_xx{envoy_response_code_class!=\"5\", cluster=\"$cluster\",local_cluster=\"$service\", consul_destination_target!=\"\"}[1m])) by (consul_destination_target) / sum(irate(envoy_cluster_upstream_rq_xx{cluster=\"$cluster\",local_cluster=\"$service\",consul_destination_target!=\"\"}[1m])) by (consul_destination_target) \nAre requests from this service to its upstreams handled in a timely manner?\nSimilar to the latency of requests departing for a service, it is useful to track the 95th percentile of the latency of requests departing from a service through its upstreams.\nenvoy_cluster_upstream_rq_time_bucket\nIf the value crosses a user defined success threshold.\nExample dashboard (plot; value)\nhistogram_quantile(0.95, sum(rate(envoy_cluster_upstream_rq_time_bucket{cluster=\"$cluster\", local_cluster=\"$service\",consul_target!=\"\"}[1m])) by (le, consul_destination_target)) \nIn this guide, you learned recommendations for monitoring your Envoy metrics, and why monitoring these metrics is important for your Consul deployment.\nTo learn about monitoring Consul components, visit our Monitoring Consul components documentation."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.18.x/connect/traffic-management/failover",
  "text": "Consul | HashiCorp Developer\nThis page does not exist for version v1.18.x."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.18.x/connect/gateways/ingress-gateway/tls-external-service",
  "text": "Serve custom TLS certificates from an external service | Consul\nThis is an advanced topic that describes how to configure ingress gateways to serve TLS certificates sourced from an external service to inbound traffic using secret discovery service (SDS). SDS is a low-level feature designed for developers building integrations with custom TLS management solutions. For instructions on more common ingress gateway implementations, refer to Implement an ingress gateway.\nThe following process describes the general procedure for configuring ingress gateways to serve TLS certificates sourced from external services:\nConfigure static SDS clusters in the ingress gateway service definition.\nRegister the service definition.\nConfigure TLS client authentication\nStart Envoy.\nConfigure SDS settings in an ingress gateway configuration entry.\nRegister the ingress gateway configuration entry with Consul.\nThe external service must implement Envoy's gRPC secret discovery service (SDS) API.\nYou should have some familiarity with Envoy configuration and the SDS protocol.\nThe connect.enabled parameter must be set to true for all server agents in the Consul datacenter.\nThe ports.grpc parameter must be configured for all server agents in the Consul datacenter.\nACL requirements\nIf ACLs are enabled, you must present a token when registering ingress gateways that grant the following permissions:\nservice:write for the ingress gateway's service name\nservice:read for all services in the ingress gateway's configuration entry\nnode:read for all nodes of the services in the ingress gateway's configuration entry.\nThese privileges authorize the token to route communications to other services in the mesh. If the Consul client agent on the gateway's node is not configured to use the default gRPC port, 8502, then the gateway's token must also provide agent:read for its node's name in order to discover the agent's gRPC port. gRPC is used to expose Envoy's xDS API to Envoy proxies.\nYou must define one or more additional static clusters in the ingress gateway service definition for each Envoy proxy associated with the gateway. The additional clusters define how Envoy should connect to the required SDS services.\nConfigure the static clusters in the Proxy.Config.envoy_envoy_extra_static_clusters_json parameter in the service definition.\nThe clusters must provide connection information and any necessary authentication information, such as mTLS credentials.\nYou must manually register the ingress gateway with Consul proxy to define extra clusters in Envoy's bootstrap configuration. You can not use the -register flag with consul connect envoy -gateway=ingress to automatically register the proxy to define static clusters.\nIn the following example, the public-ingress gateway includes a static cluster named sds-cluster that specifies paths to the SDS certificate and SDS certification validation files:\npublic-ingress-service.hcl\nServices { Name = \"public-ingress\" Kind = \"ingress-gateway\" Proxy { Config { envoy_extra_static_clusters_json = <<EOF { \"name\": \"sds-cluster\", \"connect_timeout\": \"5s\", \"typed_extension_protocol_options\": { \"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\": { \"@type\": \"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\", \"explicit_http_config\": { \"http2_protocol_options\": {} } } }, \"type\": \"LOGICAL_DNS\", \"transport_socket\": { \"name\":\"tls\", \"typed_config\": { \"@type\":\"type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\", \"common_tls_context\":{ \"tls_certificate_sds_secret_configs\": [ { \"name\":\"tls_sds\", \"sds_config\":{ \"path\":\"/certs/sds-auth-cert.json\" } } ], \"validation_context_sds_secret_config\": { \"name\":\"validation_context_sds\", \"sds_config\":{ \"path\":\"/certs/sds-validation.json\" } } } } }, \"load_assignment\": { \"cluster_name\": \"sds-cluster\", \"endpoints\": [ { \"lb_endpoints\": [ { \"endpoint\": { \"address\": { \"socket_address\": { \"address\": \"sds-server.svc.cluster.local\", \"port_value\": 8080, } } } } ] } ] } } EOF } } } \nRefer to the Envoy documentation for details about configuration parameters for SDS clusters.\nIssue the consul services register command on the Consul agent on the Envoy proxy's node to register the service. The following example command registers an ingress gateway proxy from a public-ingress.hcl file:\n$ consul services register public-ingress.hcl \nRefer to Register services and health checks for additional information about registering services in Consul.\nStore TLS client authentication files, certificate files, and keys on disk where the Envoy proxy runs and ensure that they are available to Consul. Refer to the Envoy documentation for details on configuring authentication files.\nThe following example specifies certificate chain:\ncerts/sds-auth-cert.json\n{ \"resources\": [ { \"@type\": \"type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.Secret\", \"name\": \"tls_sds\", \"tls_certificate\": { \"certificate_chain\": { \"filename\": \"/certs/sds-client-auth.crt\" }, \"private_key\": { \"filename\": \"/certs/sds-client-auth.key\" } } } ] } \nThe following example specifies the validation context:\n/certs/sds-validation.json\n{ \"resources\": [ { \"@type\": \"type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.Secret\", \"name\": \"validation_context_sds\", \"validation_context\": { \"trusted_ca\": { \"filename\": \"/certs/sds-ca.crt\" } } } ] } \nIssue the consul connect envoy command to bootstrap Envoy. The following example starts Envoy and registers it as a service called public-ingress:\n$ consul connect envoy -gateway=ingress -service public-ingress \nRefer to Consul Connect Envoy for additional information about using the consul connect envoy command.\nDefine an ingress gateway configuration entry\nCreate an ingress gateway configuration entry that enables the gateway to use certificates from SDS. The configuration entry also maps downstream ingress listeners to upstream services. Configure the following fields:\nKind: Set the value to ingress-gateway.\nName: Consul applies the configuration entry settings to ingress gateway proxies with names that match the Name field.\nTLS: The main TLS parameter for the configuration entry holds the SDS configuration. You can also specify TLS configurations per listener and per service.\nTLS.SDS: The SDS map includes the following configuration settings:\nClusterName: Specifies the name of the cluster you specified when configuring the SDS cluster.\nCertResource: Specifies the name of the certificate resource to load.\nListeners: Specify one or more listeners.\nListeners.Port: Specify a port for the listener. Each listener is uniquely identified by its port number.\nListeners.Protocol: The default protocol is tcp, but you must specify the protocol used by the services you want to allow traffic from.\nListeners.Services: The Services field contains the services that you want to expose to upstream services. The field contains several options and sub-configurations that enable granular control over ingress traffic, such as health check and TLS configurations.\nFor Consul Enterprise service meshes, you may also need to configure the Partition and Namespace fields for the gateway and for each exposed service.\nRefer to Ingress gateway configuration entry reference for details about the supported parameters.\nThe following example directs Consul to retrieve example.com-public-cert certificates from an SDS cluster named sds-cluster and serve them to all listeners:\npublic-ingress-cfg.hcl\nKind = \"ingress-gateway\" Name = \"public-ingress\" TLS { SDS { ClusterName = \"sds-cluster\" CertResource = \"example.com-public-cert\" } } Listeners = [ { Port = 8443 Protocol = \"http\" Services = [\"*\"] } ] \nRegister the ingress gateway configuration entry\nYou can register the configuration entry using the consul config command or by calling the /config API endpoint. Refer to How to Use Configuration Entries for details about applying configuration entries.\nThe following example registers an ingress gateway configuration entry named public-ingress-cfg.hcl that is stored on the local system:\n$ consul config write public-ingress-cfg.hcl \nThe Envoy instance starts a listener on the port specified in the configuration entry and fetches the TLS certificate named from the SDS server."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.17.x/connect/gateways/ingress-gateway/tls-external-service",
  "text": "Serve custom TLS certificates from an external service | Consul\nThis is an advanced topic that describes how to configure ingress gateways to serve TLS certificates sourced from an external service to inbound traffic using secret discovery service (SDS). SDS is a low-level feature designed for developers building integrations with custom TLS management solutions. For instructions on more common ingress gateway implementations, refer to Implement an ingress gateway.\nThe following process describes the general procedure for configuring ingress gateways to serve TLS certificates sourced from external services:\nConfigure static SDS clusters in the ingress gateway service definition.\nRegister the service definition.\nConfigure TLS client authentication\nStart Envoy.\nConfigure SDS settings in an ingress gateway configuration entry.\nRegister the ingress gateway configuration entry with Consul.\nThe external service must implement Envoy's gRPC secret discovery service (SDS) API.\nYou should have some familiarity with Envoy configuration and the SDS protocol.\nThe connect.enabled parameter must be set to true for all server agents in the Consul datacenter.\nThe ports.grpc parameter must be configured for all server agents in the Consul datacenter.\nACL requirements\nIf ACLs are enabled, you must present a token when registering ingress gateways that grant the following permissions:\nservice:write for the ingress gateway's service name\nservice:read for all services in the ingress gateway's configuration entry\nnode:read for all nodes of the services in the ingress gateway's configuration entry.\nThese privileges authorize the token to route communications to other services in the mesh. If the Consul client agent on the gateway's node is not configured to use the default gRPC port, 8502, then the gateway's token must also provide agent:read for its node's name in order to discover the agent's gRPC port. gRPC is used to expose Envoy's xDS API to Envoy proxies.\nYou must define one or more additional static clusters in the ingress gateway service definition for each Envoy proxy associated with the gateway. The additional clusters define how Envoy should connect to the required SDS services.\nConfigure the static clusters in the Proxy.Config.envoy_envoy_extra_static_clusters_json parameter in the service definition.\nThe clusters must provide connection information and any necessary authentication information, such as mTLS credentials.\nYou must manually register the ingress gateway with Consul proxy to define extra clusters in Envoy's bootstrap configuration. You can not use the -register flag with consul connect envoy -gateway=ingress to automatically register the proxy to define static clusters.\nIn the following example, the public-ingress gateway includes a static cluster named sds-cluster that specifies paths to the SDS certificate and SDS certification validation files:\npublic-ingress-service.hcl\nServices { Name = \"public-ingress\" Kind = \"ingress-gateway\" Proxy { Config { envoy_extra_static_clusters_json = <<EOF { \"name\": \"sds-cluster\", \"connect_timeout\": \"5s\", \"typed_extension_protocol_options\": { \"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\": { \"@type\": \"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\", \"explicit_http_config\": { \"http2_protocol_options\": {} } } }, \"type\": \"LOGICAL_DNS\", \"transport_socket\": { \"name\":\"tls\", \"typed_config\": { \"@type\":\"type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\", \"common_tls_context\":{ \"tls_certificate_sds_secret_configs\": [ { \"name\":\"tls_sds\", \"sds_config\":{ \"path\":\"/certs/sds-auth-cert.json\" } } ], \"validation_context_sds_secret_config\": { \"name\":\"validation_context_sds\", \"sds_config\":{ \"path\":\"/certs/sds-validation.json\" } } } } }, \"load_assignment\": { \"cluster_name\": \"sds-cluster\", \"endpoints\": [ { \"lb_endpoints\": [ { \"endpoint\": { \"address\": { \"socket_address\": { \"address\": \"sds-server.svc.cluster.local\", \"port_value\": 8080, } } } } ] } ] } } EOF } } } \nRefer to the Envoy documentation for details about configuration parameters for SDS clusters.\nIssue the consul services register command on the Consul agent on the Envoy proxy's node to register the service. The following example command registers an ingress gateway proxy from a public-ingress.hcl file:\n$ consul services register public-ingress.hcl \nRefer to Register services and health checks for additional information about registering services in Consul.\nStore TLS client authentication files, certificate files, and keys on disk where the Envoy proxy runs and ensure that they are available to Consul. Refer to the Envoy documentation for details on configuring authentication files.\nThe following example specifies certificate chain:\ncerts/sds-auth-cert.json\n{ \"resources\": [ { \"@type\": \"type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.Secret\", \"name\": \"tls_sds\", \"tls_certificate\": { \"certificate_chain\": { \"filename\": \"/certs/sds-client-auth.crt\" }, \"private_key\": { \"filename\": \"/certs/sds-client-auth.key\" } } } ] } \nThe following example specifies the validation context:\n/certs/sds-validation.json\n{ \"resources\": [ { \"@type\": \"type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.Secret\", \"name\": \"validation_context_sds\", \"validation_context\": { \"trusted_ca\": { \"filename\": \"/certs/sds-ca.crt\" } } } ] } \nIssue the consul connect envoy command to bootstrap Envoy. The following example starts Envoy and registers it as a service called public-ingress:\n$ consul connect envoy -gateway=ingress -service public-ingress \nRefer to Consul Connect Envoy for additional information about using the consul connect envoy command.\nDefine an ingress gateway configuration entry\nCreate an ingress gateway configuration entry that enables the gateway to use certificates from SDS. The configuration entry also maps downstream ingress listeners to upstream services. Configure the following fields:\nKind: Set the value to ingress-gateway.\nName: Consul applies the configuration entry settings to ingress gateway proxies with names that match the Name field.\nTLS: The main TLS parameter for the configuration entry holds the SDS configuration. You can also specify TLS configurations per listener and per service.\nTLS.SDS: The SDS map includes the following configuration settings:\nClusterName: Specifies the name of the cluster you specified when configuring the SDS cluster.\nCertResource: Specifies the name of the certificate resource to load.\nListeners: Specify one or more listeners.\nListeners.Port: Specify a port for the listener. Each listener is uniquely identified by its port number.\nListeners.Protocol: The default protocol is tcp, but you must specify the protocol used by the services you want to allow traffic from.\nListeners.Services: The Services field contains the services that you want to expose to upstream services. The field contains several options and sub-configurations that enable granular control over ingress traffic, such as health check and TLS configurations.\nFor Consul Enterprise service meshes, you may also need to configure the Partition and Namespace fields for the gateway and for each exposed service.\nRefer to Ingress gateway configuration entry reference for details about the supported parameters.\nThe following example directs Consul to retrieve example.com-public-cert certificates from an SDS cluster named sds-cluster and serve them to all listeners:\npublic-ingress-cfg.hcl\nKind = \"ingress-gateway\" Name = \"public-ingress\" TLS { SDS { ClusterName = \"sds-cluster\" CertResource = \"example.com-public-cert\" } } Listeners = [ { Port = 8443 Protocol = \"http\" Services = [\"*\"] } ] \nRegister the ingress gateway configuration entry\nYou can register the configuration entry using the consul config command or by calling the /config API endpoint. Refer to How to Use Configuration Entries for details about applying configuration entries.\nThe following example registers an ingress gateway configuration entry named public-ingress-cfg.hcl that is stored on the local system:\n$ consul config write public-ingress-cfg.hcl \nThe Envoy instance starts a listener on the port specified in the configuration entry and fetches the TLS certificate named from the SDS server."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.15.x/connect/gateways/ingress-gateway/tls-external-service",
  "text": "Consul | HashiCorp Developer\nThis page does not exist for version v1.15.x."
},
{
  "url": "https://developer.hashicorp.com/docs/connect/gateways/ingress-gateway",
  "text": "We couldn't find the page you're looking for.\nPlease check the url you entered for typos, go back to the page you came from, or go to one of the pages below.\nHashiCorp Cloud Platform\nTerraform\nPacker\nConsul\nVault\nBoundary\nNomad\nWaypoint\nVagrant"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.15.x/connect/gateways/ingress-gateway/ingress-gateways-tls-external-service",
  "text": "Serve custom TLS certificates from an external service | Consul\nThis is an advanced topic that describes how to configure ingress gateways to serve TLS certificates sourced from an external service to inbound traffic using secret discovery service (SDS). SDS is a low-level feature designed for developers building integrations with custom TLS management solutions. For instructions on more common ingress gateway implementations, refer to Implement an ingress gateway. \nThe following process describes the general procedure for configuring ingress gateways to serve TLS certificates sourced from external services:\nConfigure static SDS clusters in the ingress gateway service definition.\nRegister the service definition.\nConfigure TLS client authentication\nStart Envoy.\nConfigure SDS settings in an ingress gateway configuration entry.\nRegister the ingress gateway configuration entry with Consul. \nThe external service must implement Envoy's gRPC secret discovery service (SDS) API.\nYou should have some familiarity with Envoy configuration and the SDS protocol.\nThe connect.enabled parameter must be set to true for all server agents in the Consul datacenter.\nThe ports.grpc parameter must be configured for all server agents in the Consul datacenter.\nACL requirements\nIf ACLs are enabled, you must present a token when registering ingress gateways that grant the following permissions:\nservice:write for the ingress gateway's service name \nservice:read for all services in the ingress gateway's configuration entry\nnode:read for all nodes of the services in the ingress gateway's configuration entry.\nThese privileges authorize the token to route communications to other services in the mesh. If the Consul client agent on the gateway's node is not configured to use the default gRPC port, 8502, then the gateway's token must also provide agent:read for its node's name in order to discover the agent's gRPC port. gRPC is used to expose Envoy's xDS API to Envoy proxies.\nYou must define one or more additional static clusters in the ingress gateway service definition for each Envoy proxy associated with the gateway. The additional clusters define how Envoy should connect to the required SDS services.\nConfigure the static clusters in the Proxy.Config.envoy_envoy_extra_static_clusters_json parameter in the service definition. \nThe clusters must provide connection information and any necessary authentication information, such as mTLS credentials.\nYou must manually register the ingress gateway with Consul proxy to define extra clusters in Envoy's bootstrap configuration. You can not use the -register flag with consul connect envoy -gateway=ingress to automatically register the proxy to define static clusters. \nIn the following example, the public-ingress gateway includes a static cluster named sds-cluster that specifies paths to the SDS certificate and SDS certification validation files:\npublic-ingress-service.hcl\nServices { Name = \"public-ingress\" Kind = \"ingress-gateway\" Proxy { Config { envoy_extra_static_clusters_json = <<EOF { \"name\": \"sds-cluster\", \"connect_timeout\": \"5s\", \"http2_protocol_options\": {}, \"type\": \"LOGICAL_DNS\", \"transport_socket\": { \"name\":\"tls\", \"typed_config\": { \"@type\":\"type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\", \"common_tls_context\":{ \"tls_certificate_sds_secret_configs\": [ { \"name\":\"tls_sds\", \"sds_config\":{ \"path\":\"/certs/sds-auth-cert.json\" } } ], \"validation_context_sds_secret_config\": { \"name\":\"validation_context_sds\", \"sds_config\":{ \"path\":\"/certs/sds-validation.json\" } } } } }, \"load_assignment\": { \"cluster_name\": \"sds-cluster\", \"endpoints\": [ { \"lb_endpoints\": [ { \"endpoint\": { \"address\": { \"socket_address\": { \"address\": \"sds-server.svc.cluster.local\", \"port_value\": 8080, } } } } ] } ] } } EOF } } } \nRefer to the Envoy documentation for details about configuration parameters for SDS clusters. \nIssue the consul services register command on the Consul agent on the Envoy proxy's node to register the service. The following example command registers an ingress gateway proxy from a public-ingress.hcl file:\n$ consul services register public-ingress.hcl \nRefer to Register services and health checks for additional information about registering services in Consul.\nStore TLS client authentication files, certificate files, and keys on disk where the Envoy proxy runs and ensure that they are available to Consul. Refer to the Envoy documentation for details on configuring authentication files.\nThe following example specifies certificate chain:\ncerts/sds-auth-cert.json\n{ \"resources\": [ { \"@type\": \"type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.Secret\", \"name\": \"tls_sds\", \"tls_certificate\": { \"certificate_chain\": { \"filename\": \"/certs/sds-client-auth.crt\" }, \"private_key\": { \"filename\": \"/certs/sds-client-auth.key\" } } } ] } \nThe following example specifies the validation context:\n/certs/sds-validation.json\n{ \"resources\": [ { \"@type\": \"type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.Secret\", \"name\": \"validation_context_sds\", \"validation_context\": { \"trusted_ca\": { \"filename\": \"/certs/sds-ca.crt\" } } } ] } \nIssue the consul connect envoy command to bootstrap Envoy. The following example starts Envoy and registers it as a service called public-ingress:\n$ consul connect envoy -gateway=ingress -service public-ingress \nRefer to Consul Connect Envoy for additional information about using the consul connect envoy command.\nDefine an ingress gateway configuration entry\nCreate an ingress gateway configuration entry that enables the gateway to use certificates from SDS. The configuration entry also maps downstream ingress listeners to upstream services. Configure the following fields:\nKind: Set the value to ingress-gateway.\nName: Consul applies the configuration entry settings to ingress gateway proxies with names that match the Name field. \nTLS: The main TLS parameter for the configuration entry holds the SDS configuration. You can also specify TLS configurations per listener and per service. \nTLS.SDS: The SDS map includes the following configuration settings:\nClusterName: Specifies the name of the cluster you specified when configuring the SDS cluster. \nCertResource: Specifies the name of the certificate resource to load. \nListeners: Specify one or more listeners.\nListeners.Port: Specify a port for the listener. Each listener is uniquely identified by its port number.\nListeners.Protocol: The default protocol is tcp, but you must specify the protocol used by the services you want to allow traffic from.\nListeners.Services: The Services field contains the services that you want to expose to upstream services. The field contains several options and sub-configurations that enable granular control over ingress traffic, such as health check and TLS configurations. \nFor Consul Enterprise service meshes, you may also need to configure the Partition and Namespace fields for the gateway and for each exposed service. \nRefer to Ingress gateway configuration entry reference for details about the supported parameters. \nThe following example directs Consul to retrieve example.com-public-cert certificates from an SDS cluster named sds-cluster and serve them to all listeners:\npublic-ingress-cfg.hcl\nKind = \"ingress-gateway\" Name = \"public-ingress\" TLS { SDS { ClusterName = \"sds-cluster\" CertResource = \"example.com-public-cert\" } } Listeners = [ { Port = 8443 Protocol = \"http\" Services = [\"*\"] } ] \nRegister the ingress gateway configuration entry\nYou can register the configuration entry using the consul config command or by calling the /config API endpoint. Refer to How to Use Configuration Entries for details about applying configuration entries.\nThe following example registers an ingress gateway configuration entry named public-ingress-cfg.hcl that is stored on the local system:\n$ consul config write public-ingress-cfg.hcl \nThe Envoy instance starts a listener on the port specified in the configuration entry and fetches the TLS certificate named from the SDS server."
},
{
  "url": "https://developer.hashicorp.com/commands/connect/envoy",
  "text": "We couldn't find the page you're looking for.\nPlease check the url you entered for typos, go back to the page you came from, or go to one of the pages below.\nHashiCorp Cloud Platform\nTerraform\nPacker\nConsul\nVault\nBoundary\nNomad\nWaypoint\nVagrant"
},
{
  "url": "https://developer.hashicorp.com/docs/connect/l7-traffic",
  "text": "We couldn't find the page you're looking for.\nPlease check the url you entered for typos, go back to the page you came from, or go to one of the pages below.\nHashiCorp Cloud Platform\nTerraform\nPacker\nConsul\nVault\nBoundary\nNomad\nWaypoint\nVagrant"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.16.x/connect/gateways/ingress-gateway/tls-external-service",
  "text": "Serve custom TLS certificates from an external service | Consul\nThis is an advanced topic that describes how to configure ingress gateways to serve TLS certificates sourced from an external service to inbound traffic using secret discovery service (SDS). SDS is a low-level feature designed for developers building integrations with custom TLS management solutions. For instructions on more common ingress gateway implementations, refer to Implement an ingress gateway.\nThe following process describes the general procedure for configuring ingress gateways to serve TLS certificates sourced from external services:\nConfigure static SDS clusters in the ingress gateway service definition.\nRegister the service definition.\nConfigure TLS client authentication\nStart Envoy.\nConfigure SDS settings in an ingress gateway configuration entry.\nRegister the ingress gateway configuration entry with Consul.\nThe external service must implement Envoy's gRPC secret discovery service (SDS) API.\nYou should have some familiarity with Envoy configuration and the SDS protocol.\nThe connect.enabled parameter must be set to true for all server agents in the Consul datacenter.\nThe ports.grpc parameter must be configured for all server agents in the Consul datacenter.\nACL requirements\nIf ACLs are enabled, you must present a token when registering ingress gateways that grant the following permissions:\nservice:write for the ingress gateway's service name\nservice:read for all services in the ingress gateway's configuration entry\nnode:read for all nodes of the services in the ingress gateway's configuration entry.\nThese privileges authorize the token to route communications to other services in the mesh. If the Consul client agent on the gateway's node is not configured to use the default gRPC port, 8502, then the gateway's token must also provide agent:read for its node's name in order to discover the agent's gRPC port. gRPC is used to expose Envoy's xDS API to Envoy proxies.\nYou must define one or more additional static clusters in the ingress gateway service definition for each Envoy proxy associated with the gateway. The additional clusters define how Envoy should connect to the required SDS services.\nConfigure the static clusters in the Proxy.Config.envoy_envoy_extra_static_clusters_json parameter in the service definition.\nThe clusters must provide connection information and any necessary authentication information, such as mTLS credentials.\nYou must manually register the ingress gateway with Consul proxy to define extra clusters in Envoy's bootstrap configuration. You can not use the -register flag with consul connect envoy -gateway=ingress to automatically register the proxy to define static clusters.\nIn the following example, the public-ingress gateway includes a static cluster named sds-cluster that specifies paths to the SDS certificate and SDS certification validation files:\npublic-ingress-service.hcl\nServices { Name = \"public-ingress\" Kind = \"ingress-gateway\" Proxy { Config { envoy_extra_static_clusters_json = <<EOF { \"name\": \"sds-cluster\", \"connect_timeout\": \"5s\", \"typed_extension_protocol_options\": { \"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\": { \"@type\": \"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\", \"explicit_http_config\": { \"http2_protocol_options\": {} } } }, \"type\": \"LOGICAL_DNS\", \"transport_socket\": { \"name\":\"tls\", \"typed_config\": { \"@type\":\"type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\", \"common_tls_context\":{ \"tls_certificate_sds_secret_configs\": [ { \"name\":\"tls_sds\", \"sds_config\":{ \"path\":\"/certs/sds-auth-cert.json\" } } ], \"validation_context_sds_secret_config\": { \"name\":\"validation_context_sds\", \"sds_config\":{ \"path\":\"/certs/sds-validation.json\" } } } } }, \"load_assignment\": { \"cluster_name\": \"sds-cluster\", \"endpoints\": [ { \"lb_endpoints\": [ { \"endpoint\": { \"address\": { \"socket_address\": { \"address\": \"sds-server.svc.cluster.local\", \"port_value\": 8080, } } } } ] } ] } } EOF } } } \nRefer to the Envoy documentation for details about configuration parameters for SDS clusters.\nIssue the consul services register command on the Consul agent on the Envoy proxy's node to register the service. The following example command registers an ingress gateway proxy from a public-ingress.hcl file:\n$ consul services register public-ingress.hcl \nRefer to Register services and health checks for additional information about registering services in Consul.\nStore TLS client authentication files, certificate files, and keys on disk where the Envoy proxy runs and ensure that they are available to Consul. Refer to the Envoy documentation for details on configuring authentication files.\nThe following example specifies certificate chain:\ncerts/sds-auth-cert.json\n{ \"resources\": [ { \"@type\": \"type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.Secret\", \"name\": \"tls_sds\", \"tls_certificate\": { \"certificate_chain\": { \"filename\": \"/certs/sds-client-auth.crt\" }, \"private_key\": { \"filename\": \"/certs/sds-client-auth.key\" } } } ] } \nThe following example specifies the validation context:\n/certs/sds-validation.json\n{ \"resources\": [ { \"@type\": \"type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.Secret\", \"name\": \"validation_context_sds\", \"validation_context\": { \"trusted_ca\": { \"filename\": \"/certs/sds-ca.crt\" } } } ] } \nIssue the consul connect envoy command to bootstrap Envoy. The following example starts Envoy and registers it as a service called public-ingress:\n$ consul connect envoy -gateway=ingress -service public-ingress \nRefer to Consul Connect Envoy for additional information about using the consul connect envoy command.\nDefine an ingress gateway configuration entry\nCreate an ingress gateway configuration entry that enables the gateway to use certificates from SDS. The configuration entry also maps downstream ingress listeners to upstream services. Configure the following fields:\nKind: Set the value to ingress-gateway.\nName: Consul applies the configuration entry settings to ingress gateway proxies with names that match the Name field.\nTLS: The main TLS parameter for the configuration entry holds the SDS configuration. You can also specify TLS configurations per listener and per service.\nTLS.SDS: The SDS map includes the following configuration settings:\nClusterName: Specifies the name of the cluster you specified when configuring the SDS cluster.\nCertResource: Specifies the name of the certificate resource to load.\nListeners: Specify one or more listeners.\nListeners.Port: Specify a port for the listener. Each listener is uniquely identified by its port number.\nListeners.Protocol: The default protocol is tcp, but you must specify the protocol used by the services you want to allow traffic from.\nListeners.Services: The Services field contains the services that you want to expose to upstream services. The field contains several options and sub-configurations that enable granular control over ingress traffic, such as health check and TLS configurations.\nFor Consul Enterprise service meshes, you may also need to configure the Partition and Namespace fields for the gateway and for each exposed service.\nRefer to Ingress gateway configuration entry reference for details about the supported parameters.\nThe following example directs Consul to retrieve example.com-public-cert certificates from an SDS cluster named sds-cluster and serve them to all listeners:\npublic-ingress-cfg.hcl\nKind = \"ingress-gateway\" Name = \"public-ingress\" TLS { SDS { ClusterName = \"sds-cluster\" CertResource = \"example.com-public-cert\" } } Listeners = [ { Port = 8443 Protocol = \"http\" Services = [\"*\"] } ] \nRegister the ingress gateway configuration entry\nYou can register the configuration entry using the consul config command or by calling the /config API endpoint. Refer to How to Use Configuration Entries for details about applying configuration entries.\nThe following example registers an ingress gateway configuration entry named public-ingress-cfg.hcl that is stored on the local system:\n$ consul config write public-ingress-cfg.hcl \nThe Envoy instance starts a listener on the port specified in the configuration entry and fetches the TLS certificate named from the SDS server."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.11.x/connect/ingress-gateway",
  "text": "Consul | HashiCorp Developer\nThis page does not exist for version v1.11.x."
},
{
  "url": "https://developer.hashicorp.com/api/config",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/api-docs/config",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/docs/connect/l7-traffic-management",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.12.x/connect/ingress-gateway",
  "text": "This page does not exist for version v1.12.x."
},
{
  "url": "https://developer.hashicorp.com/docs/connect/config-entries/service-defaults",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/docs/connect/cluster-peering",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/api-docs/connect/intentions",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/docs/upgrading/upgrade-specific",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/docs/connect/config-entries/service-intentions",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.16.x/v1.16.x/enterprise",
  "text": "This page does not exist for version v1.16.x."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.18.x/k8s/crds/upgrade-to-crds",
  "text": "Upgrade Existing Clusters to Use Custom Resource Definitions | Consul\nUpgrading to consul-helm versions >= 0.30.0 will require some changes if you utilize the following:\nconnectInject.centralConfig.enabled\nconnectInject.centralConfig.defaultProtocol\nconnectInject.centralConfig.proxyDefaults\nmeshGateway.globalMode\nconnect annotation consul.hashicorp.com/connect-service-protocol\nIf you were previously setting centralConfig.enabled to false:\nconnectInject: centralConfig: enabled: false \nThen instead you must use server.extraConfig and client.extraConfig:\nclient: extraConfig: | {\"enable_central_service_config\": false} server: extraConfig: | {\"enable_central_service_config\": false} \nIf you were previously setting it to true, it now defaults to true so no changes are required, but you can remove it from your config if you desire.\nIf you were previously setting:\nconnectInject: centralConfig: defaultProtocol: 'http' # or any value \nNow you must use custom resources to manage the protocol for new and existing services:\nTo upgrade, first ensure you're running Consul >= 1.9.0. See Consul Version Upgrade for more information on how to upgrade Consul versions.\nThis version is required to support custom resources.\nNext, modify your Helm values:\nRemove the defaultProtocol config. This won't affect existing services.\nNow you can upgrade your Helm chart to the latest version with the new Helm values.\nFrom now on, any new service will require a ServiceDefaults resource to set its protocol:\napiVersion: consul.hashicorp.com/v1alpha1 kind: ServiceDefaults metadata: name: my-service-name spec: protocol: 'http' \nExisting services will maintain their previously set protocol. If you wish to change that protocol, you must migrate that service's service-defaults config entry to a ServiceDefaults resource. See Migrating Config Entries.\nNote: This setting was removed because it didn't support changing the protocol after a service was first run and because it didn't work in secondary datacenters.\nIf you were previously setting:\nconnectInject: centralConfig: proxyDefaults: | { \"key\": \"value\" // or any values } \nYou will need to perform the following steps to upgrade:\nYou must remove the setting from your Helm values. This won't have any effect on your existing cluster because this config is only read when the cluster is first created.\nYou can then upgrade the Helm chart.\nIf you later wish to change any of the proxy defaults settings, you will need to follow the Migrating Config Entries instructions for your proxy-defaults config entry.\nThis will require Consul >= 1.9.0.\nNote: This setting was removed because it couldn't be changed after initial installation.\nIf you were previously setting:\nmeshGateway: globalMode: 'local' # or any value \nYou will need to perform the following steps to upgrade:\nYou must remove the setting from your Helm values. This won't have any effect on your existing cluster because this config is only read when the cluster is first created.\nYou can then upgrade the Helm chart.\nIf you later wish to change the mode or any other setting in proxy-defaults, you will need to follow the Migrating Config Entries instructions to migrate your proxy-defaults config entry to a ProxyDefaults resource.\nThis will require Consul >= 1.9.0.\nNote: This setting was removed because it couldn't be changed after initial installation.\nIf any of your mesh services had the consul.hashicorp.com/connect-service-protocol annotation set, e.g.\napiVersion: apps/v1 kind: Deployment ... spec: template: metadata: annotations: \"consul.hashicorp.com/connect-inject\": \"true\" \"consul.hashicorp.com/connect-service-protocol\": \"http\" ... \nYou will need to perform the following steps to upgrade:\nEnsure you're running Consul >= 1.9.0. See Consul Version Upgrade for more information on how to upgrade Consul versions.\nThis version is required to support custom resources.\nNext, remove this annotation from existing deployments. This will have no effect on the deployments because the annotation was only used when the service was first created.\nNow you can upgrade your Helm chart to the latest version.\nFrom now on, any new service will require a ServiceDefaults resource to set its protocol:\napiVersion: consul.hashicorp.com/v1alpha1 kind: ServiceDefaults metadata: name: my-service-name spec: protocol: 'http' \nExisting services will maintain their previously set protocol. If you wish to change that protocol, you must migrate that service's service-defaults config entry to a ServiceDefaults resource. See Migrating Config Entries.\nNote: The annotation was removed because it didn't support changing the protocol and it wasn't supported in secondary datacenters.\nA config entry that already exists in Consul must be migrated into a Kubernetes custom resource in order to manage it from Kubernetes:\nDetermine the kind and name of the config entry. For example, the protocol would be set by a config entry with kind: service-defaults and name equal to the name of the service.\nIn another example, a proxy-defaults config has kind: proxy-defaults and name: global.\nOnce you've determined the kind and name, query Consul to get its contents:\n$ consul config read -kind <kind> -name <name> \nThis will require kubectl exec'ing into a Consul server or client pod. If you're using ACLs, you will also need an ACL token passed via the -token flag.\nFor example:\n$ kubectl exec consul-server-0 -- consul config read -name foo -kind service-defaults { \"Kind\": \"service-defaults\", \"Name\": \"foo\", \"Protocol\": \"http\", \"MeshGateway\": {}, \"Expose\": {}, \"CreateIndex\": 60, \"ModifyIndex\": 60 } \nNow we're ready to construct a Kubernetes resource for the config entry.\nIt will look something like:\napiVersion: consul.hashicorp.com/v1alpha1 kind: ServiceDefaults metadata: name: foo annotations: 'consul.hashicorp.com/migrate-entry': 'true' spec: protocol: 'http' \nThe apiVersion will always be consul.hashicorp.com/v1alpha1.\nThe kind will be the CamelCase version of the Consul kind, e.g. proxy-defaults becomes ProxyDefaults.\nmetadata.name will be the name of the config entry.\nmetadata.annotations will contain the \"consul.hashicorp.com/migrate-entry\": \"true\" annotation.\nThe namespace should be whatever namespace the service is deployed in. For ProxyDefaults, we recommend the namespace that Consul is deployed in.\nThe contents of spec will be a transformation from JSON keys to YAML keys.\nThe following keys can be ignored: CreateIndex, ModifyIndex and any key that has an empty object, e.g. \"Expose\": {}.\nFor example:\n{ \"Kind\": \"service-defaults\", \"Name\": \"foo\", \"Protocol\": \"http\", \"MeshGateway\": {}, \"Expose\": {}, \"CreateIndex\": 60, \"ModifyIndex\": 60 } \nBecomes:\napiVersion: consul.hashicorp.com/v1alpha1 kind: ServiceDefaults metadata: name: foo annotations: 'consul.hashicorp.com/migrate-entry': 'true' spec: protocol: 'http' \nAnd\n{ \"Kind\": \"proxy-defaults\", \"Name\": \"global\", \"MeshGateway\": { \"Mode\": \"local\" }, \"Config\": { \"local_connect_timeout_ms\": 1000, \"handshake_timeout_ms\": 10000 }, \"CreateIndex\": 60, \"ModifyIndex\": 60 } \nBecomes:\napiVersion: consul.hashicorp.com/v1alpha1 kind: ProxyDefaults metadata: name: global annotations: 'consul.hashicorp.com/migrate-entry': 'true' spec: meshGateway: mode: local config: # Note that anything under config for ProxyDefaults will use the exact # same keys. local_connect_timeout_ms: 1000 handshake_timeout_ms: 10000 \nRun kubectl apply to apply the Kubernetes resource.\nNext, check that it synced successfully:\n$ kubectl get servicedefaults foo NAME SYNCED AGE foo True 1s \nIf its SYNCED status is True then the migration for this config entry was successful.\nIf its SYNCED status is False, use kubectl describe to view the reason syncing failed:\n$ kubectl describe servicedefaults foo ... Status: Conditions: Last Transition Time: 2021-01-12T21:03:29Z Message: migration failed: Kubernetes resource does not match existing Consul config entry: consul={...}, kube={...} Reason: MigrationFailedError Status: False Type: Synced \nThe most likely reason is that the contents of the Kubernetes resource don't match the Consul resource. Make changes to the Kubernetes resource to match the Consul resource (ignoring the CreateIndex, ModifyIndex and Meta keys).\nOnce the SYNCED status is true, you can make changes to the resource and they will get synced to Consul."
},
{
  "url": "https://developer.hashicorp.com/docs/connect/cluster-peering/index.mdx",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.17.x/k8s/crds/upgrade-to-crds",
  "text": "Upgrade Existing Clusters to Use Custom Resource Definitions | Consul\nUpgrading to consul-helm versions >= 0.30.0 will require some changes if you utilize the following:\nconnectInject.centralConfig.enabled\nconnectInject.centralConfig.defaultProtocol\nconnectInject.centralConfig.proxyDefaults\nmeshGateway.globalMode\nconnect annotation consul.hashicorp.com/connect-service-protocol\nIf you were previously setting centralConfig.enabled to false:\nconnectInject: centralConfig: enabled: false \nThen instead you must use server.extraConfig and client.extraConfig:\nclient: extraConfig: | {\"enable_central_service_config\": false} server: extraConfig: | {\"enable_central_service_config\": false} \nIf you were previously setting it to true, it now defaults to true so no changes are required, but you can remove it from your config if you desire.\nIf you were previously setting:\nconnectInject: centralConfig: defaultProtocol: 'http' # or any value \nNow you must use custom resources to manage the protocol for new and existing services:\nTo upgrade, first ensure you're running Consul >= 1.9.0. See Consul Version Upgrade for more information on how to upgrade Consul versions.\nThis version is required to support custom resources.\nNext, modify your Helm values:\nRemove the defaultProtocol config. This won't affect existing services.\nNow you can upgrade your Helm chart to the latest version with the new Helm values.\nFrom now on, any new service will require a ServiceDefaults resource to set its protocol:\napiVersion: consul.hashicorp.com/v1alpha1 kind: ServiceDefaults metadata: name: my-service-name spec: protocol: 'http' \nExisting services will maintain their previously set protocol. If you wish to change that protocol, you must migrate that service's service-defaults config entry to a ServiceDefaults resource. See Migrating Config Entries.\nNote: This setting was removed because it didn't support changing the protocol after a service was first run and because it didn't work in secondary datacenters.\nconnectInject: centralConfig: proxyDefaults: | { \"key\": \"value\" // or any values } \nYou will need to perform the following steps to upgrade:\nYou must remove the setting from your Helm values. This won't have any effect on your existing cluster because this config is only read when the cluster is first created.\nYou can then upgrade the Helm chart.\nIf you later wish to change any of the proxy defaults settings, you will need to follow the Migrating Config Entries instructions for your proxy-defaults config entry.\nThis will require Consul >= 1.9.0.\nNote: This setting was removed because it couldn't be changed after initial installation.\nmeshGateway: globalMode: 'local' # or any value \nYou must remove the setting from your Helm values. This won't have any effect on your existing cluster because this config is only read when the cluster is first created.\nYou can then upgrade the Helm chart.\nIf you later wish to change the mode or any other setting in proxy-defaults, you will need to follow the Migrating Config Entries instructions to migrate your proxy-defaults config entry to a ProxyDefaults resource.\nThis will require Consul >= 1.9.0.\nNote: This setting was removed because it couldn't be changed after initial installation.\nIf any of your mesh services had the consul.hashicorp.com/connect-service-protocol annotation set, e.g.\napiVersion: apps/v1 kind: Deployment ... spec: template: metadata: annotations: \"consul.hashicorp.com/connect-inject\": \"true\" \"consul.hashicorp.com/connect-service-protocol\": \"http\" ... \nEnsure you're running Consul >= 1.9.0. See Consul Version Upgrade for more information on how to upgrade Consul versions.\nThis version is required to support custom resources.\nNext, remove this annotation from existing deployments. This will have no effect on the deployments because the annotation was only used when the service was first created.\nNow you can upgrade your Helm chart to the latest version.\nFrom now on, any new service will require a ServiceDefaults resource to set its protocol:\napiVersion: consul.hashicorp.com/v1alpha1 kind: ServiceDefaults metadata: name: my-service-name spec: protocol: 'http' \nExisting services will maintain their previously set protocol. If you wish to change that protocol, you must migrate that service's service-defaults config entry to a ServiceDefaults resource. See Migrating Config Entries.\nNote: The annotation was removed because it didn't support changing the protocol and it wasn't supported in secondary datacenters.\nA config entry that already exists in Consul must be migrated into a Kubernetes custom resource in order to manage it from Kubernetes:\nDetermine the kind and name of the config entry. For example, the protocol would be set by a config entry with kind: service-defaults and name equal to the name of the service.\nIn another example, a proxy-defaults config has kind: proxy-defaults and name: global.\nOnce you've determined the kind and name, query Consul to get its contents:\n$ consul config read -kind <kind> -name <name> \nThis will require kubectl exec'ing into a Consul server or client pod. If you're using ACLs, you will also need an ACL token passed via the -token flag.\nFor example:\n$ kubectl exec consul-server-0 -- consul config read -name foo -kind service-defaults { \"Kind\": \"service-defaults\", \"Name\": \"foo\", \"Protocol\": \"http\", \"MeshGateway\": {}, \"Expose\": {}, \"CreateIndex\": 60, \"ModifyIndex\": 60 } \nNow we're ready to construct a Kubernetes resource for the config entry.\nIt will look something like:\napiVersion: consul.hashicorp.com/v1alpha1 kind: ServiceDefaults metadata: name: foo annotations: 'consul.hashicorp.com/migrate-entry': 'true' spec: protocol: 'http' \nThe apiVersion will always be consul.hashicorp.com/v1alpha1.\nThe kind will be the CamelCase version of the Consul kind, e.g. proxy-defaults becomes ProxyDefaults.\nmetadata.name will be the name of the config entry.\nmetadata.annotations will contain the \"consul.hashicorp.com/migrate-entry\": \"true\" annotation.\nThe namespace should be whatever namespace the service is deployed in. For ProxyDefaults, we recommend the namespace that Consul is deployed in.\nThe contents of spec will be a transformation from JSON keys to YAML keys.\nThe following keys can be ignored: CreateIndex, ModifyIndex and any key that has an empty object, e.g. \"Expose\": {}.\nFor example:\n{ \"Kind\": \"service-defaults\", \"Name\": \"foo\", \"Protocol\": \"http\", \"MeshGateway\": {}, \"Expose\": {}, \"CreateIndex\": 60, \"ModifyIndex\": 60 } \nBecomes:\napiVersion: consul.hashicorp.com/v1alpha1 kind: ServiceDefaults metadata: name: foo annotations: 'consul.hashicorp.com/migrate-entry': 'true' spec: protocol: 'http' \nAnd\n{ \"Kind\": \"proxy-defaults\", \"Name\": \"global\", \"MeshGateway\": { \"Mode\": \"local\" }, \"Config\": { \"local_connect_timeout_ms\": 1000, \"handshake_timeout_ms\": 10000 }, \"CreateIndex\": 60, \"ModifyIndex\": 60 } \nBecomes:\napiVersion: consul.hashicorp.com/v1alpha1 kind: ProxyDefaults metadata: name: global annotations: 'consul.hashicorp.com/migrate-entry': 'true' spec: meshGateway: mode: local config: # Note that anything under config for ProxyDefaults will use the exact # same keys. local_connect_timeout_ms: 1000 handshake_timeout_ms: 10000 \nRun kubectl apply to apply the Kubernetes resource.\nNext, check that it synced successfully:\n$ kubectl get servicedefaults foo NAME SYNCED AGE foo True 1s \nIf its SYNCED status is True then the migration for this config entry was successful.\nIf its SYNCED status is False, use kubectl describe to view the reason syncing failed:\n$ kubectl describe servicedefaults foo ... Status: Conditions: Last Transition Time: 2021-01-12T21:03:29Z Message: migration failed: Kubernetes resource does not match existing Consul config entry: consul={...}, kube={...} Reason: MigrationFailedError Status: False Type: Synced \nThe most likely reason is that the contents of the Kubernetes resource don't match the Consul resource. Make changes to the Kubernetes resource to match the Consul resource (ignoring the CreateIndex, ModifyIndex and Meta keys).\nOnce the SYNCED status is true, you can make changes to the resource and they will get synced to Consul."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.10.x/connect/terminating-gateway",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.16.x/api-gateway/configuration/gateway",
  "text": "Gateway Resource Configuration | Consul\nThis topic provides full details about the Gateway resource.\nA Gateway is an instance of network infrastructure that determines how service traffic should be handled. A Gateway contains one or more listeners that bind to a set of IP addresses. An HTTPRoute or TCPRoute can then attach to a gateway listener to direct traffic from the gateway to a service.\nGateway instances derive their configurations from the GatewayClass resource, which acts as a template for individual Gateway deployments. Refer to GatewayClass for additional information.\nSpecify the following parameters to declare a Gateway:\nParameterDescriptionRequired\nkind\tSpecifies the type of configuration object. The value should always be Gateway.\tRequired\t\ndescription\tHuman-readable string that describes the purpose of the Gateway.\tOptional\t\nversion \tSpecifies the Kubernetes API version. The value should always be gateway.networking.k8s.io/v1alpha2\tRequired\t\nscope\tSpecifies the effective scope of the Gateway. The value should always be namespaced.\tRequired\t\nfields\tSpecifies the configurations for the Gateway. The fields are listed in the configuration model. Details for each field are described in the specification.\tRequired\t\nThe following outline shows how to format the configurations in the Gateway object. Click on a property name to view details about the configuration.\ngatewayClassName: string | required\nlisteners: array of objects | required\nallowedRoutes: object | required\nnamespaces: object | required\nfrom: string | required\nselector: object | required if from is configured to selector\nmatchExpressions: array of objects | required if matchLabels is not configured\nkey: string | required if matchExpressions is declared\noperator: string | required if matchExpressions is declared\nvalues: array of strings | required if matchExpressions is declared\nmatchLabels: map of strings | required if matchExpressions is not configured\nhostname: string | required\nname: string | required\nport: integer | required\nprotocol: string | required\ntls: object | required if protocol is set to HTTPS\ncertificateRefs: array or objects | required if tls is declared\nname: string | required if certificateRefs is declared\nnamespace: string | required if certificateRefs is declared\nmode: string | required if certificateRefs is declared\noptions: map of strings | optional\nThis topic provides details about the configuration parameters.\ngatewayClassName\nSpecifies the name of the GatewayClass resource used for the Gateway instance. Unless you are using a custom GatewayClass, this value should be set to consul.\nType: string\nRequired: required\nlisteners\nSpecifies the listeners associated with the Gateway. At least one listener must be specified. Each listener within a Gateway must have a unique combination of hostname, port, and protocol.\nType: array of objects\nRequired: required\nlisteners.allowedRoutes\nSpecifies a namespace object that defines the types of routes that may be attached to a listener.\nType: object\nRequired: required\nlisteners.allowedRoutes.namespaces\nDetermines which routes are allowed to attach to the listener. Only routes in the same namespace as the Gateway may be attached by default.\nType: string\nRequired: optional\nDefault: Same namespace as the parent Gateway\nlisteners.allowedRoutes.namespaces.from\nDetermines which namespaces are allowed to attach a route to the Gateway. You can specify one of the following strings:\nAll: Routes in all namespaces may be attached to the Gateway.\nSame (default): Only routes in the same namespace as the Gateway may be attached.\nSelector: Only routes in namespaces that match the selector may be attached.\nThis parameter is required.\nlisteners.allowedRoutes.namespaces.selector\nSpecifies a method for selecting routes that are allowed to attach to the listener. The Gateway checks for namespaces in the network that match either a regular expression or a label. Routes from the matching namespace are allowed to attach to the listener.\nYou can configure one of the following objects:\nmatchExpressions\nmatchLabels\nThis field is required when from is configured to Selector.\nlisteners.allowedRoutes.namespaces.selector.matchExpressions\nSpecifies an array of requirements for matching namespaces. If a match is found, then routes from the matching namespace(s) are allowed to attach to the Gateway. The following table describes members of the matchExpressions array:\nRequirementDescriptionTypeRequired\nkey\tSpecifies the label that the key applies to.\tstring\trequired when matchExpressions is declared\t\noperator\tSpecifies the key's relation to a set of values. You can use the following keywords: \nIn: Only routes in namespaces that contain the strings in the values field can attach to the Gateway. \nNotIn: Routes in namespaces that do not contain the strings in the values field can attach to the Gateway. \nExists: Routes in namespaces that contain the key value are allowed to attach to the Gateway.\nDoesNotExist: Routes in namespaces that do not contain the key value are allowed to attach to the Gateway.\n\tstring\trequired when matchExpressions is declared\t\nvalues\tSpecifies an array of string values. If operator is configured to In or NotIn, then the values array must contain values. If operator is configured to Exists or DoesNotExist, then the values array must be empty.\tarray of strings\trequired when matchExpressions is declared\t\nIn the following example, routes in namespaces that contain foo and bar are allowed to attach routes to the Gateway.\nnamespaceSelector: matchExpressions: - key: kubernetes.io/metadata.name operator: In values: - foo - bar \nRefer to Labels and Selectors in the Kubernetes documentation for additional information about matchExpressions.\nlisteners.allowedRoutes.namespaces.selector.matchLabels\nSpecifies an array of labels and label values. If a match is found, then routes with the matching label(s) are allowed to attach to the Gateway. This selector can contain any arbitrary key/value pair.\nIn the following example, routes in namespaces that have a bar label are allowed to attach to the Gateway.\nnamespaceSelector: matchLabels: foo: bar \nRefer to Labels and Selectors in the Kubernetes documentation for additional information about labels.\nlisteners.hostname\nSpecifies the listener's hostname.\nType: string\nRequired: required\nlisteners.name\nSpecifies the listener's name.\nType: string\nlisteners.port\nSpecifies the port number that the listener attaches to.\nType: integer\nlisteners.protocol\nSpecifies the protocol the listener communicates on.\nAllowed values are TCP, HTTP, or HTTPS\nlisteners.tls\nSpecifies the tls configurations for the Gateway. The tls object is required if protocol is set to HTTPS. The object contains the following fields:\nParameterDescriptionTypeRequired\ncertificateRefs\t\nSpecifies Kubernetes name and namespace objects that contains TLS certificates and private keys. \nThe certificates establish a TLS handshake for requests that match the hostname of the associated listener. Each reference must be a Kubernetes Secret. If you are using a Secret in a namespace other than the Gateway's, each reference must also have a corresponding ReferenceGrant.\n\tObject or array\tRequired if tls is set\t\nmode\tSpecifies the TLS Mode. Should always be set to Terminate for HTTPRoutes\tstring\tRequired if certificateRefs is set\t\noptions\tSpecifies additional Consul API Gateway options.\tMap of strings\toptional\t\nThe following keys for options are available\napi-gateway.consul.hashicorp.com/tls_min_version\napi-gateway.consul.hashicorp.com/tls_max_version\napi-gateway.consul.hashicorp.com/tls_cipher_suites\nIn the following example, tls settings are configured to use a secret named consul-server-cert in the same namespace as the Gateway and the minimum tls version is set to TLSv1_2.\ntls: certificateRefs: - name: consul-server-cert group: \"\" kind: Secret mode: Terminate options: api-gateway.consul.hashicorp.com/tls_min_version: \"TLSv1_2\" \nExample cross-namespace certificateRef\nThe following example creates a Gateway named example-gateway in namespace gateway-namespace (lines 2-4). The gateway has a certificateRef in namespace secret-namespace (lines 16-18). The reference is allowed because the ReferenceGrant configuration, named reference-grant in namespace secret-namespace (lines 24-27), allows Gateways in gateway-namespace to reference Secrets in secret-namespace (lines 31-35).\ngateway_with_referencegrant.yaml\napiVersion: gateway.networking.k8s.io/v1beta1 kind: Gateway metadata: name: example-gateway namespace: gateway-namespace spec: gatewayClassName: consul listeners: - protocol: HTTPS port: 443 name: https allowedRoutes: namespaces: from: Same tls: certificateRefs: - name: cert namespace: secret-namespace group: \"\" kind: Secret --- apiVersion: gateway.networking.k8s.io/v1alpha2 kind: ReferenceGrant metadata: name: reference-grant namespace: secret-namespace spec: from: - group: gateway.networking.k8s.io kind: Gateway namespace: gateway-namespace to: - group: \"\" kind: Secret name: cert"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.16.x/k8s/crds/upgrade-to-crds",
  "text": "Upgrade Existing Clusters to Use Custom Resource Definitions | Consul\nUpgrading to consul-helm versions >= 0.30.0 will require some changes if you utilize the following:\nconnectInject.centralConfig.enabled\nconnectInject.centralConfig.defaultProtocol\nconnectInject.centralConfig.proxyDefaults\nmeshGateway.globalMode\nconnect annotation consul.hashicorp.com/connect-service-protocol\nIf you were previously setting centralConfig.enabled to false:\nconnectInject: centralConfig: enabled: false \nThen instead you must use server.extraConfig and client.extraConfig:\nclient: extraConfig: | {\"enable_central_service_config\": false} server: extraConfig: | {\"enable_central_service_config\": false} \nIf you were previously setting it to true, it now defaults to true so no changes are required, but you can remove it from your config if you desire.\nconnectInject: centralConfig: defaultProtocol: 'http' # or any value \nNow you must use custom resources to manage the protocol for new and existing services:\nTo upgrade, first ensure you're running Consul >= 1.9.0. See Consul Version Upgrade for more information on how to upgrade Consul versions.\nNext, modify your Helm values:\nRemove the defaultProtocol config. This won't affect existing services.\nNow you can upgrade your Helm chart to the latest version with the new Helm values.\nNote: This setting was removed because it didn't support changing the protocol after a service was first run and because it didn't work in secondary datacenters.\nconnectInject: centralConfig: proxyDefaults: | { \"key\": \"value\" // or any values } \nIf you later wish to change any of the proxy defaults settings, you will need to follow the Migrating Config Entries instructions for your proxy-defaults config entry.\nmeshGateway: globalMode: 'local' # or any value \nIf you later wish to change the mode or any other setting in proxy-defaults, you will need to follow the Migrating Config Entries instructions to migrate your proxy-defaults config entry to a ProxyDefaults resource.\nIf any of your mesh services had the consul.hashicorp.com/connect-service-protocol annotation set, e.g.\napiVersion: apps/v1 kind: Deployment ... spec: template: metadata: annotations: \"consul.hashicorp.com/connect-inject\": \"true\" \"consul.hashicorp.com/connect-service-protocol\": \"http\" ... \nEnsure you're running Consul >= 1.9.0. See Consul Version Upgrade for more information on how to upgrade Consul versions.\nNext, remove this annotation from existing deployments. This will have no effect on the deployments because the annotation was only used when the service was first created.\nNow you can upgrade your Helm chart to the latest version.\nNote: The annotation was removed because it didn't support changing the protocol and it wasn't supported in secondary datacenters.\nA config entry that already exists in Consul must be migrated into a Kubernetes custom resource in order to manage it from Kubernetes:\nDetermine the kind and name of the config entry. For example, the protocol would be set by a config entry with kind: service-defaults and name equal to the name of the service.\nIn another example, a proxy-defaults config has kind: proxy-defaults and name: global.\nOnce you've determined the kind and name, query Consul to get its contents:\n$ consul config read -kind <kind> -name <name> \nThis will require kubectl exec'ing into a Consul server or client pod. If you're using ACLs, you will also need an ACL token passed via the -token flag.\n$ kubectl exec consul-server-0 -- consul config read -name foo -kind service-defaults { \"Kind\": \"service-defaults\", \"Name\": \"foo\", \"Protocol\": \"http\", \"MeshGateway\": {}, \"Expose\": {}, \"CreateIndex\": 60, \"ModifyIndex\": 60 } \nNow we're ready to construct a Kubernetes resource for the config entry.\nIt will look something like:\nThe apiVersion will always be consul.hashicorp.com/v1alpha1.\nThe kind will be the CamelCase version of the Consul kind, e.g. proxy-defaults becomes ProxyDefaults.\nmetadata.name will be the name of the config entry.\nmetadata.annotations will contain the \"consul.hashicorp.com/migrate-entry\": \"true\" annotation.\nThe namespace should be whatever namespace the service is deployed in. For ProxyDefaults, we recommend the namespace that Consul is deployed in.\nThe contents of spec will be a transformation from JSON keys to YAML keys.\nThe following keys can be ignored: CreateIndex, ModifyIndex and any key that has an empty object, e.g. \"Expose\": {}.\n{ \"Kind\": \"service-defaults\", \"Name\": \"foo\", \"Protocol\": \"http\", \"MeshGateway\": {}, \"Expose\": {}, \"CreateIndex\": 60, \"ModifyIndex\": 60 } \nAnd\n{ \"Kind\": \"proxy-defaults\", \"Name\": \"global\", \"MeshGateway\": { \"Mode\": \"local\" }, \"Config\": { \"local_connect_timeout_ms\": 1000, \"handshake_timeout_ms\": 10000 }, \"CreateIndex\": 60, \"ModifyIndex\": 60 } \napiVersion: consul.hashicorp.com/v1alpha1 kind: ProxyDefaults metadata: name: global annotations: 'consul.hashicorp.com/migrate-entry': 'true' spec: meshGateway: mode: local config: # Note that anything under config for ProxyDefaults will use the exact # same keys. local_connect_timeout_ms: 1000 handshake_timeout_ms: 10000 \nRun kubectl apply to apply the Kubernetes resource.\nNext, check that it synced successfully:\n$ kubectl get servicedefaults foo NAME SYNCED AGE foo True 1s \nIf its SYNCED status is True then the migration for this config entry was successful.\nIf its SYNCED status is False, use kubectl describe to view the reason syncing failed:\n$ kubectl describe servicedefaults foo ... Status: Conditions: Last Transition Time: 2021-01-12T21:03:29Z Message: migration failed: Kubernetes resource does not match existing Consul config entry: consul={...}, kube={...} Reason: MigrationFailedError Status: False Type: Synced \nThe most likely reason is that the contents of the Kubernetes resource don't match the Consul resource. Make changes to the Kubernetes resource to match the Consul resource (ignoring the CreateIndex, ModifyIndex and Meta keys).\nOnce the SYNCED status is true, you can make changes to the resource and they will get synced to Consul."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.16.x/api-gateway/configuration/routes",
  "text": "Route Resource Configuration | Consul\nThis topic describes how to create and configure Route resources. Routes are independent configuration objects that are associated with specific listeners.\nDeclare a route with either kind: HTTPRoute or kind: TCPRoute and configure the route parameters in the spec block. Refer to the Kubernetes Gateway API documentation for each object type for details:\nHTTPRoute\nTCPRoute\nThe following example creates a route named example-route associated with a listener defined in example-gateway.\nroutes.yaml\napiVersion: gateway.networking.k8s.io/v1beta1 kind: HTTPRoute metadata: name: example-route spec: parentRefs: - name: example-gateway rules: - backendRefs: - kind: Service name: echo port: 8080 \nThe following outline shows how to format the configurations for the Route object. The top-level spec field is the root for all configurations. Click on a property name to view details about the configuration.\nparentRefs: array of objects | optional\ngroup: string | optional\nkind: string | optional\nname: string | required\nnamespace: string | optional\nsectionName: string | optional\nrules: list of objects | optional\nbackendRefs: list of objects | optional\ngroup: string | optional\nkind: string | optional\nname: string | required\nnamespace: string | optional\nport: integer | required\nweight: integer | optional\nfilters: list of objects | optional\ntype: string | required\nrequestHeaderModifier: object | optional\nset: array of objects | optional\nname: string | required\nvalue: string | required\nadd: array of objects | optional\nvalue: string | required\nremove: array of strings | optional\nurlRewrite: object | optional\npath: object | required\nreplacePrefixMatch: string | required\ntype: string | required\nmatches: array of objects | optional\npath: list of objects | optional\ntype: string | required\nvalue: string | required\nheaders: list of objects | optional\ntype: string | required\nvalue: string | required\nqueryParams: list of objects | optional\nmethod: string | optional\nThis topic provides details about the configuration parameters.\nparentRefs\nThis field contains the list of Gateways that the route should attach to. If not set, the route will not attach to a Gateway. The following table describes the objects you can configure in the parentRefs block:\nParameterDescriptionTypeRequired\ngroup\tSpecifies the Kubernetes API group of the Gateway to attach to. You can specify the following values: \ngateway.networking.k8s.io\n. Defaults to gateway.networking.k8s.io.\tString\tOptional\t\nkind\tSpecifies the Kubernetes kind of the Gateway to attach to. you can specify the following values: \nGateway\n. Defaults to Gateway.\tString\tOptional\t\nname\tSpecifies the name of the Gateway the route is attached to.\tString\tRequired\t\nnamespace\tSpecifies the Kubernetes namespace containing the Gateway to attach to. If the Gateway is in a different Kubernetes namespace than the Route, then you must specify a value. Defaults to the Route namespace.\tString\tOptional\t\nsectionName\tSpecifies the name of a specific listener on the Gateway to attach to. The Route attempts to attach to all listeners on the Gateway.\tString\tRequired\t\nrules\nThe rules field contains a list of objects that define behaviors for network traffic that goes through the route. The rule configuration contains the following objects:\nbackendRefs: Specifies which backend services the Route references when processing traffic.\nfilters: Specifies which operations Consul API Gateway performs when traffic goes through the Route.\nmatches: Determines which requests Consul API Gateway processes.\nRules are optional.\nrules.backendRefs\nThis field specifies backend services that the Route references. The following table describes the parameters for backendRefs:\nParameterDescriptionTypeRequired\ngroup\tSpecifies the Kubernetes API Group of the referenced backend. You can specify the following values: \n\"\": Specifies the core Kubernetes API group. This value must be used when kind is set to Service. This is the default value if unspecified.\napi-gateway.consul.hashicorp.com: This value must be used when kind is set to MeshService.\n\tString\tOptional\t\nkind\tSpecifies the Kubernetes Kind of the referenced backend. You can specify the following values: \nService (default): Indicates that the backendRef references a Service in the Kubernetes cluster. \nMeshService: Indicates that the backendRef references a service in the Consul mesh. Refer to the MeshService documentation for additional information.\n\tString\tOptional\t\nname\tSpecifies the name of the Kubernetes Service or Consul mesh service resource.\tString\tRequired\t\nnamespace\tSpecifies the Kubernetes namespace containing the Kubernetes Service or Consul mesh service resource. You must specify a value if the Service or Consul mesh service is defined in a different namespace from the Route. Defaults to the namespace of the Route. \nTo create a route for a backendRef in a different namespace, you must also create a ReferenceGrant. Refer to the example route configured to reference across namespaces.\tString\tOptional\t\nport\tSpecifies the port number for accessing the Kubernetes or Consul service.\tInteger\tRequired\t\nweight\tSpecifies the proportion of requests sent to the backend. Computed as weight divided by the sum of all weights in this backendRefs list. Defaults to 1. A value of 0 indicates that no requests should be sent to the backend.\tInteger\tOptional\t\nExample cross-namespace backendRef\nThe following example creates a route named example-route in namespace gateway-namespace. This route has a backendRef in namespace service-namespace. Traffic is allowed because the ReferenceGrant, named reference-grant in namespace service-namespace, allows traffic from HTTPRoutes in gateway-namespace to Services in service-namespace.\nroute_with_referencegrant.yaml\napiVersion: gateway.networking.k8s.io/v1beta1 kind: HTTPRoute metadata: name: example-route namespace: gateway-namespace spec: parentRefs: - name: example-gateway rules: - backendRefs: - kind: Service name: echo namespace: service-namespace port: 8080 --- apiVersion: gateway.networking.k8s.io/v1alpha2 kind: ReferenceGrant metadata: name: reference-grant namespace: service-namespace spec: from: - group: gateway.networking.k8s.io kind: HTTPRoute namespace: gateway-namespace to: - group: \"\" kind: Service name: echo \nrules.filters\nThe filters block defines steps for processing requests. You can configure filters to modify the properties of matching incoming requests and enable Consul API Gateway features, such as rewriting path prefixes (refer to Reroute HTTP requests for additional information).\nType: Array of objects\nRequired: Optional\nrules.filters.type\nSpecifies the type of filter you want to apply to the route. The parameter is optional and takes a string value.\nYou can specify the following values:\nRequestHeaderModifier: The RequestHeaderModifier type modifies the HTTP headers on the incoming request. You must define the rules.filters.requestHeaderModifier configurations to use this filter type.\nURLRewrite: The URLRewrite type modifies the URL path on the incoming request. You must define the rules.filters.urlRewrite configurations to use this filter type.\nrules.filters.requestHeaderModifier\nDefines operations to perform on matching request headers when rules.filters.type is configured to RequestHeaderModifier. This field contains the following configuration objects:\nParameterDescriptionTypeRequired\nset\tConfigure this field to rewrite the HTTP request header. It specifies the name of an HTTP header to overwrite and the new value to set. Any existing values associated with the header name are overwritten. You can specify the following configurations: \nname: Required string that specifies the name of the HTTP header to set.\nvalue: Required string that specifies the value of the HTTP header to set.\n\tList of objects\tOptional\t\nadd\tConfigure this field to append the request header with a new value. It specifies the name of an HTTP header to append and the value(s) to add. You can specify the following configurations: \nname: Required string that specifies the name of the HTTP header to append.\nvalue: Required string that specifies the value of the HTTP header to add.\n\tList of objects\tOptional\t\nremove\tConfigure this field to specify an array of header names to remove from the request header.\tArray of strings\tOptional\t\nrules.filters.urlRewrite\nSpecifies rules for rewriting the URL of incoming requests when rules.filters.type is configured to URLRewrite.\nType: Object\nRequired: Optional\nrules.filters.urlRewrite.path\nSpecifies a list of objects that determine how Consul API Gateway rewrites URL paths (refer to Reroute HTTP requests for additional information).\nThe following table describes the parameters for path:\nreplacePrefixMatch\tSpecifies a value that replaces the path prefix for incoming HTTP requests. The operation only affects the path prefix. The rest of the path is unchanged.\tString\tRequired\t\ntype\tSpecifies the type of replacement to use for the URL path. You can specify the following values: \nReplacePrefixMatch: Replaces the the matched prefix of the URL path (default). \n\tString\tOptional\t\nrules.matches\nSpecifies rules for matching incoming requests. You can apply filters to requests that match the defined rules. You can match incoming requests based on the following elements:\npaths\nheaders\nquery parameters\nrequest method\nEach rule matches requests independently. As a result, a request matching any of the conditions is considered a match. You can configure several matching rules for each type to widen or narrow matches.\nrules.matches.path\nSpecifies a list of objects that define matches based on URL path. The following table describes the parameters for the path field:\ntype\tSpecifies the type of comparison to use for matching the path value. You can specify the following types. \nExact: Returns a match only when the entire path matches the value field (default).\nPathPrefix: Returns a match when the path has the prefix defined in the value field.\nRegularExpression: Returns a match when the path matches the regex defined in the value field.\n\tString\tRequired\t\nvalue\tSpecifies the value to match on. You can specify a specific string when type is Exact or PathPrefix. You can specify a regular expression if type is RegularExpression.\tString\tRequired\t\nrules.matches.headers\nSpecifies a list of objects that define matches based HTTP request headers. The following table describes the parameters for the headers field:\ntype\tSpecifies the type of comparison to use for matching the header value. You can specify the following types. \nExact: Returns a match only when the entire header matches the value field (default).\nRegularExpression: Returns a match when the header matches the regex defined in the value field.\n\tString\tRequired\t\nname\tSpecifies the name of the header to match on.\tString\tRequired\t\nvalue\tSpecifies value to match on. You can specify a specific string or a regular expression.\tString\tRequired\t\nrules.matches.queryParams\nSpecifies a list of objects that define matches based query parameters. The following table describes the parameters for the queryParams field:\ntype\tSpecifies the type of comparison to use for matching a query parameter value. You can specify the following types. \nExact: Returns a match only when the query parameter match the value field (default).\nRegularExpression: Returns a match when the query parameter matches the regex defined in the value field.\n\tString\tRequired\t\nname\tSpecifies the name of the query parameter to match on.\tString\tRequired\t\nvalue\tSpecifies value to match on. You can specify a specific string or a regular expression.\tString\tRequired\t\nrules.matches.method\nSpecifies a list of strings that define matches based on HTTP request method. You may specify the following values:\nHEAD\nPOST\nPUT\nPATCH\nGET\nDELETE\nOPTIONS\nTRACE\nCONNECT"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.16.x/api-gateway/usage/reroute-http-requests",
  "text": "Reroute HTTP Requests | Consul\nThis topic describes how to configure Consul API Gateway to reroute HTTP requests. \nVerify that the requirements have been met.\nVerify that the Consul API Gateway CRDs and controller have been installed and applied. Refer to Installation for details.\nSpecify the following fields in your Route configuration. Refer to the Route configuration reference for details about the parameters.\nrules.filters.type: Set this parameter to URLRewrite to instruct Consul API Gateway to rewrite the URL when specific conditions are met. \nrules.filters.urlRewrite: Specify the path configuration.\nrules.filters.urlRewrite.path: Contains the paths that incoming requests should be rewritten to based on the match conditions. \nTo configure the route to accept paths with or without a trailing slash, you must make two separate routes to handle each case.\nExample\nIn the following example, requests to /incoming-request-prefix/ are forwarded to the backendRef as /prefix-backend-receives/. As a result, requests to /incoming-request-prefix/request-path are received by backendRef as /prefix-backend-receives/request-path.\nroute.yaml\napiVersion: gateway.networking.k8s.io/v1beta1 kind: HTTPRoute metadata: name: example-route ##... spec: parentRefs: - group: gateway.networking.k8s.io kind: Gateway name: api-gateway rules: - backendRefs: . . . filters: - type: URLRewrite urlRewrite: path: replacePrefixMatch: /prefix-backend-receives/ type: ReplacePrefixMatch matches: - path: type: PathPrefix value: /incomingrequest-prefix/"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.15.x/api-gateway/configuration/gateway",
  "text": "Gateway Resource Configuration | Consul\nThis topic provides full details about the Gateway resource.\nA Gateway is an instance of network infrastructure that determines how service traffic should be handled. A Gateway contains one or more listeners that bind to a set of IP addresses. An HTTPRoute or TCPRoute can then attach to a gateway listener to direct traffic from the gateway to a service.\nGateway instances derive their configurations from the GatewayClass resource, which acts as a template for individual Gateway deployments. Refer to GatewayClass for additional information.\nSpecify the following parameters to declare a Gateway:\nParameterDescriptionRequired\nkind\tSpecifies the type of configuration object. The value should always be Gateway.\tRequired\t\ndescription\tHuman-readable string that describes the purpose of the Gateway.\tOptional\t\nversion \tSpecifies the Kubernetes API version. The value should always be gateway.networking.k8s.io/v1alpha2\tRequired\t\nscope\tSpecifies the effective scope of the Gateway. The value should always be namespaced.\tRequired\t\nfields\tSpecifies the configurations for the Gateway. The fields are listed in the configuration model. Details for each field are described in the specification.\tRequired\t\nThe following outline shows how to format the configurations in the Gateway object. Click on a property name to view details about the configuration.\ngatewayClassName: string | required\nlisteners: array of objects | required\nallowedRoutes: object | required\nnamespaces: object | required\nfrom: string | required\nselector: object | required if from is configured to selector\nmatchExpressions: array of objects | required if matchLabels is not configured\nkey: string | required if matchExpressions is declared\noperator: string | required if matchExpressions is declared\nvalues: array of strings | required if matchExpressions is declared\nmatchLabels: map of strings | required if matchExpressions is not configured\nhostname: string | required\nport: integer | required\nprotocol: string | required\ntls: object | required if protocol is set to HTTPS\ncertificateRefs: array or objects | required if tls is declared\nname: string | required if certificateRefs is declared\nnamespace: string | required if certificateRefs is declared\nmode: string | required if certificateRefs is declared\noptions: map of strings | optional\nThis topic provides details about the configuration parameters.\ngatewayClassName\nSpecifies the name of the GatewayClass resource used for the Gateway instance. Unless you are using a custom GatewayClass, this value should be set to consul-api-gateway.\nlisteners\nSpecifies the listeners associated with the Gateway. At least one listener must be specified. Each listener within a Gateway must have a unique combination of hostname, port, and protocol.\nType: array of objects\nlisteners.allowedRoutes\nSpecifies a namespace object that defines the types of routes that may be attached to a listener.\nType: object\nlisteners.allowedRoutes.namespaces\nDetermines which routes are allowed to attach to the listener. Only routes in the same namespace as the Gateway may be attached by default.\nRequired: optional\nDefault: Same namespace as the parent Gateway\nlisteners.allowedRoutes.namespaces.from\nDetermines which namespaces are allowed to attach a route to the Gateway. You can specify one of the following strings:\nAll: Routes in all namespaces may be attached to the Gateway.\nSame (default): Only routes in the same namespace as the Gateway may be attached.\nSelector: Only routes in namespaces that match the selector may be attached.\nThis parameter is required.\nlisteners.allowedRoutes.namespaces.selector\nSpecifies a method for selecting routes that are allowed to attach to the listener. The Gateway checks for namespaces in the network that match either a regular expression or a label. Routes from the matching namespace are allowed to attach to the listener.\nYou can configure one of the following objects:\nmatchExpressions\nmatchLabels\nThis field is required when from is configured to Selector.\nlisteners.allowedRoutes.namespaces.selector.matchExpressions\nSpecifies an array of requirements for matching namespaces. If a match is found, then routes from the matching namespace(s) are allowed to attach to the Gateway. The following table describes members of the matchExpressions array:\nRequirementDescriptionTypeRequired\nkey\tSpecifies the label that the key applies to.\tstring\trequired when matchExpressions is declared\t\noperator\tSpecifies the key's relation to a set of values. You can use the following keywords: \nIn: Only routes in namespaces that contain the strings in the values field can attach to the Gateway. \nNotIn: Routes in namespaces that do not contain the strings in the values field can attach to the Gateway. \nExists: Routes in namespaces that contain the key value are allowed to attach to the Gateway.\nDoesNotExist: Routes in namespaces that do not contain the key value are allowed to attach to the Gateway.\n\tstring\trequired when matchExpressions is declared\t\nvalues\tSpecifies an array of string values. If operator is configured to In or NotIn, then the values array must contain values. If operator is configured to Exists or DoesNotExist, then the values array must be empty.\tarray of strings\trequired when matchExpressions is declared\t\nIn the following example, routes in namespaces that contain foo and bar are allowed to attach routes to the Gateway.\nnamespaceSelector: matchExpressions: - key: kubernetes.io/metadata.name operator: In values: - foo - bar \nRefer to Labels and Selectors in the Kubernetes documentation for additional information about matchExpressions.\nlisteners.allowedRoutes.namespaces.selector.matchLabels\nSpecifies an array of labels and label values. If a match is found, then routes with the matching label(s) are allowed to attach to the Gateway. This selector can contain any arbitrary key/value pair.\nIn the following example, routes in namespaces that have a bar label are allowed to attach to the Gateway.\nnamespaceSelector: matchLabels: foo: bar \nRefer to Labels and Selectors in the Kubernetes documentation for additional information about labels.\nlisteners.hostname\nSpecifies the listener's hostname.\nlisteners.name\nSpecifies the listener's name.\nlisteners.port\nSpecifies the port number that the listener attaches to.\nType: integer\nlisteners.protocol\nSpecifies the protocol the listener communicates on.\nAllowed values are TCP, HTTP, or HTTPS\nlisteners.tls\nSpecifies the tls configurations for the Gateway. The tls object is required if protocol is set to HTTPS. The object contains the following fields:\ncertificateRefs\t\nSpecifies Kubernetes name and namespace objects that contains TLS certificates and private keys. \nThe certificates establish a TLS handshake for requests that match the hostname of the associated listener. Each reference must be a Kubernetes Secret. If you are using a Secret in a namespace other than the Gateway's, each reference must also have a corresponding ReferenceGrant.\n\tObject or array\tRequired if tls is set\t\nmode\tSpecifies the TLS Mode. Should always be set to Terminate for HTTPRoutes\tstring\tRequired if certificateRefs is set\t\noptions\tSpecifies additional Consul API Gateway options.\tMap of strings\toptional\t\nThe following keys for options are available\napi-gateway.consul.hashicorp.com/tls_min_version\napi-gateway.consul.hashicorp.com/tls_max_version\napi-gateway.consul.hashicorp.com/tls_cipher_suites\nIn the following example, tls settings are configured to use a secret named consul-server-cert in the same namespace as the Gateway and the minimum tls version is set to TLSv1_2.\ntls: certificateRefs: - name: consul-server-cert group: \"\" kind: Secret mode: Terminate options: api-gateway.consul.hashicorp.com/tls_min_version: \"TLSv1_2\" \nExample cross-namespace certificateRef\nThe following example creates a Gateway named example-gateway in namespace gateway-namespace (lines 2-4). The gateway has a certificateRef in namespace secret-namespace (lines 16-18). The reference is allowed because the ReferenceGrant configuration, named reference-grant in namespace secret-namespace (lines 24-27), allows Gateways in gateway-namespace to reference Secrets in secret-namespace (lines 31-35).\ngateway_with_referencegrant.yaml\napiVersion: gateway.networking.k8s.io/v1beta1 kind: Gateway metadata: name: example-gateway namespace: gateway-namespace spec: gatewayClassName: consul-api-gateway listeners: - protocol: HTTPS port: 443 name: https allowedRoutes: namespaces: from: Same tls: certificateRefs: - name: cert namespace: secret-namespace group: \"\" kind: Secret --- apiVersion: gateway.networking.k8s.io/v1alpha2 kind: ReferenceGrant metadata: name: reference-grant namespace: secret-namespace spec: from: - group: gateway.networking.k8s.io kind: Gateway namespace: gateway-namespace to: - group: \"\" kind: Secret name: cert"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/connect/gateways/api-gateway/configuration/http-route",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.15.x/api-gateway/usage/reroute-http-requests",
  "text": "Reroute HTTP Requests | Consul\nThis topic describes how to configure Consul API Gateway to reroute HTTP requests. \nVerify that the requirements have been met.\nVerify that the Consul API Gateway CRDs and controller have been installed and applied. Refer to Installation for details.\nSpecify the following fields in your Route configuration. Refer to the Route configuration reference for details about the parameters.\nrules.filters.type: Set this parameter to URLRewrite to instruct Consul API Gateway to rewrite the URL when specific conditions are met. \nrules.filters.urlRewrite: Specify the path configuration.\nrules.filters.urlRewrite.path: Contains the paths that incoming requests should be rewritten to based on the match conditions. \nTo configure the route to accept paths with or without a trailing slash, you must make two separate routes to handle each case.\nExample\nIn the following example, requests to /incoming-request-prefix/ are forwarded to the backendRef as /prefix-backend-receives/. As a result, requests to /incoming-request-prefix/request-path are received by backendRef as /prefix-backend-receives/request-path.\nroute.yaml\napiVersion: gateway.networking.k8s.io/v1beta1 kind: HTTPRoute metadata: name: example-route ##... spec: parentRefs: - group: gateway.networking.k8s.io kind: Gateway name: api-gateway rules: - backendRefs: . . . filters: - type: URLRewrite urlRewrite: path: replacePrefixMatch: /prefix-backend-receives/ type: ReplacePrefixMatch matches: - path: type: PathPrefix value: /incomingrequest-prefix/"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.15.x/api-gateway/configuration/routes",
  "text": "Route Resource Configuration | Consul\nThis topic describes how to create and configure Route resources. Routes are independent configuration objects that are associated with specific listeners.\nDeclare a route with either kind: HTTPRoute or kind: TCPRoute and configure the route parameters in the spec block. Refer to the Kubernetes Gateway API documentation for each object type for details:\nHTTPRoute\nTCPRoute\nThe following example creates a route named example-route associated with a listener defined in example-gateway.\nroutes.yaml\napiVersion: gateway.networking.k8s.io/v1alpha2 kind: HTTPRoute metadata: name: example-route spec: parentRefs: - name: example-gateway rules: - backendRefs: - kind: Service name: echo port: 8080 \nThe following outline shows how to format the configurations for the Route object. The top-level spec field is the root for all configurations. Click on a property name to view details about the configuration.\nparentRefs: array of objects | optional\ngroup: string | optional\nkind: string | optional\nnamespace: string | optional\nsectionName: string | optional\nrules: list of objects | optional\nbackendRefs: list of objects | optional\ngroup: string | optional\nkind: string | optional\nnamespace: string | optional\nport: integer | required\nweight: integer | optional\nfilters: list of objects | optional\nrequestHeaderModifier: object | optional\nset: array of objects | optional\nadd: array of objects | optional\nremove: array of strings | optional\nurlRewrite: object | optional\npath: object | required\nreplacePrefixMatch: string | required\nmatches: array of objects | optional\npath: list of objects | optional\nheaders: list of objects | optional\nqueryParams: list of objects | optional\nmethod: string | optional\nThis topic provides details about the configuration parameters.\nparentRefs\nThis field contains the list of Gateways that the route should attach to. If not set, the route will not attach to a Gateway. The following table describes the objects you can configure in the parentRefs block:\ngroup\tSpecifies the Kubernetes API group of the Gateway to attach to. You can specify the following values: \ngateway.networking.k8s.io\n. Defaults to gateway.networking.k8s.io.\tString\tOptional\t\nkind\tSpecifies the Kubernetes kind of the Gateway to attach to. you can specify the following values: \nGateway\n. Defaults to Gateway.\tString\tOptional\t\nname\tSpecifies the name of the Gateway the route is attached to.\tString\tRequired\t\nnamespace\tSpecifies the Kubernetes namespace containing the Gateway to attach to. If the Gateway is in a different Kubernetes namespace than the Route, then you must specify a value. Defaults to the Route namespace.\tString\tOptional\t\nsectionName\tSpecifies the name of a specific listener on the Gateway to attach to. The Route attempts to attach to all listeners on the Gateway.\tString\tRequired\t\nrules\nThe rules field contains a list of objects that define behaviors for network traffic that goes through the route. The rule configuration contains the following objects:\nbackendRefs: Specifies which backend services the Route references when processing traffic.\nfilters: Specifies which operations Consul API Gateway performs when traffic goes through the Route.\nmatches: Determines which requests Consul API Gateway processes.\nRules are optional.\nrules.backendRefs\nThis field specifies backend services that the Route references. The following table describes the parameters for backendRefs:\ngroup\tSpecifies the Kubernetes API Group of the referenced backend. You can specify the following values: \n\"\": Specifies the core Kubernetes API group. This value must be used when kind is set to Service. This is the default value if unspecified.\napi-gateway.consul.hashicorp.com: This value must be used when kind is set to MeshService.\n\tString\tOptional\t\nkind\tSpecifies the Kubernetes Kind of the referenced backend. You can specify the following values: \nService (default): Indicates that the backendRef references a Service in the Kubernetes cluster. \nMeshService: Indicates that the backendRef references a service in the Consul mesh. Refer to the MeshService documentation for additional information.\nname\tSpecifies the name of the Kubernetes Service or Consul mesh service resource.\tString\tRequired\t\nnamespace\tSpecifies the Kubernetes namespace containing the Kubernetes Service or Consul mesh service resource. You must specify a value if the Service or Consul mesh service is defined in a different namespace from the Route. Defaults to the namespace of the Route. \nTo create a route for a backendRef in a different namespace, you must also create a ReferenceGrant. Refer to the example route configured to reference across namespaces.\tString\tOptional\t\nport\tSpecifies the port number for accessing the Kubernetes or Consul service.\tInteger\tRequired\t\nweight\tSpecifies the proportion of requests sent to the backend. Computed as weight divided by the sum of all weights in this backendRefs list. Defaults to 1. A value of 0 indicates that no requests should be sent to the backend.\tInteger\tOptional\t\nExample cross-namespace backendRef\nThe following example creates a route named example-route in namespace gateway-namespace. This route has a backendRef in namespace service-namespace. Traffic is allowed because the ReferenceGrant, named reference-grant in namespace service-namespace, allows traffic from HTTPRoutes in gateway-namespace to Services in service-namespace.\nroute_with_referencegrant.yaml\napiVersion: gateway.networking.k8s.io/v1alpha2 kind: HTTPRoute metadata: name: example-route namespace: gateway-namespace spec: parentRefs: - name: example-gateway rules: - backendRefs: - kind: Service name: echo namespace: service-namespace port: 8080 --- apiVersion: gateway.networking.k8s.io/v1alpha2 kind: ReferenceGrant metadata: name: reference-grant namespace: service-namespace spec: from: - group: gateway.networking.k8s.io kind: HTTPRoute namespace: gateway-namespace to: - group: \"\" kind: Service name: echo \nrules.filters\nThe filters block defines steps for processing requests. You can configure filters to modify the properties of matching incoming requests and enable Consul API Gateway features, such as rewriting path prefixes (refer to Reroute HTTP requests for additional information).\nType: Array of objects\nRequired: Optional\nrules.filters.type\nSpecifies the type of filter you want to apply to the route. The parameter is optional and takes a string value.\nYou can specify the following values:\nRequestHeaderModifier: The RequestHeaderModifier type modifies the HTTP headers on the incoming request. You must define the rules.filters.requestHeaderModifier configurations to use this filter type.\nURLRewrite: The URLRewrite type modifies the URL path on the incoming request. You must define the rules.filters.urlRewrite configurations to use this filter type.\nrules.filters.requestHeaderModifier\nDefines operations to perform on matching request headers when rules.filters.type is configured to RequestHeaderModifier. This field contains the following configuration objects:\nset\tConfigure this field to rewrite the HTTP request header. It specifies the name of an HTTP header to overwrite and the new value to set. Any existing values associated with the header name are overwritten. You can specify the following configurations: \nname: Required string that specifies the name of the HTTP header to set.\nvalue: Required string that specifies the value of the HTTP header to set.\n\tList of objects\tOptional\t\nadd\tConfigure this field to append the request header with a new value. It specifies the name of an HTTP header to append and the value(s) to add. You can specify the following configurations: \nname: Required string that specifies the name of the HTTP header to append.\nvalue: Required string that specifies the value of the HTTP header to add.\n\tList of objects\tOptional\t\nremove\tConfigure this field to specify an array of header names to remove from the request header.\tArray of strings\tOptional\t\nrules.filters.urlRewrite\nSpecifies rules for rewriting the URL of incoming requests when rules.filters.type is configured to URLRewrite.\nType: Object\nRequired: Optional\nrules.filters.urlRewrite.path\nSpecifies a list of objects that determine how Consul API Gateway rewrites URL paths (refer to Reroute HTTP requests for additional information).\nThe following table describes the parameters for path:\nreplacePrefixMatch\tSpecifies a value that replaces the path prefix for incoming HTTP requests. The operation only affects the path prefix. The rest of the path is unchanged.\tString\tRequired\t\ntype\tSpecifies the type of replacement to use for the URL path. You can specify the following values: \nReplacePrefixMatch: Replaces the the matched prefix of the URL path (default). \nrules.matches\nSpecifies rules for matching incoming requests. You can apply filters to requests that match the defined rules. You can match incoming requests based on the following elements:\npaths\nheaders\nquery parameters\nrequest method\nEach rule matches requests independently. As a result, a request matching any of the conditions is considered a match. You can configure several matching rules for each type to widen or narrow matches.\nrules.matches.path\nSpecifies a list of objects that define matches based on URL path. The following table describes the parameters for the path field:\ntype\tSpecifies the type of comparison to use for matching the path value. You can specify the following types. \nExact: Returns a match only when the entire path matches the value field (default).\nPathPrefix: Returns a match when the path has the prefix defined in the value field.\nRegularExpression: Returns a match when the path matches the regex defined in the value field.\n\tString\tRequired\t\nvalue\tSpecifies the value to match on. You can specify a specific string when type is Exact or PathPrefix. You can specify a regular expression if type is RegularExpression.\tString\tRequired\t\nrules.matches.headers\nSpecifies a list of objects that define matches based HTTP request headers. The following table describes the parameters for the headers field:\ntype\tSpecifies the type of comparison to use for matching the header value. You can specify the following types. \nExact: Returns a match only when the entire header matches the value field (default).\nRegularExpression: Returns a match when the header matches the regex defined in the value field.\nname\tSpecifies the name of the header to match on.\tString\tRequired\t\nvalue\tSpecifies value to match on. You can specify a specific string or a regular expression.\tString\tRequired\t\nrules.matches.queryParams\nSpecifies a list of objects that define matches based query parameters. The following table describes the parameters for the queryParams field:\ntype\tSpecifies the type of comparison to use for matching a query parameter value. You can specify the following types. \nExact: Returns a match only when the query parameter match the value field (default).\nRegularExpression: Returns a match when the query parameter matches the regex defined in the value field.\nname\tSpecifies the name of the query parameter to match on.\tString\tRequired\t\nvalue\tSpecifies value to match on. You can specify a specific string or a regular expression.\tString\tRequired\t\nrules.matches.method\nSpecifies a list of strings that define matches based on HTTP request method. You may specify the following values:\nHEAD\nPOST\nPUT\nPATCH\nGET\nDELETE\nOPTIONS\nTRACE\nCONNECT"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/connect/gateways/api-gateway/configuration/tcp-route",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.12.x/connect/registration",
  "text": "Service Mesh Proxy Registration Overview | Consul\nTo make Connect aware of proxies you will need to register them in a service definition, just like you would register any other service with Consul. This section outlines your options for registering Connect proxies, either using independent registrations, or in nested sidecar registrations.\nTo register proxies with independent proxy service registrations, you can define them in either in config files or via the API just like any other service. Learn more about all of the options you can define when registering your proxy service in the proxy registration documentation.\nTo reduce the amount of boilerplate needed for a sidecar proxy, application service definitions may define an inline sidecar service block. This is an opinionated shorthand for a separate full proxy registration as described above. For a description of how to configure the sidecar proxy as well as the opinionated defaults, see the sidecar service registrations documentation."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.12.x/k8s/crds/upgrade-to-crds",
  "text": "Upgrade Existing Clusters to Use Custom Resource Definitions | Consul\nUpgrading to consul-helm versions >= 0.30.0 will require some changes if you utilize the following:\nconnectInject.centralConfig.enabled\nconnectInject.centralConfig.defaultProtocol\nconnectInject.centralConfig.proxyDefaults\nmeshGateway.globalMode\nConnect annotation consul.hashicorp.com/connect-service-protocol\nIf you were previously setting centralConfig.enabled to false:\nconnectInject: centralConfig: enabled: false \nThen instead you must use server.extraConfig and client.extraConfig:\nclient: extraConfig: | {\"enable_central_service_config\": false} server: extraConfig: | {\"enable_central_service_config\": false} \nIf you were previously setting it to true, it now defaults to true so no changes are required, but you can remove it from your config if you desire.\nconnectInject: centralConfig: defaultProtocol: 'http' # or any value \nNow you must use custom resources to manage the protocol for new and existing services:\nTo upgrade, first ensure you're running Consul >= 1.9.0. See Consul Version Upgrade for more information on how to upgrade Consul versions.\nNext, modify your Helm values:\nRemove the defaultProtocol config. This won't affect existing services.\nSet:\ncontroller: enabled: true \nNow you can upgrade your Helm chart to the latest version with the new Helm values.\nNote: This setting was removed because it didn't support changing the protocol after a service was first run and because it didn't work in secondary datacenters.\nconnectInject: centralConfig: proxyDefaults: | { \"key\": \"value\" // or any values } \nIf you later wish to change any of the proxy defaults settings, you will need to follow the Migrating Config Entries instructions for your proxy-defaults config entry.\nmeshGateway: globalMode: 'local' # or any value \nIf you later wish to change the mode or any other setting in proxy-defaults, you will need to follow the Migrating Config Entries instructions to migrate your proxy-defaults config entry to a ProxyDefaults resource.\nIf any of your Connect services had the consul.hashicorp.com/connect-service-protocol annotation set, e.g.\napiVersion: apps/v1 kind: Deployment ... spec: template: metadata: annotations: \"consul.hashicorp.com/connect-inject\": \"true\" \"consul.hashicorp.com/connect-service-protocol\": \"http\" ... \nEnsure you're running Consul >= 1.9.0. See Consul Version Upgrade for more information on how to upgrade Consul versions.\nNext, remove this annotation from existing deployments. This will have no effect on the deployments because the annotation was only used when the service was first created.\nModify your Helm values and add:\ncontroller: enabled: true \nNow you can upgrade your Helm chart to the latest version.\nNote: The annotation was removed because it didn't support changing the protocol and it wasn't supported in secondary datacenters.\nA config entry that already exists in Consul must be migrated into a Kubernetes custom resource in order to manage it from Kubernetes:\nDetermine the kind and name of the config entry. For example, the protocol would be set by a config entry with kind: service-defaults and name equal to the name of the service.\nIn another example, a proxy-defaults config has kind: proxy-defaults and name: global.\nOnce you've determined the kind and name, query Consul to get its contents:\n$ consul config read -kind <kind> -name <name> \nThis will require kubectl exec'ing into a Consul server or client pod. If you're using ACLs, you will also need an ACL token passed via the -token flag.\n$ kubectl exec consul-server-0 -- consul config read -name foo -kind service-defaults { \"Kind\": \"service-defaults\", \"Name\": \"foo\", \"Protocol\": \"http\", \"MeshGateway\": {}, \"Expose\": {}, \"CreateIndex\": 60, \"ModifyIndex\": 60 } \nNow we're ready to construct a Kubernetes resource for the config entry.\nIt will look something like:\nThe apiVersion will always be consul.hashicorp.com/v1alpha1.\nThe kind will be the CamelCase version of the Consul kind, e.g. proxy-defaults becomes ProxyDefaults.\nmetadata.name will be the name of the config entry.\nmetadata.annotations will contain the \"consul.hashicorp.com/migrate-entry\": \"true\" annotation.\nThe namespace should be whatever namespace the service is deployed in. For ProxyDefaults, we recommend the namespace that Consul is deployed in.\nThe contents of spec will be a transformation from JSON keys to YAML keys.\nThe following keys can be ignored: CreateIndex, ModifyIndex and any key that has an empty object, e.g. \"Expose\": {}.\n{ \"Kind\": \"service-defaults\", \"Name\": \"foo\", \"Protocol\": \"http\", \"MeshGateway\": {}, \"Expose\": {}, \"CreateIndex\": 60, \"ModifyIndex\": 60 } \nAnd\n{ \"Kind\": \"proxy-defaults\", \"Name\": \"global\", \"MeshGateway\": { \"Mode\": \"local\" }, \"Config\": { \"local_connect_timeout_ms\": 1000, \"handshake_timeout_ms\": 10000 }, \"CreateIndex\": 60, \"ModifyIndex\": 60 } \napiVersion: consul.hashicorp.com/v1alpha1 kind: ProxyDefaults metadata: name: global annotations: 'consul.hashicorp.com/migrate-entry': 'true' spec: meshGateway: mode: local config: # Note that anything under config for ProxyDefaults will use the exact # same keys. local_connect_timeout_ms: 1000 handshake_timeout_ms: 10000 \nRun kubectl apply to apply the Kubernetes resource.\nNext, check that it synced successfully:\n$ kubectl get servicedefaults foo NAME SYNCED AGE foo True 1s \nIf its SYNCED status is True then the migration for this config entry was successful.\nIf its SYNCED status is False, use kubectl describe to view the reason syncing failed:\n$ kubectl describe servicedefaults foo ... Status: Conditions: Last Transition Time: 2021-01-12T21:03:29Z Message: migration failed: Kubernetes resource does not match existing Consul config entry: consul={...}, kube={...} Reason: MigrationFailedError Status: False Type: Synced \nThe most likely reason is that the contents of the Kubernetes resource don't match the Consul resource. Make changes to the Kubernetes resource to match the Consul resource (ignoring the CreateIndex, ModifyIndex and Meta keys).\nOnce the SYNCED status is true, you can make changes to the resource and they will get synced to Consul."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.15.x/k8s/crds/upgrade-to-crds",
  "text": "connect annotation consul.hashicorp.com/connect-service-protocol\nIf any of your mesh services had the consul.hashicorp.com/connect-service-protocol annotation set, e.g."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.13.x/connect/registration",
  "text": "Service Mesh Proxy Registration Overview | Consul\nTo make Connect aware of proxies you will need to register them in a service definition, just like you would register any other service with Consul. This section outlines your options for registering Connect proxies, either using independent registrations, or in nested sidecar registrations.\nTo register proxies with independent proxy service registrations, you can define them in either in config files or via the API just like any other service. Learn more about all of the options you can define when registering your proxy service in the proxy registration documentation.\nTo reduce the amount of boilerplate needed for a sidecar proxy, application service definitions may define an inline sidecar service block. This is an opinionated shorthand for a separate full proxy registration as described above. For a description of how to configure the sidecar proxy as well as the opinionated defaults, see the sidecar service registrations documentation."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.14.x/connect/cluster-peering/k8s",
  "text": "This page does not exist for version v1.14.x."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.11.x/connect/registration",
  "text": "Service Mesh Proxy Registration Overview | Consul\nTo make Connect aware of proxies you will need to register them in a service definition, just like you would register any other service with Consul. This section outlines your options for registering Connect proxies, either using independent registrations, or in nested sidecar registrations.\nTo register proxies with independent proxy service registrations, you can define them in either in config files or via the API just like any other service. Learn more about all of the options you can define when registering your proxy service in the proxy registration documentation.\nTo reduce the amount of boilerplate needed for a sidecar proxy, application service definitions may define an inline sidecar service block. This is an opinionated shorthand for a separate full proxy registration as described above. For a description of how to configure the sidecar proxy as well as the opinionated defaults, see the sidecar service registrations documentation."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.13.x/k8s/crds/upgrade-to-crds",
  "text": "Connect annotation consul.hashicorp.com/connect-service-protocol\nSet:\ncontroller: enabled: true \nIf any of your Connect services had the consul.hashicorp.com/connect-service-protocol annotation set, e.g.\nModify your Helm values and add:\ncontroller: enabled: true "
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.11.x/k8s/crds/upgrade-to-crds",
  "text": "Connect annotation consul.hashicorp.com/connect-service-protocol\nSet:\nIf any of your Connect services had the consul.hashicorp.com/connect-service-protocol annotation set, e.g.\nModify your Helm values and add:"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.11.x/connect/config-entries/service-defaults",
  "text": "Service Defaults - Configuration Entry Reference | Consul\nv1.8.4+: On Kubernetes, the ServiceDefaults custom resource is supported in Consul versions 1.8.4+.\nv1.5.0+: On other platforms, this config entry is supported in Consul versions 1.5.0+.\nThe service-defaults config entry kind (ServiceDefaults on Kubernetes) controls default global values for a service, such as its protocol.\nDefault protocol\nNOTE: The default protocol can also be configured globally for all proxies using the proxy defaults config entry. However, if the protocol value is specified in a service defaults config entry for a given service, that value will take precedence over the globally configured value from proxy defaults.\nSet the default protocol for a service in the default namespace to HTTP:\nKind = \"service-defaults\" Name = \"web\" Namespace = \"default\" Protocol = \"http\" \nUpstream configuration\nSet default connection limits and mesh gateway mode across all upstreams of \"counting\", and also override the mesh gateway mode used when dialing the \"dashboard\" service.\nKind = \"service-defaults\" Name = \"counting\" UpstreamConfig = { Defaults = { MeshGateway = { Mode = \"local\" } Limits = { MaxConnections = 512 MaxPendingRequests = 512 MaxConcurrentRequests = 512 } } Overrides = [ { Name = \"dashboard\" MeshGateway = { Mode = \"remote\" } } ] } \nKind - Must be set to service-defaults\nName (string: <required>) - Set to the name of the service being configured.\nNamespace (string: \"default\")Enterprise - Specifies the namespace the config entry will apply to.\nPartition (string: \"default\")Enterprise - Specifies the name of the admin partition in which the configuration entry applies. Refer to the Admin Partitions documentation for additional information.\nMeta (map<string|string>: nil) - Specifies arbitrary KV metadata pairs. Added in Consul 1.8.4.\nProtocol (string: \"tcp\") - Sets the protocol of the service. This is used by Connect proxies for things like observability features and to unlock usage of the service-splitter and service-router config entries for a service. It also unlocks the ability to define L7 intentions via service-intentions. Supported values are one of tcp, http, http2, or grpc.\nMode (string: \"\") - One of direct or transparent. transparent represents that inbound and outbound application traffic is being captured and redirected through the proxy. This mode does not enable the traffic redirection itself. Instead it signals Consul to configure Envoy as if traffic is already being redirected. direct represents that the proxy's listeners must be dialed directly by the local application and other proxies. Added in v1.10.0.\nUpstreamConfig (UpstreamConfiguration: <optional>) - Controls default configuration settings that apply across all upstreams, and per-upstream configuration overrides. Note that per-upstream configuration applies across all federated datacenters to the pairing of source and upstream destination services. Added in v1.10.0.\nOverrides (array<UpstreamConfig>: []) - A list of optional overrides for per-upstream configuration.\nName (string: \"\") - The upstream name to apply the configuration to. This should not be set to the wildcard specifier *.\nNamespace (string: \"\") - The namespace of the upstream. This should not be set to the wildcard specifier *.\nProtocol (string: \"\") - The protocol for the upstream listener.\nNOTE: The protocol of a service should ideally be configured via the protocol field of a service-defaults config entry for the upstream destination service. Configuring it in a proxy upstream config will not fully enable some L7 features. It is supported here for backwards compatibility with Consul versions prior to 1.6.0. \nConnectTimeoutMs (int: 5000) - The number of milliseconds to allow when making upstream connections before timing out.\nNOTE: The connect timeout of a service should ideally be configured via the connect_timeout field of a service-resolver config entry for the upstream destination service. Configuring it in a proxy upstream config will not fully enable some L7 features. It is supported here for backwards compatibility with Consul versions prior to 1.6.0. \nMeshGateway (MeshGatewayConfig: <optional>) - Controls the default mesh gateway configuration for this upstream.\nMode (string: \"\") - One of none, local, or remote.\nLimits (Limits: <optional>) - A set of limits to apply when connecting to the upstream service. These limits are applied on a per-service-instance basis. The following limits are respected.\nMaxConnections (int: 0) - The maximum number of connections a service instance will be allowed to establish against the given upstream. Use this to limit HTTP/1.1 traffic, since HTTP/1.1 has a request per connection.\nMaxPendingRequests (int: 0) - The maximum number of requests that will be queued while waiting for a connection to be established. For this configuration to be respected, a L7 protocol must be defined in the protocol field.\nMaxConcurrentRequests (int: 0) - The maximum number of concurrent requests that will be allowed at a single point in time. Use this to limit HTTP/2 traffic, since HTTP/2 has many requests per connection. For this configuration to be respected, a L7 protocol must be defined in the protocol field.\nPassiveHealthCheck (PassiveHealthCheck: <optional>) - Passive health checks are used to remove hosts from the upstream cluster which are unreachable or are returning errors..\nInterval (duration: 0s) - The time between checks. Each check will cause hosts which have exceeded max_failures to be removed from the load balancer, and any hosts which have passed their ejection time to be returned to the load balancer.\nMaxFailures (int: 0) - The number of consecutive failures which cause a host to be removed from the load balancer.\nDefaults (UpstreamConfig: <optional>) - Default configuration that applies to all upstreams of this service.\nProtocol (string: \"\") - The protocol for the upstream listener.\nNOTE: The protocol of a service should ideally be configured via the protocol field of a service-defaults config entry for the upstream destination service. Configuring it in a proxy upstream config will not fully enable some L7 features. It is supported here for backwards compatibility with Consul versions prior to 1.6.0. \nConnectTimeoutMs (int: 5000) - The number of milliseconds to allow when making upstream connections before timing out.\nNOTE: The connect timeout of a service should ideally be configured via the connect_timeout field of a service-resolver config entry for the upstream destination service. Configuring it in a proxy upstream config will not fully enable some L7 features. It is supported here for backwards compatibility with Consul versions prior to 1.6.0. \nMeshGateway (MeshGatewayConfig: <optional>) - Controls the default mesh gateway configuration for this upstream.\nMode (string: \"\") - One of none, local, or remote.\nLimits (Limits: <optional>) - A set of limits to apply when connecting to the upstream service. These limits are applied on a per-service-instance basis. The following limits are respected.\nMaxConnections (int: 0) - The maximum number of connections a service instance will be allowed to establish against the given upstream. Use this to limit HTTP/1.1 traffic, since HTTP/1.1 has a request per connection.\nMaxPendingRequests (int: 0) - The maximum number of requests that will be queued while waiting for a connection to be established. For this configuration to be respected, a L7 protocol must be defined in the protocol field.\nMaxConcurrentRequests (int: 0) - The maximum number of concurrent requests that will be allowed at a single point in time. Use this to limit HTTP/2 traffic, since HTTP/2 has many requests per connection. For this configuration to be respected, a L7 protocol must be defined in the protocol field.\nPassiveHealthCheck (PassiveHealthCheck: <optional>) - Passive health checks are used to remove hosts from the upstream cluster which are unreachable or are returning errors..\nInterval (duration: 0s) - The time between checks. Each check will cause hosts which have exceeded max_failures to be removed from the load balancer, and any hosts which have passed their ejection time to be returned to the load balancer.\nMaxFailures (int: 0) - The number of consecutive failures which cause a host to be removed from the load balancer.\nTransparentProxy (TransparentProxyConfig: <optional>) - Controls configuration specific to proxies in transparent mode. Added in v1.10.0.\nOutboundListenerPort (int: \"15001\") - The port the proxy should listen on for outbound traffic. This must be the port where outbound application traffic is redirected to.\nDialedDirectly (bool: false) - Determines whether this proxy instance's IP address can be dialed directly by transparent proxies. Typically transparent proxies dial upstreams using the \"virtual\" tagged address, which load balances across instances. Dialing individual instances can be helpful in cases like stateful services such as a database cluster with a leader.\nMeshGateway (MeshGatewayConfig: <optional>) - Controls the default mesh gateway configuration for this service. Added in v1.6.0.\nMode (string: \"\") - One of none, local, or remote.\nExternalSNI (string: \"\") - This is an optional setting that allows for the TLS SNI value to be changed to a non-connect value when federating with an external system. Added in v1.6.0.\nExpose (ExposeConfig: <optional>) - Controls the default expose path configuration for Envoy. Added in v1.6.2.\nExposing paths through Envoy enables a service to protect itself by only listening on localhost, while still allowing non-Connect-enabled applications to contact an HTTP endpoint. Some examples include: exposing a /metrics path for Prometheus or /healthz for kubelet liveness checks.\nChecks (bool: false) - If enabled, all HTTP and gRPC checks registered with the agent are exposed through Envoy. Envoy will expose listeners for these checks and will only accept connections originating from localhost or Consul's advertise address. The port for these listeners are dynamically allocated from expose_min_port to expose_max_port. This flag is useful when a Consul client cannot reach registered services over localhost. One example is when running Consul on Kubernetes, and Consul agents run in their own pods.\nPaths (array<Path>: []) - A list of paths to expose through Envoy.\nPath (string: \"\") - The HTTP path to expose. The path must be prefixed by a slash. ie: /metrics.\nLocalPathPort (int: 0) - The port where the local service is listening for connections to the path.\nListenerPort (int: 0) - The port where the proxy will listen for connections. This port must be available for the listener to be set up. If the port is not free then Envoy will not expose a listener for the path, but the proxy registration will not fail.\nProtocol (string: \"http\") - Sets the protocol of the listener. One of http or http2. For gRPC use http2.\nConfiguration entries may be protected by ACLs.\nReading a service-defaults config entry requires service:read on the resource.\nCreating, updating, or deleting a service-defaults config entry requires service:write on the resource."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.14.x/k8s/crds/upgrade-to-crds",
  "text": "Connect annotation consul.hashicorp.com/connect-service-protocol\nIf any of your Connect services had the consul.hashicorp.com/connect-service-protocol annotation set, e.g."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.15.x/connect/config-entries/service-defaults",
  "text": "Service Defaults Configuration Reference | Consul\nKind\nSpecifies the configuration entry type.\nDefault: none\nData type: String value that must be set to service-defaults.\nName\nSpecifies the name of the service you are setting the defaults for.\nDefault: none\nData type: string\nNamespace \nSpecifies the Consul namespace that the configuration entry applies to.\nDefault: default\nData type: string\nPartition \nSpecifies the name of the Consul admin partition that the configuration entry applies to. Refer to Admin Partitions for additional information.\nDefault: default\nData type: string\nMeta\nSpecifies a set of custom key-value pairs to add to the Consul KV store.\nDefault: none\nData type: Map of one or more key-value pairs.\nkeys: string\nvalues: string, integer, or float\nProtocol\nSpecifies the default protocol for the service. In service mesh use cases, the protocol configuration is required to enable the following features and components:\nobservability\nservice splitter configuration entry\nservice router configuration entry\nL7 intentions\nYou can set the global protocol for proxies in the proxy-defaults configuration entry, but the protocol specified in the service-defaults configuration entry overrides the proxy-defaults configuration.\nDefault: tcp\nYou can specify one of the following string values:\ntcp (default)\nhttp\nhttp2\ngrpc\nRefer to Set the default protocol for an example configuration.\nBalanceInboundConnections\nSpecifies the strategy for allocating inbound connections to the service across Envoy proxy threads. The only supported value is exact_balance. By default, no connections are balanced. Refer to the Envoy documentation for details.\nDefault: none\nData type: string\nMode\nSpecifies a mode for how the service directs inbound and outbound traffic.\nYou can specify the following string values:\ndirect: The proxy's listeners must be dialed directly by the local application and other proxies.\ntransparent: The service captures inbound and outbound traffic and redirects it through the proxy. The mode does not enable the traffic redirection. It instructs Consul to configure Envoy as if traffic is already being redirected.\nUpstreamConfig\nControls default upstream connection settings and custom overrides for individual upstream services. If your network contains federated datacenters, individual upstream configurations apply to all pairs of source and upstream destination services in the network. Refer to the following fields for details:\nUpstreamConfig.Overrides\nUpstreamConfig.Defaults\nData type: map\nUpstreamConfig.Overrides[]\nSpecifies options that override the default upstream configurations for individual upstreams.\nData type: list\nUpstreamConfig.Overrides[].Name\nSpecifies the name of the upstream service that the configuration applies to. We recommend that you do not use the * wildcard to avoid applying the configuration to unintended upstreams.\nUpstreamConfig.Overrides[].Namespace \nSpecifies the namespace containing the upstream service that the configuration applies to. Do not use the * wildcard to prevent the configuration from appling to unintended upstreams.\nUpstreamConfig.Overrides[].Protocol\nSpecifies the protocol to use for requests to the upstream listener.\nWe recommend configuring the protocol in the main Protocol field of the configuration entry so that you can leverage L7 features. Setting the protocol in an upstream configuration limits L7 management functionality.\nUpstreamConfig.Overrides[].ConnectTimeoutMs\nSpecifies how long in milliseconds that the service should attempt to establish an upstream connection before timing out.\nWe recommend configuring the upstream timeout in the connection_timeout field of the service-resolver configuration entry for the upstream destination service. Doing so enables you to leverage L7 features. Configuring the timeout in the service-defaults upstream configuration limits L7 management functionality.\nDefault: 5000\nData type: integer\nUpstreamConfig.Overrides[].MeshGateway\nMap that contains the default mesh gateway mode field for the upstream. Refer to Service Mesh Proxy Configuration in the mesh gateway documentation for additional information.\nYou can specify the following string values for the mode field:\nnone: The service does not make outbound connections through a mesh gateway. Instead, the service makes outbound connections directly to the destination services.\nlocal: The service mesh proxy makes an outbound connection to a gateway running in the same datacenter.\nremote: The service mesh proxy makes an outbound connection to a gateway running in the destination datacenter.\nUpstreamConfig.Overrides[].BalanceOutboundConnections\nSets the strategy for allocating outbound connections from the upstream across Envoy proxy threads.\nThe only supported value is exact_balance. By default, no connections are balanced. Refer to the Envoy documentation for details.\nUpstreamConfig.Overrides[].Limits\nMap that specifies a set of limits to apply to when connecting to individual upstream services.\nThe following table describes limits you can configure:\nLimitDescriptionData typeDefault\nMaxConnections\tSpecifies the maximum number of connections a service instance can establish against the upstream. Define this limit for HTTP/1.1 traffic.\tinteger\t0\t\nMaxPendingRequests\tSpecifies the maximum number of requests that are queued while waiting for a connection to establish. An L7 protocol must be defined in the protocol field for this limit to take effect.\tinteger\t0\t\nMaxConcurrentRequests\tSpecifies the maximum number of concurrent requests. Define this limit for HTTP/2 traffic. An L7 protocol must be defined in the protocol field for this limit to take effect.\tinteger\t0\t\nRefer to the upstream configuration example for additional guidance.\nUpstreamConfig.Overrides[].PassiveHealthCheck\nMap that specifies a set of rules that enable Consul to remove hosts from the upstream cluster that are unreachable or that return errors.\nThe following table describes passive health check parameters you can configure:\nLimitDescriptionData typeDefault\nInterval\tSpecifies the time between checks.\tstring\t0s\t\nMaxFailures\tSpecifies the number of consecutive failures allowed per check interval. If exceeded, Consul removes the host from the load balancer.\tinteger\t0\t\nEnforcingConsecutive5xx\tSpecifies a percentage that indicates how many times out of 100 that Consul ejects the host when it detects an outlier status. The outlier status is determined by consecutive errors in the 500-599 response range.\tinteger\t100\t\nMaxEjectionPercent\tSpecifies the maximum percentage of an upstream cluster that Consul ejects when the proxy reports an outlier. Consul ejects at least one host when an outlier is detected regardless of the value.\tinteger\t10\t\nBaseEjectionTime\tSpecifies the minimum amount of time that an ejected host must remain outside the cluster before rejoining. The real time is equal to the value of the BaseEjectionTime multiplied by the number of times the host has been ejected.\tstring\t30s\t\nUpstreamConfig.Defaults\nSpecifies configurations that set default upstream settings. For information about overriding the default configurations for in for individual upstreams, refer to UpstreamConfig.Overrides.\nData type: map\nUpstreamConfig.Defaults.Protocol\nSpecifies default protocol for upstream listeners.\nWe recommend configuring the protocol in the main Protocol field of the configuration entry so that you can leverage L7 features. Setting the protocol in an upstream configuration limits L7 management functionality.\nUpstreamConfig.Defaults.ConnectTimeoutMs\nSpecifies how long in milliseconds that all services should continue attempting to establish an upstream connection before timing out.\nFor non-Kubernetes environments, we recommend configuring the upstream timeout in the connection_timeout field of the service-resolver configuration entry for the upstream destination service. Doing so enables you to leverage L7 features. Configuring the timeout in the service-defaults upstream configuration limits L7 management functionality.\nDefault: 5000\nData type: integer\nUpstreamConfig.Defaults.MeshGateway\nSpecifies the default mesh gateway mode field for all upstreams. Refer to Service Mesh Proxy Configuration in the mesh gateway documentation for additional information.\nYou can specify the following string values for the mode field:\nnone: The service does not make outbound connections through a mesh gateway. Instead, the service makes outbound connections directly to the destination services.\nlocal: The service mesh proxy makes an outbound connection to a gateway running in the same datacenter.\nremote: The service mesh proxy makes an outbound connection to a gateway running in the destination datacenter.\nUpstreamConfig.Defaults.BalanceOutboundConnections\nSets the strategy for allocating outbound connections from upstreams across Envoy proxy threads. The only supported value is exact_balance. By default, no connections are balanced. Refer to the Envoy documentation for details.\nUpstreamConfig.Defaults.Limits\nMap that specifies a set of limits to apply to when connecting upstream services. The following table describes limits you can configure:\nLimitDescriptionData typeDefault\nMaxConnections\tSpecifies the maximum number of connections a service instance can establish against the upstream. Define this limit for HTTP/1.1 traffic.\tinteger\t0\t\nMaxPendingRequests\tSpecifies the maximum number of requests that are queued while waiting for a connection to establish. An L7 protocol must be defined in the protocol field for this limit to take effect.\tinteger\t0\t\nMaxConcurrentRequests\tSpecifies the maximum number of concurrent requests. Define this limit for HTTP/2 traffic. An L7 protocol must be defined in the protocol field for this limit to take effect.\tinteger\t0\t\nUpstreamConfig.Defaults.PassiveHealthCheck\nMap that specifies a set of rules that enable Consul to remove hosts from the upstream cluster that are unreachable or that return errors. The following table describes the health check parameters you can configure:\nLimitDescriptionData typeDefault\nInterval\tSpecifies the time between checks.\tstring\t0s\t\nMaxFailures\tSpecifies the number of consecutive failures allowed per check interval. If exceeded, Consul removes the host from the load balancer.\tinteger\t0\t\nEnforcingConsecutive5xx \tSpecifies a percentage that indicates how many times out of 100 that Consul ejects the host when it detects an outlier status. The outlier status is determined by consecutive errors in the 500-599 response range.\tinteger\t100\t\nMaxEjectionPercent\tSpecifies the maximum percentage of an upstream cluster that Consul ejects when the proxy reports an outlier. Consul ejects at least one host when an outlier is detected regardless of the value.\tinteger\t10\t\nBaseEjectionTime\tSpecifies the minimum amount of time that an ejected host must remain outside the cluster before rejoining. The real time is equal to the value of the BaseEjectionTime multiplied by the number of times the host has been ejected.\tstring\t30s\t\nTransparentProxy\nControls configurations specific to proxies in transparent mode. Refer to Transparent Proxy for additional information.\nYou can configure the following parameters in the TransparentProxy block:\nOutboundListenerPort\tSpecifies the port that the proxy listens on for outbound traffic. This must be the same port number where outbound application traffic is redirected.\tinteger\t15001\t\nDialedDirectly\tEnables transparent proxies to dial the proxy instance's IP address directly when set to true. Transparent proxies commonly dial upstreams at the \"virtual\" tagged address, which load balances across instances. Dialing individual instances can be helpful for stateful services, such as a database cluster with a leader.\tboolean\tfalse\t\nEnvoyExtensions\nList of extensions to modify Envoy proxy configuration. Refer to Envoy Extensions for additional information.\nYou can configure the following parameters in the EnvoyExtensions block:\nName\tName of the extension.\tstring\t\"\"\t\nRequired\tWhen Required is true and the extension does not update any Envoy resources, an error is returned. Use this parameter to ensure that extensions required for secure communication are not unintentionally bypassed.\tstring\t\"\"\t\nArguments\tArguments to pass to the extension executable.\tmap\tnil\t\nDestination[]\nConfigures the destination for service traffic through terminating gateways. Refer to Terminating Gateway for additional information.\nYou can configure the following parameters in the Destination block:\nAddress\tSpecifies a list of addresses for the destination. You can configure a list of hostnames and IP addresses. Wildcards are not supported.\tlist\tnone\t\nPort\tSpecifies the port number of the destination.\tinteger\t0\t\nMaxInboundConnections\nSpecifies the maximum number of concurrent inbound connections to each service instance.\nDefault: 0\nData type: integer\nLocalConnectTimeoutMs\nSpecifies the number of milliseconds allowed for establishing connections to the local application instance before timing out.\nDefault: 5000\nData type: integer\nLocalRequestTimeoutMs\nSpecifies the timeout for HTTP requests to the local application instance. Applies to HTTP-based protocols only. If not specified, inherits the Envoy default for route timeouts.\nDefault: Inherits 15s from Envoy as the default\nMeshGateway\nSpecifies the default mesh gateway mode field for the service. Refer to Service Mesh Proxy Configuration in the mesh gateway documentation for additional information.\nYou can specify the following string values for the mode field:\nnone: The service does not make outbound connections through a mesh gateway. Instead, the service makes outbound connections directly to the destination services.\nlocal: The service mesh proxy makes an outbound connection to a gateway running in the same datacenter.\nremote: The service mesh proxy makes an outbound connection to a gateway running in the destination datacenter.\nExternalSNI\nSpecifies the TLS server name indication (SNI) when federating with an external system.\nExpose\nSpecifies default configurations for exposing HTTP paths through Envoy. Exposing paths through Envoy enables services to listen on localhost only. Applications that are not Consul service mesh-enabled can still contact an HTTP endpoint. Refer to Expose Paths Configuration Reference for additional information and example configurations.\nData type: map\nExpose.Checks\nExposes all HTTP and gRPC checks registered with the agent if set to true. Envoy exposes listeners for the checks and only accepts connections originating from localhost or Consul's advertise_addr. The ports for the listeners are dynamically allocated from the agent's expose_min_port and expose_max_port configurations.\nWe recommend enabling the Checks configuration when a Consul client cannot reach registered services over localhost, such as when Consul agents run in their own pods in Kubernetes.\nDefault: false\nData type: boolean\nExpose.Paths[]\nSpecifies a list of configuration maps that define paths to expose through Envoy when Expose.Checks is set to true. You can configure the following parameters for each map in the list:\nPath\tSpecifies the HTTP path to expose. You must prepend the path with a forward slash (/).\tstring\tnone\t\nLocalPathPort\tSpecifies the port where the local service listens for connections to the path.\tinteger\t0\t\nListenPort\tSpecifies the port where the proxy listens for connections. The port must be available. If the port is unavailable, Envoy does not expose a listener for the path and the proxy registration still succeeds.\tinteger\t0\t\nProtocol\tSpecifies the protocol of the listener. You can configure one of the following values: \nhttp\nhttp2: Use with gRPC traffic\n\tinteger\thttp"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/connect/config-entries/partition-exports",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.16.x/connect/config-entries/sameness-group",
  "text": "Sameness group configuration entry reference | Consul\nThis page provides reference information for sameness group configuration entries. Sameness groups associate services with identical names across partitions and cluster peers.\nTo learn more about creating a sameness group, refer to Create sameness groups or Create sameness groups on Kubernetes.\nWarning\nSameness groups are a beta feature for all Consul v1.16.x releases. Functionality is subject to change. You should never use the beta release in secure environments or production scenarios. Features in beta may experience performance issues, scaling issues, and limited support.\nThe following list outlines field hierarchy, language-specific data types, and requirements in the sameness group configuration entry. Click on a property name to view additional details, including default values.\nKind: string | required | must be set to sameness-group\nName: string | required\nPartition: string | default\nDefaultForFailover: boolean | false\nMembers: list of maps | required\nPartition: string\nPeer: string\nWhen every field is defined, a sameness group configuration entry has the following form:\nKind = \"sameness-group\" # required Name = \"<group-name>\" # required Partition = \"<partition-configuration-applies-to>\" DefaultForFailover = false Members = [ # required { Partition = \"<partition-with-services-in-group>\" }, { Peer = \"<cluster-peer-with-services-in-group>\" } ] \nThis section provides details about the fields you can configure in the sameness group configuration entry.\nKind\nSpecifies the type of configuration entry to implement. Must be set to sameness-group.\nData type: String value that must be set to sameness-group.\nSpecifies a name for the configuration entry that is used to identify the sameness group. To ensure consistency, use descriptive names and make sure that the same name is used when creating configuration entries to add each member to the sameness group.\nPartition\nSpecifies the local admin partition that the sameness group applies to. Refer to admin partitions for more information.\nDefault: default\nDefaultForFailover\nDetermines whether the sameness group should be used to establish connections to services with the same name during failover scenarios. When this field is set to true, DNS queries and upstream requests automatically failover to services in the sameness group according to the order of the members in the Members list.\nWhen this field is set to true, upstream requests automatically fail over to services in the sameness group according to the order of the members in the Members list. It impacts all services on the partition.\nWhen this field is set to false, you can use a sameness group for failover by configuring the Failover block of a service resolver configuration entry.\nWhen you query Consul DNS using sameness groups, DefaultForFailover must be set to true. Otherwise, Consul DNS returns an error.\nData type: Boolean\nIncludeLocal\nDetermines whether the local partition should be considered the first member of the sameness group. When this field is set to true, DNS queries, upstream requests, and failover traffic returns a health instance from the local partition unless one does not exist.\nIf you enable this parameter, you do not need to list the local partition as the first member in the group.\nMembers\nSpecifies the partitions and cluster peers that are members of the sameness group from the perspective of the local partition.\nThe local partition should be the first member listed. The order of the members determines their precedence during failover scenarios. If a member is listed but Consul cannot connect to it, failover proceeds with the next healthy member in the list. For an example demonstrating how to configure this parameter, refer to Failover between sameness groups.\nEach partition can belong to a single sameness group. You cannot associate a partition or cluster peer with multiple sameness groups.\nData type: List that can contain maps of the following parameters:\nPartition\nPeer\nMembers[].Partition\nSpecifies a partition in the local datacenter that is a member of the sameness group. When the value of this field is set to *, all local partitions become members of the sameness group.\nMembers[].Peer\nSpecifies the name of a cluster peer that is a member of the sameness group.\nCluster peering connections must be established before adding a peer to the list of members. Refer to establish cluster peering connections for more information.\nThe following examples demonstrate common sameness group configuration patterns for specific use cases.\nFailover between members of a sameness group\nIn the following example, the configuration entry defines a sameness group named products-api that applies to the store-east partition in the local datacenter. The sameness group is configured so that when its services instances in store-east fails, Consul will attempt to establish a failover connection in the following order:\nServices with the same name in the store-east partition\nServices with the same name in the inventory-east partition in the same datacenter\nServices with the same name in the store-west partition of datacenter dc2, which has an established cluster peering connection.\nServices with the same name in the inventory-west partition of dc2, which has an established cluster peering connection.\nKind = \"sameness-group\" Name = \"products-api\" Partition = \"store-east\" DefaultForFailover = true Members = [ { Partition = \"store-east\" }, { Partition = \"inventory-east\" }, { Peer = \"dc2-store-west\" }, { Peer = \"dc2-inventory-west\" } ]"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.14.x/connect/config-entries/service-defaults",
  "text": "Service Defaults Configuration Reference | Consul\nKind\nSpecifies the configuration entry type.\nData type: String value that must be set to service-defaults.\nSpecifies the name of the service you are setting the defaults for. \nNamespace \nSpecifies the Consul namespace that the configuration entry applies to. \nDefault: default\nPartition \nSpecifies the name of the Consul admin partition that the configuration entry applies to. Refer to Admin Partitions for additional information.\nMeta\nSpecifies a set of custom key-value pairs to add to the Consul KV store. \nData type: Map of one or more key-value pairs.\nkeys: string\nvalues: string, integer, or float\nProtocol\nSpecifies the default protocol for the service. In service mesh use cases, the protocol configuration is required to enable the following features and components:\nobservability\nservice splitter configuration entry \nservice router configuration entry \nL7 intentions\nYou can set the global protocol for proxies in the proxy-defaults configuration entry, but the protocol specified in the service-defaults configuration entry overrides the proxy-defaults configuration. \nDefault: tcp\nYou can speciyf one of the following string values:\ntcp (default)\nhttp \nhttp2\ngrpc \nRefer to Set the default protocol for an example configuration.\nBalanceInboundConnections\nSpecifies the strategy for allocating inbound connections to the service across Envoy proxy threads. The only supported value is exact_balance. By default, no connections are balanced. Refer to the Envoy documentation for details. \nMode\nSpecifies a mode for how the service directs inbound and outbound traffic. \nYou can specify the following string values:\ndirect: The proxy's listeners must be dialed directly by the local application and other proxies. \ntransparent: The service captures inbound and outbound traffic and redirects it through the proxy. The mode does not enable the traffic redirection. It instructs Consul to configure Envoy as if traffic is already being redirected. \nUpstreamConfig\nControls default upstream connection settings and custom overrides for individual upstream services. If your network contains federated datacenters, individual upstream configurations apply to all pairs of source and upstream destination services in the network. Refer to the following fields for details: \nUpstreamConfig.Overrides\nUpstreamConfig.Defaults\nData type: map\nUpstreamConfig.Overrides[]\nSpecifies options that override the default upstream configurations for individual upstreams. \nData type: list\nUpstreamConfig.Overrides[].Name\nSpecifies the name of the upstream service that the configuration applies to. We recommend that you do not use the * wildcard to avoid applying the configuration to unintended upstreams.\nUpstreamConfig.Overrides[].Namespace \nSpecifies the namespace containing the upstream service that the configuration applies to. Do not use the * wildcard to prevent the configuration from appling to unintended upstreams.\nUpstreamConfig.Overrides[].Protocol\nSpecifies the protocol to use for requests to the upstream listener. \nWe recommend configuring the protocol in the main Protocol field of the configuration entry so that you can leverage L7 features. Setting the protocol in an upstream configuration limits L7 management functionality.\nUpstreamConfig.Overrides[].ConnectTimeoutMs\nSpecifies how long in milliseconds that the service should attempt to establish an upstream connection before timing out. \nWe recommend configuring the upstream timeout in the connection_timeout field of the service-resolver configuration entry for the upstream destination service. Doing so enables you to leverage L7 features. Configuring the timeout in the service-defaults upstream configuration limits L7 management functionality.\nDefault: 5000\nUpstreamConfig.Overrides[].MeshGateway\nMap that contains the default mesh gateway mode field for the upstream. Refer to Connect Proxy Configuration in the mesh gateway documentation for additional information. \nYou can specify the following string values for the mode field:\nnone: The service does not make outbound connections through a mesh gateway. Instead, the service makes outbound connections directly to the destination services.\nlocal: The service mesh proxy makes an outbound connection to a gateway running in the same datacenter. \nremote: The service mesh proxy makes an outbound connection to a gateway running in the destination datacenter. \nUpstreamConfig.Overrides[].BalanceOutboundConnections\nSets the strategy for allocating outbound connections from the upstream across Envoy proxy threads. \nThe only supported value is exact_balance. By default, no connections are balanced. Refer to the Envoy documentation for details. \nUpstreamConfig.Overrides[].Limits\nMap that specifies a set of limits to apply to when connecting to individual upstream services. \nThe following table describes limits you can configure:\nMaxConnections\tSpecifies the maximum number of connections a service instance can establish against the upstream. Define this limit for HTTP/1.1 traffic.\tinteger\t0\t\nMaxPendingRequests\tSpecifies the maximum number of requests that are queued while waiting for a connection to establish. An L7 protocol must be defined in the protocol field for this limit to take effect.\tinteger\t0\t\nMaxConcurrentRequests\tSpecifies the maximum number of concurrent requests. Define this limit for HTTP/2 traffic. An L7 protocol must be defined in the protocol field for this limit to take effect.\tinteger\t0\t\nRefer to the upstream configuration example for additional guidance.\nUpstreamConfig.Overrides[].PassiveHealthCheck\nMap that specifies a set of rules that enable Consul to remove hosts from the upstream cluster that are unreachable or that return errors. \nThe following table describes passive health check parameters you can configure:\nInterval\tSpecifies the time between checks.\tstring\t0s\t\nMaxFailures\tSpecifies the number of consecutive failures allowed per check interval. If exceeded, Consul removes the host from the load balancer.\tinteger\t0\t\nEnforcingConsecutive5xx \tSpecifies a percentage that indicates how many times out of 100 that Consul ejects the host when it detects an outlier status. The outlier status is determined by consecutive errors in the 500-599 response range.\tinteger\t100\t\nUpstreamConfig.Defaults\nSpecifies configurations that set default upstream settings. For information about overriding the default configurations for in for individual upstreams, refer to UpstreamConfig.Overrides. \nUpstreamConfig.Defaults.Protocol\nSpecifies default protocol for upstream listeners. \nWe recommend configuring the protocol in the main Protocol field of the configuration entry so that you can leverage L7 features. Setting the protocol in an upstream configuration limits L7 management functionality.\nUpstreamConfig.Defaults.ConnectTimeoutMs\nSpecifies how long in milliseconds that all services should continue attempting to establish an upstream connection before timing out. \nFor non-Kubernetes environments, we recommend configuring the upstream timeout in the connection_timeout field of the service-resolver configuration entry for the upstream destination service. Doing so enables you to leverage L7 features. Configuring the timeout in the service-defaults upstream configuration limits L7 management functionality.\nUpstreamConfig.Defaults.MeshGateway\nSpecifies the default mesh gateway mode field for all upstreams. Refer to Connect Proxy Configuration in the mesh gateway documentation for additional information. \nlocal: The service mesh proxy makes an outbound connection to a gateway running in the same datacenter. \nremote: The service mesh proxy makes an outbound connection to a gateway running in the destination datacenter. \nUpstreamConfig.Defaults.BalanceOutboundConnections\nSets the strategy for allocating outbound connections from upstreams across Envoy proxy threads. The only supported value is exact_balance. By default, no connections are balanced. Refer to the Envoy documentation for details. \nUpstreamConfig.Defaults.Limits\nMap that specifies a set of limits to apply to when connecting upstream services. The following table describes limits you can configure:\nMaxConnections\tSpecifies the maximum number of connections a service instance can establish against the upstream. Define this limit for HTTP/1.1 traffic.\tinteger\t0\t\nMaxPendingRequests\tSpecifies the maximum number of requests that are queued while waiting for a connection to establish. An L7 protocol must be defined in the protocol field for this limit to take effect.\tinteger\t0\t\nMaxConcurrentRequests\tSpecifies the maximum number of concurrent requests. Define this limit for HTTP/2 traffic. An L7 protocol must be defined in the protocol field for this limit to take effect.\tinteger\t0\t\nUpstreamConfig.Defaults.PassiveHealthCheck\nMap that specifies a set of rules that enable Consul to remove hosts from the upstream cluster that are unreachable or that return errors. The following table describes the health check parameters you can configure:\nInterval\tSpecifies the time between checks.\tstring\t0s\t\nMaxFailures\tSpecifies the number of consecutive failures allowed per check interval. If exceeded, Consul removes the host from the load balancer.\tinteger\t0\t\nEnforcingConsecutive5xx \tSpecifies a percentage that indicates how many times out of 100 that Consul ejects the host when it detects an outlier status. The outlier status is determined by consecutive errors in the 500-599 response range.\tinteger\t100\t\nTransparentProxy\nControls configurations specific to proxies in transparent mode. Refer to Transparent Proxy for additional information. \nYou can configure the following parameters in the TransparentProxy block:\nOutboundListenerPort\tSpecifies the port that the proxy listens on for outbound traffic. This must be the same port number where outbound application traffic is redirected.\tinteger\t15001\t\nDialedDirectly\tEnables transparent proxies to dial the proxy instance's IP address directly when set to true. Transparent proxies commonly dial upstreams at the \"virtual\" tagged address, which load balances across instances. Dialing individual instances can be helpful for stateful services, such as a database cluster with a leader.\tboolean\tfalse\t\nDestination[]\nConfigures the destination for service traffic through terminating gateways. Refer to Terminating Gateway for additional information. \nYou can configure the following parameters in the Destination block:\nAddress\tSpecifies a list of addresses for the destination. You can configure a list of hostnames and IP addresses. Wildcards are not supported.\tlist\tnone\t\nPort\tSpecifies the port number of the destination.\tinteger\t0\t\nMaxInboundConnections\nSpecifies the maximum number of concurrent inbound connections to each service instance.\nDefault: 0\nLocalConnectTimeoutMs\nSpecifies the number of milliseconds allowed for establishing connections to the local application instance before timing out.\nLocalRequestTimeoutMs\nSpecifies the timeout for HTTP requests to the local application instance. Applies to HTTP-based protocols only. If not specified, inherits the Envoy default for route timeouts.\nDefault: Inherits 15s from Envoy as the default\nMeshGateway\nSpecifies the default mesh gateway mode field for the service. Refer to Connect Proxy Configuration in the mesh gateway documentation for additional information. \nlocal: The service mesh proxy makes an outbound connection to a gateway running in the same datacenter. \nremote: The service mesh proxy makes an outbound connection to a gateway running in the destination datacenter. \nExternalSNI\nSpecifies the TLS server name indication (SNI) when federating with an external system. \nExpose\nSpecifies default configurations for exposing HTTP paths through Envoy. Exposing paths through Envoy enables services to listen on localhost only. Applications that are not Consul service mesh-enabled can still contact an HTTP endpoint. Refer to Expose Paths Configuration Reference for additional information and example configurations.\nExpose.Checks\nExposes all HTTP and gRPC checks registered with the agent if set to true. Envoy exposes listeners for the checks and only accepts connections originating from localhost or Consul's advertise_addr. The ports for the listeners are dynamically allocated from the agent's expose_min_port and expose_max_port configurations. \nWe recommend enabling the Checks configuration when a Consul client cannot reach registered services over localhost, such as when Consul agents run in their own pods in Kubernetes.\nData type: boolean\nExpose.Paths[]\nSpecifies a list of configuration maps that define paths to expose through Envoy when Expose.Checks is set to true. You can configure the following parameters for each map in the list:\nPath\tSpecifies the HTTP path to expose. You must prepend the path with a forward slash (/).\tstring\tnone\t\nLocalPathPort\tSpecifies the port where the local service listens for connections to the path.\tinteger\t0\t\nListenPort\tSpecifies the port where the proxy listens for connections. The port must be available. If the port is unavailable, Envoy does not expose a listener for the path and the proxy registration still succeeds.\tinteger\t0\t\nProtocol\tSpecifies the protocol of the listener. You can configure one of the following values: \nhttp\nhttp2: Use with gRPC traffic\n\tinteger\thttp"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.15.x/connect/config-entries/service-intentions",
  "text": "Service intentions configuration entry reference | Consul\nKind\nSpecifies the type of configuration entry to implement. Must be set to service-intentions.\nData type: String value that must be set to service-intentions.\nSpecifies a name of the destination service for all intentions defined in the configuration entry. \nDefault: Defaults to the name of the node after writing the entry to the Consul server.\nYou can also specify a wildcard character (*) to match all services without intentions. Intentions that are applied with a wildcard, however, are not supported when defining L7 Permissions. \nNamespace Enterprise\nSpecifies the namespace that the configuration entry applies to. Services in the namespace are the traffic destinations that the intentions allow or deny traffic to. \nYou can also specify a wildcard character (*) to match all namespaces. Intentions that are applied with a wildcard, however, are not supported when defining L7 Permissions. \nPartition Enterprise\nSpecifies the admin partition to apply the configuration entry. Services in the specified partition are the traffic destinations that the intentions allow or deny traffic to. \nMeta\nSpecifies key-value pairs to add to the KV store when the configuration entry is evaluated.\nData type: Map of one or more key-value pairs\nkeys: String\nvalues: String, integer, or float\nSources[]\nList of configurations that define intention sources and the authorization granted to the sources. You can specify source configurations in any order, but Consul stores and evaluates them in order of reverse precedence at runtime. Refer to Precedence for additional information.\nList of objects that contain the following fields:\nPeer\nNamespace Enterprise\nPartition Enterprise\nAction\nPermissions\nPrecedence\nType\nDescription\nLegacyID\nLegacyMeta\nLegacyCreateTime\nLegacyUpdateTime\nSources[].Name\nSpecifies the name of the source that the intention allows or denies traffic from. If Type is set to consul, then the value refers to the name of a Consul service. The source is not required to be registered into the Consul catalog. \nSources[].Peer\nSpecifies the name of a peered Consul cluster that the intention allows or denies traffic from. Refer to Cluster peering overview for additional information about peers. \nThe Peer and Partition fields are mutually exclusive. \nSources[].Namespace Enterprise\nSpecifies the traffic source namespace that the intention allows or denies traffic from. \nDefault: If Peer is unspecified, defaults to the destination Namespace.\nSources[].Partition Enterprise\nSpecifies the name of an admin partition that the intention allows or denies traffic from. Refer to Admin Partitions for additional information about partitions. \nThe Peer and Partition fields are mutually exclusive. \nDefault: If Peer is unspecified, defaults to the destination Partition.\nSources[].Action\nSpecifies the action to take when the source sends traffic to the destination service. The value is either allow or deny. Do not configure this field to apply L7 intentions to the same source. Configure the Permissions field instead. \nThis field is required for L4 intentions. \nData type: String value set to either allow or deny\nRefer to the following examples for additional guidance:\nL4 Intentions for specific sources and destinations\nL4 intentions for all destinations\nL4 intentions for all sources\nL4 and L7\nSources[].Permissions[]\nSpecifies a list of permissions for L7 traffic sources. The list contains one or more actions and a set of match criteria for each action. \nConsul applies permissions in the order specified in the configuration. Beginning at the top of the list, Consul applies the first matching request and stops evaluating against the remaining configurations. \nFor requests that do not match any of the defined permissions, Consul applies the intention behavior defined in the acl_default_policy configuration. \nDo not configure this field for L4 intentions. Use the Sources.Action parameter instead. \nThe Permissions only applies to services with a compatible protocol. Permissions are not supported when the Name or Namespace field is configured with a wildcard because service instances or services in a namespace may use different protocols.\nList of objects that contain the following fields:\nAction\nHTTP\nRefer to the following examples for additional guidance:\nRest access\ngRPC\nCluster peering\nL4 and L7\nSources[].Permissions[].Action\nSpecifies the action to take when the source sends traffic to the destination service. The value is either allow or deny. \nThis field is required. \nData type: String value set to either allow or deny.\nSources[].Permissions[].HTTP\nSpecifies a set of HTTP-specific match criteria. Consul applies the action defined in the Action field to source traffic that matches the criteria.\nData type: Map \nThe following table describes the parameters that the HTTP map may contain:\nPathExact\tSpecifies an exact path to match on the HTTP request path. Do not specify PathExact if PathPrefix or PathRegex are configured in the same HTTP configuration.\tstring\tnone\t\nPathPrefix\tSpecifies a path prefix to match on the HTTP request path. Do not specify PathPrefix if PathExact or PathRegex are configured in the same HTTP configuration.\tstring\tnone\t\nPathRegex\tDefines a regular expression to match on the HTTP request path. Do not specify PathRegex if PathExact or PathPrefix are configured in the same HTTP configuration. The regex syntax is proxy-specific. If using Envoy, refer to the re2 documentation for details.\tstring\tnone\t\nMethods\tSpecifies a list of HTTP methods. Consul applies the permission if a request matches the PathExact, PathPrefix, PathRegex, or Header, and the source sent the request using one of the specified methods. Refer to the Mozilla documentation for a list of supported request headers.\tlist\tAll request methods\t\nHeader\tSpecifies a header name and matching criteria for HTTP request headers. Refer to Sources[].Permissions[].HTTP[].Header for details.\tlist of maps\tnone\t\nSources[].Permissions[].HTTP[].Header[]\nSpecifies a header name and matching criteria for HTTP request headers. The request header must match all specified criteria for the permission to apply. \nData type: list of objects \nEach member of the Header list is a map that contains a Name field and at least one match criterion. The following table describes the parameters that each member of the Header list may contain: \nParameterDescriptionData typeRequired\nName\tSpecifies the name of the header to match.\tstring\trequired\t\nPresent\tEnables a match if the header configured in the Name field appears in the request. Consul matches on any value as long as the header key appears in the request. Do not specify Present if Exact, Prefix, Suffix, or Regex are configured in the same Header configuration.\tboolean\toptional\t\nExact\tSpecifies a value for the header key set in the Name field. If the request header value matches the Exact value, Consul applies the permission. Do not specify Exact if Present, Prefix, Suffix, or Regex are configured in the same Header configuration.\tstring\toptional\t\nPrefix\tSpecifies a prefix value for the header key set in the Name field. If the request header value starts with the Prefix value, Consul applies the permission. Do not specify Prefix if Present, Exact, Suffix, or Regex are configured in the same Header configuration.\tstring\toptional\t\nSuffix\tSpecifies a suffix value for the header key set in the Name field. If the request header value ends with the Suffix value, Consul applies the permission. Do not specify Suffix if Present, Exact, Prefix, or Regex are configured in the same Header configuration.\tstring\toptional\t\nRegex\tSpecifies a regular expression pattern as the value for the header key set in the Name field. If the request header value matches the regex, Consul applies the permission. Do not specify Regex if Present, Exact, Prefix, or Suffix are configured in the same Header configuration. The regex syntax is proxy-specific. If using Envoy, refer to the re2 documentation for details.\tstring\toptional\t\nInvert\tInverts the matching logic configured in the Header. Default is false.\tboolean\toptional\t\nSources[].Precedence\nThe Precedence field contains a read-only integer. Consul generates the value based on name configurations for the source and destination services. Refer to Precedence and matching order for additional information. \nSources[].Type\nSpecifies the type of destination service that the configuration entry applies to. The only value supported is consul. \nDefault: consul\nSources[].Description\nSpecifies a description of the intention. Consul presents the description in API responses to assist other tools integrated into the network. \nSources[].LegacyID\nRead-only unique user ID (UUID) for the intention in the system. Consul generates the value and exposes it in the configuration entry so that legacy API endpoints continue to function. Refer to Read Specific Intention by ID for additional information.\nSources[].LegacyMeta\nRead-only set of arbitrary key-value pairs to attach to the intention. Consul generates the metadata and exposes it in the configuration entry so that legacy intention API endpoints continue to function. Refer to Read Specific Intention by ID for additional information.\nSources[].CreateTime\nRead-only timestamp for the intention creation. Consul exposes the timestamp in the configuration entry to allow legacy intention API endpoints to continue functioning. Refer to Read Specific Intention by ID for additional information.\nSources[].LegacyUpdateTime\nRead-only timestamp marking the most recent intention update. Consul exposes the timestamp in the configuration entry to allow legacy intention API endpoints to continue functioning. Refer to Read Specific Intention by ID for additional information."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.16.x/connect/config-entries/service-defaults",
  "text": "Service Defaults Configuration Reference | Consul\nSpecifies the configuration entry type. The value must be set to service-defaults.\nData type: String value that must be set to service-defaults.\nSpecifies the name of the service you are setting the defaults for.\nNamespace Enterprise\nSpecifies the Consul namespace that the configuration entry applies to.\nPartition Enterprise\nSpecifies the name of the Consul admin partition that the configuration entry applies to. Refer to Admin Partitions for additional information.\nMeta\nSpecifies a set of custom key-value pairs to add to the Consul KV store.\nData type: Map of one or more key-value pairs.\nkeys: String\nvalues: String, integer, or float\nProtocol\nSpecifies the default protocol for the service. In service mesh use cases, the protocol configuration is required to enable the following features and components:\nobservability\nservice splitter configuration entry\nservice router configuration entry\nL7 intentions\nYou can set the global protocol for proxies in the proxy-defaults configuration entry, but the protocol specified in the service-defaults configuration entry overrides the proxy-defaults configuration.\nDefault: tcp\nYou can specify one of the following String values:\ntcp (default)\nhttp2\ngrpc\nRefer to Set the default protocol for an example configuration.\nBalanceInboundConnections\nSpecifies the strategy for allocating inbound connections to the service across Envoy proxy threads. The only supported value is exact_balance. By default, no connections are balanced. Refer to the Envoy documentation for details.\nMode\nSpecifies a mode for how the service directs inbound and outbound traffic.\nYou can specify the following string values:\ndirect: The proxy's listeners must be dialed directly by the local application and other proxies.\ntransparent: The service captures inbound and outbound traffic and redirects it through the proxy. The mode does not enable the traffic redirection. It instructs Consul to configure Envoy as if traffic is already being redirected.\nUpstreamConfig\nControls default upstream connection settings and custom overrides for individual upstream services. If your network contains federated datacenters, individual upstream configurations apply to all pairs of source and upstream destination services in the network. Refer to the following fields for details:\nUpstreamConfig.Overrides\nUpstreamConfig.Overrides[]\nSpecifies options that override the default upstream configurations for individual upstreams.\nData type: List\nUpstreamConfig.Overrides[].Name\nSpecifies the name of the upstream service that the configuration applies to. We recommend that you do not use the * wildcard to avoid applying the configuration to unintended upstreams.\nUpstreamConfig.Overrides[].Namespace \nSpecifies the namespace containing the upstream service that the configuration applies to. Do not use the * wildcard to prevent the configuration from applying to unintended upstreams.\nUpstreamConfig.Overrides[].Peer\nSpecifies the peer name of the upstream service that the configuration applies to. The * wildcard is not supported.\nUpstreamConfig.Overrides[].Protocol\nSpecifies the protocol to use for requests to the upstream listener.\nUpstreamConfig.Overrides[].ConnectTimeoutMs\nSpecifies how long in milliseconds that the service should attempt to establish an upstream connection before timing out.\nWe recommend configuring the upstream timeout in the connection_timeout field of the service-resolver configuration entry for the upstream destination service. Doing so enables you to leverage L7 features. Configuring the timeout in the service-defaults upstream configuration limits L7 management functionality.\nUpstreamConfig.Overrides[].MeshGateway\nMap that contains the default mesh gateway mode field for the upstream. Refer to Service Mesh Proxy Configuration in the mesh gateway documentation for additional information.\nlocal: The service mesh proxy makes an outbound connection to a gateway running in the same datacenter.\nremote: The service mesh proxy makes an outbound connection to a gateway running in the destination datacenter.\nUpstreamConfig.Overrides[].BalanceOutboundConnections\nSets the strategy for allocating outbound connections from the upstream across Envoy proxy threads.\nThe only supported value is exact_balance. By default, no connections are balanced. Refer to the Envoy documentation for details.\nUpstreamConfig.Overrides[].Limits\nMap that specifies a set of limits to apply to when connecting to individual upstream services.\nThe following table describes limits you can configure:\nMaxConnections\tSpecifies the maximum number of connections a service instance can establish against the upstream. Define this limit for HTTP/1.1 traffic.\tInteger\t0\t\nMaxPendingRequests\tSpecifies the maximum number of requests that are queued while waiting for a connection to establish. An L7 protocol must be defined in the protocol field for this limit to take effect.\tInteger\t0\t\nMaxConcurrentRequests\tSpecifies the maximum number of concurrent requests. Define this limit for HTTP/2 traffic. An L7 protocol must be defined in the protocol field for this limit to take effect.\tInteger\t0\t\nRefer to the upstream configuration example for additional guidance.\nUpstreamConfig.Overrides[].PassiveHealthCheck\nMap that specifies a set of rules that enable Consul to remove hosts from the upstream cluster that are unreachable or that return errors.\nThe following table describes passive health check parameters you can configure:\nInterval\tSpecifies the time between checks.\tString\t0s\t\nMaxFailures\tSpecifies the number of consecutive failures allowed per check interval. If exceeded, Consul removes the host from the load balancer.\tInteger\t0\t\nEnforcingConsecutive5xx\tSpecifies a percentage that indicates how many times out of 100 that Consul ejects the host when it detects an outlier status. The outlier status is determined by consecutive errors in the 500-599 response range.\tInteger\t100\t\nMaxEjectionPercent\tSpecifies the maximum percentage of an upstream cluster that Consul ejects when the proxy reports an outlier. Consul ejects at least one host when an outlier is detected regardless of the value.\tInteger\t10\t\nBaseEjectionTime\tSpecifies the minimum amount of time that an ejected host must remain outside the cluster before rejoining. The real time is equal to the value of the BaseEjectionTime multiplied by the number of times the host has been ejected.\tString\t30s\t\nSpecifies configurations that set default upstream settings. For information about overriding the default configurations for in for individual upstreams, refer to UpstreamConfig.Overrides.\nUpstreamConfig.Defaults.Protocol\nSpecifies default protocol for upstream listeners.\nUpstreamConfig.Defaults.ConnectTimeoutMs\nSpecifies how long in milliseconds that all services should continue attempting to establish an upstream connection before timing out.\nFor non-Kubernetes environments, we recommend configuring the upstream timeout in the connection_timeout field of the service-resolver configuration entry for the upstream destination service. Doing so enables you to leverage L7 features. Configuring the timeout in the service-defaults upstream configuration limits L7 management functionality.\nUpstreamConfig.Defaults.MeshGateway\nSpecifies the default mesh gateway mode field for all upstreams. Refer to Service Mesh Proxy Configuration in the mesh gateway documentation for additional information.\nNone: The service does not make outbound connections through a mesh gateway. Instead, the service makes outbound connections directly to the destination services.\nUpstreamConfig.Defaults.BalanceOutboundConnections\nSets the strategy for allocating outbound connections from upstreams across Envoy proxy threads. The only supported value is exact_balance. By default, no connections are balanced. Refer to the Envoy documentation for details.\nUpstreamConfig.Defaults.Limits\nMap that specifies a set of limits to apply to when connecting upstream services. The following table describes limits you can configure:\nMaxConnections\tSpecifies the maximum number of connections a service instance can establish against the upstream. Define this limit for HTTP/1.1 traffic.\tInteger\t0\t\nMaxPendingRequests\tSpecifies the maximum number of requests that are queued while waiting for a connection to establish. An L7 protocol must be defined in the protocol field for this limit to take effect.\tInteger\t0\t\nMaxConcurrentRequests\tSpecifies the maximum number of concurrent requests. Define this limit for HTTP/2 traffic. An L7 protocol must be defined in the protocol field for this limit to take effect.\tInteger\t0\t\nUpstreamConfig.Defaults.PassiveHealthCheck\nMap that specifies a set of rules that enable Consul to remove hosts from the upstream cluster that are unreachable or that return errors. The following table describes the health check parameters you can configure:\nInterval\tSpecifies the time between checks.\tString\t0s\t\nMaxFailures\tSpecifies the number of consecutive failures allowed per check interval. If exceeded, Consul removes the host from the load balancer.\tInteger\t0\t\nEnforcingConsecutive5xx \tSpecifies a percentage that indicates how many times out of 100 that Consul ejects the host when it detects an outlier status. The outlier status is determined by consecutive errors in the 500-599 response range.\tInteger\t100\t\nMaxEjectionPercent\tSpecifies the maximum percentage of an upstream cluster that Consul ejects when the proxy reports an outlier. Consul ejects at least one host when an outlier is detected regardless of the value.\tInteger\t10\t\nBaseEjectionTime\tSpecifies the minimum amount of time that an ejected host must remain outside the cluster before rejoining. The real time is equal to the value of the BaseEjectionTime multiplied by the number of times the host has been ejected.\tString\t30s\t\nTransparentProxy\nControls configurations specific to proxies in transparent mode. Refer to Transparent Proxy Mode for additional information.\nYou can configure the following parameters in the TransparentProxy block:\nOutboundListenerPort\tSpecifies the port that the proxy listens on for outbound traffic. This must be the same port number where outbound application traffic is redirected.\tInteger\t15001\t\nDialedDirectly\tEnables transparent proxies to dial the proxy instance's IP address directly when set to true. Transparent proxies commonly dial upstreams at the \"virtual\" tagged address, which load balances across instances. Dialing individual instances can be helpful for stateful services, such as a database cluster with a leader.\tBoolean\tfalse\t\nMutualTLSMode\nControls whether mutual TLS is required for incoming connections to this service. This setting is only supported for services with transparent proxy enabled. We recommend only using permissive mode if necessary while onboarding services to the service mesh.\nYou can specify the following string values for the MutualTLSMode field:\n\"\": When this field is empty, the value is inherited from the proxy-defaults config entry.\nstrict: The sidecar proxy requires mutual TLS for incoming traffic.\npermissive: The sidecar proxy accepts mutual TLS traffic on the sidecar proxy service port, and accepts any traffic on the destination service's port.\nEnvoyExtensions\nList of extensions to modify Envoy proxy configuration. Refer to Envoy Extensions for additional information.\nThe following table describes how to configure values in the EnvoyExtensions map:\nName\tSpecifies the name of the extension.\tString\tNone\t\nRequired\tSpecify true to require the extension to apply successfully. \nUse this parameter to ensure that extensions required for secure communication are not unintentionally bypassed.\nWhen Envoy fails to apply a required extension, Consul logs an error and skips all extensions, leaving xDS resources unchanged. \n\tString\tNone\t\nArguments\tSpecifies the arguments to pass to the extension. Refer to the documentation for the extension you want to implement for additional information.\tMap\tNone\t\nConsulVersion\tSpecifies the Consul version constraint for the extension. Consul validates the version constraint against the runtime version during xDS updates. If a non-matching version is in use, Consul logs and skips the extension. \nUse this parameter to avoid upgrade issues when a configured extension is not compatible with a new version of Consul.\n\tString\tNone\t\nEnvoyVersion\tSpecifies the Envoy version constraint for the extension. Consul validates the version constraint against the version of the running Envoy proxy during xDS updates. If a non-matching version is in use, Consul logs and skips the extension. \nUse this parameter to avoid upgrade issues when a configured extension is not compatible with a new version of Envoy.\n\tString\tNone\t\nDestination{}\nConfigures the destination for service traffic through terminating gateways. Refer to Terminating Gateway for additional information.\nTo use the Destination block, proxy services must be in transparent proxy mode. Refer to Enable transparent proxy mode for additional information.\nYou can configure the following parameters in the Destination block:\nAddresses\tSpecifies a list of addresses for the destination. You can configure a list of hostnames and IP addresses. Wildcards are not supported.\tList\tNone\t\nPort\tSpecifies the port number of the destination.\tInteger\t0\t\nMaxInboundConnections\nSpecifies the maximum number of concurrent inbound connections to each service instance.\nDefault: 0\nLocalConnectTimeoutMs\nSpecifies the number of milliseconds allowed for establishing connections to the local application instance before timing out.\nLocalRequestTimeoutMs\nSpecifies the timeout for HTTP requests to the local application instance. Applies to HTTP-based protocols only. If not specified, inherits the Envoy default for route timeouts.\nDefault: Inherits 15s from Envoy as the default\nMeshGateway\nSpecifies the default mesh gateway mode field for the service. Refer to Service Mesh Proxy Configuration in the mesh gateway documentation for additional information.\nExternalSNI\nSpecifies the TLS server name indication (SNI) when federating with an external system.\nExpose\nSpecifies default configurations for exposing HTTP paths through Envoy. Exposing paths through Envoy enables services to listen on localhost only. Applications that are not Consul service mesh-enabled can still contact an HTTP endpoint. Refer to Expose Paths Configuration Reference for additional information and example configurations.\nExpose.Checks\nExposes all HTTP and gRPC checks registered with the agent if set to true. Envoy exposes listeners for the checks and only accepts connections originating from localhost or Consul's advertise_addr. The ports for the listeners are dynamically allocated from the agent's expose_min_port and expose_max_port configurations.\nWe recommend enabling the Checks configuration when a Consul client cannot reach registered services over localhost, such as when Consul agents run in their own pods in Kubernetes.\nExpose.Paths[]\nSpecifies a list of configuration maps that define paths to expose through Envoy when Expose.Checks is set to true. You can configure the following parameters for each map in the list:\nPath\tSpecifies the HTTP path to expose. You must prepend the path with a forward slash (/).\tString\tNone\t\nLocalPathPort\tSpecifies the port where the local service listens for connections to the path.\tInteger\t0\t\nListenPort\tSpecifies the port where the proxy listens for connections. The port must be available. If the port is unavailable, Envoy does not expose a listener for the path and the proxy registration still succeeds.\tInteger\t0\t\nProtocol\tSpecifies the protocol of the listener. You can configure one of the following values: \nhttp2: Use with gRPC traffic\n\tInteger\thttp"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/k8s/connect/cluster-peering/usage/create-sameness-groups",
  "text": "Create sameness groups | Consul\nCreate sameness groups on Kubernetes\nThis topic describes how to create a sameness group, which designates a set of admin partitions as functionally identical in a Consul deployment running Kubernetes. Adding an admin partition to a sameness group enables Consul to recognize services registered to remote partitions with cluster peering connections as instances of the same service when they share a name and Consul namespace.\nFor information about configuring a failover strategy using sameness groups, refer to Failover with sameness groups.\nSameness groups are a user-defined set of partitions with identical configurations, includingcustom resource definitions (CRDs) for service and proxy defaults. Partitions on separate clusters should have an established cluster peering connection in order to recognize each other.\nTo create and use sameness groups in your network, complete the following steps:\nCreate sameness group custom resource definitions (CRDs) for each member of the group. For each partition that you want to include in the sameness group, you must write and apply a sameness group CRD that defines the groups members from that partitions perspective. Refer to the sameness group configuration entry reference for details on configuration hierarchy, default values, and specifications.\nExport services to members of the sameness group. You must write and apply an exported services CRD that makes the partitions services available to other members of the group. Refer to exported services configuration entry reference for additional specification information.\nCreate service intentions for each member of the sameness group. For each partition that you want to include in the sameness group, you must write and apply service intentions CRDs to authorize traffic to your services from all members of the group. Refer to the service intentions configuration entry reference for additional specification information.\nAll datacenters where you want to create sameness groups must run Consul v1.16 or later. Refer to upgrade instructions for more information about how to upgrade your deployment.\nA Consul Enterprise license is required.\nBefore you begin\nBefore creating a sameness group, take the following actions to prepare your network:\nCheck Consul namespaces and service naming conventions\nSameness groups are defined at the partition level. Consul assumes all partitions in the group have identical configurations, including identical service names and identical Consul namespaces. This behavior occurs even when two partitions in the group contain functionally different services that share a common name and namespace. For example, if distinct services both named api were registered to different members of a sameness group, it could lead to errors because requests may be sent to the incorrect service.\nTo prevent errors, check the names of the services deployed to your network and the namespaces they are deployed in. Pay particular attention to the default namespace to confirm that services have unique names. If different services share a name, you should either change one of the services names or deploy one of the services to a different namespace.\nDeploy mesh gateways for each partition\nMesh gateways are required for cluster peering connections and recommended to secure cross-partition traffic in a single datacenter. Therefore, we recommend securing your network, and especially your production environment, by deploying mesh gateways to each datacenter. Refer to mesh gateways specifications for more information about configuring mesh gateways.\nEstablish cluster peering relationships between remote partitions\nYou must establish connections with cluster peers before you can create a sameness group that includes them. A cluster peering connection exists between two admin partitions in different datacenters, and each connection between two partitions must be established separately with each peer. Refer to establish cluster peering connections for step-by-step instructions.\nYou can establish and manage cluster peering relationships between all of your self-managed clusters using HCP Consul Central. For more information, refer to cluster peering global view in the HCP documentation.\nTo establish cluster peering connections and define a group as part of the same workflow, follow instructions up to Export services between clusters. You can use the same exported services and service intention configuration entries to establish the cluster peering connection and create the sameness group.\nTo create a sameness group, you must write and apply a set of three CRDs for each partition that is a member of the group:\nSameness group CRDs: Define the sameness group from each partitions perspective.\nExported services CRDs: Make services available to other partitions in the group.\nService intentions CRDs: Authorize traffic between services across partitions.\nDefine the sameness group from each partitions perspective\nTo define a sameness group for a partition, create a sameness group CRD that describes the partitions and cluster peers that are part of the group. Typically, this order follows this pattern:\nThe local partition\nOther partitions in the same datacenter\nPartitions with established cluster peering relationships\nIf you want all services to failover to other instances in the sameness group by default, set spec.defaultForFailover=true and list the group members in the order you want to use in a failover scenario. Refer to failover with sameness groups for more information.\nBe aware that the sameness group CRDs are different for each partition. The following example demonstrates how to format three different CRDs for three partitions that are part of the sameness group product-group when Partition 1 and Partition 2 are in DC1, and the third partition is Partition 1 in DC2:\nproduct-group.yaml\napiVersion: consul.hashicorp.com/v1alpha1 kind: SamenessGroup metadata: name: product-group spec: defaultForFailover: true members: - partition: partition-1 - partition: partition-2 - peer: dc2-partition-1 \nAfter you create the CRD, apply it to the Consul server with the following kubectl CLI command:\n$ kubectl apply -f product-group.yaml \nThen, repeat the process to create and apply a CRD for every partition that is a member of the sameness group.\nExport services to other partitions in the sameness group\nTo make services available to other members of the sameness group, you must write and apply an exported services CRD for each partition in the group. This CRD exports the local partition's services to the rest of the group members. In each CRD, set the sameness group as the consumer for the exported services. You can export multiple services in a single exported services configuration entry.\nBecause you are configuring the consumer to reference the sameness group instead of listing out each partition and cluster peer, you do not need to edit this configuration again when you add a partition or peer to the group.\nThe following example demonstrates how to format three different ExportedServices CRDs to make a service named api deployed to the store namespace of each partition available to all other group members:\napiVersion: consul.hashicorp.com/v1alpha1 Kind: ExportedServices metadata: name: partition-1 spec: services: - name: api namespace: store consumers: - samenessGroup: product-group \nFor more information about exporting services, including examples of CRDs that export multiple services at the same time, refer to the exported services configuration entry reference.\nAfter you create each exported services configuration entry, apply it to the Consul server with the following CLI command:\n$ kubectl apply -f product-group-export.yaml \nExport services for cluster peers and sameness groups as part of the same workflow\nCreating a cluster peering connection between two partitions and then adding the partitions to a sameness group requires that you write and apply two separate exported services CRDs. One CRD exports services to the peer, and a second CRD exports services to other members of the group.\nIf your goal for peering clusters is to create a sameness group, you can write and apply a single exported services configuration entry by configuring the services[].consumers block with the samenessGroup field instead of the peer field. Be aware that this scenario requires you to write the SamenessGroup CRD to Kubernetes before you apply the ExportedServices CRD that references the sameness group.\nExporting the service to other members of the sameness group makes the services visible to remote partitions, but you must also create service intentions so that local services are authorized to send and receive traffic from a member of the sameness group.\nFor each partition that is a member of the group, write and apply a service intentions CRD that defines intentions for the services that are part of the group. In the sources block of the configuration entry, include the service name, its namespace, the sameness group and grant allow permissions.\nBecause you are using the sameness group in the sources block rather than listing out each partition and cluster peer, you do not have to make further edits to the service intentions configuration entries when members are added to or removed from the group.\nThe following example demonstrates how to format three different ServiceIntentions CRDs to make a service named api available to all instances of payments deployed in all members of the sameness group including the local partition. In this example, api is deployed to the store namespace in all three partitions.\napiVersion: consul.hashicorp.com/v1alpha1 kind: ServiceIntentions metadata: name: api-intentions spec: sources: - name: api action: allow namespace: store samenessGroup: product-group \nRefer to create and manage intentions for more information about how to create and apply service intentions in Consul.\nAfter you create each service intentions configuration entry, apply it to the Consul server with the following CLI command:\n$ kubectl apply -f api-intentions.yaml \nCreate service intentions for cluster peers and sameness groups as part of the same workflow\nCreating a cluster peering connection between two partitions and then adding the partitions to a sameness group requires that you write and apply two separate service intentions CRDs. One CRD authorizes services for the peer, and a second CRD authorizes services for other members of the group.\nIf your goal for peering clusters is to create a sameness group, you can write and apply a single service intentions CRD by configuring the sources block with the samenessGroup field instead of the peer field. Be aware that this scenario requires you to write the SamenessGroup CRD to Kubernetes before you apply the ServiceIntentions CRD that references the sameness group.\nWhen defaultForFailover=true in a sameness group CRD, additional upstream configuration is not required.\nAfter creating a sameness group, you can also set up failover between services in a sameness group. Refer to Failover with sameness groups for more information."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.9.x/connect/config-entries/service-defaults",
  "text": "Configuration Entry Kind: Service Defaults | Consul\nv1.8.4+: On Kubernetes, the ServiceDefaults custom resource is supported in Consul versions 1.8.4+.\nv1.5.0+: On other platforms, this config entry is supported in Consul versions 1.5.0+.\nThe service-defaults config entry kind (ServiceDefaults on Kubernetes) controls default global values for a service, such as its protocol.\nDefault protocol\nSet the default protocol for a service in the default namespace to HTTP:\nKind = \"service-defaults\" Name = \"web\" Namespace = \"default\" Protocol = \"http\" \nKind - Must be set to service-defaults\nName (string: <required>) - Set to the name of the service being configured.\nNamespace (string: \"default\")Enterprise - Specifies the namespace the config entry will apply to.\nMeta (map<string|string>: nil) - Specifies arbitrary KV metadata pairs. Added in Consul 1.8.4.\nProtocol (string: \"tcp\") - Sets the protocol of the service. This is used by Connect proxies for things like observability features and to unlock usage of the service-splitter and service-router config entries for a service. It also unlocks the ability to define L7 intentions via service-intentions. Supported values are one of tcp, http, http2, or grpc.\nMeshGateway (MeshGatewayConfig: <optional>) - Controls the default mesh gateway configuration for this service. Added in v1.6.0.\nMode (string: \"\") - One of none, local, or remote.\nExternalSNI (string: \"\") - This is an optional setting that allows for the TLS SNI value to be changed to a non-connect value when federating with an external system. Added in v1.6.0.\nExpose (ExposeConfig: <optional>) - Controls the default expose path configuration for Envoy. Added in v1.6.2.\nExposing paths through Envoy enables a service to protect itself by only listening on localhost, while still allowing non-Connect-enabled applications to contact an HTTP endpoint. Some examples include: exposing a /metrics path for Prometheus or /healthz for kubelet liveness checks.\nChecks (bool: false) - If enabled, all HTTP and gRPC checks registered with the agent are exposed through Envoy. Envoy will expose listeners for these checks and will only accept connections originating from localhost or Consul's advertise address. The port for these listeners are dynamically allocated from expose_min_port to expose_max_port. This flag is useful when a Consul client cannot reach registered services over localhost. One example is when running Consul on Kubernetes, and Consul agents run in their own pods.\nPaths (array<Path>: []) - A list of paths to expose through Envoy.\nPath (string: \"\") - The HTTP path to expose. The path must be prefixed by a slash. ie: /metrics.\nLocalPathPort (int: 0) - The port where the local service is listening for connections to the path.\nListenerPort (int: 0) - The port where the proxy will listen for connections. This port must be available for the listener to be set up. If the port is not free then Envoy will not expose a listener for the path, but the proxy registration will not fail.\nProtocol (string: \"http\") - Sets the protocol of the listener. One of http or http2. For gRPC use http2.\nConfiguration entries may be protected by ACLs.\nReading a service-defaults config entry requires service:read on the resource.\nCreating, updating, or deleting a service-defaults config entry requires service:write on the resource."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.16.x/connect/config-entries/service-intentions",
  "text": "Service intentions configuration entry reference | Consul\nSpecifies the type of configuration entry to implement. Must be set to service-intentions.\nData type: String value that must be set to service-intentions.\nSpecifies a name of the destination service for all intentions defined in the configuration entry.\nDefault: Defaults to the name of the node after writing the entry to the Consul server.\nYou can also specify a wildcard character (*) to match all services without intentions. Intentions that are applied with a wildcard, however, are not supported when defining L7 Permissions.\nNamespace Enterprise\nSpecifies the namespace that the configuration entry applies to. Services in the namespace are the traffic destinations that the intentions allow or deny traffic to.\nYou can also specify a wildcard character (*) to match all namespaces. Intentions that are applied with a wildcard, however, are not supported when defining L7 Permissions.\nPartition Enterprise\nSpecifies the admin partition to apply the configuration entry. Services in the specified partition are the traffic destinations that the intentions allow or deny traffic to.\nSpecifies key-value pairs to add to the KV store when the configuration entry is evaluated.\nData type: Map of one or more key-value pairs\nkeys: String\nvalues: String, integer, or float\nJWT\nSpecifies a JSON Web Token provider configured in a JWT provider configuration entry, as well as additional configurations for verifying a service's JWT before authorizing communication between services\nData type: Map that contains JWT{}.Providers\nJWT{}.Providers\nSpecifies the names of one or more previously configured JWT provider configuration entries, which include the information necessary to validate a JSON web token.\nData type: List of maps\nJWT{}.Providers[].Name\nSpecifies the name of a JWT provider defined in the Name field of the jwt-provider configuration entry. You must write the JWT Provider to Consul before referencing it in a service intention.\nJWT{}.Providers[].VerifyClaims\nSpecifies additional token information to verify beyond what is configured in the JWT provider configuration entry. This map takes the form of a JSON web token claim and a value to match for verification.\nData type: List of maps that can contain the following parameters:\nPath\nValue\nJWT{}.Providers[].VerifyClaims[].Path\nSpecifies the path to the claim in the JSON web token. For more information about JWT claims, refer to the IETF standards documentation.\nData type: List of strings\nJWT{}.Providers[].VerifyClaims.Value\nSpecifies the value to match on when verifying the the claim designated in JWT{}.Providers[].VerifyClaims[].Path.\nSources[]\nList of configurations that define intention sources and the authorization granted to the sources. You can specify source configurations in any order, but Consul stores and evaluates them in order of reverse precedence at runtime. Refer to Precedence for additional information.\nList of objects that contain the following fields:\nPeer\nSamenessGroup Enterprise\nAction\nPermissions\nPrecedence\nType\nDescription\nLegacyID\nLegacyMeta\nLegacyCreateTime\nLegacyUpdateTime\nSources[].Name\nSpecifies the name of the source that the intention allows or denies traffic from. If Type is set to consul, then the value refers to the name of a Consul service. The source is not required to be registered into the Consul catalog.\nSources[].Peer\nSpecifies the name of a peered Consul cluster that the intention allows or denies traffic from. Refer to Cluster peering overview for additional information about peers.\nThe Peer and Partition fields are mutually exclusive.\nSources[].Namespace Enterprise\nSpecifies the traffic source namespace that the intention allows or denies traffic from.\nDefault: If Peer is unspecified, defaults to the destination Namespace.\nSources[].Partition Enterprise\nSpecifies the name of an admin partition that the intention allows or denies traffic from. Refer to Admin Partitions for additional information about partitions.\nThe Peer and Partition fields are mutually exclusive.\nDefault: If Peer is unspecified, defaults to the destination Partition.\nSources[].SamenessGroup Enterprise\nSpecifies the name of a sameness group that the intention allows or denies traffic from. Refer to create sameness groups for additional information.\nSources[].Action\nSpecifies the action to take when the source sends traffic to the destination service. The value is either allow or deny. Do not configure this field to apply L7 intentions to the same source. Configure the Permissions field instead.\nThis field is required for L4 intentions.\nData type: String value set to either allow or deny\nRefer to the following examples for additional guidance:\nL4 Intentions for specific sources and destinations\nL4 intentions for all destinations\nL4 intentions for all sources\nL4 and L7\nSources[].Permissions[]\nSpecifies a list of permissions for L7 traffic sources. The list contains one or more actions and a set of match criteria for each action.\nConsul applies permissions in the order specified in the configuration. Beginning at the top of the list, Consul applies the first matching request and stops evaluating against the remaining configurations.\nFor requests that do not match any of the defined permissions, Consul applies the intention behavior defined in the acl_default_policy configuration.\nDo not configure this field for L4 intentions. Use the Sources.Action parameter instead.\nThe Permissions only applies to services with a compatible protocol. Permissions are not supported when the Name or Namespace field is configured with a wildcard because service instances or services in a namespace may use different protocols.\nList of objects that contain the following fields:\nAction\nHTTP\nRefer to the following examples for additional guidance:\nRest access\ngRPC\nCluster peering\nL4 and L7\nSources[].Permissions[].Action\nSpecifies the action to take when the source sends traffic to the destination service. The value is either allow or deny.\nData type: String value set to either allow or deny.\nSources[].Permissions[].HTTP\nSpecifies a set of HTTP-specific match criteria. Consul applies the action defined in the Action field to source traffic that matches the criteria.\nThe following table describes the parameters that the HTTP map may contain:\nPathExact\tSpecifies an exact path to match on the HTTP request path. Do not specify PathExact if PathPrefix or PathRegex are configured in the same HTTP configuration.\tstring\tnone\t\nPathPrefix\tSpecifies a path prefix to match on the HTTP request path. Do not specify PathPrefix if PathExact or PathRegex are configured in the same HTTP configuration.\tstring\tnone\t\nPathRegex\tDefines a regular expression to match on the HTTP request path. Do not specify PathRegex if PathExact or PathPrefix are configured in the same HTTP configuration. The regex syntax is proxy-specific. If using Envoy, refer to the re2 documentation for details.\tstring\tnone\t\nMethods\tSpecifies a list of HTTP methods. Consul applies the permission if a request matches the PathExact, PathPrefix, PathRegex, or Header, and the source sent the request using one of the specified methods. Refer to the Mozilla documentation for a list of supported request headers.\tlist\tAll request methods\t\nHeader\tSpecifies a header name and matching criteria for HTTP request headers. Refer to Sources[].Permissions[].HTTP[].Header for details.\tlist of maps\tnone\t\nSources[].Permissions[].HTTP[].Header[]\nSpecifies a header name and matching criteria for HTTP request headers. The request header must match all specified criteria for the permission to apply.\nData type: list of objects\nEach member of the Header list is a map that contains a Name field and at least one match criterion. The following table describes the parameters that each member of the Header list may contain:\nParameterDescriptionData typeRequired\nName\tSpecifies the name of the header to match.\tstring\trequired\t\nPresent\tEnables a match if the header configured in the Name field appears in the request. Consul matches on any value as long as the header key appears in the request. Do not specify Present if Exact, Prefix, Suffix, or Regex are configured in the same Header configuration.\tboolean\toptional\t\nExact\tSpecifies a value for the header key set in the Name field. If the request header value matches the Exact value, Consul applies the permission. Do not specify Exact if Present, Prefix, Suffix, or Regex are configured in the same Header configuration.\tstring\toptional\t\nPrefix\tSpecifies a prefix value for the header key set in the Name field. If the request header value starts with the Prefix value, Consul applies the permission. Do not specify Prefix if Present, Exact, Suffix, or Regex are configured in the same Header configuration.\tstring\toptional\t\nSuffix\tSpecifies a suffix value for the header key set in the Name field. If the request header value ends with the Suffix value, Consul applies the permission. Do not specify Suffix if Present, Exact, Prefix, or Regex are configured in the same Header configuration.\tstring\toptional\t\nRegex\tSpecifies a regular expression pattern as the value for the header key set in the Name field. If the request header value matches the regex, Consul applies the permission. Do not specify Regex if Present, Exact, Prefix, or Suffix are configured in the same Header configuration. The regex syntax is proxy-specific. If using Envoy, refer to the re2 documentation for details.\tstring\toptional\t\nInvert\tInverts the matching logic configured in the Header. Default is false.\tboolean\toptional\t\nSources[].Precedence\nThe Precedence field contains a read-only integer. Consul generates the value based on name configurations for the source and destination services. Refer to Precedence and matching order for additional information.\nSources[].Type\nSpecifies the type of destination service that the configuration entry applies to. The only value supported is consul.\nDefault: consul\nSources[].Description\nSpecifies a description of the intention. Consul presents the description in API responses to assist other tools integrated into the network.\nSources[].LegacyID\nRead-only unique user ID (UUID) for the intention in the system. Consul generates the value and exposes it in the configuration entry so that legacy API endpoints continue to function. Refer to Read Specific Intention by ID for additional information.\nSources[].LegacyMeta\nRead-only set of arbitrary key-value pairs to attach to the intention. Consul generates the metadata and exposes it in the configuration entry so that legacy intention API endpoints continue to function. Refer to Read Specific Intention by ID for additional information.\nSources[].CreateTime\nRead-only timestamp for the intention creation. Consul exposes the timestamp in the configuration entry to allow legacy intention API endpoints to continue functioning. Refer to Read Specific Intention by ID for additional information.\nSources[].LegacyUpdateTime\nRead-only timestamp marking the most recent intention update. Consul exposes the timestamp in the configuration entry to allow legacy intention API endpoints to continue functioning. Refer to Read Specific Intention by ID for additional information."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.18.x/connect/config-entries/sameness-group",
  "text": "Sameness group configuration entry reference | Consul\nThis page provides reference information for sameness group configuration entries. Sameness groups associate identical admin partitions to facilitate traffic between identical services. When partitions are part of the same Consul datacenter, you can create a sameness group by listing them in the Members[].Partition field. When partitions are located on remote clusters, you must establish cluster peering connections between remote partitions in order to add them to a sameness group in the Members[].Peer field.\nTo learn more about creating a sameness group, refer to Create sameness groups or Create sameness groups on Kubernetes.\nThe following list outlines field hierarchy, language-specific data types, and requirements in the sameness group configuration entry. Click on a property name to view additional details, including default values.\nKind: string | required | must be set to sameness-group\nName: string | required\nPartition: string | default\nDefaultForFailover: boolean | false\nIncludeLocal: boolean | false\nMembers: list of maps | required\nPartition: string\nPeer: string\nWhen every field is defined, a sameness group configuration entry has the following form:\nKind = \"sameness-group\" # required Name = \"<group-name>\" # required Partition = \"<partition-configuration-applies-to>\" DefaultForFailover = false IncludeLocal = true Members = [ # required { Partition = \"<partition-with-services-in-group>\" }, { Peer = \"<cluster-peer-with-services-in-group>\" } ] \nThis section provides details about the fields you can configure in the sameness group configuration entry.\nSpecifies the type of configuration entry to implement. Must be set to sameness-group.\nData type: String value that must be set to sameness-group.\nSpecifies a name for the configuration entry that is used to identify the sameness group. To ensure consistency, use descriptive names and make sure that the same name is used when creating configuration entries to add each member to the sameness group.\nPartition\nSpecifies the local admin partition that the sameness group applies to. Refer to admin partitions for more information.\nDefaultForFailover\nDetermines whether the sameness group should be used to establish connections to services with the same name during failover scenarios.\nWhen this field is set to true, upstream requests automatically fail over to services in the sameness group according to the order of the members in the Members list. It impacts all services on the partition.\nWhen this field is set to false, you can use a sameness group for failover by configuring the Failover block of a service resolver configuration entry.\nWhen you query Consul DNS using sameness groups, DefaultForFailover must be set to true. Otherwise, Consul DNS returns an error.\nIncludeLocal\nDetermines whether the local partition should be considered the first member of the sameness group. When this field is set to true, DNS queries, upstream requests, and failover traffic returns a health instance from the local partition unless one does not exist.\nIf you enable this parameter, you do not need to list the local partition as the first member in the group.\nMembers\nSpecifies the partitions and cluster peers that are members of the sameness group from the perspective of the local partition.\nThe local partition should be the first member listed unless IncludeLocal=true. The order of the members determines their precedence during failover scenarios. If a member is listed but Consul cannot connect to it, failover proceeds with the next healthy member in the list. For an example demonstrating how to configure this parameter, refer to Failover between sameness groups.\nEach partition can belong to a single sameness group. You cannot associate a partition or cluster peer with multiple sameness groups.\nData type: List that can contain maps of the following parameters:\nPartition\nPeer\nMembers[].Partition\nSpecifies a partition in the local datacenter that is a member of the sameness group. Local partitions do not require cluster peering connections before they are added to a sameness group.\nMembers[].Peer\nSpecifies the name of a cluster peer that is a member of the sameness group.\nCluster peering connections must be established before adding a remote partition to the list of members. Refer to establish cluster peering connections for more information.\nThe following examples demonstrate common sameness group configuration patterns for specific use cases.\nFailover between members of a sameness group\nIn the following example, the configuration entry defines a sameness group named products-api that applies to the store-east partition in the local datacenter. The sameness group is configured so that when a service instance in store-east fails, Consul attempts to establish a failover connection in the following order:\nServices with the same name in the store-east partition\nServices with the same name in the inventory-east partition in the same datacenter\nServices with the same name in the store-west partition of datacenter dc2, which has an established cluster peering connection.\nServices with the same name in the inventory-west partition of dc2, which has an established cluster peering connection.\nKind = \"sameness-group\" Name = \"products-api\" Partition = \"store-east\" Members = [ { Partition = \"store-east\" }, { Partition = \"inventory-east\" }, { Peer = \"dc2-store-west\" }, { Peer = \"dc2-inventory-west\" } ]"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.18.x/connect/config-entries/service-defaults",
  "text": "Service defaults configuration entry reference | Consul\nSpecifies the configuration entry type. The value must be set to service-defaults.\nData type: String value that must be set to service-defaults.\nSpecifies the name of the service you are setting the defaults for.\nSpecifies the Consul namespace that the configuration entry applies to.\nSpecifies the name of the Consul admin partition that the configuration entry applies to. Refer to Admin Partitions for additional information.\nSpecifies a set of custom key-value pairs to add to the Consul KV store.\nData type: Map of one or more key-value pairs.\nkeys: String\nvalues: String, integer, or float\nSpecifies the default protocol for the service. In service mesh use cases, the protocol configuration is required to enable the following features and components:\nobservability\nservice splitter configuration entry\nservice router configuration entry\nL7 intentions\nYou can set the global protocol for proxies in the proxy-defaults configuration entry, but the protocol specified in the service-defaults configuration entry overrides the proxy-defaults configuration.\nDefault: tcp\nYou can specify one of the following String values:\ntcp (default)\nhttp2\ngrpc\nRefer to Set the default protocol for an example configuration.\nBalanceInboundConnections\nSpecifies the strategy for allocating inbound connections to the service across Envoy proxy threads. The only supported value is exact_balance. By default, no connections are balanced. Refer to the Envoy documentation for details.\nMode\nSpecifies a mode for how the service directs inbound and outbound traffic.\nYou can specify the following string values:\ndirect: The proxy's listeners must be dialed directly by the local application and other proxies.\ntransparent: The service captures inbound and outbound traffic and redirects it through the proxy. The mode does not enable the traffic redirection. It instructs Consul to configure Envoy as if traffic is already being redirected.\nRateLimits Enterprise\nMap containing an instance-level configuration for limiting the service's traffic rate.\nRateLimits{}.InstanceLevel\nMap containing a set of request rate limit configurations for instances of the service.\nRateLimits{}.InstanceLevel{}.RequestsPerSecond\nSpecifies the average number of requests per second allowed to the service. If the RequestsMaxBurst parameter is set, the number of requests per second to the service can temporarily exceed the limit specified in the RequestsPerSecond up to the value specified in RequestsMaxBurst. Internally, this is the refill rate of the token bucket used for rate limiting.\nRateLimits{}.InstanceLevel{}.RequestsMaxBurst\nSpecifies the maximum number of concurrent requests momentarily allowed to the service. When the limit is reached, Consul blocks additional requests. You must specify a value equal to or greater than the RequestsPerSecond parameter. If unspecified, this parameter defaults to RequestsPerSecond. Internally, this is the maximum size of the token bucket used for rate limiting.\nRateLimits{}.InstanceLevel{}.Routes\nSpecifies a list of rate limiting configurations to apply to specific routes to the service. Each member of the Routes list must configure the RequestsPerSecond parameter and one of the following route-matching parameters:\nPathExact\nPathPrefix\nPathRegex\nConsul applies the rate limit configuration to the first matching route for each request. Refer to Examples for example configurations.\nThe following table describes the parameters you can specify in the Routes map:\nPathExact\tSpecifies the exact path to match on the request path. When using this field, do not configure PathPrefix or PathRegex in the same Routes map.\tString\tNone\t\nPathPrefix\tSpecifies the path prefix to match on the request path. When using this field, do not configure PathExact or PathRegex in the same Routes map.\tString\tNone\t\nPathRegex\tSpecifies a regular expression to match on the request path. When using this field, do not configure PathExact or PathPrefix in the same Routes map. The syntax is proxy-specific. When using Envoy, refer to the documentation for Envoy v1.11.2 or newer or the documentation for Envoy v1.11.1 or older, depending on the version of Envoy you use.\tString\tNone\t\nRequestsPerSecond\tSpecifies the average number of requests per second allowed to the service. Overrides the RequestsPerSecond parameter specified for the service.\tInteger\tNone\t\nRequestsMaxBurst\tSpecifies the maximum number of concurrent requests temporarily allowed to the service. When the limit is reached, Consul blocks additional requests. You must specify a value equal to or greater than the Routes.RequestsPerSecond parameter. Overrides the RequestsMaxBurst parameter specified for the service.\tInteger\tNone\t\nUpstreamConfig\nControls default upstream connection settings and custom overrides for individual upstream services. If your network contains federated datacenters, individual upstream configurations apply to all pairs of source and upstream destination services in the network. Refer to the following fields for details:\nUpstreamConfig.Overrides\nUpstreamConfig.Overrides[]\nSpecifies options that override the default upstream configurations for individual upstreams.\nData type: List\nUpstreamConfig.Overrides[].Name\nSpecifies the name of the upstream service that the configuration applies to. We recommend that you do not use the * wildcard to avoid applying the configuration to unintended upstreams.\nUpstreamConfig.Overrides[].Namespace Enterprise\nSpecifies the namespace containing the upstream service that the configuration applies to. Do not use the * wildcard to prevent the configuration from applying to unintended upstreams.\nUpstreamConfig.Overrides[].Peer\nSpecifies the peer name of the upstream service that the configuration applies to. The * wildcard is not supported.\nUpstreamConfig.Overrides[].Protocol\nSpecifies the protocol to use for requests to the upstream listener.\nUpstreamConfig.Overrides[].ConnectTimeoutMs\nSpecifies how long in milliseconds that the service should attempt to establish an upstream connection before timing out.\nWe recommend configuring the upstream timeout in the connection_timeout field of the service-resolver configuration entry for the upstream destination service. Doing so enables you to leverage L7 features. Configuring the timeout in the service-defaults upstream configuration limits L7 management functionality.\nUpstreamConfig.Overrides[].MeshGateway\nMap that contains the default mesh gateway mode field for the upstream. Refer to Service Mesh Proxy Configuration in the mesh gateway documentation for additional information.\nUpstreamConfig.Overrides[].BalanceOutboundConnections\nSets the strategy for allocating outbound connections from the upstream across Envoy proxy threads.\nThe only supported value is exact_balance. By default, no connections are balanced. Refer to the Envoy documentation for details.\nUpstreamConfig.Overrides[].Limits\nMap that specifies a set of limits to apply to when connecting to individual upstream services.\nThe following table describes limits you can configure:\nMaxConnections\tSpecifies the maximum number of connections a service instance can establish against the upstream. Define this limit for HTTP/1.1 traffic.\tInteger\t0\t\nMaxPendingRequests\tSpecifies the maximum number of requests that are queued while waiting for a connection to establish. An L7 protocol must be defined in the protocol field for this limit to take effect.\tInteger\t0\t\nMaxConcurrentRequests\tSpecifies the maximum number of concurrent requests. Define this limit for HTTP/2 traffic. An L7 protocol must be defined in the protocol field for this limit to take effect.\tInteger\t0\t\nRefer to the upstream configuration example for additional guidance.\nUpstreamConfig.Overrides[].PassiveHealthCheck\nMap that specifies a set of rules that enable Consul to remove hosts from the upstream cluster that are unreachable or that return errors.\nThe following table describes passive health check parameters you can configure:\nInterval\tSpecifies the time between checks.\tString\t0s\t\nMaxFailures\tSpecifies the number of consecutive failures allowed per check interval. If exceeded, Consul removes the host from the load balancer.\tInteger\t0\t\nEnforcingConsecutive5xx\tSpecifies a percentage that indicates how many times out of 100 that Consul ejects the host when it detects an outlier status. The outlier status is determined by consecutive errors in the 500-599 response range.\tInteger\t100\t\nMaxEjectionPercent\tSpecifies the maximum percentage of an upstream cluster that Consul ejects when the proxy reports an outlier. Consul ejects at least one host when an outlier is detected regardless of the value.\tInteger\t10\t\nBaseEjectionTime\tSpecifies the minimum amount of time that an ejected host must remain outside the cluster before rejoining. The real time is equal to the value of the BaseEjectionTime multiplied by the number of times the host has been ejected.\tString\t30s\t\nSpecifies configurations that set default upstream settings. For information about overriding the default configurations for in for individual upstreams, refer to UpstreamConfig.Overrides.\nUpstreamConfig.Defaults.Protocol\nSpecifies default protocol for upstream listeners.\nUpstreamConfig.Defaults.ConnectTimeoutMs\nSpecifies how long in milliseconds that all services should continue attempting to establish an upstream connection before timing out.\nFor non-Kubernetes environments, we recommend configuring the upstream timeout in the connection_timeout field of the service-resolver configuration entry for the upstream destination service. Doing so enables you to leverage L7 features. Configuring the timeout in the service-defaults upstream configuration limits L7 management functionality.\nUpstreamConfig.Defaults.MeshGateway\nSpecifies the default mesh gateway mode field for all upstreams. Refer to Service Mesh Proxy Configuration in the mesh gateway documentation for additional information.\nNone: The service does not make outbound connections through a mesh gateway. Instead, the service makes outbound connections directly to the destination services.\nUpstreamConfig.Defaults.BalanceOutboundConnections\nSets the strategy for allocating outbound connections from upstreams across Envoy proxy threads. The only supported value is exact_balance. By default, no connections are balanced. Refer to the Envoy documentation for details.\nUpstreamConfig.Defaults.Limits\nMap that specifies a set of limits to apply to when connecting upstream services. The following table describes limits you can configure:\nMaxConnections\tSpecifies the maximum number of connections a service instance can establish against the upstream. Define this limit for HTTP/1.1 traffic.\tInteger\t0\t\nMaxPendingRequests\tSpecifies the maximum number of requests that are queued while waiting for a connection to establish. An L7 protocol must be defined in the protocol field for this limit to take effect.\tInteger\t0\t\nMaxConcurrentRequests\tSpecifies the maximum number of concurrent requests. Define this limit for HTTP/2 traffic. An L7 protocol must be defined in the protocol field for this limit to take effect.\tInteger\t0\t\nUpstreamConfig.Defaults.PassiveHealthCheck\nMap that specifies a set of rules that enable Consul to remove hosts from the upstream cluster that are unreachable or that return errors. The following table describes the health check parameters you can configure:\nInterval\tSpecifies the time between checks.\tString\t0s\t\nMaxFailures\tSpecifies the number of consecutive failures allowed per check interval. If exceeded, Consul removes the host from the load balancer.\tInteger\t0\t\nEnforcingConsecutive5xx \tSpecifies a percentage that indicates how many times out of 100 that Consul ejects the host when it detects an outlier status. The outlier status is determined by consecutive errors in the 500-599 response range.\tInteger\t100\t\nMaxEjectionPercent\tSpecifies the maximum percentage of an upstream cluster that Consul ejects when the proxy reports an outlier. Consul ejects at least one host when an outlier is detected regardless of the value.\tInteger\t10\t\nBaseEjectionTime\tSpecifies the minimum amount of time that an ejected host must remain outside the cluster before rejoining. The real time is equal to the value of the BaseEjectionTime multiplied by the number of times the host has been ejected.\tString\t30s\t\nTransparentProxy\nControls configurations specific to proxies in transparent mode. Refer to Transparent Proxy Mode for additional information.\nYou can configure the following parameters in the TransparentProxy block:\nOutboundListenerPort\tSpecifies the port that the proxy listens on for outbound traffic. This must be the same port number where outbound application traffic is redirected.\tInteger\t15001\t\nDialedDirectly\tEnables transparent proxies to dial the proxy instance's IP address directly when set to true. Transparent proxies commonly dial upstreams at the \"virtual\" tagged address, which load balances across instances. Dialing individual instances can be helpful for stateful services, such as a database cluster with a leader.\tBoolean\tfalse\t\nMutualTLSMode\nControls whether mutual TLS is required for incoming connections to this service. This setting is only supported for services with transparent proxy enabled. We recommend only using permissive mode if necessary while onboarding services to the service mesh.\nYou can specify the following string values for the MutualTLSMode field:\n\"\": When this field is empty, the value is inherited from the proxy-defaults config entry.\nstrict: The sidecar proxy requires mutual TLS for incoming traffic.\npermissive: The sidecar proxy accepts mutual TLS traffic on the sidecar proxy service port, and accepts any traffic on the destination service's port.\nEnvoyExtensions\nList of extensions to modify Envoy proxy configuration. Refer to Envoy Extensions for additional information.\nThe following table describes how to configure values in the EnvoyExtensions map:\nName\tSpecifies the name of the extension.\tString\tNone\t\nRequired\tSpecify true to require the extension to apply successfully. \nUse this parameter to ensure that extensions required for secure communication are not unintentionally bypassed.\nWhen Envoy fails to apply a required extension, Consul logs an error and skips all extensions, leaving xDS resources unchanged. \n\tString\tNone\t\nArguments\tSpecifies the arguments to pass to the extension. Refer to the documentation for the extension you want to implement for additional information.\tMap\tNone\t\nConsulVersion\tSpecifies the Consul version constraint for the extension. Consul validates the version constraint against the runtime version during xDS updates. If a non-matching version is in use, Consul logs and skips the extension. \nUse this parameter to avoid upgrade issues when a configured extension is not compatible with a new version of Consul.\nEnvoyVersion\tSpecifies the Envoy version constraint for the extension. Consul validates the version constraint against the version of the running Envoy proxy during xDS updates. If a non-matching version is in use, Consul logs and skips the extension. \nUse this parameter to avoid upgrade issues when a configured extension is not compatible with a new version of Envoy.\nDestination{}\nConfigures the destination for service traffic through terminating gateways. Refer to Terminating Gateway for additional information.\nTo use the Destination block, proxy services must be in transparent proxy mode. Refer to Enable transparent proxy mode for additional information.\nYou can configure the following parameters in the Destination block:\nAddresses\tSpecifies a list of addresses for the destination. You can configure a list of hostnames and IP addresses. Wildcards are not supported.\tList\tNone\t\nPort\tSpecifies the port number of the destination.\tInteger\t0\t\nMaxInboundConnections\nSpecifies the maximum number of concurrent inbound connections to each service instance.\nDefault: 0\nLocalConnectTimeoutMs\nSpecifies the number of milliseconds allowed for establishing connections to the local application instance before timing out.\nLocalRequestTimeoutMs\nSpecifies the timeout for HTTP requests to the local application instance. Applies to HTTP-based protocols only. If not specified, inherits the Envoy default for route timeouts.\nDefault: Inherits 15s from Envoy as the default\nMeshGateway\nSpecifies the default mesh gateway mode field for the service. Refer to Service Mesh Proxy Configuration in the mesh gateway documentation for additional information.\nExternalSNI\nSpecifies the TLS server name indication (SNI) when federating with an external system.\nExpose\nSpecifies default configurations for exposing HTTP paths through Envoy. Exposing paths through Envoy enables services to listen on localhost only. Applications that are not Consul service mesh-enabled can still contact an HTTP endpoint. Refer to Expose Paths Configuration Reference for additional information and example configurations.\nExpose.Checks\nExposes all HTTP and gRPC checks registered with the agent if set to true. Envoy exposes listeners for the checks and only accepts connections originating from localhost or Consul's advertise_addr. The ports for the listeners are dynamically allocated from the agent's expose_min_port and expose_max_port configurations.\nWe recommend enabling the Checks configuration when a Consul client cannot reach registered services over localhost, such as when Consul agents run in their own pods in Kubernetes.\nExpose.Paths[]\nSpecifies a list of configuration maps that define paths to expose through Envoy when Expose.Checks is set to true. You can configure the following parameters for each map in the list:\nPath\tSpecifies the HTTP path to expose. You must prepend the path with a forward slash (/).\tString\tNone\t\nLocalPathPort\tSpecifies the port where the local service listens for connections to the path.\tInteger\t0\t\nListenPort\tSpecifies the port where the proxy listens for connections. The port must be available. If the port is unavailable, Envoy does not expose a listener for the path and the proxy registration still succeeds.\tInteger\t0\t\nProtocol\tSpecifies the protocol of the listener. You can configure one of the following values: \nhttp2: Use with gRPC traffic\n\tInteger\thttp"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.17.x/connect/config-entries/service-defaults",
  "text": "Service defaults configuration entry reference | Consul\nSpecifies the configuration entry type. The value must be set to service-defaults.\nSpecifies the name of the service you are setting the defaults for.\nSpecifies the Consul namespace that the configuration entry applies to.\nSpecifies a set of custom key-value pairs to add to the Consul KV store.\nservice splitter configuration entry\nservice router configuration entry\nYou can set the global protocol for proxies in the proxy-defaults configuration entry, but the protocol specified in the service-defaults configuration entry overrides the proxy-defaults configuration.\nYou can specify one of the following String values:\ngrpc\nSpecifies the strategy for allocating inbound connections to the service across Envoy proxy threads. The only supported value is exact_balance. By default, no connections are balanced. Refer to the Envoy documentation for details.\nSpecifies a mode for how the service directs inbound and outbound traffic.\ndirect: The proxy's listeners must be dialed directly by the local application and other proxies.\ntransparent: The service captures inbound and outbound traffic and redirects it through the proxy. The mode does not enable the traffic redirection. It instructs Consul to configure Envoy as if traffic is already being redirected.\nRateLimits Enterprise\nMap containing an instance-level configuration for limiting the service's traffic rate.\nRateLimits{}.InstanceLevel\nMap containing a set of request rate limit configurations for instances of the service.\nRateLimits{}.InstanceLevel{}.RequestsPerSecond\nSpecifies the average number of requests per second allowed to the service. If the RequestsMaxBurst parameter is set, the number of requests per second to the service can temporarily exceed the limit specified in the RequestsPerSecond up to the value specified in RequestsMaxBurst. Internally, this is the refill rate of the token bucket used for rate limiting.\nRateLimits{}.InstanceLevel{}.RequestsMaxBurst\nSpecifies the maximum number of concurrent requests momentarily allowed to the service. When the limit is reached, Consul blocks additional requests. You must specify a value equal to or greater than the RequestsPerSecond parameter. If unspecified, this parameter defaults to RequestsPerSecond. Internally, this is the maximum size of the token bucket used for rate limiting.\nRateLimits{}.InstanceLevel{}.Routes\nSpecifies a list of rate limiting configurations to apply to specific routes to the service. Each member of the Routes list must configure the RequestsPerSecond parameter and one of the following route-matching parameters:\nPathExact\nPathPrefix\nPathRegex\nConsul applies the rate limit configuration to the first matching route for each request. Refer to Examples for example configurations.\nThe following table describes the parameters you can specify in the Routes map:\nPathExact\tSpecifies the exact path to match on the request path. When using this field, do not configure PathPrefix or PathRegex in the same Routes map.\tString\tNone\t\nPathPrefix\tSpecifies the path prefix to match on the request path. When using this field, do not configure PathExact or PathRegex in the same Routes map.\tString\tNone\t\nPathRegex\tSpecifies a regular expression to match on the request path. When using this field, do not configure PathExact or PathPrefix in the same Routes map. The syntax is proxy-specific. When using Envoy, refer to the documentation for Envoy v1.11.2 or newer or the documentation for Envoy v1.11.1 or older, depending on the version of Envoy you use.\tString\tNone\t\nRequestsPerSecond\tSpecifies the average number of requests per second allowed to the service. Overrides the RequestsPerSecond parameter specified for the service.\tInteger\tNone\t\nRequestsMaxBurst\tSpecifies the maximum number of concurrent requests temporarily allowed to the service. When the limit is reached, Consul blocks additional requests. You must specify a value equal to or greater than the Routes.RequestsPerSecond parameter. Overrides the RequestsMaxBurst parameter specified for the service.\tInteger\tNone\t\nControls default upstream connection settings and custom overrides for individual upstream services. If your network contains federated datacenters, individual upstream configurations apply to all pairs of source and upstream destination services in the network. Refer to the following fields for details:\nSpecifies options that override the default upstream configurations for individual upstreams.\nData type: List\nUpstreamConfig.Overrides[].Namespace Enterprise\nSpecifies the namespace containing the upstream service that the configuration applies to. Do not use the * wildcard to prevent the configuration from applying to unintended upstreams.\nUpstreamConfig.Overrides[].Peer\nSpecifies the peer name of the upstream service that the configuration applies to. The * wildcard is not supported.\nSpecifies the protocol to use for requests to the upstream listener.\nSpecifies how long in milliseconds that the service should attempt to establish an upstream connection before timing out.\nMap that contains the default mesh gateway mode field for the upstream. Refer to Service Mesh Proxy Configuration in the mesh gateway documentation for additional information.\nSets the strategy for allocating outbound connections from the upstream across Envoy proxy threads.\nThe only supported value is exact_balance. By default, no connections are balanced. Refer to the Envoy documentation for details.\nMap that specifies a set of limits to apply to when connecting to individual upstream services.\nMap that specifies a set of rules that enable Consul to remove hosts from the upstream cluster that are unreachable or that return errors.\nEnforcingConsecutive5xx\tSpecifies a percentage that indicates how many times out of 100 that Consul ejects the host when it detects an outlier status. The outlier status is determined by consecutive errors in the 500-599 response range.\tInteger\t100\t\nSpecifies configurations that set default upstream settings. For information about overriding the default configurations for in for individual upstreams, refer to UpstreamConfig.Overrides.\nSpecifies default protocol for upstream listeners.\nSpecifies how long in milliseconds that all services should continue attempting to establish an upstream connection before timing out.\nSpecifies the default mesh gateway mode field for all upstreams. Refer to Service Mesh Proxy Configuration in the mesh gateway documentation for additional information.\nNone: The service does not make outbound connections through a mesh gateway. Instead, the service makes outbound connections directly to the destination services.\nSets the strategy for allocating outbound connections from upstreams across Envoy proxy threads. The only supported value is exact_balance. By default, no connections are balanced. Refer to the Envoy documentation for details.\nEnforcingConsecutive5xx \tSpecifies a percentage that indicates how many times out of 100 that Consul ejects the host when it detects an outlier status. The outlier status is determined by consecutive errors in the 500-599 response range.\tInteger\t100\t\nControls configurations specific to proxies in transparent mode. Refer to Transparent Proxy Mode for additional information.\nOutboundListenerPort\tSpecifies the port that the proxy listens on for outbound traffic. This must be the same port number where outbound application traffic is redirected.\tInteger\t15001\t\nDialedDirectly\tEnables transparent proxies to dial the proxy instance's IP address directly when set to true. Transparent proxies commonly dial upstreams at the \"virtual\" tagged address, which load balances across instances. Dialing individual instances can be helpful for stateful services, such as a database cluster with a leader.\tBoolean\tfalse\t\nMutualTLSMode\nControls whether mutual TLS is required for incoming connections to this service. This setting is only supported for services with transparent proxy enabled. We recommend only using permissive mode if necessary while onboarding services to the service mesh.\nYou can specify the following string values for the MutualTLSMode field:\n\"\": When this field is empty, the value is inherited from the proxy-defaults config entry.\nstrict: The sidecar proxy requires mutual TLS for incoming traffic.\npermissive: The sidecar proxy accepts mutual TLS traffic on the sidecar proxy service port, and accepts any traffic on the destination service's port.\nEnvoyExtensions\nList of extensions to modify Envoy proxy configuration. Refer to Envoy Extensions for additional information.\nThe following table describes how to configure values in the EnvoyExtensions map:\nName\tSpecifies the name of the extension.\tString\tNone\t\nRequired\tSpecify true to require the extension to apply successfully. \nUse this parameter to ensure that extensions required for secure communication are not unintentionally bypassed.\nWhen Envoy fails to apply a required extension, Consul logs an error and skips all extensions, leaving xDS resources unchanged. \nArguments\tSpecifies the arguments to pass to the extension. Refer to the documentation for the extension you want to implement for additional information.\tMap\tNone\t\nConsulVersion\tSpecifies the Consul version constraint for the extension. Consul validates the version constraint against the runtime version during xDS updates. If a non-matching version is in use, Consul logs and skips the extension. \nUse this parameter to avoid upgrade issues when a configured extension is not compatible with a new version of Consul.\nEnvoyVersion\tSpecifies the Envoy version constraint for the extension. Consul validates the version constraint against the version of the running Envoy proxy during xDS updates. If a non-matching version is in use, Consul logs and skips the extension. \nUse this parameter to avoid upgrade issues when a configured extension is not compatible with a new version of Envoy.\nDestination{}\nConfigures the destination for service traffic through terminating gateways. Refer to Terminating Gateway for additional information.\nTo use the Destination block, proxy services must be in transparent proxy mode. Refer to Enable transparent proxy mode for additional information.\nAddresses\tSpecifies a list of addresses for the destination. You can configure a list of hostnames and IP addresses. Wildcards are not supported.\tList\tNone\t\nPort\tSpecifies the port number of the destination.\tInteger\t0\t\nSpecifies the default mesh gateway mode field for the service. Refer to Service Mesh Proxy Configuration in the mesh gateway documentation for additional information.\nSpecifies the TLS server name indication (SNI) when federating with an external system.\nExposes all HTTP and gRPC checks registered with the agent if set to true. Envoy exposes listeners for the checks and only accepts connections originating from localhost or Consul's advertise_addr. The ports for the listeners are dynamically allocated from the agent's expose_min_port and expose_max_port configurations.\nPath\tSpecifies the HTTP path to expose. You must prepend the path with a forward slash (/).\tString\tNone\t\nLocalPathPort\tSpecifies the port where the local service listens for connections to the path.\tInteger\t0\t\nListenPort\tSpecifies the port where the proxy listens for connections. The port must be available. If the port is unavailable, Envoy does not expose a listener for the path and the proxy registration still succeeds.\tInteger\t0\t\n\tInteger\thttp"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.13.x/connect/config-entries/service-defaults",
  "text": "Service Defaults - Configuration Entry Reference | Consul\nv1.8.4+: On Kubernetes, the ServiceDefaults custom resource is supported in Consul versions 1.8.4+.\nv1.5.0+: On other platforms, this config entry is supported in Consul versions 1.5.0+.\nThe service-defaults config entry kind (ServiceDefaults on Kubernetes) controls default global values for a service, such as its protocol.\nDefault protocol\nNOTE: The default protocol can also be configured globally for all proxies using the proxy defaults config entry. However, if the protocol value is specified in a service defaults config entry for a given service, that value will take precedence over the globally configured value from proxy defaults.\nSet the default protocol for a service in the default namespace to HTTP:\nKind = \"service-defaults\" Name = \"web\" Namespace = \"default\" Protocol = \"http\" \nUpstream configuration\nSet default connection limits and mesh gateway mode across all upstreams of \"counting\", and also override the mesh gateway mode used when dialing the \"dashboard\" service.\nKind = \"service-defaults\" Name = \"counting\" UpstreamConfig = { Defaults = { MeshGateway = { Mode = \"local\" } Limits = { MaxConnections = 512 MaxPendingRequests = 512 MaxConcurrentRequests = 512 } } Overrides = [ { Name = \"dashboard\" MeshGateway = { Mode = \"remote\" } } ] } \nTerminating gateway destination\nCreate a default destination that will be assigned to a terminating gateway. A destination represents a location outside the Consul cluster. They can be dialed directly when transparent proxy mode is enabled.\nKind = \"service-defaults\" Name = \"test-destination\" Protocol = \"tcp\" Destination { Addresses = [\"test.com\",\"test.org\"] Port = 443 } \nKind - Must be set to service-defaults\nName (string: <required>) - Set to the name of the service being configured.\nNamespace (string: \"default\")Enterprise - Specifies the namespace the config entry will apply to.\nPartition (string: \"default\")Enterprise - Specifies the name of the admin partition in which the configuration entry applies. Refer to the Admin Partitions documentation for additional information.\nMeta (map<string|string>: nil) - Specifies arbitrary KV metadata pairs. Added in Consul 1.8.4.\nProtocol (string: \"tcp\") - Sets the protocol of the service. This is used by Connect proxies for things like observability features and to unlock usage of the service-splitter and service-router config entries for a service. It also unlocks the ability to define L7 intentions via service-intentions. Supported values are one of tcp, http, http2, or grpc.\nMode (string: \"\") - One of direct or transparent. transparent represents that inbound and outbound application traffic is being captured and redirected through the proxy. This mode does not enable the traffic redirection itself. Instead it signals Consul to configure Envoy as if traffic is already being redirected. direct represents that the proxy's listeners must be dialed directly by the local application and other proxies. Added in v1.10.0.\nUpstreamConfig (UpstreamConfiguration: <optional>) - Controls default configuration settings that apply across all upstreams, and per-upstream configuration overrides. Note that per-upstream configuration applies across all federated datacenters to the pairing of source and upstream destination services. Added in v1.10.0.\nOverrides (array<UpstreamConfig>: []) - A list of optional overrides for per-upstream configuration.\nName (string: \"\") - The upstream name to apply the configuration to. This should not be set to the wildcard specifier *.\nNamespace (string: \"\") - The namespace of the upstream. This should not be set to the wildcard specifier *.\nProtocol (string: \"\") - The protocol for the upstream listener.\nNOTE: The protocol of a service should ideally be configured via the protocol field of a service-defaults config entry for the upstream destination service. Configuring it in a proxy upstream config will not fully enable some L7 features. It is supported here for backwards compatibility with Consul versions prior to 1.6.0. \nConnectTimeoutMs (int: 5000) - The number of milliseconds to allow when making upstream connections before timing out.\nNOTE: The connect timeout of a service should ideally be configured via the connect_timeout field of a service-resolver config entry for the upstream destination service. Configuring it in a proxy upstream config will not fully enable some L7 features. It is supported here for backwards compatibility with Consul versions prior to 1.6.0. \nMeshGateway (MeshGatewayConfig: <optional>) - Controls the default mesh gateway configuration for this upstream.\nLimits (Limits: <optional>) - A set of limits to apply when connecting to the upstream service. These limits are applied on a per-service-instance basis. The following limits are respected.\nMaxConnections (int: 0) - The maximum number of connections a service instance will be allowed to establish against the given upstream. Use this to limit HTTP/1.1 traffic, since HTTP/1.1 has a request per connection.\nMaxPendingRequests (int: 0) - The maximum number of requests that will be queued while waiting for a connection to be established. For this configuration to be respected, a L7 protocol must be defined in the protocol field.\nMaxConcurrentRequests (int: 0) - The maximum number of concurrent requests that will be allowed at a single point in time. Use this to limit HTTP/2 traffic, since HTTP/2 has many requests per connection. For this configuration to be respected, a L7 protocol must be defined in the protocol field.\nPassiveHealthCheck (PassiveHealthCheck: <optional>) - Passive health checks are used to remove hosts from the upstream cluster which are unreachable or are returning errors..\nInterval (duration: 0s) - The time between checks. Each check will cause hosts which have exceeded max_failures to be removed from the load balancer, and any hosts which have passed their ejection time to be returned to the load balancer.\nMaxFailures (int: 0) - The number of consecutive failures which cause a host to be removed from the load balancer.\nEnforcingConsecutive5xx (int: 100) - The % chance that a host will be actually ejected when an outlier status is detected through consecutive 5xx.\nDefaults (UpstreamConfig: <optional>) - Default configuration that applies to all upstreams of this service.\nProtocol (string: \"\") - The protocol for the upstream listener.\nNOTE: The protocol of a service should ideally be configured via the protocol field of a service-defaults config entry for the upstream destination service. Configuring it in a proxy upstream config will not fully enable some L7 features. It is supported here for backwards compatibility with Consul versions prior to 1.6.0. \nConnectTimeoutMs (int: 5000) - The number of milliseconds to allow when making upstream connections before timing out.\nNOTE: The connect timeout of a service should ideally be configured via the connect_timeout field of a service-resolver config entry for the upstream destination service. Configuring it in a proxy upstream config will not fully enable some L7 features. It is supported here for backwards compatibility with Consul versions prior to 1.6.0. \nMeshGateway (MeshGatewayConfig: <optional>) - Controls the default mesh gateway configuration for this upstream.\nLimits (Limits: <optional>) - A set of limits to apply when connecting to the upstream service. These limits are applied on a per-service-instance basis. The following limits are respected.\nMaxConnections (int: 0) - The maximum number of connections a service instance will be allowed to establish against the given upstream. Use this to limit HTTP/1.1 traffic, since HTTP/1.1 has a request per connection.\nMaxPendingRequests (int: 0) - The maximum number of requests that will be queued while waiting for a connection to be established. For this configuration to be respected, a L7 protocol must be defined in the protocol field.\nMaxConcurrentRequests (int: 0) - The maximum number of concurrent requests that will be allowed at a single point in time. Use this to limit HTTP/2 traffic, since HTTP/2 has many requests per connection. For this configuration to be respected, a L7 protocol must be defined in the protocol field.\nPassiveHealthCheck (PassiveHealthCheck: <optional>) - Passive health checks are used to remove hosts from the upstream cluster which are unreachable or are returning errors..\nInterval (duration: 0s) - The time between checks. Each check will cause hosts which have exceeded max_failures to be removed from the load balancer, and any hosts which have passed their ejection time to be returned to the load balancer.\nMaxFailures (int: 0) - The number of consecutive failures which cause a host to be removed from the load balancer.\nEnforcingConsecutive5xx (int: 100) - The % chance that a host will be actually ejected when an outlier status is detected through consecutive 5xx.\nTransparentProxy (TransparentProxyConfig: <optional>) - Controls configuration specific to proxies in transparent mode. Added in v1.10.0.\nOutboundListenerPort (int: \"15001\") - The port the proxy should listen on for outbound traffic. This must be the port where outbound application traffic is redirected to.\nDialedDirectly (bool: false) - Determines whether this proxy instance's IP address can be dialed directly by transparent proxies. Typically transparent proxies dial upstreams using the \"virtual\" tagged address, which load balances across instances. Dialing individual instances can be helpful in cases like stateful services such as a database cluster with a leader.\nDestination (DestinationConfig: <optional>) - Controls configuration specific to destinations through terminating-gateway. Added in v1.13.0.\nAddresses (array<string>: []) - List of addresses associated with the destination. This can be a hostname or an IP address. Wildcards are not accepted.\nPort (int: 0) - Port number associated with the destination.\nMaxInboundConnections (int: 0) - The maximum number of concurrent inbound connections to each service instance.\nLocalConnectTimeoutMs (int: 0) - The number of milliseconds allowed to make connections to the local application instance before timing out. Defaults to 5000.\nLocalRequestTimeoutMs (int: 0) - In milliseconds, the timeout for HTTP requests to the local application instance. Applies to HTTP-based protocols only. If not specified, inherits the Envoy default for route timeouts (15s).\nMeshGateway (MeshGatewayConfig: <optional>) - Controls the default mesh gateway configuration for this service. Added in v1.6.0.\nExternalSNI (string: \"\") - This is an optional setting that allows for the TLS SNI value to be changed to a non-connect value when federating with an external system. Added in v1.6.0.\nExpose (ExposeConfig: <optional>) - Controls the default expose path configuration for Envoy. Added in v1.6.2.\nExposing paths through Envoy enables a service to protect itself by only listening on localhost, while still allowing non-Connect-enabled applications to contact an HTTP endpoint. Some examples include: exposing a /metrics path for Prometheus or /healthz for kubelet liveness checks.\nChecks (bool: false) - If enabled, all HTTP and gRPC checks registered with the agent are exposed through Envoy. Envoy will expose listeners for these checks and will only accept connections originating from localhost or Consul's advertise address. The port for these listeners are dynamically allocated from expose_min_port to expose_max_port. This flag is useful when a Consul client cannot reach registered services over localhost. One example is when running Consul on Kubernetes, and Consul agents run in their own pods.\nPaths (array<Path>: []) - A list of paths to expose through Envoy.\nPath (string: \"\") - The HTTP path to expose. The path must be prefixed by a slash. ie: /metrics.\nLocalPathPort (int: 0) - The port where the local service is listening for connections to the path.\nListenerPort (int: 0) - The port where the proxy will listen for connections. This port must be available for the listener to be set up. If the port is not free then Envoy will not expose a listener for the path, but the proxy registration will not fail.\nProtocol (string: \"http\") - Sets the protocol of the listener. One of http or http2. For gRPC use http2.\nConfiguration entries may be protected by ACLs.\nReading a service-defaults config entry requires service:read on the resource.\nCreating, updating, or deleting a service-defaults config entry requires service:write on the resource."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.10.x/connect/config-entries/service-defaults",
  "text": "Configuration Entry Kind: Service Defaults | Consul\nv1.8.4+: On Kubernetes, the ServiceDefaults custom resource is supported in Consul versions 1.8.4+.\nv1.5.0+: On other platforms, this config entry is supported in Consul versions 1.5.0+.\nThe service-defaults config entry kind (ServiceDefaults on Kubernetes) controls default global values for a service, such as its protocol.\nDefault protocol\nNOTE: The default protocol can also be configured globally for all proxies using the proxy defaults config entry. However, if the protocol value is specified in a service defaults config entry for a given service, that value will take precedence over the globally configured value from proxy defaults.\nSet the default protocol for a service in the default namespace to HTTP:\nKind = \"service-defaults\" Name = \"web\" Namespace = \"default\" Protocol = \"http\" \nUpstream configuration Beta\nSet default connection limits and mesh gateway mode across all upstreams of \"counting\", and also override the mesh gateway mode used when dialing the \"dashboard\" service.\nKind = \"service-defaults\" Name = \"counting\" UpstreamConfig = { Defaults = { MeshGateway = { Mode = \"local\" } Limits = { MaxConnections = 512 MaxPendingRequests = 512 MaxConcurrentRequests = 512 } } Overrides = [ { Name = \"dashboard\" MeshGateway = { Mode = \"remote\" } } ] } \nKind - Must be set to service-defaults\nName (string: <required>) - Set to the name of the service being configured.\nNamespace (string: \"default\")Enterprise - Specifies the namespace the config entry will apply to.\nMeta (map<string|string>: nil) - Specifies arbitrary KV metadata pairs. Added in Consul 1.8.4.\nProtocol (string: \"tcp\") - Sets the protocol of the service. This is used by Connect proxies for things like observability features and to unlock usage of the service-splitter and service-router config entries for a service. It also unlocks the ability to define L7 intentions via service-intentions. Supported values are one of tcp, http, http2, or grpc.\nMode (string: \"\") - One of direct or transparent. transparent represents that inbound and outbound application traffic is being captured and redirected through the proxy. This mode does not enable the traffic redirection itself. Instead it signals Consul to configure Envoy as if traffic is already being redirected. direct represents that the proxy's listeners must be dialed directly by the local application and other proxies. Added in v1.10.0.\nUpstreamConfig (UpstreamConfiguration: <optional>) - Controls default configuration settings that apply across all upstreams, and per-upstream configuration overrides. Note that per-upstream configuration applies across all federated datacenters to the pairing of source and upstream destination services. Added in v1.10.0.\nOverrides (array<UpstreamConfig>: []) - A list of optional overrides for per-upstream configuration.\nName (string: \"\") - The upstream name to apply the configuration to. This should not be set to the wildcard specifier *.\nNamespace (string: \"\") - The namespace of the upstream. This should not be set to the wildcard specifier *.\nDefaults (UpstreamConfig: <optional>) - Default configuration that applies to all upstreams of this service.\nTransparentProxy (TransparentProxyConfig: <optional>) - Controls configuration specific to proxies in transparent mode. Added in v1.10.0.\nOutboundListenerPort (int: \"15001\") - The port the proxy should listen on for outbound traffic. This must be the port where outbound application traffic is redirected to.\nDialedDirectly (bool: false) - Determines whether this proxy instance's IP address can be dialed directly by transparent proxies. Typically transparent proxies dial upstreams using the \"virtual\" tagged address, which load balances across instances. Dialing individual instances can be helpful in cases like stateful services such as a database cluster with a leader.\nMeshGateway (MeshGatewayConfig: <optional>) - Controls the default mesh gateway configuration for this service. Added in v1.6.0.\nExternalSNI (string: \"\") - This is an optional setting that allows for the TLS SNI value to be changed to a non-connect value when federating with an external system. Added in v1.6.0.\nExpose (ExposeConfig: <optional>) - Controls the default expose path configuration for Envoy. Added in v1.6.2.\nExposing paths through Envoy enables a service to protect itself by only listening on localhost, while still allowing non-Connect-enabled applications to contact an HTTP endpoint. Some examples include: exposing a /metrics path for Prometheus or /healthz for kubelet liveness checks.\nChecks (bool: false) - If enabled, all HTTP and gRPC checks registered with the agent are exposed through Envoy. Envoy will expose listeners for these checks and will only accept connections originating from localhost or Consul's advertise address. The port for these listeners are dynamically allocated from expose_min_port to expose_max_port. This flag is useful when a Consul client cannot reach registered services over localhost. One example is when running Consul on Kubernetes, and Consul agents run in their own pods.\nPaths (array<Path>: []) - A list of paths to expose through Envoy.\nPath (string: \"\") - The HTTP path to expose. The path must be prefixed by a slash. ie: /metrics.\nLocalPathPort (int: 0) - The port where the local service is listening for connections to the path.\nListenerPort (int: 0) - The port where the proxy will listen for connections. This port must be available for the listener to be set up. If the port is not free then Envoy will not expose a listener for the path, but the proxy registration will not fail.\nProtocol (string: \"http\") - Sets the protocol of the listener. One of http or http2. For gRPC use http2.\nConfiguration entries may be protected by ACLs.\nReading a service-defaults config entry requires service:read on the resource.\nCreating, updating, or deleting a service-defaults config entry requires service:write on the resource."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.18.x/connect/config-entries/service-intentions",
  "text": "Service intentions configuration entry reference | Consul\nSpecifies the type of configuration entry to implement. Must be set to service-intentions.\nData type: String value that must be set to service-intentions.\nSpecifies a name of the destination service for all intentions defined in the configuration entry.\nDefault: Defaults to the name of the node after writing the entry to the Consul server.\nYou can also specify a wildcard character (*) to match all services without intentions. Intentions that are applied with a wildcard, however, are not supported when defining L7 Permissions.\nSpecifies the namespace that the configuration entry applies to. Services in the namespace are the traffic destinations that the intentions allow or deny traffic to.\nYou can also specify a wildcard character (*) to match all namespaces. Intentions that are applied with a wildcard, however, are not supported when defining L7 Permissions.\nSpecifies the admin partition to apply the configuration entry. Services in the specified partition are the traffic destinations that the intentions allow or deny traffic to.\nSpecifies key-value pairs to add to the KV store when the configuration entry is evaluated.\nData type: Map of one or more key-value pairs\nJWT\nSpecifies a JSON Web Token provider configured in a JWT provider configuration entry, as well as additional configurations for verifying a service's JWT before authorizing communication between services\nData type: Map that contains JWT{}.Providers\nJWT{}.Providers\nSpecifies the names of one or more previously configured JWT provider configuration entries, which include the information necessary to validate a JSON web token.\nData type: List of maps\nJWT{}.Providers[].Name\nSpecifies the name of a JWT provider defined in the Name field of the jwt-provider configuration entry. You must write the JWT Provider to Consul before referencing it in a service intention.\nJWT{}.Providers[].VerifyClaims\nSpecifies additional token information to verify beyond what is configured in the JWT provider configuration entry. This map takes the form of a JSON web token claim and a value to match for verification.\nData type: List of maps that can contain the following parameters:\nPath\nValue\nJWT{}.Providers[].VerifyClaims[].Path\nSpecifies the path to the claim in the JSON web token. For more information about JWT claims, refer to the IETF standards documentation.\nData type: List of strings\nJWT{}.Providers[].VerifyClaims.Value\nSpecifies the value to match on when verifying the claim designated in JWT{}.Providers[].VerifyClaims[].Path.\nSources[]\nList of configurations that define intention sources and the authorization granted to the sources. You can specify source configurations in any order, but Consul stores and evaluates them in order of reverse precedence at runtime. Refer to Precedence for additional information.\nSamenessGroup Enterprise\nPermissions\nPrecedence\nType\nDescription\nLegacyID\nLegacyMeta\nLegacyCreateTime\nLegacyUpdateTime\nSources[].Name\nSpecifies the name of the source that the intention allows or denies traffic from. If Type is set to consul, then the value refers to the name of a Consul service. The source is not required to be registered into the Consul catalog.\nSources[].Peer\nSpecifies the name of a peered Consul cluster that the intention allows or denies traffic from. Refer to Cluster peering overview for additional information about peers.\nThe Peer and Partition fields are mutually exclusive.\nSources[].Namespace Enterprise\nSpecifies the traffic source namespace that the intention allows or denies traffic from.\nDefault: If Peer is unspecified, defaults to the destination Namespace.\nSources[].Partition Enterprise\nSpecifies the name of an admin partition that the intention allows or denies traffic from. Refer to Admin Partitions for additional information about partitions.\nThe Peer and Partition fields are mutually exclusive.\nDefault: If Peer is unspecified, defaults to the destination Partition.\nSources[].SamenessGroup Enterprise\nSpecifies the name of a sameness group that the intention allows or denies traffic from. Refer to create sameness groups for additional information.\nSources[].Action\nSpecifies the action to take when the source sends traffic to the destination service. The value is either allow or deny. Do not configure this field to apply L7 intentions to the same source. Configure the Permissions field instead.\nThis field is required for L4 intentions.\nData type: String value set to either allow or deny\nL4 Intentions for specific sources and destinations\nL4 intentions for all destinations\nL4 intentions for all sources\nSources[].Permissions[]\nSpecifies a list of permissions for L7 traffic sources. The list contains one or more actions and a set of match criteria for each action.\nConsul applies permissions in the order specified in the configuration. Beginning at the top of the list, Consul applies the first matching request and stops evaluating against the remaining configurations.\nFor requests that do not match any of the defined permissions, Consul applies the intention behavior defined in the acl_default_policy configuration.\nDo not configure this field for L4 intentions. Use the Sources.Action parameter instead.\nThe Permissions only applies to services with a compatible protocol. Permissions are not supported when the Name or Namespace field is configured with a wildcard because service instances or services in a namespace may use different protocols.\nHTTP\nRest access\ngRPC\nCluster peering\nSources[].Permissions[].Action\nSpecifies the action to take when the source sends traffic to the destination service. The value is either allow or deny.\nData type: String value set to either allow or deny.\nSources[].Permissions[].HTTP\nSpecifies a set of HTTP-specific match criteria. Consul applies the action defined in the Action field to source traffic that matches the criteria.\nThe following table describes the parameters that the HTTP map may contain:\nPathExact\tSpecifies an exact path to match on the HTTP request path. Do not specify PathExact if PathPrefix or PathRegex are configured in the same HTTP configuration.\tstring\tnone\t\nPathPrefix\tSpecifies a path prefix to match on the HTTP request path. Do not specify PathPrefix if PathExact or PathRegex are configured in the same HTTP configuration.\tstring\tnone\t\nPathRegex\tDefines a regular expression to match on the HTTP request path. Do not specify PathRegex if PathExact or PathPrefix are configured in the same HTTP configuration. The regex syntax is proxy-specific. If using Envoy, refer to the re2 documentation for details.\tstring\tnone\t\nMethods\tSpecifies a list of HTTP methods. Consul applies the permission if a request matches the PathExact, PathPrefix, PathRegex, or Header, and the source sent the request using one of the specified methods. Refer to the Mozilla documentation for a list of supported request headers.\tlist\tAll request methods\t\nHeader\tSpecifies a header name and matching criteria for HTTP request headers. Refer to Sources[].Permissions[].HTTP[].Header for details.\tlist of maps\tnone\t\nSources[].Permissions[].HTTP[].Header[]\nSpecifies a header name and matching criteria for HTTP request headers. The request header must match all specified criteria for the permission to apply.\nData type: list of objects\nEach member of the Header list is a map that contains a Name field and at least one match criterion. The following table describes the parameters that each member of the Header list may contain:\nParameterDescriptionData typeRequired\nName\tSpecifies the name of the header to match.\tstring\trequired\t\nPresent\tEnables a match if the header configured in the Name field appears in the request. Consul matches on any value as long as the header key appears in the request. Do not specify Present if Exact, Prefix, Suffix, or Regex are configured in the same Header configuration.\tboolean\toptional\t\nExact\tSpecifies a value for the header key set in the Name field. If the request header value matches the Exact value, Consul applies the permission. Do not specify Exact if Present, Prefix, Suffix, or Regex are configured in the same Header configuration.\tstring\toptional\t\nPrefix\tSpecifies a prefix value for the header key set in the Name field. If the request header value starts with the Prefix value, Consul applies the permission. Do not specify Prefix if Present, Exact, Suffix, or Regex are configured in the same Header configuration.\tstring\toptional\t\nSuffix\tSpecifies a suffix value for the header key set in the Name field. If the request header value ends with the Suffix value, Consul applies the permission. Do not specify Suffix if Present, Exact, Prefix, or Regex are configured in the same Header configuration.\tstring\toptional\t\nRegex\tSpecifies a regular expression pattern as the value for the header key set in the Name field. If the request header value matches the regex, Consul applies the permission. Do not specify Regex if Present, Exact, Prefix, or Suffix are configured in the same Header configuration. The regex syntax is proxy-specific. If using Envoy, refer to the re2 documentation for details.\tstring\toptional\t\nInvert\tInverts the matching logic configured in the Header. Default is false.\tboolean\toptional\t\nSources[].Precedence\nThe Precedence field contains a read-only integer. Consul generates the value based on name configurations for the source and destination services. Refer to Precedence and matching order for additional information.\nSources[].Type\nSpecifies the type of destination service that the configuration entry applies to. The only value supported is consul.\nDefault: consul\nSources[].Description\nSpecifies a description of the intention. Consul presents the description in API responses to assist other tools integrated into the network.\nSources[].LegacyID\nRead-only unique user ID (UUID) for the intention in the system. Consul generates the value and exposes it in the configuration entry so that legacy API endpoints continue to function. Refer to Read Specific Intention by ID for additional information.\nSources[].LegacyMeta\nRead-only set of arbitrary key-value pairs to attach to the intention. Consul generates the metadata and exposes it in the configuration entry so that legacy intention API endpoints continue to function. Refer to Read Specific Intention by ID for additional information.\nSources[].CreateTime\nRead-only timestamp for the intention creation. Consul exposes the timestamp in the configuration entry to allow legacy intention API endpoints to continue functioning. Refer to Read Specific Intention by ID for additional information.\nSources[].LegacyUpdateTime\nRead-only timestamp marking the most recent intention update. Consul exposes the timestamp in the configuration entry to allow legacy intention API endpoints to continue functioning. Refer to Read Specific Intention by ID for additional information."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/enterprise/namespace",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.10.x/connect/config-entries/mesh",
  "text": "Configuration Entry Kind: Mesh | Consul\nv1.10.0+: This config entry is supported in Consul versions 1.10.0+.\nThe mesh config entry kind allows for globally defining default configuration that applies to all service mesh proxies. Settings in this config entry apply across all namespaces and federated datacenters.\nMesh Destinations Only\nOnly allow transparent proxies to dial addresses in the mesh.\nKind = \"mesh\" TransparentProxy { MeshDestinationsOnly = true } \nKind - Must be set to mesh\nNamespace (string: \"default\")Enterprise - Must be set to default. Config will apply to all namespaces.\nTransparentProxy (TransparentProxyConfig: <optional>) - Controls configuration specific to proxies in transparent mode. Added in v1.10.0.\nMeshDestinationsOnly (bool: false) - Determines whether sidecar proxies operating in transparent mode can proxy traffic to IP addresses not registered in Consul's mesh. If enabled, traffic will only be proxied to upstream proxies or Connect-native services. If disabled, requests will be proxied as-is to the original destination IP address. Consul will not encrypt the connection.\nReading a mesh config entry requires no specific privileges.\nCreating, updating, or deleting a mesh config entry requires operator:write."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.17.x/connect/config-entries/sameness-group",
  "text": "Sameness group configuration entry reference | Consul\nThis page provides reference information for sameness group configuration entries. Sameness groups associate identical admin partitions to facilitate traffic between identical services. When partitions are part of the same Consul datacenter, you can create a sameness group by listing them in the Members[].Partition field. When partitions are located on remote clusters, you must establish cluster peering connections between remote partitions in order to add them to a sameness group in the Members[].Peer field.\nTo learn more about creating a sameness group, refer to Create sameness groups or Create sameness groups on Kubernetes.\nThe following list outlines field hierarchy, language-specific data types, and requirements in the sameness group configuration entry. Click on a property name to view additional details, including default values.\nKind: string | required | must be set to sameness-group\nName: string | required\nPartition: string | default\nDefaultForFailover: boolean | false\nIncludeLocal: boolean | false\nMembers: list of maps | required\nPartition: string\nPeer: string\nWhen every field is defined, a sameness group configuration entry has the following form:\nKind = \"sameness-group\" # required Name = \"<group-name>\" # required Partition = \"<partition-configuration-applies-to>\" DefaultForFailover = false IncludeLocal = true Members = [ # required { Partition = \"<partition-with-services-in-group>\" }, { Peer = \"<cluster-peer-with-services-in-group>\" } ] \nThis section provides details about the fields you can configure in the sameness group configuration entry.\nSpecifies the type of configuration entry to implement. Must be set to sameness-group.\nData type: String value that must be set to sameness-group.\nSpecifies a name for the configuration entry that is used to identify the sameness group. To ensure consistency, use descriptive names and make sure that the same name is used when creating configuration entries to add each member to the sameness group.\nSpecifies the local admin partition that the sameness group applies to. Refer to admin partitions for more information.\nDefaultForFailover\nDetermines whether the sameness group should be used to establish connections to services with the same name during failover scenarios.\nWhen this field is set to true, upstream requests automatically fail over to services in the sameness group according to the order of the members in the Members list. It impacts all services on the partition.\nWhen this field is set to false, you can use a sameness group for failover by configuring the Failover block of a service resolver configuration entry.\nWhen you query Consul DNS using sameness groups, DefaultForFailover must be set to true. Otherwise, Consul DNS returns an error.\nIncludeLocal\nDetermines whether the local partition should be considered the first member of the sameness group. When this field is set to true, DNS queries, upstream requests, and failover traffic returns a health instance from the local partition unless one does not exist.\nIf you enable this parameter, you do not need to list the local partition as the first member in the group.\nMembers\nSpecifies the partitions and cluster peers that are members of the sameness group from the perspective of the local partition.\nThe local partition should be the first member listed unless IncludeLocal=true. The order of the members determines their precedence during failover scenarios. If a member is listed but Consul cannot connect to it, failover proceeds with the next healthy member in the list. For an example demonstrating how to configure this parameter, refer to Failover between sameness groups.\nEach partition can belong to a single sameness group. You cannot associate a partition or cluster peer with multiple sameness groups.\nData type: List that can contain maps of the following parameters:\nMembers[].Partition\nSpecifies a partition in the local datacenter that is a member of the sameness group. Local partitions do not require cluster peering connections before they are added to a sameness group.\nMembers[].Peer\nSpecifies the name of a cluster peer that is a member of the sameness group.\nCluster peering connections must be established before adding a remote partition to the list of members. Refer to establish cluster peering connections for more information.\nThe following examples demonstrate common sameness group configuration patterns for specific use cases.\nFailover between members of a sameness group\nIn the following example, the configuration entry defines a sameness group named products-api that applies to the store-east partition in the local datacenter. The sameness group is configured so that when a service instance in store-east fails, Consul attempts to establish a failover connection in the following order:\nServices with the same name in the store-east partition\nServices with the same name in the inventory-east partition in the same datacenter\nServices with the same name in the store-west partition of datacenter dc2, which has an established cluster peering connection.\nServices with the same name in the inventory-west partition of dc2, which has an established cluster peering connection.\nKind = \"sameness-group\" Name = \"products-api\" Partition = \"store-east\" Members = [ { Partition = \"store-east\" }, { Partition = \"inventory-east\" }, { Peer = \"dc2-store-west\" }, { Peer = \"dc2-inventory-west\" } ]"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.11.x/connect/config-entries/mesh",
  "text": "Mesh - Configuration Entry Reference | Consul\nv1.10.0+: This configuration entry is supported in Consul versions 1.10.0+.\nThe mesh configuration entry allows you to define a global default configuration that applies to all service mesh proxies. Settings in this config entry apply across all namespaces and federated datacenters.\nMesh Destinations Only\nOnly allow transparent proxies to dial addresses in the mesh.\nKind = \"mesh\" TransparentProxy { MeshDestinationsOnly = true } \nNote that the Kubernetes example does not include a partition field. Configuration entries are applied on Kubernetes using custom resource definitions (CRD), which can only be scoped to their own partition.\nKind - Must be set to mesh\nNamespace (string: \"default\")Enterprise - Must be set to default. The configuration will apply to all namespaces.\nPartition (string: \"default\")Enterprise - Specifies the name of the admin partition in which the configuration entry applies. Refer to the Admin Partitions documentation for additional information.\nMeshDestinationsOnly (bool: false) - Determines whether sidecar proxies operating in transparent mode can proxy traffic to IP addresses not registered in Consul's mesh. If enabled, traffic will only be proxied to upstream proxies or Connect-native services. If disabled, requests will be proxied as-is to the original destination IP address. Consul will not encrypt the connection.\nReading a mesh config entry requires no specific privileges.\nCreating, updating, or deleting a mesh config entry requires operator:write."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.17.x/connect/config-entries/service-intentions",
  "text": "Service intentions configuration entry reference | Consul\nSpecifies the type of configuration entry to implement. Must be set to service-intentions.\nData type: String value that must be set to service-intentions.\nSpecifies a name of the destination service for all intentions defined in the configuration entry.\nDefault: Defaults to the name of the node after writing the entry to the Consul server.\nYou can also specify a wildcard character (*) to match all services without intentions. Intentions that are applied with a wildcard, however, are not supported when defining L7 Permissions.\nSpecifies the namespace that the configuration entry applies to. Services in the namespace are the traffic destinations that the intentions allow or deny traffic to.\nYou can also specify a wildcard character (*) to match all namespaces. Intentions that are applied with a wildcard, however, are not supported when defining L7 Permissions.\nSpecifies the admin partition to apply the configuration entry. Services in the specified partition are the traffic destinations that the intentions allow or deny traffic to.\nSpecifies key-value pairs to add to the KV store when the configuration entry is evaluated.\nData type: Map of one or more key-value pairs\nJWT\nSpecifies a JSON Web Token provider configured in a JWT provider configuration entry, as well as additional configurations for verifying a service's JWT before authorizing communication between services\nData type: Map that contains JWT{}.Providers\nJWT{}.Providers\nSpecifies the names of one or more previously configured JWT provider configuration entries, which include the information necessary to validate a JSON web token.\nData type: List of maps\nJWT{}.Providers[].Name\nSpecifies the name of a JWT provider defined in the Name field of the jwt-provider configuration entry. You must write the JWT Provider to Consul before referencing it in a service intention.\nJWT{}.Providers[].VerifyClaims\nSpecifies additional token information to verify beyond what is configured in the JWT provider configuration entry. This map takes the form of a JSON web token claim and a value to match for verification.\nData type: List of maps that can contain the following parameters:\nValue\nJWT{}.Providers[].VerifyClaims[].Path\nSpecifies the path to the claim in the JSON web token. For more information about JWT claims, refer to the IETF standards documentation.\nData type: List of strings\nJWT{}.Providers[].VerifyClaims.Value\nSpecifies the value to match on when verifying the the claim designated in JWT{}.Providers[].VerifyClaims[].Path.\nSources[]\nList of configurations that define intention sources and the authorization granted to the sources. You can specify source configurations in any order, but Consul stores and evaluates them in order of reverse precedence at runtime. Refer to Precedence for additional information.\nSamenessGroup Enterprise\nPermissions\nPrecedence\nType\nDescription\nLegacyID\nLegacyMeta\nLegacyCreateTime\nLegacyUpdateTime\nSources[].Name\nSpecifies the name of the source that the intention allows or denies traffic from. If Type is set to consul, then the value refers to the name of a Consul service. The source is not required to be registered into the Consul catalog.\nSources[].Peer\nSpecifies the name of a peered Consul cluster that the intention allows or denies traffic from. Refer to Cluster peering overview for additional information about peers.\nSources[].Namespace Enterprise\nSpecifies the traffic source namespace that the intention allows or denies traffic from.\nDefault: If Peer is unspecified, defaults to the destination Namespace.\nSources[].Partition Enterprise\nSpecifies the name of an admin partition that the intention allows or denies traffic from. Refer to Admin Partitions for additional information about partitions.\nDefault: If Peer is unspecified, defaults to the destination Partition.\nSources[].SamenessGroup Enterprise\nSpecifies the name of a sameness group that the intention allows or denies traffic from. Refer to create sameness groups for additional information.\nSources[].Action\nSpecifies the action to take when the source sends traffic to the destination service. The value is either allow or deny. Do not configure this field to apply L7 intentions to the same source. Configure the Permissions field instead.\nThis field is required for L4 intentions.\nData type: String value set to either allow or deny\nL4 Intentions for specific sources and destinations\nL4 intentions for all destinations\nL4 intentions for all sources\nSources[].Permissions[]\nSpecifies a list of permissions for L7 traffic sources. The list contains one or more actions and a set of match criteria for each action.\nConsul applies permissions in the order specified in the configuration. Beginning at the top of the list, Consul applies the first matching request and stops evaluating against the remaining configurations.\nFor requests that do not match any of the defined permissions, Consul applies the intention behavior defined in the acl_default_policy configuration.\nDo not configure this field for L4 intentions. Use the Sources.Action parameter instead.\nThe Permissions only applies to services with a compatible protocol. Permissions are not supported when the Name or Namespace field is configured with a wildcard because service instances or services in a namespace may use different protocols.\nHTTP\nRest access\ngRPC\nCluster peering\nSources[].Permissions[].Action\nSpecifies the action to take when the source sends traffic to the destination service. The value is either allow or deny.\nData type: String value set to either allow or deny.\nSources[].Permissions[].HTTP\nSpecifies a set of HTTP-specific match criteria. Consul applies the action defined in the Action field to source traffic that matches the criteria.\nThe following table describes the parameters that the HTTP map may contain:\nPathExact\tSpecifies an exact path to match on the HTTP request path. Do not specify PathExact if PathPrefix or PathRegex are configured in the same HTTP configuration.\tstring\tnone\t\nPathPrefix\tSpecifies a path prefix to match on the HTTP request path. Do not specify PathPrefix if PathExact or PathRegex are configured in the same HTTP configuration.\tstring\tnone\t\nPathRegex\tDefines a regular expression to match on the HTTP request path. Do not specify PathRegex if PathExact or PathPrefix are configured in the same HTTP configuration. The regex syntax is proxy-specific. If using Envoy, refer to the re2 documentation for details.\tstring\tnone\t\nMethods\tSpecifies a list of HTTP methods. Consul applies the permission if a request matches the PathExact, PathPrefix, PathRegex, or Header, and the source sent the request using one of the specified methods. Refer to the Mozilla documentation for a list of supported request headers.\tlist\tAll request methods\t\nHeader\tSpecifies a header name and matching criteria for HTTP request headers. Refer to Sources[].Permissions[].HTTP[].Header for details.\tlist of maps\tnone\t\nSources[].Permissions[].HTTP[].Header[]\nSpecifies a header name and matching criteria for HTTP request headers. The request header must match all specified criteria for the permission to apply.\nData type: list of objects\nEach member of the Header list is a map that contains a Name field and at least one match criterion. The following table describes the parameters that each member of the Header list may contain:\nParameterDescriptionData typeRequired\nName\tSpecifies the name of the header to match.\tstring\trequired\t\nPresent\tEnables a match if the header configured in the Name field appears in the request. Consul matches on any value as long as the header key appears in the request. Do not specify Present if Exact, Prefix, Suffix, or Regex are configured in the same Header configuration.\tboolean\toptional\t\nExact\tSpecifies a value for the header key set in the Name field. If the request header value matches the Exact value, Consul applies the permission. Do not specify Exact if Present, Prefix, Suffix, or Regex are configured in the same Header configuration.\tstring\toptional\t\nPrefix\tSpecifies a prefix value for the header key set in the Name field. If the request header value starts with the Prefix value, Consul applies the permission. Do not specify Prefix if Present, Exact, Suffix, or Regex are configured in the same Header configuration.\tstring\toptional\t\nSuffix\tSpecifies a suffix value for the header key set in the Name field. If the request header value ends with the Suffix value, Consul applies the permission. Do not specify Suffix if Present, Exact, Prefix, or Regex are configured in the same Header configuration.\tstring\toptional\t\nRegex\tSpecifies a regular expression pattern as the value for the header key set in the Name field. If the request header value matches the regex, Consul applies the permission. Do not specify Regex if Present, Exact, Prefix, or Suffix are configured in the same Header configuration. The regex syntax is proxy-specific. If using Envoy, refer to the re2 documentation for details.\tstring\toptional\t\nInvert\tInverts the matching logic configured in the Header. Default is false.\tboolean\toptional\t\nSources[].Precedence\nThe Precedence field contains a read-only integer. Consul generates the value based on name configurations for the source and destination services. Refer to Precedence and matching order for additional information.\nSources[].Type\nSpecifies the type of destination service that the configuration entry applies to. The only value supported is consul.\nDefault: consul\nSources[].Description\nSpecifies a description of the intention. Consul presents the description in API responses to assist other tools integrated into the network.\nSources[].LegacyID\nRead-only unique user ID (UUID) for the intention in the system. Consul generates the value and exposes it in the configuration entry so that legacy API endpoints continue to function. Refer to Read Specific Intention by ID for additional information.\nSources[].LegacyMeta\nRead-only set of arbitrary key-value pairs to attach to the intention. Consul generates the metadata and exposes it in the configuration entry so that legacy intention API endpoints continue to function. Refer to Read Specific Intention by ID for additional information.\nSources[].CreateTime\nRead-only timestamp for the intention creation. Consul exposes the timestamp in the configuration entry to allow legacy intention API endpoints to continue functioning. Refer to Read Specific Intention by ID for additional information.\nSources[].LegacyUpdateTime\nRead-only timestamp marking the most recent intention update. Consul exposes the timestamp in the configuration entry to allow legacy intention API endpoints to continue functioning. Refer to Read Specific Intention by ID for additional information."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.12.x/connect/config-entries/service-defaults",
  "text": "Service Defaults - Configuration Entry Reference | Consul\nNOTE: The default protocol can also be configured globally for all proxies using the proxy defaults config entry. However, if the protocol value is specified in a service defaults config entry for a given service, that value will take precedence over the globally configured value from proxy defaults.\nUpstream configuration\nSet default connection limits and mesh gateway mode across all upstreams of \"counting\", and also override the mesh gateway mode used when dialing the \"dashboard\" service.\nKind = \"service-defaults\" Name = \"counting\" UpstreamConfig = { Defaults = { MeshGateway = { Mode = \"local\" } Limits = { MaxConnections = 512 MaxPendingRequests = 512 MaxConcurrentRequests = 512 } } Overrides = [ { Name = \"dashboard\" MeshGateway = { Mode = \"remote\" } } ] } \nPartition (string: \"default\")Enterprise - Specifies the name of the admin partition in which the configuration entry applies. Refer to the Admin Partitions documentation for additional information.\nMode (string: \"\") - One of direct or transparent. transparent represents that inbound and outbound application traffic is being captured and redirected through the proxy. This mode does not enable the traffic redirection itself. Instead it signals Consul to configure Envoy as if traffic is already being redirected. direct represents that the proxy's listeners must be dialed directly by the local application and other proxies. Added in v1.10.0.\nUpstreamConfig (UpstreamConfiguration: <optional>) - Controls default configuration settings that apply across all upstreams, and per-upstream configuration overrides. Note that per-upstream configuration applies across all federated datacenters to the pairing of source and upstream destination services. Added in v1.10.0.\nOverrides (array<UpstreamConfig>: []) - A list of optional overrides for per-upstream configuration.\nName (string: \"\") - The upstream name to apply the configuration to. This should not be set to the wildcard specifier *.\nNamespace (string: \"\") - The namespace of the upstream. This should not be set to the wildcard specifier *.\nEnforcingConsecutive5xx (int: 100) - The % chance that a host will be actually ejected when an outlier status is detected through consecutive 5xx.\nDefaults (UpstreamConfig: <optional>) - Default configuration that applies to all upstreams of this service.\nEnforcingConsecutive5xx (int: 100) - The % chance that a host will be actually ejected when an outlier status is detected through consecutive 5xx.\nOutboundListenerPort (int: \"15001\") - The port the proxy should listen on for outbound traffic. This must be the port where outbound application traffic is redirected to.\nDialedDirectly (bool: false) - Determines whether this proxy instance's IP address can be dialed directly by transparent proxies. Typically transparent proxies dial upstreams using the \"virtual\" tagged address, which load balances across instances. Dialing individual instances can be helpful in cases like stateful services such as a database cluster with a leader."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.15.x/connect/config-entries/mesh",
  "text": "Mesh - Configuration Entry Reference | Consul\nThe mesh configuration entry allows you to define a global default configuration that applies to all service mesh proxies. Settings in this config entry apply across all namespaces and federated datacenters.\nMesh-wide TLS Min Version\nEnforce that service mesh mTLS traffic uses TLS v1.2 or newer.\nKind = \"mesh\" TLS { Incoming { TLSMinVersion = \"TLSv1_2\" } } \nNote that the Kubernetes example does not include a partition field. Configuration entries are applied on Kubernetes using custom resource definitions (CRD), which can only be scoped to their own partition.\nMesh Destinations Only\nOnly allow transparent proxies to dial addresses in the mesh.\nKind = \"mesh\" TransparentProxy { MeshDestinationsOnly = true } \nNote that the Kubernetes example does not include a partition field. Configuration entries are applied on Kubernetes using custom resource definitions (CRD), which can only be scoped to their own partition.\nPeer Through Mesh Gateways\nSet the PeerThroughMeshGateways parameter to true to route peering control plane traffic through mesh gateways.\nKind = \"mesh\" Peering { PeerThroughMeshGateways = true } \nNote that the Kubernetes example does not include a partition field. Configuration entries are applied on Kubernetes using custom resource definitions (CRD), which can only be scoped to their own partition.\nKind - Must be set to mesh\nNamespace (string: \"default\")Enterprise - Must be set to default. The configuration will apply to all namespaces.\nMeshDestinationsOnly (bool: false) - Determines whether sidecar proxies operating in transparent mode can proxy traffic to IP addresses not registered in Consul's mesh. If enabled, traffic will only be proxied to upstream proxies or mesh-native services. If disabled, requests will be proxied as-is to the original destination IP address. Consul will not encrypt the connection.\nTLS (TLSConfig: <optional>) - TLS configuration for the service mesh.\nIncoming (TLSDirectionConfig: <optional>) - TLS configuration for inbound mTLS connections targeting the public listener on connect-proxy and terminating-gateway proxy kinds.\nTLSMinVersion (string: \"\") - Set the default minimum TLS version supported. One of TLS_AUTO, TLSv1_0, TLSv1_1, TLSv1_2, or TLSv1_3. If unspecified, Envoy v1.22.0 and newer will default to TLS 1.2 as a min version, while older releases of Envoy default to TLS 1.0.\nTLSMaxVersion (string: \"\") - Set the default maximum TLS version supported. Must be greater than or equal to TLSMinVersion. One of TLS_AUTO, TLSv1_0, TLSv1_1, TLSv1_2, or TLSv1_3. If unspecified, Envoy will default to TLS 1.3 as a max version for incoming connections.\nCipherSuites (array<string>: <optional>) - Set the default list of TLS cipher suites to support when negotiating connections using TLS 1.2 or earlier. If unspecified, Envoy will use a default server cipher list. The list of supported cipher suites can seen in consul/types/tls.go and is dependent on underlying support in Envoy. Future releases of Envoy may remove currently-supported but insecure cipher suites, and future releases of Consul may add new supported cipher suites if any are added to Envoy.\nOutgoing (TLSDirectionConfig: <optional>) - TLS configuration for outbound mTLS connections dialing upstreams from connect-proxy and ingress-gateway proxy kinds.\nTLSMinVersion (string: \"\") - Set the default minimum TLS version supported. One of TLS_AUTO, TLSv1_0, TLSv1_1, TLSv1_2, or TLSv1_3. If unspecified, Envoy v1.22.0 and newer will default to TLS 1.2 as a min version, while older releases of Envoy default to TLS 1.0.\nTLSMaxVersion (string: \"\") - Set the default maximum TLS version supported. Must be greater than or equal to TLSMinVersion. One of TLS_AUTO, TLSv1_0, TLSv1_1, TLSv1_2, or TLSv1_3. If unspecified, Envoy will default to TLS 1.2 as a max version for outgoing connections, but future Envoy releases may change this to TLS 1.3.\nCipherSuites (array<string>: <optional>) - Set the default list of TLS cipher suites to support when negotiating connections using TLS 1.2 or earlier. If unspecified, Envoy will use a default server cipher list. The list of supported cipher suites can seen in consul/types/tls.go and is dependent on underlying support in Envoy. Future releases of Envoy may remove currently-supported but insecure cipher suites, and future releases of Consul may add new supported cipher suites if any are added to Envoy.\nHTTP (HTTPConfig: <optional>) - HTTP configuration for the service mesh.\nSanitizeXForwardedClientCert (bool: <optional>) - If configured to true, the forward_client_cert_details option will be set to SANITIZE for all Envoy proxies. As a result, Consul will not include the x-forwarded-client-cert header in the next hop. If set to false (default), the XFCC header is propagated to upstream applications.\nPeering (PeeringMeshConfig: <optional>) - Controls configuration specific to peering connections.\nPeerThroughMeshGateways (bool: <optional>) - Determines if peering control-plane traffic should be routed through mesh gateways. When enabled, dialing cluster attempt to contact peers through their mesh gateway. Clusters that accept calls advertise the address of their mesh gateways, rather than the address of their Consul servers.\nReading a mesh config entry requires no specific privileges.\nCreating, updating, or deleting a mesh config entry requires operator:write."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vsphere/latest/components/builder/vsphere-clone",
  "text": "VMware vSphere Builder | Integrations | Packer\nType: vsphere-clone\nArtifact BuilderId: jetbrains.vsphere\nThis builder clones an existing template, modifies the virtual machine image, and saves the result as a new template using the vSphere API.\nNote: This builder is developed to maintain compatibility with VMware vSphere versions until their respective End of General Support dates. For detailed information, refer to the Broadcom Product Lifecycle.\nExamples\nExamples are available in the examples directory of the GitHub repository.\nConfiguration Reference\nThere are many configuration options available for this builder. In addition to the items listed here, you will want to review the general configuration references for Hardware, Output, Boot, Run, Shutdown, Communicator, and Export configuration references, which are necessary for a build to succeed and can be found further down the page.\nOptional:\ncreate_snapshot (bool) - Create a snapshot of the virtual machine to use as a base for linked clones. Defaults to false.\nsnapshot_name (string) - The name of the snapshot when create_snapshot is true. Defaults to Created By Packer.\nconvert_to_template (bool) - Convert the cloned virtual machine to a template after the build is complete. Defaults to false. If set to true, the virtual machine can not be imported to a content library.\nexport (*common.ExportConfig) - The configuration for exporting the virtual machine to an OVF. The virtual machine is not exported if export configuration is not specified.\ncontent_library_destination (*common.ContentLibraryDestinationConfig) - The configuration for importing a VM template or OVF template to a content library. The template will not be imported if no content library import configuration is specified. If set, convert_to_template must be set to false.\ncustomize (*CustomizeConfig) - The customization options for the virtual machine. Refer to the customization options section for more information.\nClone Configuration\nOptional:\ntemplate (string) - The name of the source virtual machine to clone.\ndisk_size (int64) - The size of the primary disk in MiB. Cannot be used with linked_clone. -> Note: Only the primary disk size can be specified. Additional disks are not supported.\nlinked_clone (bool) - Create the virtual machine as a linked clone from the latest snapshot. Defaults to false. Cannot be used with disk_size.`\nnetwork (string) - The network to which the virtual machine will connect.\nName: <NetworkName>\nInventory Path: /<DatacenterName>/<FolderName>/<NetworkName>\nManaged Object ID (Port Group): Network:network-<xxxxx>\nManaged Object ID (Distributed Port Group): DistributedVirtualPortgroup::dvportgroup-<xxxxx>\nLogical Switch UUID: <uuid>\nSegment ID: /infra/segments/<SegmentID>\nNote: If more than one network resolves to the same name, either the inventory path to network or an ID must be provided.\nNote: If no network is specified, provide host to allow the plugin to search for an available network.\nmac_address (string) - The network card MAC address. For example 00:50:56:00:00:00. If set, the network must be also specified.\nnotes (string) - The annotations for the virtual machine.\ndestroy (bool) - Destroy the virtual machine after the build is complete. Defaults to false.\nvapp (vAppConfig) - The vApp Options for the virtual machine. For more information, refer to the vApp Options Configuration section.\ndisk_controller_type ([]string) - The disk controller type. One of lsilogic, lsilogic-sas, pvscsi, nvme, scsi, or sata. Defaults to lsilogic. Use a list to define additional controllers. Refer to SCSI, SATA, and NVMe Storage Controller Conditions, Limitations, and Compatibility for additional information.\nstorage ([]DiskConfig) - A collection of one or more disks to be provisioned. Refer to the Storage Configuration section for additional information.\nStorage Configuration\nWhen cloning a virtual machine, the storage configuration can be used to add additional storage and disk controllers. The resulting virtual machine will contain the origin virtual machine storage and disk controller plus the new configured ones.\nThe following example that will create a 15GB and a 20GB disk on the virtual machine. The second disk will be thin provisioned:\nHCL Example:\nstorage { disk_size = 15000 } storage { disk_size = 20000 disk_thin_provisioned = true } \nJSON Example:\n\"storage\": [ { \"disk_size\": 15000 }, { \"disk_size\": 20000, \"disk_thin_provisioned\": true } ], \nThe following example will use two PVSCSI controllers and two disks on each controller.\nHCL Example:\ndisk_controller_type = [\"pvscsi\", \"pvscsi\"] storage { disk_size = 15000, disk_controller_index = 0 } storage { disk_size = 15000 disk_controller_index = 0 } storage { disk_size = 15000 disk_controller_index = 1 } storage { disk_size = 15000 disk_controller_index = 1 } \nJSON Example:\n\"disk_controller_type\": [\"pvscsi\", \"pvscsi\"], \"storage\": [ { \"disk_size\": 15000, \"disk_controller_index\": 0 }, { \"disk_size\": 15000, \"disk_controller_index\": 0 }, { \"disk_size\": 15000, \"disk_controller_index\": 1 }, { \"disk_size\": 15000, \"disk_controller_index\": 1 } ], \nRequired:\ndisk_size (int64) - The size of the disk in MiB.\nOptional:\ndisk_thin_provisioned (bool) - Enable thin provisioning for the disk. Defaults to false.\ndisk_eagerly_scrub (bool) - Enable eager scrubbing for the disk. Defaults to false.\ndisk_controller_index (int) - The assigned disk controller for the disk. Defaults to the first controller, (0).\nvApp Options Configuration\nOptional:\nproperties (map[string]string) - The values for the available vApp properties. These are used to supply configuration parameters to a virtual machine. This machine is cloned from a template that originated from an imported OVF or OVA file.\nNote: The only supported usage path for vApp properties is for existing user-configurable keys. These generally come from an existing template that was created from an imported OVF or OVA file.\nYou cannot set values for vApp properties on virtual machines created from scratch, on virtual machines that lack a vApp configuration, or on property keys that do not exist.\nHCL Example:\nvapp { properties = { hostname = var.hostname user-data = base64encode(var.user_data) } } \nJSON Example:\n\"vapp\": { \"properties\": { \"hostname\": \"{{ user `hostname`}}\", \"user-data\": \"{{ env `USERDATA`}}\" } } \nA user-data field requires the content of a YAML file to be encoded with base64. This can be done using an environment variable:\nexport USERDATA=$(gzip -c9 <userdata.yaml | { base64 -w0 2>/dev/null || base64; }) \nExtra Configuration Parameters\nconfiguration_parameters (map[string]string) - configuration_parameters is a direct passthrough to the vSphere API's VirtualMachineConfigSpec\ntools_sync_time (bool) - Enable time synchronization with the ESXi host where the virtual machine is running. Defaults to false.\ntools_upgrade_policy (bool) - Automatically check for and upgrade VMware Tools after a virtual machine power cycle. Defaults to false.\nCustomization\nA cloned virtual machine can be customized to configure host, network, or licensing settings.\nTo perform virtual machine customization as a part of the clone process, specify the customize block with the respective customization options. Windows guests are customized using Sysprep, which will result in the machine SID being reset. Before using customization, check that your source virtual machine meets the requirements for guest OS customization on vSphere. Refer to the customization example for a usage synopsis.\nThe settings for guest customization include:\nlinux_options (*LinuxOptions) - Settings for the guest customization of Linux operating systems. Refer to the Linux options section for additional details.\nwindows_options (*WindowsOptions) - Settings for the guest customization of Windows operating systems. Refer to the Windows options section for additional details.\nwindows_sysprep_file (string) - Provide a sysprep.xml file to allow control of the customization process independent of vSphere. This option is deprecated, please use windows_sysprep_text.\nwindows_sysprep_text (string) - Provide the text for the sysprep.xml content to allow control of the customization process independent of vSphere.\nHCL Examples:\ncustomize { windows_sysprep_text = file(\"<path-to-sysprep.xml>\") } \ncustomize { windows_sysprep_text = templatefile(\"<path-to-sysprep.xml>\", { var1=\"example\" var2=\"example-2\" }) } \nJSON Examples\n{ \"customize\": { \"windows_sysprep_text\": \"<path-to-sysprep.xml>\" } } \n{ \"customize\": { \"windows_sysprep_text\": \"<path-to-sysprep.xml>\", \"var1\": \"example\", \"var2\": \"example-2\" } } \nnetwork_interface (NetworkInterfaces) - Set up network interfaces individually to correspond with the network adapters on the virtual machine. To use DHCP, specify an empty network_interface for each configured adapter. This field is mandatory. Refer to the network interface section for additional details.\nNetwork Interface Settings\ndns_server_list ([]string) - The DNS servers for a specific network interface on a Windows guest operating system. Ignored on Linux. Refer to the global DNS settings section for additional details.\ndns_domain (string) - The DNS search domain for a specific network interface on a Windows guest operating system. Ignored on Linux. Refer to the global DNS settings section for additional details.\nipv4_address (string) - The IPv4 address assigned to the network adapter. If left blank or not included, DHCP is used.\nipv4_netmask (int) - The IPv4 subnet mask, in bits, for the network adapter. For example, 24 for a /24 subnet.\nipv6_address (string) - The IPv6 address assigned to the network adapter. If left blank or not included, auto-configuration is used.\nipv6_netmask (int) - The IPv6 subnet mask, in bits, for the network adapter. For example, 64 for a /64 subnet.\nremove_network_adapter (bool) - Remove all network adapters from template. Defaults to false.\nGlobal Routing Settings\nThe settings must match the IP address and subnet mask of at least one network_interface for the customization.\nipv4_gateway (string) - The IPv4 default gateway when using network_interface customization.\nipv6_gateway (string) - The IPv6 default gateway when using network_interface customization.\nGlobal DNS Settings\nThe following settings configure DNS globally for Linux guest operating systems. For Windows guest operating systems, this is set for each network interface. Refer to the network interface section for additional details.\ndns_server_list ([]string) - A list of DNS servers to configure on the guest operating system.\ndns_suffix_list ([]string) - A list of DNS search domains to add to the DNS configuration on the guest operating system.\nLinux Customization Settings\ndomain (string) - The domain name for the guest operating system. Used with host_name to construct the fully qualified domain name (FQDN).\nhost_name (string) - The hostname for the guest operating system. Used with domain to construct the fully qualified domain name (FQDN).\nhw_clock_utc (boolean) - Set the hardware clock to Coordinated Universal Time (UTC). Defaults to true.\ntime_zone (string) - The time zone for the guest operating system.\nLinux Customization Example\nHCL Example:\ncustomize { linux_options { host_name = \"foo\" domain = \"example.com\" } network_interface { ipv4_address = \"10.0.0.10\" ipv4_netmask = \"24\" } ipv4_gateway = 10.0.0.1 dns_server_list = [\"10.0.0.18\"] } \nJSON Example:\n\"customize\": { \"linux_options\": { \"host_name\": \"foo\", \"domain\": \"example.com\" }, \"network_interface\": { \"ipv4_address\": \"10.0.0.10\", \"ipv4_netmask\": \"24\" }, \"ipv4_gateway\": \"10.0.0.1\", \"dns_server_list\": [\"10.0.0.18\"] } \nWindows Customization Settings\nrun_once_command_list ([]string) - A list of commands to run at first logon after the guest operating system is customized.\nauto_logon (*bool) - Automatically log on the Administrator account after the guest operating system is customized.\nauto_logon_count (*int32) - The number of times the guest operating system should auto-logon the Administrator account when auto_logon is set to true. Defaults to 1.\nadmin_password (*string) - The password for the guest operating system's Administrator account.\ntime_zone (*int32) - The time zone for the guest operating system. Defaults to 85 (Pacific Time).\nworkgroup (string) - The workgroup for the guest operating system. Joining an Active Directory domain is not supported.\ncomputer_name (string) - The hostname for the guest operating system.\nfull_name (string) - The full name for the guest operating system's Administrator account. Defaults to Administrator.\norganization_name (string) - The organization name for the guest operating system. Defaults to Built by Packer.\nproduct_key (string) - The product key for the guest operating system.\nWindows Customization Example\ncustomize { windows_options { computer_name = \"foo\" workgroup = \"example\" product_key = \"XXXXX-XXXXX-XXXXX-XXXXX-XXXXX\" admin_password = \"password\" } network_interface { ipv4_address = \"10.0.0.10\" ipv4_netmask = \"24\" } ipv4_gateway = 10.0.0.1 dns_server_list = [\"10.0.0.18\"] } \n\"customize\": { \"windows_options\": { \"host_name\": \"foo\", \"workgroup\": \"example\", \"product_key\": \"XXXXX-XXXXX-XXXXX-XXXXX-XXXXX\", \"admin_password\": \"password\" }, \"network_interface\": { \"ipv4_address\": \"10.0.0.10\", \"ipv4_netmask\": \"24\" }, \"ipv4_gateway\": \"10.0.0.1\", \"dns_server_list\": [\"10.0.0.18\"] } \nBoot Configuration\nThe boot configuration is very important: boot_command specifies the keys to type when the virtual machine is first booted in order to start the OS installer. This command is typed after boot_wait, which gives the virtual machine some time to actually load.\nThe boot_command is an array of strings. The strings are all typed in sequence. It is an array only to improve readability within the template.\nThere are a set of special keys available. If these are in your boot command, they will be replaced by the proper key:\n<bs> - Backspace\n<del> - Delete\n<enter> <return> - Simulates an actual \"enter\" or \"return\" keypress.\n<esc> - Simulates pressing the escape key.\n<tab> - Simulates pressing the tab key.\n<f1> - <f12> - Simulates pressing a function key.\n<up> <down> <left> <right> - Simulates pressing an arrow key.\n<spacebar> - Simulates pressing the spacebar.\n<insert> - Simulates pressing the insert key.\n<home> <end> - Simulates pressing the home and end keys.\n<pageUp> <pageDown> - Simulates pressing the page up and page down keys.\n<menu> - Simulates pressing the Menu key.\n<leftAlt> <rightAlt> - Simulates pressing the alt key.\n<leftCtrl> <rightCtrl> - Simulates pressing the ctrl key.\n<leftShift> <rightShift> - Simulates pressing the shift key.\n<leftSuper> <rightSuper> - Simulates pressing the  or Windows key.\n<wait> <wait5> <wait10> - Adds a 1, 5 or 10 second pause before sending any additional keys. This is useful if you have to generally wait for the UI to update before typing more.\n<waitXX> - Add an arbitrary pause before sending any additional keys. The format of XX is a sequence of positive decimal numbers, each with optional fraction and a unit suffix, such as 300ms, 1.5h or 2h45m. Valid time units are ns, us (or s), ms, s, m, h. For example <wait10m> or <wait1m20s>.\n<XXXOn> <XXXOff> - Any printable keyboard character, and of these \"special\" expressions, with the exception of the <wait> types, can also be toggled on or off. For example, to simulate ctrl+c, use <leftCtrlOn>c<leftCtrlOff>. Be sure to release them, otherwise they will be held down until the machine reboots. To hold the c key down, you would use <cOn>. Likewise, <cOff> to release.\n{{ .HTTPIP }} {{ .HTTPPort }} - The IP and port, respectively of an HTTP server that is started serving the directory specified by the http_directory configuration parameter. If http_directory isn't specified, these will be blank!\n{{ .Name }} - The name of the VM.\nExample boot command. This is actually a working boot command used to start an CentOS 6.4 installer:\nIn JSON:\n\"boot_command\": [ \"<tab><wait>\", \" ks=http://{{ .HTTPIP }}:{{ .HTTPPort }}/centos6-ks.cfg<enter>\" ] \nIn HCL2:\nboot_command = [ \"<tab><wait>\", \" ks=http://{{ .HTTPIP }}:{{ .HTTPPort }}/centos6-ks.cfg<enter>\" ] \nThe example shown below is a working boot command used to start an Ubuntu 12.04 installer:\nIn JSON:\n\"boot_command\": [ \"<esc><esc><enter><wait>\", \"/install/vmlinuz noapic \", \"preseed/url=http://{{ .HTTPIP }}:{{ .HTTPPort }}/preseed.cfg \", \"debian-installer=en_US auto locale=en_US kbd-chooser/method=us \", \"hostname={{ .Name }} \", \"fb=false debconf/frontend=noninteractive \", \"keyboard-configuration/modelcode=SKIP keyboard-configuration/layout=USA \", \"keyboard-configuration/variant=USA console-setup/ask_detect=false \", \"initrd=/install/initrd.gz -- <enter>\" ] \nIn HCL2:\nboot_command = [ \"<esc><esc><enter><wait>\", \"/install/vmlinuz noapic \", \"preseed/url=http://{{ .HTTPIP }}:{{ .HTTPPort }}/preseed.cfg \", \"debian-installer=en_US auto locale=en_US kbd-chooser/method=us \", \"hostname={{ .Name }} \", \"fb=false debconf/frontend=noninteractive \", \"keyboard-configuration/modelcode=SKIP keyboard-configuration/layout=USA \", \"keyboard-configuration/variant=USA console-setup/ask_detect=false \", \"initrd=/install/initrd.gz -- <enter>\" ] \nFor more examples of various boot commands, see the sample projects from our community templates page.\nboot_keygroup_interval (duration string | ex: \"1h5m2s\") - Time to wait after sending a group of key pressses. The value of this should be a duration. Examples are 5s and 1m30s which will cause Packer to wait five seconds and one minute 30 seconds, respectively. If this isn't specified, a sensible default value is picked depending on the builder type.\nboot_wait (duration string | ex: \"1h5m2s\") - The time to wait after booting the initial virtual machine before typing the boot_command. The value of this should be a duration. Examples are 5s and 1m30s which will cause Packer to wait five seconds and one minute 30 seconds, respectively. If this isn't specified, the default is 10s or 10 seconds. To set boot_wait to 0s, use a negative number, such as \"-1s\"\nboot_command ([]string) - This is an array of commands to type when the virtual machine is first booted. The goal of these commands should be to type just enough to initialize the operating system installer. Special keys can be typed as well, and are covered in the section below on the boot command. If this is not specified, it is assumed the installer will start itself.\nHTTP Directory Configuration\nPacker will create an http server serving http_directory when it is set, a random free port will be selected and the architecture of the directory referenced will be available in your builder.\nExample usage from a builder:\nwget http://{{ .HTTPIP }}:{{ .HTTPPort }}/foo/bar/preseed.cfg \nhttp_directory (string) - Path to a directory to serve using an HTTP server. The files in this directory will be available over HTTP that will be requestable from the virtual machine. This is useful for hosting kickstart files and so on. By default this is an empty string, which means no HTTP server will be started. The address and port of the HTTP server will be available as variables in boot_command. This is covered in more detail below.\nhttp_content (map[string]string) - Key/Values to serve using an HTTP server. http_content works like and conflicts with http_directory. The keys represent the paths and the values contents, the keys must start with a slash, ex: /path/to/file. http_content is useful for hosting kickstart files and so on. By default this is empty, which means no HTTP server will be started. The address and port of the HTTP server will be available as variables in boot_command. This is covered in more detail below. Example:\nhttp_content = { \"/a/b\" = file(\"http/b\") \"/foo/bar\" = templatefile(\"${path.root}/preseed.cfg\", { packages = [\"nginx\"] }) } \nhttp_port_min (int) - These are the minimum and maximum port to use for the HTTP server started to serve the http_directory. Because Packer often runs in parallel, Packer will choose a randomly available port in this range to run the HTTP server. If you want to force the HTTP server to be on one port, make this minimum and maximum port the same. By default the values are 8000 and 9000, respectively.\nhttp_port_max (int) - HTTP Port Max\nhttp_bind_address (string) - This is the bind address for the HTTP server. Defaults to 0.0.0.0 so that it will work with any network interface.\nhttp_interface (string) - The network interface (for example, en0, ens192, etc.) that the HTTP server will use to serve the http_directory. The plugin will identify the IP address associated with this network interface and bind to it.\nhttp_ip (string) - The IP address to use for the HTTP server to serve the http_directory.\nNotes:\nThe options http_bind_address and http_interface are mutually exclusive.\nBoth http_bind_address and http_interface have higher priority than http_ip.\nThe http_bind_address is matched against the IP addresses of the host's network interfaces. If no match is found, the plugin will terminate.\nSimilarly, http_interface is compared with the host's network interfaces. If there's no corresponding network interface, the plugin will also terminate.\nIf neither http_bind_address, http_interface, and http_ip are provided, the plugin will automatically find and use the IP address of the first non-loopback interface for http_ip.\nFloppy Configuration\nfloppy_img_path (string) - Datastore path to a floppy image that will be mounted to the virtual machine. Example: [datastore] iso/foo.flp.\nfloppy_files ([]string) - A list of local files to be mounted to the virtual machine's floppy drive.\nfloppy_dirs ([]string) - A list of directories to copy files from.\nfloppy_content (map[string]string) - Key/Values to add to the floppy disk. The keys represent the paths, and the values contents. It can be used alongside floppy_files or floppy_dirs, which is useful to add large files without loading them into memory. If any paths are specified by both, the contents in floppy_content will take precedence.\nfloppy_content = { \"meta-data\" = jsonencode(local.instance_data) \"user-data\" = templatefile(\"user-data\", { packages = [\"nginx\"] }) } \nfloppy_label (string) - The label to use for the floppy disk that is attached when the virtual machine is booted. This is most useful for cloud-init, Kickstart or other early initialization tools, which can benefit from labelled floppy disks. By default, the floppy label will be 'packer'.\nConnection Configuration\nvcenter_server (string) - The fully qualified domain name or IP address of the vCenter Server instance.\nusername (string) - The username to authenticate with the vCenter Server instance.\npassword (string) - The password to authenticate with the vCenter Server instance.\ninsecure_connection (bool) - Do not validate the certificate of the vCenter Server instance. Defaults to false.\nNote: This option is beneficial in scenarios where the certificate is self-signed or does not meet standard validation criteria.\ndatacenter (string) - The name of the datacenter object in the vSphere inventory.\nNote: Required if more than one datacenter object exists in the vSphere inventory.\nHardware Configuration\nCPUs (int32) - The number of virtual CPUs cores for the virtual machine.\ncpu_cores (int32) - The number of virtual CPU cores per socket for the virtual machine.\nCPU_reservation (int64) - The CPU reservation in MHz.\nCPU_limit (int64) - The upper limit of available CPU resources in MHz.\nCPU_hot_plug (bool) - Enable CPU hot plug setting for virtual machine. Defaults to false\nRAM (int64) - The amount of memory for the virtual machine in MB.\nRAM_reservation (int64) - The guaranteed minimum allocation of memory for the virtual machine in MB.\nRAM_reserve_all (bool) - Reserve all allocated memory. Defaults to false.\nNote: May not be used together with RAM_reservation.\nRAM_hot_plug (bool) - Enable memory hot add setting for virtual machine. Defaults to false.\nvideo_ram (int64) - The amount of video memory in KB. Defaults to 4096 KB.\nNote: Refer to the vSphere documentation for supported maximums.\ndisplays (int32) - The number of video displays. Defaults to 1.\n`-> Note: Refer to the vSphere documentation for supported maximums.\npci_passthrough_allowed_device ([]PCIPassthroughAllowedDevice) - Configure Dynamic DirectPath I/O PCI Passthrough for virtual machine. Refer to the vSphere documentation\nvgpu_profile (string) - vGPU profile for accelerated graphics. Refer to the NVIDIA GRID vGPU documentation for examples of profile names. Defaults to none.\nNestedHV (bool) - Enable nested hardware virtualization for the virtual machine. Defaults to false.\nfirmware (string) - The firmware for the virtual machine.\nThe available options for this setting are: 'bios', 'efi', and 'efi-secure'.\nNote: Use efi-secure for UEFI Secure Boot.\nforce_bios_setup (bool) - Force entry into the BIOS setup screen during boot. Defaults to false.\nvTPM (bool) - Enable virtual trusted platform module (TPM) device for the virtual machine. Defaults to false.\nprecision_clock (string) - The virtual precision clock device for the virtual machine. Defaults to none.\nThe available options for this setting are: none, ntp, and ptp.\nLocation Configuration\nvm_name (string) - The name of the virtual machine.\nfolder (string) - The virtual machine folder where the virtual machine is created.\ncluster (string) - The cluster where the virtual machine is created. Refer to the Working With Clusters And Hosts section for more details.\nhost (string) - The ESXi host where the virtual machine is created. A full path must be specified if the ESXi host is in a folder. For example folder/host. Refer to the Working With Clusters And Hosts section for more details.\nresource_pool (string) - The resource pool where the virtual machine is created. If this is not specified, the root resource pool associated with the host or cluster is used.\nNote: The full path to the resource pool must be provided. For example, a simple resource pool path might resemble rp-packer and a nested path might resemble 'rp-packer/rp-linux-images'.\ndatastore (string) - The datastore where the virtual machine is created. Required if host is a cluster, or if host has multiple datastores.\nset_host_for_datastore_uploads (bool) - The ESXI host used for uploading files to the datastore. Defaults to false.\nRun Configuration\nboot_order (string) - The priority of boot devices. Defaults to disk,cdrom.\nThe available boot devices are: floppy, cdrom, ethernet, and disk.\nNote: If not set, the boot order is temporarily set to disk,cdrom for the duration of the build and then cleared upon build completion.\nShutdown Configuration\nshutdown_command (string) - Specify a VM guest shutdown command. This command will be executed using the communicator. Otherwise, the VMware Tools are used to gracefully shutdown the VM.\nshutdown_timeout (duration string | ex: \"1h5m2s\") - Amount of time to wait for graceful VM shutdown. Defaults to 5m or five minutes. This will likely need to be modified if the communicator is 'none'.\ndisable_shutdown (bool) - Packer normally halts the virtual machine after all provisioners have run when no shutdown_command is defined. If this is set to true, Packer will not halt the virtual machine but will assume that you will send the stop signal yourself through a preseed.cfg, a script or the final provisioner. Packer will wait for a default of five minutes until the virtual machine is shutdown. The timeout can be changed using shutdown_timeout option.\nWait Configuration\nip_wait_timeout (duration string | ex: \"1h5m2s\") - Amount of time to wait for VM's IP, similar to 'ssh_timeout'. Defaults to 30m (30 minutes). See the Golang ParseDuration documentation for full details.\nip_settle_timeout (duration string | ex: \"1h5m2s\") - Amount of time to wait for VM's IP to settle down, sometimes VM may report incorrect IP initially, then its recommended to set that parameter to apx. 2 minutes. Examples 45s and 10m. Defaults to 5s(5 seconds). Refer to the Golang ParseDuration documentation for full details.\nip_wait_address (*string) - Set this to a CIDR address to cause the service to wait for an address that is contained in this network range. Defaults to \"0.0.0.0/0\" for any ipv4 address. Examples include:\nempty string (\"\") - remove all filters\n0:0:0:0:0:0:0:0/0 - allow only ipv6 addresses\n192.168.1.0/24 - only allow ipv4 addresses from 192.168.1.1 to 192.168.1.254\nCD-ROM Configuration\nAn iso (CD) containing custom files can be made available for your build.\nBy default, no extra CD will be attached. All files listed in this setting get placed into the root directory of the CD and the CD is attached as the second CD device.\nThis config exists to work around modern operating systems that have no way to mount floppy disks, which was our previous go-to for adding files at boot time.\ncd_files ([]string) - A list of files to place onto a CD that is attached when the VM is booted. This can include either files or directories; any directories will be copied onto the CD recursively, preserving directory structure hierarchy. Symlinks will have the link's target copied into the directory tree on the CD where the symlink was. File globbing is allowed.\nUsage example (JSON):\n\"cd_files\": [\"./somedirectory/meta-data\", \"./somedirectory/user-data\"], \"cd_label\": \"cidata\", \nUsage example (HCL):\ncd_files = [\"./somedirectory/meta-data\", \"./somedirectory/user-data\"] cd_label = \"cidata\" \nThe above will create a CD with two files, user-data and meta-data in the CD root. This specific example is how you would create a CD that can be used for an Ubuntu 20.04 autoinstall.\nSince globbing is also supported,\ncd_files = [\"./somedirectory/*\"] cd_label = \"cidata\" \nWould also be an acceptable way to define the above cd. The difference between providing the directory with or without the glob is whether the directory itself or its contents will be at the CD root.\nUse of this option assumes that you have a command line tool installed that can handle the iso creation. Packer will use one of the following tools:\nxorriso\nmkisofs\nhdiutil (normally found in macOS)\noscdimg (normally found in Windows as part of the Windows ADK)\ncd_content (map[string]string) - Key/Values to add to the CD. The keys represent the paths, and the values contents. It can be used alongside cd_files, which is useful to add large files without loading them into memory. If any paths are specified by both, the contents in cd_content will take precedence.\nUsage example (HCL):\ncd_files = [\"vendor-data\"] cd_content = { \"meta-data\" = jsonencode(local.instance_data) \"user-data\" = templatefile(\"user-data\", { packages = [\"nginx\"] }) } cd_label = \"cidata\" \ncd_label (string) - CD Label\ncdrom_type (string) - The type of controller to use for the CD-ROM device. Defaults to ide.\nThe available options for this setting are: ide and sata.\niso_paths ([]string) - A list of paths to ISO files in either a datastore or a content library that will be attached to the virtual machine.\niso_paths = [ \"[nfs] iso/ubuntu-server-amd64.iso\", \"Example Content Library/ubuntu-server-amd64/ubuntu-server-amd64.iso\" ] \n\"iso_paths\": [ \"[nfs] iso/ubuntu-server-amd64.iso\", \"Example Content Library/ubuntu-server-amd64/ubuntu-server-amd64.iso\" ] \nTwo ISOs are referenced:\nAn ISO in the \"iso\" folder of the \"nfs\" datastore with the file name of \"ubuntu-server-amd64.iso\". \"ubuntu-server-amd64.iso\".\nAn ISO in the \"Example Content Library\" content library with the item name of \"ubuntu-server-amd64\".\nNote: All files in a content library have an associated item name. To determine the file name, view the datastore backing the content library or use the govc vSphere CLI.\nremove_cdrom (bool) - Remove all CD-ROM devices from the virtual machine when the build is complete. Defaults to false.\nCommunicator Configuration\nCommon\ncommunicator (string) - Packer currently supports three kinds of communicators:\nnone - No communicator will be used. If this is set, most provisioners also can't be used.\nssh - An SSH connection will be established to the machine. This is usually the default.\nwinrm - A WinRM connection will be established.\nIn addition to the above, some builders have custom communicators they can use. For example, the Docker builder has a \"docker\" communicator that uses docker exec and docker cp to execute scripts and copy files.\npause_before_connecting (duration string | ex: \"1h5m2s\") - We recommend that you enable SSH or WinRM as the very last step in your guest's bootstrap script, but sometimes you may have a race condition where you need Packer to wait before attempting to connect to your guest.\nIf you end up in this situation, you can use the template option pause_before_connecting. By default, there is no pause. For example if you set pause_before_connecting to 10m Packer will check whether it can connect, as normal. But once a connection attempt is successful, it will disconnect and then wait 10 minutes before connecting to the guest and beginning provisioning.\nSSH\nssh_host (string) - The address to SSH to. This usually is automatically configured by the builder.\nssh_port (int) - The port to connect to SSH. This defaults to 22.\nssh_username (string) - The username to connect to SSH with. Required if using SSH.\nssh_password (string) - A plaintext password to use to authenticate with SSH.\nssh_ciphers ([]string) - This overrides the value of ciphers supported by default by Golang. The default value is [ \"aes128-gcm@openssh.com\", \"chacha20-poly1305@openssh.com\", \"aes128-ctr\", \"aes192-ctr\", \"aes256-ctr\", ]\nValid options for ciphers include: \"aes128-ctr\", \"aes192-ctr\", \"aes256-ctr\", \"aes128-gcm@openssh.com\", \"chacha20-poly1305@openssh.com\", \"arcfour256\", \"arcfour128\", \"arcfour\", \"aes128-cbc\", \"3des-cbc\",\nssh_clear_authorized_keys (bool) - If true, Packer will attempt to remove its temporary key from ~/.ssh/authorized_keys and /root/.ssh/authorized_keys. This is a mostly cosmetic option, since Packer will delete the temporary private key from the host system regardless of whether this is set to true (unless the user has set the -debug flag). Defaults to \"false\"; currently only works on guests with sed installed.\nssh_key_exchange_algorithms ([]string) - If set, Packer will override the value of key exchange (kex) algorithms supported by default by Golang. Acceptable values include: \"curve25519-sha256@libssh.org\", \"ecdh-sha2-nistp256\", \"ecdh-sha2-nistp384\", \"ecdh-sha2-nistp521\", \"diffie-hellman-group14-sha1\", and \"diffie-hellman-group1-sha1\".\nssh_certificate_file (string) - Path to user certificate used to authenticate with SSH. The ~ can be used in path and will be expanded to the home directory of current user.\nssh_pty (bool) - If true, a PTY will be requested for the SSH connection. This defaults to false.\nssh_timeout (duration string | ex: \"1h5m2s\") - The time to wait for SSH to become available. Packer uses this to determine when the machine has booted so this is usually quite long. Example value: 10m. This defaults to 5m, unless ssh_handshake_attempts is set.\nssh_disable_agent_forwarding (bool) - If true, SSH agent forwarding will be disabled. Defaults to false.\nssh_handshake_attempts (int) - The number of handshakes to attempt with SSH once it can connect. This defaults to 10, unless a ssh_timeout is set.\nssh_bastion_host (string) - A bastion host to use for the actual SSH connection.\nssh_bastion_port (int) - The port of the bastion host. Defaults to 22.\nssh_bastion_agent_auth (bool) - If true, the local SSH agent will be used to authenticate with the bastion host. Defaults to false.\nssh_bastion_username (string) - The username to connect to the bastion host.\nssh_bastion_password (string) - The password to use to authenticate with the bastion host.\nssh_bastion_interactive (bool) - If true, the keyboard-interactive used to authenticate with bastion host.\nssh_bastion_private_key_file (string) - Path to a PEM encoded private key file to use to authenticate with the bastion host. The ~ can be used in path and will be expanded to the home directory of current user.\nssh_bastion_certificate_file (string) - Path to user certificate used to authenticate with bastion host. The ~ can be used in path and will be expanded to the home directory of current user.\nssh_file_transfer_method (string) - scp or sftp - How to transfer files, Secure copy (default) or SSH File Transfer Protocol.\nNOTE: Guests using Windows with Win32-OpenSSH v9.1.0.0p1-Beta, scp (the default protocol for copying data) returns a a non-zero error code since the MOTW cannot be set, which cause any file transfer to fail. As a workaround you can override the transfer protocol with SFTP instead ssh_file_transfer_method = \"sftp\".\nssh_proxy_host (string) - A SOCKS proxy host to use for SSH connection\nssh_proxy_port (int) - A port of the SOCKS proxy. Defaults to 1080.\nssh_proxy_username (string) - The optional username to authenticate with the proxy server.\nssh_proxy_password (string) - The optional password to use to authenticate with the proxy server.\nssh_keep_alive_interval (duration string | ex: \"1h5m2s\") - How often to send \"keep alive\" messages to the server. Set to a negative value (-1s) to disable. Example value: 10s. Defaults to 5s.\nssh_read_write_timeout (duration string | ex: \"1h5m2s\") - The amount of time to wait for a remote command to end. This might be useful if, for example, packer hangs on a connection after a reboot. Example: 5m. Disabled by default.\nssh_remote_tunnels ([]string) - \nssh_local_tunnels ([]string) - \ntemporary_key_pair_type (string) - dsa | ecdsa | ed25519 | rsa ( the default )\nSpecifies the type of key to create. The possible values are 'dsa', 'ecdsa', 'ed25519', or 'rsa'.\nNOTE: DSA is deprecated and no longer recognized as secure, please consider other alternatives like RSA or ED25519.\ntemporary_key_pair_bits (int) - Specifies the number of bits in the key to create. For RSA keys, the minimum size is 1024 bits and the default is 4096 bits. Generally, 3072 bits is considered sufficient. DSA keys must be exactly 1024 bits as specified by FIPS 186-2. For ECDSA keys, bits determines the key length by selecting from one of three elliptic curve sizes: 256, 384 or 521 bits. Attempting to use bit lengths other than these three values for ECDSA keys will fail. Ed25519 keys have a fixed length and bits will be ignored.\nNOTE: DSA is deprecated and no longer recognized as secure as specified by FIPS 186-5, please consider other alternatives like RSA or ED25519.\nssh_keypair_name (string) - If specified, this is the key that will be used for SSH with the machine. The key must match a key pair name loaded up into the remote. By default, this is blank, and Packer will generate a temporary keypair unless ssh_password is used. ssh_private_key_file or ssh_agent_auth must be specified when ssh_keypair_name is utilized.\nssh_private_key_file (string) - Path to a PEM encoded private key file to use to authenticate with SSH. The ~ can be used in path and will be expanded to the home directory of current user.\nssh_agent_auth (bool) - If true, the local SSH agent will be used to authenticate connections to the source instance. No temporary keypair will be created, and the values of ssh_password and ssh_private_key_file will be ignored. The environment variable SSH_AUTH_SOCK must be set for this option to work properly.\nNOTE: The builder uses vApp Options to inject SSH public keys to the virtual machine. The temporary_key_pair_name will only work if the template being cloned contains the vApp property public-keys. If using ssh_private_key_file, provide the public key using the configuration_parameters or vApp Options Configuration whenever the guestinto.userdata is available. Refer to the VMware datasource in cloud-init 21.3 and later for additional information.\nWindows Remote Management (WinRM)\nwinrm_username (string) - The username to use to connect to WinRM.\nwinrm_password (string) - The password to use to connect to WinRM.\nwinrm_host (string) - The address for WinRM to connect to.\nNOTE: If using an Amazon EBS builder, you can specify the interface WinRM connects to via ssh_interface\nwinrm_no_proxy (bool) - Setting this to true adds the remote host:port to the NO_PROXY environment variable. This has the effect of bypassing any configured proxies when connecting to the remote host. Default to false.\nwinrm_port (int) - The WinRM port to connect to. This defaults to 5985 for plain unencrypted connection and 5986 for SSL when winrm_use_ssl is set to true.\nwinrm_timeout (duration string | ex: \"1h5m2s\") - The amount of time to wait for WinRM to become available. This defaults to 30m since setting up a Windows machine generally takes a long time.\nwinrm_use_ssl (bool) - If true, use HTTPS for WinRM.\nwinrm_insecure (bool) - If true, do not check server certificate chain and host name.\nwinrm_use_ntlm (bool) - If true, NTLMv2 authentication (with session security) will be used for WinRM, rather than default (basic authentication), removing the requirement for basic authentication to be enabled within the target guest. Further reading for remote connection authentication can be found here.\nExport Configuration\nYou can export an image in Open Virtualization Format (OVF) to the Packer host.\n# ... vm_name = \"example-ubuntu\" # ... export { force = true output_directory = \"./output-artifacts\" } \n... \"vm_name\": \"example-ubuntu\", ... \"export\": { \"force\": true, \"output_directory\": \"./output-artifacts\" }, \nThe above configuration would create the following files:\n./output-artifacts/example-ubuntu-disk-0.vmdk ./output-artifacts/example-ubuntu.mf ./output-artifacts/example-ubuntu.ovf \nname (string) - The name of the exported image in Open Virtualization Format (OVF).\nNote: The name of the virtual machine with the .ovf extension is used if this option is not specified.\nforce (bool) - Forces the export to overwrite existing files. Defaults to false. If set to false, an error is returned if the file(s) already exists.\nimage_files (bool) - Include additional image files that are that are associated with the virtual machine. Defaults to false. For example, .nvram and .log files.\nmanifest (string) - The hash algorithm to use when generating a manifest file. Defaults to sha256.\nThe available options for this setting are: 'none', 'sha1', 'sha256', and 'sha512'.\n--> Tip: Use none to disable the creation of a manifest file.\noptions ([]string) - Advanced image export options. Available options include:\nmac - MAC address is exported for each Ethernet device.\nuuid - UUID is exported for the virtual machine.\nextraconfig - Extra configuration options are exported for the virtual machine.\nnodevicesubtypes - Resource subtypes for CD/DVD drives, floppy drives, and SCSI controllers are not exported.\nFor example, adding the following export configuration option outputs the MAC addresses for each Ethernet device in the OVF descriptor:\n... export { options = [\"mac\"] } \nJSON: Example:\n... \"export\": { \"options\": [\"mac\"] }, \noutput_format (string) - The output format for the exported virtual machine image. Defaults to ovf. Available options include ovf and ova.\nWhen set to ova, the image is first exported using Open Virtualization Format (.ovf) and then converted to an Open Virtualization Archive (.ova) using the VMware Open Virtualization Format Tool (ovftool). The intermediate files are removed after the conversion.\nNote: To use the ova format option, VMware ovftool must be installed on the Packer host and accessible in either the system PATH or the user's PATH.\nOutput Configuration\noutput_directory (string) - The directory where artifacts from the build, such as the virtual machine files and disks, will be output to. The path to the directory may be relative or absolute. If relative, the path is relative to the working directory Packer is run from. This directory must not exist or, if created, must be empty prior to running the builder. By default this is \"output-\" where \"buildName\" is the name of the build.\ndirectory_permission (os.FileMode) - The permissions to apply to the \"output_directory\", and to any parent directories that get created for output_directory. By default this is \"0750\". You should express the permission as quoted string with a leading zero such as \"0755\" in JSON file, because JSON does not support octal value. In Unix-like OS, the actual permission may differ from this value because of umask.\nContent Library Configuration\nCreate a content library item in a content library whose content is a VM template or an OVF template created from the virtual machine image after the build is complete.\nThe template is stored in a existing or newly created library item.\nlibrary (string) - The name of the content library in which the new content library item containing the template will be created or updated. The content library must be of type Local to allow deploying virtual machines.\nname (string) - The name of the content library item that will be created or updated. For VM templates, the name of the item should be different from vm_name and the default is vm_name + timestamp when not set. VM templates will be always imported to a new library item. For OVF templates, the name defaults to vm_name when not set, and if an item with the same name already exists it will be then updated with the new OVF template, otherwise a new item will be created.\nNote: It's not possible to update existing content library items with a new VM template. If updating an existing content library item is necessary, use an OVF template instead by setting the ovf option as true.\ndescription (string) - A description for the content library item that will be created. Defaults to \"Packer imported vm_name VM template\".\ncluster (string) - The cluster where the VM template will be placed. If cluster and resource_pool are both specified, resource_pool must belong to cluster. If cluster and host are both specified, the ESXi host must be a member of the cluster. This option is not used when importing OVF templates. Defaults to cluster.\nfolder (string) - The virtual machine folder where the VM template will be placed. This option is not used when importing OVF templates. Defaults to the same folder as the source virtual machine.\nhost (string) - The ESXi host where the virtual machine template will be placed. If host and resource_pool are both specified, resource_pool must belong to host. If host and cluster are both specified, host must be a member of the cluster. This option is not used when importing OVF templates. Defaults to host.\nresource_pool (string) - The resource pool where the virtual machine template will be placed. Defaults to resource_pool. If resource_pool is unset, the system will attempt to choose a suitable resource pool for the VM template.\ndatastore (string) - The datastore for the virtual machine template's configuration and log files. This option is not used when importing OVF templates. Defaults to the storage backing associated with the content library.\ndestroy (bool) - Destroy the virtual machine after the import to the content library. Defaults to false.\novf (bool) - Import an OVF template to the content library item. Defaults to false.\nskip_import (bool) - Skip the import to the content library item. Useful during a build test stage. Defaults to false.\novf_flags ([]string) - Flags to use for OVF package creation. The supported flags can be obtained using ExportFlag.list. If unset, no flags will be used. Known values: EXTRA_CONFIG, PRESERVE_MAC.\ncontent_library_destination { library = \"Example Content Library\" } \n\"content_library_destination\" : { \"library\": \"Example Content Library\" } \nWorking with Clusters and Hosts\nStandalone ESXi Hosts\nOnly use the host option. Optionally, specify a resource_pool:\nhost = \"esxi-01.example.com\" resource_pool = \"example_resource_pool\" \n\"host\": \"esxi-01.example.com\", \"resource_pool\": \"example_resource_pool\", \nClusters with Distributed Resource Scheduler Enabled\nOnly use the cluster option. Optionally, specify a resource_pool:\ncluster = \"cluster-01\" resource_pool = \"example_resource_pool\" \n\"cluster\": \"cluster-01\", \"resource_pool\": \"example_resource_pool\", \nClusters without Distributed Resource Scheduler Enabled\nUse the cluster and host parameters:\ncluster = \"cluster-01\" host = \"esxi-01.example.com\" \n\"cluster\": \"cluster-01\", \"host\": \"esxi-01.example.com\", \nPrivileges\nVM folder (this object and children):\nVirtual machine > Inventory Virtual machine > Configuration Virtual machine > Interaction Virtual machine > Snapshot management Virtual machine > Provisioning \nResource pool, host, or cluster (this object):\nResource -> Assign virtual machine to resource pool \nHost in clusters without DRS (this object):\nDatastore (this object):\nDatastore > Allocate space Datastore > Browse datastore Datastore > Low level file operations \nNetwork (this object):\nDistributed switch (this object):\nDatacenter (this object):\nDatastore > Low level file operations \nHost (this object):\nHost > Configuration > System Management"
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vsphere/latest/components/builder/vsphere-supervisor",
  "text": "VMware vSphere Builder | Integrations | Packer\nThe vSphere plugin is able to create vSphere virtual machines for use with any VMware product.\nOfficial\nHCP Ready\nUpdated 2 years ago\nGitHub\n(opens in new tab)\nType: vsphere-supervisor\nArtifact BuilderId: vsphere.supervisor\nThis builder creates a virtual machine on a vSphere Supervisor cluster using the VM-Operator API.\nRefer to Deploying and Managing Virtual Machines in vSphere with Tanzu for more information on the VM Service functionality in vSphere with Tanzu.\nIt uses a kubeconfig file to connect to the vSphere Supervisor cluster.\nIt uses the VM-Operator API to deploy and configure the source virtual machine.\nIt cna use Packer provisioners to customize the virtual machine after establishing a successful connection.\nIt publishes the customized virtual machine as a new virtual machine image to the designated content library in vSphere.\nNote: This builder is developed to maintain compatibility with VMware vSphere versions until their respective End of General Support dates. For detailed information, refer to the Broadcom Product Lifecycle.\nExamples\nExamples are available in the examples directory of the GitHub repository.\nsource \"vsphere-supervisor\" \"example-vm\" { image_name = \"<Image name of the source VM, e.g. 'ubuntu-impish-21.10-cloudimg'>\" class_name = \"<VM class that describes the virtual hardware settings, e.g. 'best-effort-large'>\" storage_class = \"<Storage class that provides the backing storage for volume, e.g. 'wcplocal-storage-profile'>\" bootstrap_provider = \"<CloudInit, Sysprep, or vAppConfig to customize the guest OS>\" bootstrap_data_file = \"<Path to the file containing the bootstrap data for guest OS customization>\" publish_location_name = \"<target location / content library for the published image, optional, e.g. 'cl-6066c61f7931c5ef9'>\" } build { sources = [\"source.vsphere-supervisor.example-vm\"] } \nHCL Example with image import:\nsource \"vsphere-supervisor\" \"example-vm\" { import_source_url = \"<Remote URL to import image from, optional, e.g. 'https://example.com/example.ovf'>\" import_source_ssl_certificate = \"<SSL certificate of the remote HTTPS server, optional, e.g. '-----BEGIN CERTIFICATE-----xxxxx-----END CERTIFICATE-----'>\" import_target_location_name = \"<Target location / content library for the imported image, optional, e.g. 'cl-6066c61f7931c5ef9'>\" import_target_image_type = \"<Target image type of the imported image, optional, e.g. 'ovf'>\" import_target_image_name = \"<Target image name of the imported image for the source VM, e.g. 'ubuntu-impish-21.10-cloudimg'>\" class_name = \"<VM class that describes the virtual hardware settings, e.g. 'best-effort-large'>\" storage_class = \"<Storage class that provides the backing storage for volume, e.g. 'wcplocal-storage-profile'>\" bootstrap_provider = \"<CloudInit, Sysprep, or vAppConfig to customize the guest OS>\" bootstrap_data_file = \"<Path to the file containing the bootstrap data for guest OS customization>\" publish_location_name = \"<target location / content library for the published image, optional, e.g. 'cl-6066c61f7931c5ef9'>\" } build { sources = [\"source.vsphere-supervisor.example-vm\"] } \n{ \"builders\": [ { \"type\": \"vsphere-supervisor\", \"image_name\": \"<Image name of the source VM, e.g. 'ubuntu-impish-21.10-cloudimg'>\", \"class_name\": \"<VM class that describes the virtual hardware settings, e.g. 'best-effort-large'>\", \"storage_class\": \"<Storage class that provides the backing storage for volume, e.g. 'wcplocal-storage-profile'>\", \"bootstrap_provider\": \"<CloudInit, Sysprep, or vAppConfig to customize the guest OS>\", \"bootstrap_data_file\": \"<Path to the file containing the bootstrap data for guest OS customization>\", \"publish_location_name\": \"<target location / content library for the published image, optional, e.g. 'cl-6066c61f7931c5ef9'>\" } ] } \nJSON Example with image import:\n{ \"builders\": [ { \"type\": \"vsphere-supervisor\", \"import_source_url\": \"<Remote URL to import image from, optional, e.g. 'https://example.com/example.ovf'>\", \"import_source_ssl_certificate\": \"<SSL certificate of the remote HTTPS server, optional, e.g. '-----BEGIN CERTIFICATE-----xxxxx-----END CERTIFICATE-----'>\", \"import_target_location_name\": \"<Target location / content library for the import image, optional, e.g. 'cl-6066c61f7931c5ef9'>\", \"import_target_image_type\": \"<Target image type of the imported image, optional, e.g. 'ovf'>\", \"import_target_image_name\": \"<Target image name of the imported image for the source VM, e.g. 'ubuntu-impish-21.10-cloudimg'>\", \"class_name\": \"<VM class that describes the virtual hardware settings, e.g. 'best-effort-large'>\", \"storage_class\": \"<Storage class that provides the backing storage for volume, e.g. 'wcplocal-storage-profile'>\", \"bootstrap_provider\": \"<CloudInit, Sysprep, or vAppConfig to customize the guest OS>\", \"bootstrap_data_file\": \"<Path to the file containing the bootstrap data for guest OS customization>\", \"publish_location_name\": \"<target location / content library for the published image, optional, e.g. 'cl-6066c61f7931c5ef9'>\" } ] } \nConfiguration Reference\nThere are various configuration options available for each step in this builder. The required items are listed below as well as the optional configurations.\nRequired:\nclass_name (string) - Name of the VM class that describes virtual hardware settings.\nstorage_class (string) - Name of the storage class that configures storage-related attributes.\nSupervisor Connection\nkubeconfig_path (string) - The path to kubeconfig file for accessing to the vSphere Supervisor cluster. Defaults to the value of KUBECONFIG envvar or $HOME/.kube/config if the envvar is not set.\nsupervisor_namespace (string) - The Supervisor namespace to deploy the source VM. Defaults to the current context's namespace in kubeconfig.\nSource VM Image Importing\nimport_source_url (string) - The remote URL where the to-be-imported image is hosted.\nimport_source_ssl_certificate (string) - The SSL certificate of the remote HTTP server that hosts the to-be-imported image.\nimport_target_location_name (string) - Name of a writable and import-allowed ContentLibrary resource in the namespace where the image will be imported.\nimport_target_image_type (string) - The type of imported image. Defaults to ovf. Available options include ovf.\nimport_target_image_name (string) - Name of the imported image. Defaults to the file name of the image referenced in the source URL.\nimport_request_name (string) - The name of the image import request. Defaults to packer-vsphere-supervisor-import-req-<random-suffix>.\nwatch_import_timeout_sec (int) - The timeout in seconds to wait for the image to be imported. Defaults to 600.\nkeep_import_request (bool) - Preserve the import request in the Supervisor cluster after the build finishes. Defaults to false.\nclean_imported_image (bool) - Clean the imported image after the build finishes. If set to true, the imported image will be deleted. Defaults to false.\nSource Virtual Machine Creation\nimage_name (string) - Name of the source virtual machine (VM) image. If it is specified, the image with the name will be used for the source VM, otherwise the image name from imported image will be used.\nsource_name (string) - Name of the source VM. Defaults to packer-vsphere-supervisor-<random-suffix>.\nnetwork_type (string) - Name of the network type to attach to the source VM's network interface. Defaults to empty.\nnetwork_name (string) - Name of the network to attach to the source VM's network interface. Defaults to empty.\nkeep_input_artifact (bool) - Preserve all the created objects in Supervisor cluster after the build finishes. Defaults to false.\nbootstrap_provider (string) - Name of the bootstrap provider to use for configuring the source VM. Supported values are CloudInit, Sysprep, and vAppConfig. Defaults to CloudInit.\nbootstrap_data_file (string) - Path to a file with bootstrap configuration data. Required if bootstrap_provider is not set to CloudInit. Defaults to a basic cloud config that sets up the user account from the SSH communicator config.\nSource Virtual Machine Watching\nwatch_source_timeout_sec (int) - The timeout in seconds to wait for the source VM to be ready. Defaults to 1800.\nSource Virtual Machine Publishing\npublish_image_name (string) - The name of the published VM image. If not specified, the vm-operator API will set a default name.\nwatch_publish_timeout_sec (int) - The timeout in seconds to wait for the VM to be published. Defaults to 600.\nCommunicator Configuration\nssh_host (string) - The address to SSH to. This usually is automatically configured by the builder.\nssh_port (int) - The port to connect to SSH. This defaults to 22.\nssh_username (string) - The username to connect to SSH with. Required if using SSH.\nssh_password (string) - A plaintext password to use to authenticate with SSH.\nssh_ciphers ([]string) - This overrides the value of ciphers supported by default by Golang. The default value is [ \"aes128-gcm@openssh.com\", \"chacha20-poly1305@openssh.com\", \"aes128-ctr\", \"aes192-ctr\", \"aes256-ctr\", ]\nValid options for ciphers include: \"aes128-ctr\", \"aes192-ctr\", \"aes256-ctr\", \"aes128-gcm@openssh.com\", \"chacha20-poly1305@openssh.com\", \"arcfour256\", \"arcfour128\", \"arcfour\", \"aes128-cbc\", \"3des-cbc\",\nssh_clear_authorized_keys (bool) - If true, Packer will attempt to remove its temporary key from ~/.ssh/authorized_keys and /root/.ssh/authorized_keys. This is a mostly cosmetic option, since Packer will delete the temporary private key from the host system regardless of whether this is set to true (unless the user has set the -debug flag). Defaults to \"false\"; currently only works on guests with sed installed.\nssh_key_exchange_algorithms ([]string) - If set, Packer will override the value of key exchange (kex) algorithms supported by default by Golang. Acceptable values include: \"curve25519-sha256@libssh.org\", \"ecdh-sha2-nistp256\", \"ecdh-sha2-nistp384\", \"ecdh-sha2-nistp521\", \"diffie-hellman-group14-sha1\", and \"diffie-hellman-group1-sha1\".\nssh_certificate_file (string) - Path to user certificate used to authenticate with SSH. The ~ can be used in path and will be expanded to the home directory of current user.\nssh_pty (bool) - If true, a PTY will be requested for the SSH connection. This defaults to false.\nssh_timeout (duration string | ex: \"1h5m2s\") - The time to wait for SSH to become available. Packer uses this to determine when the machine has booted so this is usually quite long. Example value: 10m. This defaults to 5m, unless ssh_handshake_attempts is set.\nssh_disable_agent_forwarding (bool) - If true, SSH agent forwarding will be disabled. Defaults to false.\nssh_handshake_attempts (int) - The number of handshakes to attempt with SSH once it can connect. This defaults to 10, unless a ssh_timeout is set.\nssh_bastion_host (string) - A bastion host to use for the actual SSH connection.\nssh_bastion_port (int) - The port of the bastion host. Defaults to 22.\nssh_bastion_agent_auth (bool) - If true, the local SSH agent will be used to authenticate with the bastion host. Defaults to false.\nssh_bastion_username (string) - The username to connect to the bastion host.\nssh_bastion_password (string) - The password to use to authenticate with the bastion host.\nssh_bastion_interactive (bool) - If true, the keyboard-interactive used to authenticate with bastion host.\nssh_bastion_private_key_file (string) - Path to a PEM encoded private key file to use to authenticate with the bastion host. The ~ can be used in path and will be expanded to the home directory of current user.\nssh_bastion_certificate_file (string) - Path to user certificate used to authenticate with bastion host. The ~ can be used in path and will be expanded to the home directory of current user.\nssh_file_transfer_method (string) - scp or sftp - How to transfer files, Secure copy (default) or SSH File Transfer Protocol.\nNOTE: Guests using Windows with Win32-OpenSSH v9.1.0.0p1-Beta, scp (the default protocol for copying data) returns a a non-zero error code since the MOTW cannot be set, which cause any file transfer to fail. As a workaround you can override the transfer protocol with SFTP instead ssh_file_transfer_method = \"sftp\".\nssh_proxy_host (string) - A SOCKS proxy host to use for SSH connection\nssh_proxy_port (int) - A port of the SOCKS proxy. Defaults to 1080.\nssh_proxy_username (string) - The optional username to authenticate with the proxy server.\nssh_proxy_password (string) - The optional password to use to authenticate with the proxy server.\nssh_keep_alive_interval (duration string | ex: \"1h5m2s\") - How often to send \"keep alive\" messages to the server. Set to a negative value (-1s) to disable. Example value: 10s. Defaults to 5s.\nssh_read_write_timeout (duration string | ex: \"1h5m2s\") - The amount of time to wait for a remote command to end. This might be useful if, for example, packer hangs on a connection after a reboot. Example: 5m. Disabled by default.\nssh_remote_tunnels ([]string) - \nssh_local_tunnels ([]string) - \ntemporary_key_pair_type (string) - dsa | ecdsa | ed25519 | rsa ( the default )\nSpecifies the type of key to create. The possible values are 'dsa', 'ecdsa', 'ed25519', or 'rsa'.\nNOTE: DSA is deprecated and no longer recognized as secure, please consider other alternatives like RSA or ED25519.\ntemporary_key_pair_bits (int) - Specifies the number of bits in the key to create. For RSA keys, the minimum size is 1024 bits and the default is 4096 bits. Generally, 3072 bits is considered sufficient. DSA keys must be exactly 1024 bits as specified by FIPS 186-2. For ECDSA keys, bits determines the key length by selecting from one of three elliptic curve sizes: 256, 384 or 521 bits. Attempting to use bit lengths other than these three values for ECDSA keys will fail. Ed25519 keys have a fixed length and bits will be ignored.\nNOTE: DSA is deprecated and no longer recognized as secure as specified by FIPS 186-5, please consider other alternatives like RSA or ED25519.\nwinrm_username (string) - The username to use to connect to WinRM.\nwinrm_password (string) - The password to use to connect to WinRM.\nwinrm_host (string) - The address for WinRM to connect to.\nNOTE: If using an Amazon EBS builder, you can specify the interface WinRM connects to via ssh_interface\nwinrm_no_proxy (bool) - Setting this to true adds the remote host:port to the NO_PROXY environment variable. This has the effect of bypassing any configured proxies when connecting to the remote host. Default to false.\nwinrm_port (int) - The WinRM port to connect to. This defaults to 5985 for plain unencrypted connection and 5986 for SSL when winrm_use_ssl is set to true.\nwinrm_timeout (duration string | ex: \"1h5m2s\") - The amount of time to wait for WinRM to become available. This defaults to 30m since setting up a Windows machine generally takes a long time.\nwinrm_use_ssl (bool) - If true, use HTTPS for WinRM.\nwinrm_insecure (bool) - If true, do not check server certificate chain and host name.\nwinrm_use_ntlm (bool) - If true, NTLMv2 authentication (with session security) will be used for WinRM, rather than default (basic authentication), removing the requirement for basic authentication to be enabled within the target guest. Further reading for remote connection authentication can be found here.\nDeprovisioning Tasks\nIf you would like to clean up the virtual machine after the build is complete, you can use the Ansible provisioner to run the following tasks to delete machine-specific files and data.\nbuild { sources = [\"source.vsphere-supervisor.vm\"] provisioner \"ansible\" { playbook_file = \"cleanup-playbook.yml\" } } \n{ \"builders\": [ { \"type\": \"vsphere-supervisor\" } ], \"provisioners\": [ { \"type\": \"ansible\", \"playbook_file\": \"./cleanup-playbook.yml\" } ] } \nContent of cleanup-playbook.yml:\n--- # cleanup-playbook.yml - name: Clean up source virtual machine hosts: default become: true tasks: - name: Truncate machine id file: state: \"{{ item.state }}\" path: \"{{ item.path }}\" owner: root group: root mode: \"{{ item.mode }}\" loop: - { path: /etc/machine-id, state: absent, mode: \"0644\" } - { path: /etc/machine-id, state: touch, mode: \"0644\" } - name: Truncate audit logs file: state: \"{{ item.state }}\" path: \"{{ item.path }}\" owner: root group: utmp mode: \"{{ item.mode }}\" loop: - { path: /var/log/wtmp, state: absent, mode: \"0664\" } - { path: /var/log/lastlog, state: absent, mode: \"0644\" } - { path: /var/log/wtmp, state: touch, mode: \"0664\" } - { path: /var/log/lastlog, state: touch, mode: \"0644\" } - name: Remove cloud-init lib dir and logs file: state: absent path: \"{{ item }}\" loop: - /var/lib/cloud - /var/log/cloud-init.log - /var/log/cloud-init-output.log - /var/run/cloud-init - name: Truncate all remaining log files in /var/log shell: cmd: | find /var/log -type f -iname '*.log' | xargs truncate -s 0 - name: Delete all logrotated log zips shell: cmd: | find /var/log -type f -name '*.gz' -exec rm {} + - name: Find temp files find: depth: 1 file_type: any paths: - /tmp - /var/tmp pattern: \"*\" register: temp_files - name: Reset temp space file: state: absent path: \"{{ item.path }}\" loop: \"{{ temp_files.files }}\" - name: Truncate shell history file: state: absent path: \"{{ item.path }}\" loop: - { path: /root/.bash_history } - { path: \"/home/{{ ansible_env.SUDO_USER | default(ansible_user_id) }}/.bash_history\", }"
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vsphere/latest/components/post-processor/vsphere",
  "text": "VMware vSphere Post-Processor | Integrations | Packer\nThe vSphere plugin is able to create vSphere virtual machines for use with any VMware product.\nOfficial\nHCP Ready\nUpdated 2 years ago\nGitHub\n(opens in new tab)\nType: vsphere\nArtifact BuilderId: packer.post-processor.vsphere\nThis post-processor uploads an artifact to a vSphere endpoint.\nThe artifact must be a VMX, OVA, or OVF file.\nNote: This post-processor is developed to maintain compatibility with VMware vSphere versions until their respective End of General Support dates. For detailed information, refer to the Broadcom Product Lifecycle.\nExamples\nExamples are available in the examples directory of the GitHub repository.\nConfiguration Reference\nThe following configuration options are available for the post-processor.\nRequired:\ncluster (string) - The cluster or ESXi host to upload the virtual machine. This can be either the name of the vSphere cluster or the fully qualified domain name (FQDN) or IP address of the ESXi host.\ndatacenter (string) - The name of the vSphere datacenter object to place the virtual machine. This is not required if resource_pool is specified.\ndatastore (string) - The name of the vSphere datastore to place the virtual machine.\nhost (string) - The fully qualified domain name or IP address of the vCenter Server or ESXi host.\npassword (string) - The password to use to authenticate to the vSphere endpoint.\nusername (string) - The username to use to authenticate to the vSphere endpoint.\ndisk_mode (string) - The disk format of the target virtual machine. One of thin, thick,\nesxi_host (string) - The fully qualified domain name or IP address of the ESXi host to upload the virtual machine. This is not required if host is a vCenter Server.\ninsecure (bool) - Skip the verification of the server certificate. Defaults to false.\noptions ([]string) - Options to send to ovftool when uploading the virtual machine. Use ovftool --help to list all the options available.\noverwrite (bool) - Overwrite existing files. If true, forces overwrites of existing files. Defaults to false.\nresource_pool (string) - The name of the resource pool to place the virtual machine.\nvm_folder (string) - The name of the virtual machine folder path where the virtual machine will be placed.\nvm_name (string) - The name of the virtual machine to be created on the vSphere endpoint.\nvm_network (string) - The name of the network in which to place the virtual machine.\nhardware_version (string) - The maximum virtual hardware version for the deployed virtual machine.\nIt does not upgrade the virtual hardware version of the source VM. Instead, it limits the virtual hardware version of the deployed virtual machine to the specified version. If the source virtual machine's hardware version is higher than the specified version, the deployed virtual machine's hardware version will be downgraded to the specified version.\nIf the source virtual machine's hardware version is lower than or equal to the specified version, the deployed virtual machine's hardware version will be the same as the source virtual machine's.\nThis option is useful when deploying to vCenter Server instance ot an ESXi host whose version is different than the one used to create the artifact.\nRefer to KB 315655 for more information on supported virtual hardware versions.\nmax_retries (int) - The maximum number of times to retry the upload operation if it fails. Defaults to 5.\nkeep_input_artifact (boolean) - Preserve the local virtual machines files, even after importing them to the vSphere endpoint. Defaults to false.\nExample Usage\nThe following is an example of the post-processor used in conjunction with the null builder to upload a VMX to a vSphere cluster. You can also use this post-processor with the VMX artifact from a build.\nAn example is shown below, showing only the post-processor configuration:\nsource \"null\" \"example\" { communicator = \"none\" } build { sources = [ \"source.null.example\" ] post-processors { post-processor \"vsphere\"{ vm_name = \"foo\" host = \"vcenter.example.com\" username = \"administrator@vsphere.local\" password = \"VMw@re1!\" datacenter = \"dc-01\" cluster = \"cluster-01\" datastore = \"datastore-01\" vm_network = \"VM Network\" keep_input_artifact = true } } } \n{ \"builders\": [ { \"type\": \"null\", \"communicator\": \"none\" } ], \"post-processors\": [ [ { \"type\": \"vsphere\", \"vm_name\": \"foo\", \"host\": \"vcenter.example.com\", \"username\": \"administrator@vsphere.local\", \"password\": \"VMw@re1!\", \"datacenter\": \"dc-01\", \"cluster\": \"cluster-01\", \"datastore\": \"datastore-01\", \"vm_network\": \"VM Network\", \"keep_input_artifact\": true } ] ] } \nPrivileges\nThe post-processor uses ovftool and needs several privileges to be able to run ovftool.\nRather than giving Administrator access, you can create a role to give the post-processor the privileges necessary to run.\nBelow is an example role that will work. Please note that this is a user-supplied list so there may be a few extraneous privileges that are not strictly required.\nFor vSphere, the role needs the following privileges:\nDatastore.AllocateSpace\nHost.Config.AdvancedConfig\nHost.Config.NetService\nHost.Config.Network\nNetwork.Assign\nSystem.Anonymous\nSystem.Read\nSystem.View\nVApp.Import\nVirtualMachine.Config.AddNewDisk\nVirtualMachine.Config.AdvancedConfig\nVirtualMachine.Inventory.Delete\nThe role must be authorized on the:\nCluster of the host.\nThe destination folder.\nThe destination datastore.\nThe network to be assigned."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vsphere/latest/components/post-processor/vsphere-template",
  "text": "VMware vSphere Post-Processor | Integrations | Packer\nThe vSphere plugin is able to create vSphere virtual machines for use with any VMware product.\nOfficial\nHCP Ready\nUpdated 2 years ago\nGitHub\n(opens in new tab)\nType: vsphere-template\nArtifact BuilderId: packer.post-processor.vsphere\nThis post-processor uses an artifact from the vmware-iso builder with an ESXi host or an artifact from the vSphere post-processor. It then marks the virtual machine as a template and moves it to your specified path.\nNote: This post-processor is developed to maintain compatibility with VMware vSphere versions until their respective End of General Support dates. For detailed information, refer to the Broadcom Product Lifecycle.\nExamples\nExamples are available in the examples directory of the GitHub repository.\nConfiguration Reference\nThe following configuration options are available for the post-processor.\nRequired:\nhost (string) - The fully qualified domain name or IP address of the vSphere endpoint.\nusername (string) - The username to use to authenticate to the vSphere endpoint.\npassword (string) - The password to use to authenticate to the vSphere endpoint.\ninsecure (bool) - Skip the verification of the server certificate. Defaults to false.\ndatacenter (string) - The name of the datacenter to use. Required when the vCenter Server instance endpoint has more than one datacenter.\ntemplate_name (string) - The name of the template. If not specified, the name of the virtual machine will be used.\nfolder (string) - The name of the virtual machine folder path where the template will be created.\nsnapshot_enable (bool) - Create a snapshot before marking as a template. Defaults to false.\nsnapshot_name (string) - The name of the snapshot. Required when snapshot_enable is true.\nsnapshot_description (string) - A description for the snapshot. Required when snapshot_enable is true.\nreregister_vm (boolean) - Keepe the virtual machine registered after marking as a template.\nkeep_input_artifact (boolean) - This option is not applicable to vsphere-template. For a template to function, the original virtual machine from which it was generated cannot be deleted. Therefore, the vSphere Template post-processor always preserves the original virtual machine.\nNote: If you are getting permission denied errors when trying to mark as a template, but it works in the vSphere UI, set this to false. Default is true.\nExample Usage\nAn example is shown below, showing only the post-processor configuration:\nsource \"null\" \"example\" { communicator = \"none\" } build { sources = [ \"source.null.example\" ] post-processors { post-processor \"vsphere-template\"{ host = \"vcenter.example.com\" insecure = false username = \"administrator@vsphere.local\" password = \"VMw@re1!\" datacenter = \"dc-01\" folder = \"/templates/os/distro\" } } } \n{ \"builders\": [ { \"type\": \"null\", \"communicator\": \"none\" } ], \"post-processors\": [ [ { \"type\": \"vsphere-template\", \"host\": \"vcenter.example.com\", \"insecure\": true, \"username\": \"administrator@vsphere.local\", \"password\": \"VMw@re1!\", \"datacenter\": \"dc-01\", \"folder\": \"/templates/os/distro\" } ] ] } \nUsing the vSphere Template with Local Builders\nOnce the vSphere post-processor takes an artifact from the builder and uploads it to a vSphere endpoint, you may want the virtual machine to be marked as a template.\nbuild { sources = [ \"source.null.example\" ] post-processors { post-processor \"vsphere\" { # ... } post-processor \"vsphere-template\" { # ... } } } \n{ \"builders\": [ { \"type\": \"null\", \"communicator\": \"none\" } ], \"post-processors\": [ [ { \"type\": \"vsphere\", ... }, { \"type\": \"vsphere-template\", ... } ], { \"type\": \"...\", ... } ] } \nIn the example above, the result of each builder is passed through the defined sequence of post-processors starting with the vsphere post-processor which will upload the artifact to a vSphere endpoint. The resulting artifact is then passed on to the vsphere-template post-processor which handles marking a virtual machine as a template.\nIn JSON, note that the vsphere and vsphere-template post-processors can be paired together in their own array.\nPrivileges\nThe post processor needs several privileges to be able to mark the virtual as a template.\nRather than giving full administrator access, you can create a role to give the post-processor the privileges necessary to run.\nBelow is an example role that will work. Please note that this is a user-supplied list so there may be a few extraneous privileges that are not strictly required.\nFor vSphere, the role needs the following privileges:\nDatastore.AllocateSpace\nHost.Config.AdvancedConfig\nHost.Config.NetService\nHost.Config.Network\nNetwork.Assign\nSystem.Anonymous\nSystem.Read\nSystem.View\nVApp.Import\nVirtualMachine.Config.AddNewDisk\nVirtualMachine.Config.AdvancedConfig\nVirtualMachine.Inventory.Delete\nand either (if reregister_vm is false):\nVirtualMachine.Provisioning.MarkAsTemplate\nor (if reregister_vm is true or unset):\nVirtualMachine.Inventory.Register\nVirtualMachine.Inventory.Unregister\nThe role must be authorized on the:\nCluster of the host.\nThe destination folder.\nThe destination datastore.\nThe network to be assigned.\nTroubleshooting\nSome users have reported that vSphere templates created from local vSphere builds get their boot order reset to CD-ROM only instead of the original boot order defined by the template. If this issue affects you, the solution is to set \"bios.hddOrder\": \"scsi0:0\" in your builder's vmx_data."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vsphere/v1.4.0",
  "text": "VMware vSphere (v1.4.0) | Integrations | Packer\nThe vSphere plugin is able to create vSphere virtual machines for use with any VMware product.\nOfficial\nHCP Ready\nUpdated 2 years ago\nGitHub\n(opens in new tab)\nThe vSphere plugin is able to create vSphere virtual machines for use with VMware products.\nTo achieve this, the plugin comes with three builders, and two post-processors to build the virtual machine depending on the strategy you want to use.\nThe Packer Plugin for VMware vSphere is a multi-component plugin can be used with HashiCorp Packer to create virtual machine images for VMware vSphere.\nThe plugin includes three builders which are able to create images, depending on your desired strategy:\nInstallation\nTo install this plugin add this code into your Packer configuration and run packer init\npacker { required_plugins { vsphere = { version = \"~> 1\" source = \"github.com/hashicorp/vsphere\" } } } \nAlternatively, you can use packer plugins install to manage installation of this plugin.\npacker plugins install github.com/hashicorp/vsphere \nComponents\nBuilders\nvsphere-iso - This builder starts from an ISO file and uses the vSphere API to build a virtual machine image on an ESXi host.\nvsphere-clone - This builder clones a virtual machine from an existing template using the uses the vSphere API and then modifies and saves it as a new template.\nvsphere-supervisor - This builder deploys and publishes new virtual machine to a vSphere Supervisor cluster using VM Service.\nPost-Processors\nvsphere - This post-processor uploads an artifact to a vSphere endpoint. The artifact must be a VMX, OVA, or OVF file.\nvsphere-template - This post-processor uses an artifact from the vmware-iso builder with an ESXi host or an artifact from the vSphere post-processor. It then marks the virtual machine as a template and moves it to your specified path.\nDifferences from the Packer Plugin for VMware\nWhile both this plugin and the packer-plugin-vmware are designed to create virtual machine images, there are some key differences:\nPlatforms: This plugin is specifically developed to utilize the VMware vSphere API, facilitating the creation of virtual machine images by integrating with VMware vCenter Server and the VMware vSphere Hypervisor. On the other hand, packer-plugin-vmware supports a variety of platforms including VMware vSphere Hypervisor and desktop virtualization products such as VMware Fusion, VMware Workstation, and VMware Player, though it does not utilize the vSphere API for its operations.\nFocus: This plugin is purpose-built with a focus on VMware vSphere, offering capabilities such as creating virtual machine images, cloning and modifying base virtual machine images, and exporting artifacts in specified locations and formats. In contrast, packer-plugin-vmware includes builders that operate on both VMware vSphere Hypervisor and the aforementioned desktop virtualization products, providing a different set of functionalities, including support for Vagrant.\nPlease refer to the documentation for each plugin to understand the specific capabilities and configuration options."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vsphere/v1.3.0/components/builder/vsphere-iso",
  "text": "VMware vSphere Builder (1.3.0) | Integrations | Packer\nType: vsphere-iso Artifact BuilderId: jetbrains.vsphere\nThis builder uses the vSphere API, and creates virtual machines remotely. It starts from an ISO file and creates new VMs from scratch.\nVMware Player is not required.\nIt uses the official vCenter Server API, and does not require ESXi host modification\nThe builder supports versions following the VMware Product Lifecycle Matrix from General Availability to End of General Support. Builds on versions that are end of support may work, but configuration options may throw errors if they do not exist in the vSphere API for those versions.\nSee example templates in the examples folder.\nThere are many configuration options available for this builder. In addition to the items listed here, you will want to look at the general configuration references for HTTP, Floppy, Boot, Hardware, Output, Run, Shutdown, Communicator, Export, configuration references, which are necessary for this build to succeed and can be found further down the page.\ncreate_snapshot (bool) - Create a snapshot of the virtual machine to use as a base for linked clones. Defaults to false.\nsnapshot_name (string) - The name of the snapshot when create_snapshot is true. Defaults to Created By Packer.\nconvert_to_template (bool) - Convert the virtual machine to a template after the build is complete. Defaults to false. If set to true, the virtual machine can not be imported into a content library.\nexport (*common.ExportConfig) - The configuration for exporting the virtual machine to an OVF. The virtual machine is not exported if export configuration is not specified.\ncontent_library_destination (*common.ContentLibraryDestinationConfig) - Import the virtual machine as a VM template or OVF template to a content library. The template will not be imported if no content library import configuration is specified. If set, convert_to_template must be set to false.\nlocal_cache_overwrite (bool) - Overwrite files in the local cache if they already exist. Defaults to false.\nremote_cache_cleanup (bool) - Cleanup items added to the remote cache after the build is complete. Defaults to false.\nNote: If the local cache overwrite flag is set to true, RemoteCacheOverwrite will implicitly be set to true. This is to ensure consistency between the local and remote cache.\nremote_cache_overwrite (bool) - Overwrite files in the remote cache if they already exist. Defaults to false.\nremote_cache_datastore (string) - The remote cache datastore to use for the build. If not set, the datastore of the virtual machine is used.\nremote_cache_path (string) - The directory path on the remote cache datastore to use for the build. If not set, the default path is packer_cache/.\nBoot Configuration\nThe boot configuration is very important: boot_command specifies the keys to type when the virtual machine is first booted in order to start the OS installer. This command is typed after boot_wait, which gives the virtual machine some time to actually load.\nThe boot_command is an array of strings. The strings are all typed in sequence. It is an array only to improve readability within the template.\nThere are a set of special keys available. If these are in your boot command, they will be replaced by the proper key:\n<bs> - Backspace\n<del> - Delete\n<enter> <return> - Simulates an actual \"enter\" or \"return\" keypress.\n<esc> - Simulates pressing the escape key.\n<tab> - Simulates pressing the tab key.\n<f1> - <f12> - Simulates pressing a function key.\n<up> <down> <left> <right> - Simulates pressing an arrow key.\n<spacebar> - Simulates pressing the spacebar.\n<insert> - Simulates pressing the insert key.\n<home> <end> - Simulates pressing the home and end keys.\n<pageUp> <pageDown> - Simulates pressing the page up and page down keys.\n<menu> - Simulates pressing the Menu key.\n<leftAlt> <rightAlt> - Simulates pressing the alt key.\n<leftCtrl> <rightCtrl> - Simulates pressing the ctrl key.\n<leftShift> <rightShift> - Simulates pressing the shift key.\n<leftSuper> <rightSuper> - Simulates pressing the  or Windows key.\n<wait> <wait5> <wait10> - Adds a 1, 5 or 10 second pause before sending any additional keys. This is useful if you have to generally wait for the UI to update before typing more.\n<waitXX> - Add an arbitrary pause before sending any additional keys. The format of XX is a sequence of positive decimal numbers, each with optional fraction and a unit suffix, such as 300ms, 1.5h or 2h45m. Valid time units are ns, us (or s), ms, s, m, h. For example <wait10m> or <wait1m20s>.\n<XXXOn> <XXXOff> - Any printable keyboard character, and of these \"special\" expressions, with the exception of the <wait> types, can also be toggled on or off. For example, to simulate ctrl+c, use <leftCtrlOn>c<leftCtrlOff>. Be sure to release them, otherwise they will be held down until the machine reboots. To hold the c key down, you would use <cOn>. Likewise, <cOff> to release.\n{{ .HTTPIP }} {{ .HTTPPort }} - The IP and port, respectively of an HTTP server that is started serving the directory specified by the http_directory configuration parameter. If http_directory isn't specified, these will be blank!\n{{ .Name }} - The name of the VM.\nExample boot command. This is actually a working boot command used to start an CentOS 6.4 installer:\nIn JSON:\n\"boot_command\": [ \"<tab><wait>\", \" ks=http://{{ .HTTPIP }}:{{ .HTTPPort }}/centos6-ks.cfg<enter>\" ] \nIn HCL2:\nboot_command = [ \"<tab><wait>\", \" ks=http://{{ .HTTPIP }}:{{ .HTTPPort }}/centos6-ks.cfg<enter>\" ] \nThe example shown below is a working boot command used to start an Ubuntu 12.04 installer:\nIn JSON:\n\"boot_command\": [ \"<esc><esc><enter><wait>\", \"/install/vmlinuz noapic \", \"preseed/url=http://{{ .HTTPIP }}:{{ .HTTPPort }}/preseed.cfg \", \"debian-installer=en_US auto locale=en_US kbd-chooser/method=us \", \"hostname={{ .Name }} \", \"fb=false debconf/frontend=noninteractive \", \"keyboard-configuration/modelcode=SKIP keyboard-configuration/layout=USA \", \"keyboard-configuration/variant=USA console-setup/ask_detect=false \", \"initrd=/install/initrd.gz -- <enter>\" ] \nIn HCL2:\nboot_command = [ \"<esc><esc><enter><wait>\", \"/install/vmlinuz noapic \", \"preseed/url=http://{{ .HTTPIP }}:{{ .HTTPPort }}/preseed.cfg \", \"debian-installer=en_US auto locale=en_US kbd-chooser/method=us \", \"hostname={{ .Name }} \", \"fb=false debconf/frontend=noninteractive \", \"keyboard-configuration/modelcode=SKIP keyboard-configuration/layout=USA \", \"keyboard-configuration/variant=USA console-setup/ask_detect=false \", \"initrd=/install/initrd.gz -- <enter>\" ] \nFor more examples of various boot commands, see the sample projects from our community templates page.\nWe send each character to the VM with a default delay of 100ms between groups. The delay alleviates possible issues with latency and CPU contention. If you notice missing keys, you can tune this delay by specifying \"boot_keygroup_interval\" in your Packer template, for example:\nJSON\n{ \"builders\": [ { \"type\": \"vsphere-iso\", \"boot_keygroup_interval\": \"500ms\" ... } ] } \nHCL2\nsource \"vsphere-iso\" \"example\" { boot_keygroup_interval = \"500ms\" # ... } \nboot_keygroup_interval (duration string | ex: \"1h5m2s\") - Time to wait after sending a group of key pressses. The value of this should be a duration. Examples are 5s and 1m30s which will cause Packer to wait five seconds and one minute 30 seconds, respectively. If this isn't specified, a sensible default value is picked depending on the builder type.\nboot_wait (duration string | ex: \"1h5m2s\") - The time to wait after booting the initial virtual machine before typing the boot_command. The value of this should be a duration. Examples are 5s and 1m30s which will cause Packer to wait five seconds and one minute 30 seconds, respectively. If this isn't specified, the default is 10s or 10 seconds. To set boot_wait to 0s, use a negative number, such as \"-1s\"\nboot_command ([]string) - This is an array of commands to type when the virtual machine is first booted. The goal of these commands should be to type just enough to initialize the operating system installer. Special keys can be typed as well, and are covered in the section below on the boot command. If this is not specified, it is assumed the installer will start itself.\nhttp_ip (string) - The IP address to use for the HTTP server started to serve the http_directory. If unset, Packer will automatically discover and assign an IP.\nHttp directory configuration\nPacker will create an http server serving http_directory when it is set, a random free port will be selected and the architecture of the directory referenced will be available in your builder.\nExample usage from a builder:\nwget http://{{ .HTTPIP }}:{{ .HTTPPort }}/foo/bar/preseed.cfg \nhttp_directory (string) - Path to a directory to serve using an HTTP server. The files in this directory will be available over HTTP that will be requestable from the virtual machine. This is useful for hosting kickstart files and so on. By default this is an empty string, which means no HTTP server will be started. The address and port of the HTTP server will be available as variables in boot_command. This is covered in more detail below.\nhttp_content (map[string]string) - Key/Values to serve using an HTTP server. http_content works like and conflicts with http_directory. The keys represent the paths and the values contents, the keys must start with a slash, ex: /path/to/file. http_content is useful for hosting kickstart files and so on. By default this is empty, which means no HTTP server will be started. The address and port of the HTTP server will be available as variables in boot_command. This is covered in more detail below. Example:\nhttp_content = { \"/a/b\" = file(\"http/b\") \"/foo/bar\" = templatefile(\"${path.root}/preseed.cfg\", { packages = [\"nginx\"] }) } \nhttp_port_min (int) - These are the minimum and maximum port to use for the HTTP server started to serve the http_directory. Because Packer often runs in parallel, Packer will choose a randomly available port in this range to run the HTTP server. If you want to force the HTTP server to be on one port, make this minimum and maximum port the same. By default the values are 8000 and 9000, respectively.\nhttp_port_max (int) - HTTP Port Max\nhttp_bind_address (string) - This is the bind address for the HTTP server. Defaults to 0.0.0.0 so that it will work with any network interface.\nFloppy configuration\nfloppy_img_path (string) - Datastore path to a floppy image that will be mounted to the VM. Example: [datastore1] ISO/pvscsi-Windows8.flp.\nfloppy_files ([]string) - List of local files to be mounted to the VM floppy drive. Can be used to make Debian preseed or RHEL kickstart files available to the VM.\nfloppy_dirs ([]string) - List of directories to copy files from.\nfloppy_content (map[string]string) - Key/Values to add to the floppy disk. The keys represent the paths, and the values contents. It can be used alongside floppy_files or floppy_dirs, which is useful to add large files without loading them into memory. If any paths are specified by both, the contents in floppy_content will take precedence.\nUsage example (HCL):\nfloppy_content = { \"meta-data\" = jsonencode(local.instance_data) \"user-data\" = templatefile(\"user-data\", { packages = [\"nginx\"] }) } \nfloppy_label (string) - The label to use for the floppy disk that is attached when the VM is booted. This is most useful for cloud-init, Kickstart or other early initialization tools, which can benefit from labelled floppy disks. By default, the floppy label will be 'packer'.\nConnection Configuration\nvcenter_server (string) - vCenter Server hostname.\nusername (string) - vSphere username.\npassword (string) - vSphere password.\ninsecure_connection (bool) - Do not validate the vCenter Server TLS certificate. Defaults to false.\ndatacenter (string) - vSphere datacenter name. Required if there is more than one datacenter in the vSphere inventory.\nHardware Configuration\nCPUs (int32) - Number of CPU cores.\ncpu_cores (int32) - Number of CPU cores per socket.\nCPU_reservation (int64) - Amount of reserved CPU resources in MHz.\nCPU_limit (int64) - Upper limit of available CPU resources in MHz.\nCPU_hot_plug (bool) - Enable CPU hot plug setting for virtual machine. Defaults to false.\nRAM (int64) - Amount of RAM in MB.\nRAM_reservation (int64) - Amount of reserved RAM in MB.\nRAM_reserve_all (bool) - Reserve all available RAM. Defaults to false. Cannot be used together with RAM_reservation.\nRAM_hot_plug (bool) - Enable RAM hot plug setting for virtual machine. Defaults to false.\nvideo_ram (int64) - Amount of video memory in KB. See vSphere documentation for supported maximums. Defaults to 4096 KB.\ndisplays (int32) - Number of video displays. See vSphere documentation for supported maximums. Defaults to 1.\npci_passthrough_allowed_device ([]PCIPassthroughAllowedDevice) - Configure Dynamic DirectPath I/O PCI Passthrough for virtual machine. See vSphere documentation\nvgpu_profile (string) - vGPU profile for accelerated graphics. vGPU profile for accelerated graphics. See NVIDIA GRID vGPU documentation for examples of profile names. Defaults to none.\nNestedHV (bool) - Enable nested hardware virtualization for VM. Defaults to false.\nfirmware (string) - Set the Firmware for virtual machine. Supported values: bios, efi or efi-secure. Defaults to bios.\nforce_bios_setup (bool) - During the boot, force entry into the BIOS setup screen. Defaults to false.\nvTPM (bool) - Add virtual TPM device for virtual machine. Defaults to false.\nprecision_clock (string) - Add a precision clock device for virtual machine. Defaults to none.\nPCI Passthrough Configuration\nDynamic DirectPath I/O is component of the Assignable Hardware framework in VMware vSphere. Dynamic DirectPath I/O enables the Assignable Hardware intelligence for passthrough devices and the hardware address of the PCIe device is no longer directly mapped to the virtual machine configuration. Instead, the attributes, or capabilities, are exposed to the virtual machine.\nJSON\n{ \"pci_passthrough_allowed_device\": { \"vendor_id\": \"8086\", \"device_id\": \"100e\", \"sub_device_id\": \"8086\", \"sub_vendor_id\": \"100e\" } } \nHCL2\npci_passthrough_allowed_device { \"vendor_id\": \"8086\", \"device_id\": \"100e\", \"sub_device_id\": \"8086\", \"sub_vendor_id\": \"100e\" } \nvendor_id (string) - The sub-vendor ID of the PCI device.\ndevice_id (string) - The vendor ID of the PCI device.\nsub_vendor_id (string) - The sub-vendor ID of the PCI device.\nsub_device_id (string) - The sub-device ID of the PCI device.\nLocation Configuration\nvm_name (string) - Name of the virtual machine.\nfolder (string) - VM folder where the virtual machine is created.\ncluster (string) - vSphere cluster where the virtual machine is created. See the Working With Clusters And Hosts section above for more details.\nhost (string) - ESXi host where the virtual machine is created. A full path must be specified if the host is in a folder. For example folder/host. See the Working With Clusters And Hosts section above for more details.\nresource_pool (string) - vSphere resource pool where the virtual machine is created. If this is not specified, the root resource pool associated with the host or cluster is used. Note that the full path to the resource pool must be provided. For example, a simple resource pool path might resemble rp-packer and a nested path might resemble 'rp-packer/rp-linux-images'.\ndatastore (string) - vSphere datastore where the virtual machine is created. Required if host is a cluster, or if host has multiple datastores.\nset_host_for_datastore_uploads (bool) - Specifies that the host is used for uploading files to the datastore. Defaults to false.\nRun Configuration\nboot_order (string) - Priority of boot devices. Defaults to disk,cdrom\nShutdown Configuration\nshutdown_command (string) - Specify a VM guest shutdown command. This command will be executed using the communicator. Otherwise, the VMware Tools are used to gracefully shutdown the VM.\nshutdown_timeout (duration string | ex: \"1h5m2s\") - Amount of time to wait for graceful VM shutdown. Defaults to 5m or five minutes. This will likely need to be modified if the communicator is 'none'.\ndisable_shutdown (bool) - Packer normally halts the virtual machine after all provisioners have run when no shutdown_command is defined. If this is set to true, Packer will not halt the virtual machine but will assume that you will send the stop signal yourself through a preseed.cfg, a script or the final provisioner. Packer will wait for a default of five minutes until the virtual machine is shutdown. The timeout can be changed using shutdown_timeout option.\nWait Configuration\nip_wait_timeout (duration string | ex: \"1h5m2s\") - Amount of time to wait for VM's IP, similar to 'ssh_timeout'. Defaults to 30m (30 minutes). See the Golang ParseDuration documentation for full details.\nip_settle_timeout (duration string | ex: \"1h5m2s\") - Amount of time to wait for VM's IP to settle down, sometimes VM may report incorrect IP initially, then its recommended to set that parameter to apx. 2 minutes. Examples 45s and 10m. Defaults to 5s(5 seconds). See the Golang ParseDuration documentation for full details.\nip_wait_address (*string) - Set this to a CIDR address to cause the service to wait for an address that is contained in this network range. Defaults to \"0.0.0.0/0\" for any ipv4 address. Examples include:\nempty string (\"\") - remove all filters\n0:0:0:0:0:0:0:0/0 - allow only ipv6 addresses\n192.168.1.0/24 - only allow ipv4 addresses from 192.168.1.1 to 192.168.1.254\nISO Configuration\nBy default, Packer will symlink, download or copy image files to the Packer cache into a \"hash($iso_url+$iso_checksum).$iso_target_extension\" file. Packer uses hashicorp/go-getter in file mode in order to perform a download.\ngo-getter supports the following protocols:\nLocal files\nGit\nMercurial\nAmazon S3\nExamples: go-getter can guess the checksum type based on iso_checksum length, and it is also possible to specify the checksum type.\n\"iso_checksum\": \"946a6077af6f5f95a51f82fdc44051c7aa19f9cfc5f737954845a6050543d7c2\", \"iso_url\": \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \n\"iso_checksum\": \"file:ubuntu.org/..../ubuntu-14.04.1-server-amd64.iso.sum\", \"iso_url\": \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \n\"iso_checksum\": \"file://./shasums.txt\", \"iso_url\": \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \n\"iso_checksum\": \"file:./shasums.txt\", \"iso_url\": \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \niso_checksum = \"946a6077af6f5f95a51f82fdc44051c7aa19f9cfc5f737954845a6050543d7c2\" iso_url = \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \niso_checksum = \"file:ubuntu.org/..../ubuntu-14.04.1-server-amd64.iso.sum\" iso_url = \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \niso_checksum = \"file://./shasums.txt\" iso_url = \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \niso_checksum = \"file:./shasums.txt\", iso_url = \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \niso_checksum (string) - The checksum for the ISO file or virtual hard drive file. The type of the checksum is specified within the checksum field as a prefix, ex: \"md5:{$checksum}\". The type of the checksum can also be omitted and Packer will try to infer it based on string length. Valid values are \"none\", \"{$checksum}\", \"md5:{$checksum}\", \"sha1:{$checksum}\", \"sha256:{$checksum}\", \"sha512:{$checksum}\" or \"file:{$path}\". Here is a list of valid checksum values:\nmd5:090992ba9fd140077b0661cb75f7ce13\n090992ba9fd140077b0661cb75f7ce13\nsha1:ebfb681885ddf1234c18094a45bbeafd91467911\nebfb681885ddf1234c18094a45bbeafd91467911\nsha256:ed363350696a726b7932db864dda019bd2017365c9e299627830f06954643f93\ned363350696a726b7932db864dda019bd2017365c9e299627830f06954643f93\nfile:http://releases.ubuntu.com/20.04/SHA256SUMS\nfile:file://./local/path/file.sum\nfile:./local/path/file.sum\nnone Although the checksum will not be verified when it is set to \"none\", this is not recommended since these files can be very large and corruption does happen from time to time.\niso_url (string) - A URL to the ISO containing the installation image or virtual hard drive (VHD or VHDX) file to clone.\niso_urls ([]string) - Multiple URLs for the ISO to download. Packer will try these in order. If anything goes wrong attempting to download or while downloading a single URL, it will move on to the next. All URLs must point to the same file (same checksum). By default this is empty and iso_url is used. Only one of iso_url or iso_urls can be specified.\niso_target_path (string) - The path where the iso should be saved after download. By default will go in the packer cache, with a hash of the original filename and checksum as its name.\niso_target_extension (string) - The extension of the iso file after download. This defaults to iso.\nCDRom Configuration\nEach iso defined in the CDRom Configuration adds a new drive. If the \"iso_url\" is defined in addition to the \"iso_paths\", the \"iso_url\" is added to the VM first. This keeps the \"iso_url\" first in the boot order by default allowing the boot iso being defined by the iso_url and the vmware tools iso added from the datastore. Example:\nJSON\n\"iso_urls\": [ \"win10.iso\", \"http://example.org/isos/win10.iso\" ], \"iso_paths\": [ \"[] /usr/lib/vmware/isoimages/windows.iso\" ], \nHCL2\niso_urls = [ \"win10.iso\", \"http://example.org/isos/win10.iso\" ] iso_paths = [ \"[] /usr/lib/vmware/isoimages/windows.iso\" ] \ncdrom_type (string) - Which controller to use. Example: sata. Defaults to ide.\niso_paths ([]string) - A list of paths to ISO files in either a datastore or a content library that will be mounted to the VM.\nUsage example (HCL):\niso_paths = [ \"[nfs] iso/ubuntu-server-amd64.iso\", \"Packer/ubuntu-server-amd64/ubuntu-server-amd64.iso\" ] \nTwo ISOs are referenced:\nAn ISO in the \"iso\" folder of the \"nfs\" datastore with the file name of \"ubuntu-server-amd64.iso\".\nAn ISO in the \"Packer\" content library with the item name of \"ubuntu-server-amd64\".\nNote: All files in a content library have an associated item name. To determine the file name, view the datastore backing the content library or use the govc vSphere CLI.\nremove_cdrom (bool) - Remove CD-ROM devices from template. Defaults to false.\nreattach_cdroms (int) - Reattach one or more configured CD-ROM devices. Range: 1-4. You can reattach up to 4 CD-ROM devices to the final build artifact. If set to 0, reattach_cdroms is ignored and the step is skipped. When set to a value in the range, remove_cdrom is ignored and the CD-ROM devices are kept without any attached media.\nAn iso (CD) containing custom files can be made available for your build.\nBy default, no extra CD will be attached. All files listed in this setting get placed into the root directory of the CD and the CD is attached as the second CD device.\nThis config exists to work around modern operating systems that have no way to mount floppy disks, which was our previous go-to for adding files at boot time.\ncd_files ([]string) - A list of files to place onto a CD that is attached when the VM is booted. This can include either files or directories; any directories will be copied onto the CD recursively, preserving directory structure hierarchy. Symlinks will have the link's target copied into the directory tree on the CD where the symlink was. File globbing is allowed.\nUsage example (JSON):\n\"cd_files\": [\"./somedirectory/meta-data\", \"./somedirectory/user-data\"], \"cd_label\": \"cidata\", \ncd_files = [\"./somedirectory/meta-data\", \"./somedirectory/user-data\"] cd_label = \"cidata\" \nThe above will create a CD with two files, user-data and meta-data in the CD root. This specific example is how you would create a CD that can be used for an Ubuntu 20.04 autoinstall.\nSince globbing is also supported,\ncd_files = [\"./somedirectory/*\"] cd_label = \"cidata\" \nWould also be an acceptable way to define the above cd. The difference between providing the directory with or without the glob is whether the directory itself or its contents will be at the CD root.\nUse of this option assumes that you have a command line tool installed that can handle the iso creation. Packer will use one of the following tools:\nxorriso\nmkisofs\nhdiutil (normally found in macOS)\noscdimg (normally found in Windows as part of the Windows ADK)\ncd_content (map[string]string) - Key/Values to add to the CD. The keys represent the paths, and the values contents. It can be used alongside cd_files, which is useful to add large files without loading them into memory. If any paths are specified by both, the contents in cd_content will take precedence.\ncd_files = [\"vendor-data\"] cd_content = { \"meta-data\" = jsonencode(local.instance_data) \"user-data\" = templatefile(\"user-data\", { packages = [\"nginx\"] }) } cd_label = \"cidata\" \ncd_label (string) - CD Label\nCreate Configuration\nvm_version (uint) - Specifies the virtual machine hardware version. Defaults to the most current virtual machine hardware version supported by the ESXi host. Refer to KB 315655 for more information on supported virtual hardware versions.\nguest_os_type (string) - Specifies the guest operating system identifier for the virtual machine. If not specified, the setting defaults to otherGuest.\nTo get a list of supported guest operating system identifiers for your ESXi host, run the following PowerShell command using VMware.PowerCLI:\nConnect-VIServer -Server \"vc.example.com\" -User \"administrator@vsphere\" -Password \"password\" $esxiHost = Get-VMHost -Name \"esxi.example.com\" $environmentBrowser = Get-View -Id $esxiHost.ExtensionData.Parent.ExtensionData.ConfigManager.EnvironmentBrowser $vmxVersion = ($environmentBrowser.QueryConfigOptionDescriptor() | Where-Object DefaultConfigOption).Key $osDescriptor = $environmentBrowser.QueryConfigOption($vmxVersion, $null).GuestOSDescriptor $osDescriptor | Select-Object Id, Fullname \nnetwork_adapters ([]NIC) - Specifies the network adapters for the virtual machine. If no network adapter is defined, all network-related operations will be skipped.\nusb_controller ([]string) - Specifies the USB controllers for the virtual machine. Use usb for a USB 2.0 controller and `xhci`` for a USB 3.0 controller. -> Note: Maximum of one controller of each type.\nnotes (string) - Specifies the annotations for the virtual machine.\ndestroy (bool) - Specifies whether to destroy the virtual machine after the build is complete.\ndisk_controller_type ([]string) - Set VM disk controller type. Example lsilogic, lsilogic-sas, pvscsi, nvme, or scsi. Use a list to define additional controllers. Defaults to lsilogic. See SCSI, SATA, and NVMe Storage Controller Conditions, Limitations, and Compatibility for additional details.\nstorage ([]DiskConfig) - Configures a collection of one or more disks to be provisioned along with the VM. See the Storage Configuration.\nNetwork Adapter Configuration\nDefines a Network Adapter If no adapter is defined, network tasks (communicators, most provisioners) won't work, so it's advised to define one.\nExample that creates two network adapters:\n\"network_adapters\": [ { \"network\": \"VM Network\", \"network_card\": \"vmxnet3\" }, { \"network\": \"OtherNetwork\", \"network_card\": \"vmxnet3\" } ], \nnetwork_adapters { network = \"VM Network\" network_card = \"vmxnet3\" } network_adapters { network = \"OtherNetwork\" network_card = \"vmxnet3\" } \nnetwork_card (string) - Specifies the virtual machine network card type. For example vmxnet3.\nremove_network_adapter (bool) - Remove all network adapters from template. Defaults to false.\nOptional\nnetwork (string) - Specifies the network to which the virtual machine will connect.\nName: <NetworkName>\nInventory Path: /<DatacenterName>/<FolderName>/<NetworkName>\nManaged Object ID (Port Group): Network:network-<xxxxx>\nManaged Object ID (Distributed Port Group): DistributedVirtualPortgroup::dvportgroup-<xxxxx>\nLogical Switch UUID: <uuid>\nSegment ID: /infra/segments/<SegmentID>\nNote: If more than one network resolves to the same name, either the inventory path to network or an ID must be provided.\nNote: If no network is specified, provide host to allow the plugin to search for an available network.\nmac_address (string) - Specifies the network card MAC address. For example 00:50:56:00:00:00.\npassthrough (*bool) - Specifies whether to enable DirectPath I/O passthrough for the network device.\nStorage Configuration\nDefines the disk storage for a VM.\nExample that will create a 15GB and a 20GB disk on the VM. The second disk will be thin provisioned:\n\"storage\": [ { \"disk_size\": 15000 }, { \"disk_size\": 20000, \"disk_thin_provisioned\": true } ], \nstorage { disk_size = 15000 } storage { disk_size = 20000 disk_thin_provisioned = true } \nExample that creates 2 pvscsi controllers and adds 2 disks to each one:\n\"disk_controller_type\": [\"pvscsi\", \"pvscsi\"], \"storage\": [ { \"disk_size\": 15000, \"disk_controller_index\": 0 }, { \"disk_size\": 15000, \"disk_controller_index\": 0 }, { \"disk_size\": 15000, \"disk_controller_index\": 1 }, { \"disk_size\": 15000, \"disk_controller_index\": 1 } ], \ndisk_controller_type = [\"pvscsi\", \"pvscsi\"] storage { disk_size = 15000, disk_controller_index = 0 } storage { disk_size = 15000 disk_controller_index = 0 } storage { disk_size = 15000 disk_controller_index = 1 } storage { disk_size = 15000 disk_controller_index = 1 } \ndisk_size (int64) - The size of the disk in MiB.\nOptional\ndisk_thin_provisioned (bool) - Enable VMDK thin provisioning for VM. Defaults to false.\ndisk_eagerly_scrub (bool) - Enable VMDK eager scrubbing for VM. Defaults to false.\ndisk_controller_index (int) - The assigned disk controller. Defaults to the first one (0).\nExport Configuration\nYou can export an image in Open Virtualization Format (OVF) to the Packer host.\nExample usage:\n... \"vm_name\": \"example-ubuntu\", ... \"export\": { \"force\": true, \"output_directory\": \"./output-artifacts\" }, \n# ... vm_name = \"example-ubuntu\" # ... export { force = true output_directory = \"./output-artifacts\" } \nThe above configuration would create the following files:\n./output-artifacts/example-ubuntu-disk-0.vmdk ./output-artifacts/example-ubuntu.mf ./output-artifacts/example-ubuntu.ovf \nname (string) - Name of the exported image in Open Virtualization Format (OVF). The name of the virtual machine with the .ovf extension is used if this option is not specified.\nforce (bool) - Forces the export to overwrite existing files. Defaults to false. If set to false, an error is returned if the file(s) already exists.\nimage_files (bool) - Include additional image files that are that are associated with the virtual machine. Defaults to false. For example, .nvram and .log files.\nmanifest (string) - Generate a manifest file with the specified hash algorithm. Defaults to sha256. Available options include none, sha1, sha256, and sha512. Use none for no manifest.\noptions ([]string) - Advanced image export options. Available options can include:\nmac - MAC address is exported for each Ethernet device.\nuuid - UUID is exported for the virtual machine.\nextraconfig - Extra configuration options are exported for the virtual machine.\nnodevicesubtypes - Resource subtypes for CD/DVD drives, floppy drives, and serial and parallel ports are not exported.\nFor example, adding the following export config option outputs the MAC addresses for each Ethernet device in the OVF descriptor:\n... \"export\": { \"options\": [\"mac\"] }, \n... export { options = [\"mac\"] } \noutput_format (string) - The output format for the exported virtual machine image. Defaults to ovf. Available options include ovf and ova.\nWhen set to ova, the image is first exported using Open Virtualization / Format (.ovf) and then converted to an Open Virtualization Archive (.ova) using the VMware Open Virtualization Format Tool (ovftool). The intermediate files are removed after the conversion.\nNote: To use the ova format option, VMware ovftool must be installed on the Packer host and accessible in either the system PATH or the user's PATH.\nOutput Configuration:\noutput_directory (string) - This setting specifies the directory that artifacts from the build, such as the virtual machine files and disks, will be output to. The path to the directory may be relative or absolute. If relative, the path is relative to the working directory packer is executed from. This directory must not exist or, if created, must be empty prior to running the builder. By default this is \"output-BUILDNAME\" where \"BUILDNAME\" is the name of the build.\ndirectory_permission (os.FileMode) - The permissions to apply to the \"output_directory\", and to any parent directories that get created for output_directory. By default this is \"0750\". You should express the permission as quoted string with a leading zero such as \"0755\" in JSON file, because JSON does not support octal value. In Unix-like OS, the actual permission may differ from this value because of umask.\nContent Library Import Configuration\nWith this configuration Packer creates a library item in a content library whose content is a VM template or an OVF template created from the just built VM. The template is stored in a existing or newly created library item.\nlibrary (string) - Name of the library in which the new library item containing the template should be created/updated. The Content Library should be of type Local to allow deploying virtual machines.\nname (string) - Name of the library item that will be created or updated. For VM templates, the name of the item should be different from vm_name and the default is vm_name + timestamp when not set. VM templates will be always imported to a new library item. For OVF templates, the name defaults to vm_name when not set, and if an item with the same name already exists it will be then updated with the new OVF template, otherwise a new item will be created.\nNote: It's not possible to update existing library items with a new VM template. If updating an existing library item is necessary, use an OVF template instead by setting the ovf option as true.\ndescription (string) - Description of the library item that will be created. Defaults to \"Packer imported vm_name VM template\".\ncluster (string) - Cluster onto which the virtual machine template should be placed. If cluster and resource_pool are both specified, resource_pool must belong to cluster. If cluster and host are both specified, host must be a member of cluster. This option is not used when importing OVF templates. Defaults to cluster.\nfolder (string) - Virtual machine folder into which the virtual machine template should be placed. This option is not used when importing OVF templates. Defaults to the same folder as the source virtual machine.\nhost (string) - Host onto which the virtual machine template should be placed. If host and resource_pool are both specified, resource_pool must belong to host. If host and cluster are both specified, host must be a member of cluster. This option is not used when importing OVF templates. Defaults to host.\nresource_pool (string) - Resource pool into which the virtual machine template should be placed. Defaults to resource_pool. if resource_pool is also unset, the system will attempt to choose a suitable resource pool for the virtual machine template.\ndatastore (string) - The datastore for the virtual machine template's configuration and log files. This option is not used when importing OVF templates. Defaults to the storage backing associated with the library specified by library.\ndestroy (bool) - If set to true, the VM will be destroyed after deploying the template to the Content Library. Defaults to false.\novf (bool) - When set to true, Packer will import and OVF template to the content library item. Defaults to false.\nskip_import (bool) - When set to true, the VM won't be imported to the content library item. Useful for setting to true during a build test stage. Defaults to false.\novf_flags ([]string) - Flags to use for OVF package creation. The supported flags can be obtained using ExportFlag.list. If unset, no flags will be used. Known values: EXTRA_CONFIG, PRESERVE_MAC\nMinimal example of usage to import a VM template:\nJSON\n\"content_library_destination\" : { \"library\": \"Packer Library Test\" } \nHCL2\ncontent_library_destination { library = \"Packer Library Test\" } \nMinimal example of usage to import a OVF template:\n\"content_library_destination\" : { \"library\": \"Packer Library Test\", \"ovf\": true } \ncontent_library_destination { library = \"Packer Library Test\" ovf = true } \nExtra Configuration Parameters\nconfiguration_parameters (map[string]string) - configuration_parameters is a direct passthrough to the vSphere API's VirtualMachineConfigSpec\ntools_sync_time (bool) - Enables time synchronization with the host. Defaults to false.\ntools_upgrade_policy (bool) - If sets to true, vSphere will automatically check and upgrade VMware Tools upon a system power cycle. If not set, defaults to manual upgrade.\nCommunicator configuration\nOptional common fields:\ncommunicator (string) - Packer currently supports three kinds of communicators:\nnone - No communicator will be used. If this is set, most provisioners also can't be used.\nssh - An SSH connection will be established to the machine. This is usually the default.\nwinrm - A WinRM connection will be established.\nIn addition to the above, some builders have custom communicators they can use. For example, the Docker builder has a \"docker\" communicator that uses docker exec and docker cp to execute scripts and copy files.\npause_before_connecting (duration string | ex: \"1h5m2s\") - We recommend that you enable SSH or WinRM as the very last step in your guest's bootstrap script, but sometimes you may have a race condition where you need Packer to wait before attempting to connect to your guest.\nIf you end up in this situation, you can use the template option pause_before_connecting. By default, there is no pause. For example if you set pause_before_connecting to 10m Packer will check whether it can connect, as normal. But once a connection attempt is successful, it will disconnect and then wait 10 minutes before connecting to the guest and beginning provisioning.\nOptional SSH fields:\nssh_host (string) - The address to SSH to. This usually is automatically configured by the builder.\nssh_port (int) - The port to connect to SSH. This defaults to 22.\nssh_username (string) - The username to connect to SSH with. Required if using SSH.\nssh_password (string) - A plaintext password to use to authenticate with SSH.\nssh_ciphers ([]string) - This overrides the value of ciphers supported by default by Golang. The default value is [ \"aes128-gcm@openssh.com\", \"chacha20-poly1305@openssh.com\", \"aes128-ctr\", \"aes192-ctr\", \"aes256-ctr\", ]\nValid options for ciphers include: \"aes128-ctr\", \"aes192-ctr\", \"aes256-ctr\", \"aes128-gcm@openssh.com\", \"chacha20-poly1305@openssh.com\", \"arcfour256\", \"arcfour128\", \"arcfour\", \"aes128-cbc\", \"3des-cbc\",\nssh_clear_authorized_keys (bool) - If true, Packer will attempt to remove its temporary key from ~/.ssh/authorized_keys and /root/.ssh/authorized_keys. This is a mostly cosmetic option, since Packer will delete the temporary private key from the host system regardless of whether this is set to true (unless the user has set the -debug flag). Defaults to \"false\"; currently only works on guests with sed installed.\nssh_key_exchange_algorithms ([]string) - If set, Packer will override the value of key exchange (kex) algorithms supported by default by Golang. Acceptable values include: \"curve25519-sha256@libssh.org\", \"ecdh-sha2-nistp256\", \"ecdh-sha2-nistp384\", \"ecdh-sha2-nistp521\", \"diffie-hellman-group14-sha1\", and \"diffie-hellman-group1-sha1\".\nssh_certificate_file (string) - Path to user certificate used to authenticate with SSH. The ~ can be used in path and will be expanded to the home directory of current user.\nssh_pty (bool) - If true, a PTY will be requested for the SSH connection. This defaults to false.\nssh_timeout (duration string | ex: \"1h5m2s\") - The time to wait for SSH to become available. Packer uses this to determine when the machine has booted so this is usually quite long. Example value: 10m. This defaults to 5m, unless ssh_handshake_attempts is set.\nssh_disable_agent_forwarding (bool) - If true, SSH agent forwarding will be disabled. Defaults to false.\nssh_handshake_attempts (int) - The number of handshakes to attempt with SSH once it can connect. This defaults to 10, unless a ssh_timeout is set.\nssh_bastion_host (string) - A bastion host to use for the actual SSH connection.\nssh_bastion_port (int) - The port of the bastion host. Defaults to 22.\nssh_bastion_agent_auth (bool) - If true, the local SSH agent will be used to authenticate with the bastion host. Defaults to false.\nssh_bastion_username (string) - The username to connect to the bastion host.\nssh_bastion_password (string) - The password to use to authenticate with the bastion host.\nssh_bastion_interactive (bool) - If true, the keyboard-interactive used to authenticate with bastion host.\nssh_bastion_private_key_file (string) - Path to a PEM encoded private key file to use to authenticate with the bastion host. The ~ can be used in path and will be expanded to the home directory of current user.\nssh_bastion_certificate_file (string) - Path to user certificate used to authenticate with bastion host. The ~ can be used in path and will be expanded to the home directory of current user.\nssh_file_transfer_method (string) - scp or sftp - How to transfer files, Secure copy (default) or SSH File Transfer Protocol.\nNOTE: Guests using Windows with Win32-OpenSSH v9.1.0.0p1-Beta, scp (the default protocol for copying data) returns a a non-zero error code since the MOTW cannot be set, which cause any file transfer to fail. As a workaround you can override the transfer protocol with SFTP instead ssh_file_transfer_protocol = \"sftp\".\nssh_proxy_host (string) - A SOCKS proxy host to use for SSH connection\nssh_proxy_port (int) - A port of the SOCKS proxy. Defaults to 1080.\nssh_proxy_username (string) - The optional username to authenticate with the proxy server.\nssh_proxy_password (string) - The optional password to use to authenticate with the proxy server.\nssh_keep_alive_interval (duration string | ex: \"1h5m2s\") - How often to send \"keep alive\" messages to the server. Set to a negative value (-1s) to disable. Example value: 10s. Defaults to 5s.\nssh_read_write_timeout (duration string | ex: \"1h5m2s\") - The amount of time to wait for a remote command to end. This might be useful if, for example, packer hangs on a connection after a reboot. Example: 5m. Disabled by default.\nssh_remote_tunnels ([]string) - \nssh_local_tunnels ([]string) - \nssh_private_key_file (string) - Path to a PEM encoded private key file to use to authenticate with SSH. The ~ can be used in path and will be expanded to the home directory of current user.\nOptional WinRM fields:\nwinrm_username (string) - The username to use to connect to WinRM.\nwinrm_password (string) - The password to use to connect to WinRM.\nwinrm_host (string) - The address for WinRM to connect to.\nNOTE: If using an Amazon EBS builder, you can specify the interface WinRM connects to via ssh_interface\nwinrm_no_proxy (bool) - Setting this to true adds the remote host:port to the NO_PROXY environment variable. This has the effect of bypassing any configured proxies when connecting to the remote host. Default to false.\nwinrm_port (int) - The WinRM port to connect to. This defaults to 5985 for plain unencrypted connection and 5986 for SSL when winrm_use_ssl is set to true.\nwinrm_timeout (duration string | ex: \"1h5m2s\") - The amount of time to wait for WinRM to become available. This defaults to 30m since setting up a Windows machine generally takes a long time.\nwinrm_use_ssl (bool) - If true, use HTTPS for WinRM.\nwinrm_insecure (bool) - If true, do not check server certificate chain and host name.\nwinrm_use_ntlm (bool) - If true, NTLMv2 authentication (with session security) will be used for WinRM, rather than default (basic authentication), removing the requirement for basic authentication to be enabled within the target guest. Further reading for remote connection authentication can be found here.\nWorking With Clusters And Hosts\nStandalone Hosts\nOnly use the host option. Optionally specify a resource_pool:\n\"host\": \"esxi-01.example.com\", \"resource_pool\": \"pool1\", \nhost = \"\"esxi-01.example.com\"\" resource_pool = \"pool1\" \nClusters Without DRS\nUse the cluster and hostparameters:\n\"cluster\": \"cluster1\", \"host\": \"esxi-02.example.com\", \ncluster = \"cluster1\" host = \"esxi-02.example.com\" \nClusters With DRS\nOnly use the cluster option. Optionally specify a resource_pool:\n\"cluster\": \"cluster2\", \"resource_pool\": \"pool1\", \ncluster = \"cluster2\" resource_pool = \"pool1\" \nRequired vSphere Privileges\nIt is recommended to create a custom vSphere role with the required privileges to integrate Packer with vSphere. Accounts or groups can be added to the role to ensure that Packer has the least privileged access to the infrastructure. For example, a named service account (e.g. svc-packer-vsphere@example.com).\nClone the default Read-Only vSphere role and add the following privileges, which are based on the capabilities of the vsphere-iso plugin:\nCategoryPrivilegeReference\nContent Library\tAdd library item\tContentLibrary.AddLibraryItem\t\n...\tUpdate Library Item\tContentLibrary.UpdateLibraryItem\t\nDatastore\tAllocate space\tDatastore.AllocateSpace\t\n...\tBrowse datastore\tDatastore.Browse\t\n...\tLow level file operations\tDatastore.FileManagement\t\nNetwork\tAssign network\tNetwork.Assign\t\nResource\tAssign virtual machine to resource pool\tResource.AssignVMToPool\t\nvApp\tExport\tvApp.Export\t\nVirtual Machine\tConfiguration > Add new disk\tVirtualMachine.Config.AddNewDisk\t\n...\tConfiguration > Add or remove device\tVirtualMachine.Config.AddRemoveDevice\t\n...\tConfiguration > Advanced configuration\tVirtualMachine.Config.AdvancedConfig\t\n...\tConfiguration > Change CPU count\tVirtualMachine.Config.CPUCount\t\n...\tConfiguration > Change memory\tVirtualMachine.Config.Memory\t\n...\tConfiguration > Change settings\tVirtualMachine.Config.Settings\t\n...\tConfiguration > Change Resource\tVirtualMachine.Config.Resource\t\n...\tConfiguration > Set annotation\tVirtualMachine.Config.Annotation\t\n...\tEdit Inventory > Create from existing\tVirtualMachine.Inventory.CreateFromExisting\t\n...\tEdit Inventory > Create new\tVirtualMachine.Inventory.Create\t\n...\tEdit Inventory > Remove\tVirtualMachine.Inventory.Delete\t\n...\tInteraction > Configure CD media\tVirtualMachine.Interact.SetCDMedia\t\n...\tInteraction > Configure floppy media\tVirtualMachine.Interact.SetFloppyMedia\t\n...\tInteraction > Connect devices\tVirtualMachine.Interact.DeviceConnection\t\n...\tInteraction > Inject USB HID scan codes\tVirtualMachine.Interact.PutUsbScanCodes\t\n...\tInteraction > Power off\tVirtualMachine.Interact.PowerOff\t\n...\tInteraction > Power on\tVirtualMachine.Interact.PowerOn\t\n...\tProvisioning > Create template from virtual machine\tVirtualMachine.Provisioning.CreateTemplateFromVM\t\n...\tProvisioning > Mark as template\tVirtualMachine.Provisioning.MarkAsTemplate\t\n...\tProvisioning > Mark as virtual machine\tVirtualMachine.Provisioning.MarkAsVM\t\n...\tState > Create snapshot\tVirtualMachine.State.CreateSnapshot\t\nGlobal permissions are required for the content library based on the hierarchical inheritance of permissions. Once the custom vSphere role is created, assign Global Permissions in vSphere to the accounts or groups used for the Packer to vSphere integration, if using the content library.\nLog in to the vCenter Server at https://<management_vcenter_server_fqdn>/ui as administrator@vsphere.local.\nSelect Menu > Administration.\nIn the left pane, select Access control > Global permissions and click the Add permissions icon.\nIn the Add permissions dialog box, enter the service account (e.g. svc-packer-vsphere@example.com), select the custom role (e.g. Packer to vSphere Integration Role) and the Propagate to children check box, and click OK.\nIn an environment with many vCenter Server instances, such as management and workload, in enhanced linked-mode, you may wish to further reduce the scope of access across the vSphere infrastructure. For example, if you do not want Packer to have access to the management vCenter Server instance, but only allow access to workload vCenter Server instances:\nFrom the Hosts and clusters inventory, select management vCenter Server to restrict scope, and click the Permissions tab.\nSelect the service account with the custom role assigned and click the Change role icon.\nIn the Change role dialog box, from the Role drop-down menu, select No Access, select the Propagate to children check box, and click OK."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vsphere/v1.2.7/components/builder/vsphere-iso",
  "text": "VMware vSphere Builder (1.2.7) | Integrations | Packer\nType: vsphere-iso Artifact BuilderId: jetbrains.vsphere\nThis builder uses the vSphere API, and creates virtual machines remotely. It starts from an ISO file and creates new VMs from scratch.\nVMware Player is not required.\nIt uses the official vCenter Server API, and does not require ESXi host modification\nThe builder supports versions following the VMware Product Lifecycle Matrix from General Availability to End of General Support. Builds on versions that are end of support may work, but configuration options may throw errors if they do not exist in the vSphere API for those versions.\nSee example templates in the examples folder.\nThere are many configuration options available for this builder. In addition to the items listed here, you will want to look at the general configuration references for HTTP, Floppy, Boot, Hardware, Output, Run, Shutdown, Communicator, Export, configuration references, which are necessary for this build to succeed and can be found further down the page.\ncreate_snapshot (bool) - Specifies to create a snapshot of the virtual machine to use as a base for linked clones. Defaults to false.\nsnapshot_name (string) - Specifies the name of the snapshot when create_snapshot is true. Defaults to Created By Packer.\nconvert_to_template (bool) - Specifies to convert the cloned virtual machine to a template after the build is complete. Defaults to false. If set to true, the virtual machine can not be imported to a content library.\nexport (*common.ExportConfig) - Specifies the configuration for exporting the virtual machine to an OVF. The virtual machine is not exported if export configuration is not specified.\ncontent_library_destination (*common.ContentLibraryDestinationConfig) - Specifies the configuration for importing a VM template or OVF template to a content library. The template will not be imported if no content library import configuration is specified. If set, convert_to_template must be set to false.\nBoot Configuration\nThe boot configuration is very important: boot_command specifies the keys to type when the virtual machine is first booted in order to start the OS installer. This command is typed after boot_wait, which gives the virtual machine some time to actually load.\nThe boot_command is an array of strings. The strings are all typed in sequence. It is an array only to improve readability within the template.\nThere are a set of special keys available. If these are in your boot command, they will be replaced by the proper key:\n<bs> - Backspace\n<del> - Delete\n<enter> <return> - Simulates an actual \"enter\" or \"return\" keypress.\n<esc> - Simulates pressing the escape key.\n<tab> - Simulates pressing the tab key.\n<f1> - <f12> - Simulates pressing a function key.\n<up> <down> <left> <right> - Simulates pressing an arrow key.\n<spacebar> - Simulates pressing the spacebar.\n<insert> - Simulates pressing the insert key.\n<home> <end> - Simulates pressing the home and end keys.\n<pageUp> <pageDown> - Simulates pressing the page up and page down keys.\n<menu> - Simulates pressing the Menu key.\n<leftAlt> <rightAlt> - Simulates pressing the alt key.\n<leftCtrl> <rightCtrl> - Simulates pressing the ctrl key.\n<leftShift> <rightShift> - Simulates pressing the shift key.\n<leftSuper> <rightSuper> - Simulates pressing the  or Windows key.\n<wait> <wait5> <wait10> - Adds a 1, 5 or 10 second pause before sending any additional keys. This is useful if you have to generally wait for the UI to update before typing more.\n<waitXX> - Add an arbitrary pause before sending any additional keys. The format of XX is a sequence of positive decimal numbers, each with optional fraction and a unit suffix, such as 300ms, 1.5h or 2h45m. Valid time units are ns, us (or s), ms, s, m, h. For example <wait10m> or <wait1m20s>.\n<XXXOn> <XXXOff> - Any printable keyboard character, and of these \"special\" expressions, with the exception of the <wait> types, can also be toggled on or off. For example, to simulate ctrl+c, use <leftCtrlOn>c<leftCtrlOff>. Be sure to release them, otherwise they will be held down until the machine reboots. To hold the c key down, you would use <cOn>. Likewise, <cOff> to release.\n{{ .HTTPIP }} {{ .HTTPPort }} - The IP and port, respectively of an HTTP server that is started serving the directory specified by the http_directory configuration parameter. If http_directory isn't specified, these will be blank!\n{{ .Name }} - The name of the VM.\nExample boot command. This is actually a working boot command used to start an CentOS 6.4 installer:\n\"boot_command\": [ \"<tab><wait>\", \" ks=http://{{ .HTTPIP }}:{{ .HTTPPort }}/centos6-ks.cfg<enter>\" ] \nboot_command = [ \"<tab><wait>\", \" ks=http://{{ .HTTPIP }}:{{ .HTTPPort }}/centos6-ks.cfg<enter>\" ] \nThe example shown below is a working boot command used to start an Ubuntu 12.04 installer:\n\"boot_command\": [ \"<esc><esc><enter><wait>\", \"/install/vmlinuz noapic \", \"preseed/url=http://{{ .HTTPIP }}:{{ .HTTPPort }}/preseed.cfg \", \"debian-installer=en_US auto locale=en_US kbd-chooser/method=us \", \"hostname={{ .Name }} \", \"fb=false debconf/frontend=noninteractive \", \"keyboard-configuration/modelcode=SKIP keyboard-configuration/layout=USA \", \"keyboard-configuration/variant=USA console-setup/ask_detect=false \", \"initrd=/install/initrd.gz -- <enter>\" ] \nboot_command = [ \"<esc><esc><enter><wait>\", \"/install/vmlinuz noapic \", \"preseed/url=http://{{ .HTTPIP }}:{{ .HTTPPort }}/preseed.cfg \", \"debian-installer=en_US auto locale=en_US kbd-chooser/method=us \", \"hostname={{ .Name }} \", \"fb=false debconf/frontend=noninteractive \", \"keyboard-configuration/modelcode=SKIP keyboard-configuration/layout=USA \", \"keyboard-configuration/variant=USA console-setup/ask_detect=false \", \"initrd=/install/initrd.gz -- <enter>\" ] \nFor more examples of various boot commands, see the sample projects from our community templates page.\nWe send each character to the VM with a default delay of 100ms between groups. The delay alleviates possible issues with latency and CPU contention. If you notice missing keys, you can tune this delay by specifying \"boot_keygroup_interval\" in your Packer template, for example:\n{ \"builders\": [ { \"type\": \"vsphere-iso\", \"boot_keygroup_interval\": \"500ms\" ... } ] } \nsource \"vsphere-iso\" \"example\" { boot_keygroup_interval = \"500ms\" # ... } \nboot_keygroup_interval (duration string | ex: \"1h5m2s\") - Time to wait after sending a group of key pressses. The value of this should be a duration. Examples are 5s and 1m30s which will cause Packer to wait five seconds and one minute 30 seconds, respectively. If this isn't specified, a sensible default value is picked depending on the builder type.\nboot_wait (duration string | ex: \"1h5m2s\") - The time to wait after booting the initial virtual machine before typing the boot_command. The value of this should be a duration. Examples are 5s and 1m30s which will cause Packer to wait five seconds and one minute 30 seconds, respectively. If this isn't specified, the default is 10s or 10 seconds. To set boot_wait to 0s, use a negative number, such as \"-1s\"\nboot_command ([]string) - This is an array of commands to type when the virtual machine is first booted. The goal of these commands should be to type just enough to initialize the operating system installer. Special keys can be typed as well, and are covered in the section below on the boot command. If this is not specified, it is assumed the installer will start itself.\nhttp_ip (string) - The IP address to use for the HTTP server started to serve the http_directory. If unset, Packer will automatically discover and assign an IP.\nHttp directory configuration\nPacker will create an http server serving http_directory when it is set, a random free port will be selected and the architecture of the directory referenced will be available in your builder.\nExample usage from a builder:\nwget http://{{ .HTTPIP }}:{{ .HTTPPort }}/foo/bar/preseed.cfg \nhttp_directory (string) - Path to a directory to serve using an HTTP server. The files in this directory will be available over HTTP that will be requestable from the virtual machine. This is useful for hosting kickstart files and so on. By default this is an empty string, which means no HTTP server will be started. The address and port of the HTTP server will be available as variables in boot_command. This is covered in more detail below.\nhttp_content (map[string]string) - Key/Values to serve using an HTTP server. http_content works like and conflicts with http_directory. The keys represent the paths and the values contents, the keys must start with a slash, ex: /path/to/file. http_content is useful for hosting kickstart files and so on. By default this is empty, which means no HTTP server will be started. The address and port of the HTTP server will be available as variables in boot_command. This is covered in more detail below. Example:\nhttp_content = { \"/a/b\" = file(\"http/b\") \"/foo/bar\" = templatefile(\"${path.root}/preseed.cfg\", { packages = [\"nginx\"] }) } \nhttp_port_min (int) - These are the minimum and maximum port to use for the HTTP server started to serve the http_directory. Because Packer often runs in parallel, Packer will choose a randomly available port in this range to run the HTTP server. If you want to force the HTTP server to be on one port, make this minimum and maximum port the same. By default the values are 8000 and 9000, respectively.\nhttp_port_max (int) - HTTP Port Max\nhttp_bind_address (string) - This is the bind address for the HTTP server. Defaults to 0.0.0.0 so that it will work with any network interface.\nFloppy configuration\nfloppy_img_path (string) - Datastore path to a floppy image that will be mounted to the VM. Example: [datastore1] ISO/pvscsi-Windows8.flp.\nfloppy_files ([]string) - List of local files to be mounted to the VM floppy drive. Can be used to make Debian preseed or RHEL kickstart files available to the VM.\nfloppy_dirs ([]string) - List of directories to copy files from.\nfloppy_content (map[string]string) - Key/Values to add to the floppy disk. The keys represent the paths, and the values contents. It can be used alongside floppy_files or floppy_dirs, which is useful to add large files without loading them into memory. If any paths are specified by both, the contents in floppy_content will take precedence.\nfloppy_content = { \"meta-data\" = jsonencode(local.instance_data) \"user-data\" = templatefile(\"user-data\", { packages = [\"nginx\"] }) } \nfloppy_label (string) - The label to use for the floppy disk that is attached when the VM is booted. This is most useful for cloud-init, Kickstart or other early initialization tools, which can benefit from labelled floppy disks. By default, the floppy label will be 'packer'.\nConnection Configuration\nvcenter_server (string) - vCenter Server hostname.\nusername (string) - vSphere username.\npassword (string) - vSphere password.\ninsecure_connection (bool) - Do not validate the vCenter Server TLS certificate. Defaults to false.\ndatacenter (string) - vSphere datacenter name. Required if there is more than one datacenter in the vSphere inventory.\nHardware Configuration\nCPUs (int32) - Number of CPU cores.\ncpu_cores (int32) - Number of CPU cores per socket.\nCPU_reservation (int64) - Amount of reserved CPU resources in MHz.\nCPU_limit (int64) - Upper limit of available CPU resources in MHz.\nCPU_hot_plug (bool) - Enable CPU hot plug setting for virtual machine. Defaults to false.\nRAM (int64) - Amount of RAM in MB.\nRAM_reservation (int64) - Amount of reserved RAM in MB.\nRAM_reserve_all (bool) - Reserve all available RAM. Defaults to false. Cannot be used together with RAM_reservation.\nRAM_hot_plug (bool) - Enable RAM hot plug setting for virtual machine. Defaults to false.\nvideo_ram (int64) - Amount of video memory in KB. See vSphere documentation for supported maximums. Defaults to 4096 KB.\ndisplays (int32) - Number of video displays. See vSphere documentation for supported maximums. Defaults to 1.\nvgpu_profile (string) - vGPU profile for accelerated graphics. See NVIDIA GRID vGPU documentation for examples of profile names. Defaults to none.\nNestedHV (bool) - Enable nested hardware virtualization for VM. Defaults to false.\nfirmware (string) - Set the Firmware for virtual machine. Supported values: bios, efi or efi-secure. Defaults to bios.\nforce_bios_setup (bool) - During the boot, force entry into the BIOS setup screen. Defaults to false.\nvTPM (bool) - Add virtual TPM device for virtual machine. Defaults to false.\nprecision_clock (string) - Add a precision clock device for virtual machine. Defaults to none.\nLocation Configuration\nvm_name (string) - Name of the virtual machine.\nfolder (string) - VM folder where the virtual machine is created.\ncluster (string) - vSphere cluster where the virtual machine is created. See the Working With Clusters And Hosts section above for more details.\nhost (string) - ESXi host where the virtual machine is created. A full path must be specified if the host is in a folder. For example folder/host. See the Working With Clusters And Hosts section above for more details.\nresource_pool (string) - vSphere resource pool where the virtual machine is created. If this is not specified, the root resource pool associated with the host or cluster is used. Note that the full path to the resource pool must be provided. For example, a simple resource pool path might resemble rp-packer and a nested path might resemble 'rp-packer/rp-linux-images'.\ndatastore (string) - vSphere datastore where the virtual machine is created. Required if host is a cluster, or if host has multiple datastores.\nset_host_for_datastore_uploads (bool) - Specifies that the host is used for uploading files to the datastore. Defaults to false.\nRun Configuration\nboot_order (string) - Priority of boot devices. Defaults to disk,cdrom\nShutdown Configuration\nshutdown_command (string) - Specify a VM guest shutdown command. This command will be executed using the communicator. Otherwise, the VMware Tools are used to gracefully shutdown the VM.\nshutdown_timeout (duration string | ex: \"1h5m2s\") - Amount of time to wait for graceful VM shutdown. Defaults to 5m or five minutes. This will likely need to be modified if the communicator is 'none'.\ndisable_shutdown (bool) - Packer normally halts the virtual machine after all provisioners have run when no shutdown_command is defined. If this is set to true, Packer will not halt the virtual machine but will assume that you will send the stop signal yourself through a preseed.cfg, a script or the final provisioner. Packer will wait for a default of five minutes until the virtual machine is shutdown. The timeout can be changed using shutdown_timeout option.\nWait Configuration\nip_wait_timeout (duration string | ex: \"1h5m2s\") - Amount of time to wait for VM's IP, similar to 'ssh_timeout'. Defaults to 30m (30 minutes). See the Golang ParseDuration documentation for full details.\nip_settle_timeout (duration string | ex: \"1h5m2s\") - Amount of time to wait for VM's IP to settle down, sometimes VM may report incorrect IP initially, then its recommended to set that parameter to apx. 2 minutes. Examples 45s and 10m. Defaults to 5s(5 seconds). See the Golang ParseDuration documentation for full details.\nip_wait_address (*string) - Set this to a CIDR address to cause the service to wait for an address that is contained in this network range. Defaults to \"0.0.0.0/0\" for any ipv4 address. Examples include:\nempty string (\"\") - remove all filters\n0:0:0:0:0:0:0:0/0 - allow only ipv6 addresses\n192.168.1.0/24 - only allow ipv4 addresses from 192.168.1.1 to 192.168.1.254\nISO Configuration\nBy default, Packer will symlink, download or copy image files to the Packer cache into a \"hash($iso_url+$iso_checksum).$iso_target_extension\" file. Packer uses hashicorp/go-getter in file mode in order to perform a download.\ngo-getter supports the following protocols:\nLocal files\nGit\nMercurial\nAmazon S3\nExamples: go-getter can guess the checksum type based on iso_checksum length, and it is also possible to specify the checksum type.\n\"iso_checksum\": \"946a6077af6f5f95a51f82fdc44051c7aa19f9cfc5f737954845a6050543d7c2\", \"iso_url\": \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \n\"iso_checksum\": \"file:ubuntu.org/..../ubuntu-14.04.1-server-amd64.iso.sum\", \"iso_url\": \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \n\"iso_checksum\": \"file://./shasums.txt\", \"iso_url\": \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \n\"iso_checksum\": \"file:./shasums.txt\", \"iso_url\": \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \niso_checksum = \"946a6077af6f5f95a51f82fdc44051c7aa19f9cfc5f737954845a6050543d7c2\" iso_url = \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \niso_checksum = \"file:ubuntu.org/..../ubuntu-14.04.1-server-amd64.iso.sum\" iso_url = \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \niso_checksum = \"file://./shasums.txt\" iso_url = \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \niso_checksum = \"file:./shasums.txt\", iso_url = \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \niso_checksum (string) - The checksum for the ISO file or virtual hard drive file. The type of the checksum is specified within the checksum field as a prefix, ex: \"md5:{$checksum}\". The type of the checksum can also be omitted and Packer will try to infer it based on string length. Valid values are \"none\", \"{$checksum}\", \"md5:{$checksum}\", \"sha1:{$checksum}\", \"sha256:{$checksum}\", \"sha512:{$checksum}\" or \"file:{$path}\". Here is a list of valid checksum values:\nmd5:090992ba9fd140077b0661cb75f7ce13\n090992ba9fd140077b0661cb75f7ce13\nsha1:ebfb681885ddf1234c18094a45bbeafd91467911\nebfb681885ddf1234c18094a45bbeafd91467911\nsha256:ed363350696a726b7932db864dda019bd2017365c9e299627830f06954643f93\ned363350696a726b7932db864dda019bd2017365c9e299627830f06954643f93\nfile:http://releases.ubuntu.com/20.04/SHA256SUMS\nfile:file://./local/path/file.sum\nfile:./local/path/file.sum\nnone Although the checksum will not be verified when it is set to \"none\", this is not recommended since these files can be very large and corruption does happen from time to time.\niso_url (string) - A URL to the ISO containing the installation image or virtual hard drive (VHD or VHDX) file to clone.\niso_urls ([]string) - Multiple URLs for the ISO to download. Packer will try these in order. If anything goes wrong attempting to download or while downloading a single URL, it will move on to the next. All URLs must point to the same file (same checksum). By default this is empty and iso_url is used. Only one of iso_url or iso_urls can be specified.\niso_target_path (string) - The path where the iso should be saved after download. By default will go in the packer cache, with a hash of the original filename and checksum as its name.\niso_target_extension (string) - The extension of the iso file after download. This defaults to iso.\nCDRom Configuration\nEach iso defined in the CDRom Configuration adds a new drive. If the \"iso_url\" is defined in addition to the \"iso_paths\", the \"iso_url\" is added to the VM first. This keeps the \"iso_url\" first in the boot order by default allowing the boot iso being defined by the iso_url and the vmware tools iso added from the datastore. Example:\n\"iso_urls\": [ \"win10.iso\", \"http://example.org/isos/win10.iso\" ], \"iso_paths\": [ \"[] /usr/lib/vmware/isoimages/windows.iso\" ], \niso_urls = [ \"win10.iso\", \"http://example.org/isos/win10.iso\" ] iso_paths = [ \"[] /usr/lib/vmware/isoimages/windows.iso\" ] \ncdrom_type (string) - Which controller to use. Example: sata. Defaults to ide.\niso_paths ([]string) - A list of paths to ISO files in either a datastore or a content library that will be mounted to the VM.\niso_paths = [ \"[nfs] iso/ubuntu-server-amd64.iso\", \"Packer/ubuntu-server-amd64/ubuntu-server-amd64.iso\" ] \nTwo ISOs are referenced:\nAn ISO in the \"iso\" folder of the \"nfs\" datastore with the file name of \"ubuntu-server-amd64.iso\".\nAn ISO in the \"Packer\" content library with the item name of \"ubuntu-server-amd64\".\nNote: All files in a content library have an associated item name. To determine the file name, view the datastore backing the content library or use the govc vSphere CLI.\nremove_cdrom (bool) - Remove CD-ROM devices from template. Defaults to false.\nreattach_cdroms (int) - Reattach one or more configured CD-ROM devices. Range: 1-4. You can reattach up to 4 CD-ROM devices to the final build artifact. If set to 0, reattach_cdroms is ignored and the step is skipped. When set to a value in the range, remove_cdrom is ignored and the CD-ROM devices are kept without any attached media.\nAn iso (CD) containing custom files can be made available for your build.\nBy default, no extra CD will be attached. All files listed in this setting get placed into the root directory of the CD and the CD is attached as the second CD device.\nThis config exists to work around modern operating systems that have no way to mount floppy disks, which was our previous go-to for adding files at boot time.\ncd_files ([]string) - A list of files to place onto a CD that is attached when the VM is booted. This can include either files or directories; any directories will be copied onto the CD recursively, preserving directory structure hierarchy. Symlinks will have the link's target copied into the directory tree on the CD where the symlink was. File globbing is allowed.\nUsage example (JSON):\n\"cd_files\": [\"./somedirectory/meta-data\", \"./somedirectory/user-data\"], \"cd_label\": \"cidata\", \ncd_files = [\"./somedirectory/meta-data\", \"./somedirectory/user-data\"] cd_label = \"cidata\" \nThe above will create a CD with two files, user-data and meta-data in the CD root. This specific example is how you would create a CD that can be used for an Ubuntu 20.04 autoinstall.\nSince globbing is also supported,\ncd_files = [\"./somedirectory/*\"] cd_label = \"cidata\" \nWould also be an acceptable way to define the above cd. The difference between providing the directory with or without the glob is whether the directory itself or its contents will be at the CD root.\nUse of this option assumes that you have a command line tool installed that can handle the iso creation. Packer will use one of the following tools:\nxorriso\nmkisofs\nhdiutil (normally found in macOS)\noscdimg (normally found in Windows as part of the Windows ADK)\ncd_content (map[string]string) - Key/Values to add to the CD. The keys represent the paths, and the values contents. It can be used alongside cd_files, which is useful to add large files without loading them into memory. If any paths are specified by both, the contents in cd_content will take precedence.\ncd_files = [\"vendor-data\"] cd_content = { \"meta-data\" = jsonencode(local.instance_data) \"user-data\" = templatefile(\"user-data\", { packages = [\"nginx\"] }) } cd_label = \"cidata\" \ncd_label (string) - CD Label\nCreate Configuration\nvm_version (uint) - Specifies the virtual machine hardware version. Defaults to the most current virtual machine hardware version supported by the ESXi host. Refer to VMware KB article 1003746 for the list of supported virtual machine hardware versions.\nguest_os_type (string) - Specifies the guest operating system identifier for the virtual machine. If not specified, the setting defaults to otherGuest.\nTo get a list of supported guest operating system identifiers for your ESXi host, run the following PowerShell command using VMware.PowerCLI:\nConnect-VIServer -Server \"vc.example.com\" -User \"administrator@vsphere\" -Password \"password\" $esxiHost = Get-VMHost -Name \"esxi.example.com\" $environmentBrowser = Get-View -Id $esxiHost.ExtensionData.Parent.ExtensionData.ConfigManager.EnvironmentBrowser $vmxVersion = ($environmentBrowser.QueryConfigOptionDescriptor() | Where-Object DefaultConfigOption).Key $osDescriptor = $environmentBrowser.QueryConfigOption($vmxVersion, $null).GuestOSDescriptor $osDescriptor | Select-Object Id, Fullname \nnetwork_adapters ([]NIC) - Specifies the network adapters for the virtual machine. If no network adapter is defined, all network-related operations will be skipped.\nusb_controller ([]string) - Specifies the USB controllers for the virtual machine. Use usb for a USB 2.0 controller and `xhci`` for a USB 3.0 controller. -> Note: Maximum of one controller of each type.\nnotes (string) - Specifies the annotations for the virtual machine.\ndestroy (bool) - Specifies whether to destroy the virtual machine after the build is complete.\ndisk_controller_type ([]string) - Set VM disk controller type. Example lsilogic, lsilogic-sas, pvscsi, nvme, or scsi. Use a list to define additional controllers. Defaults to lsilogic. See SCSI, SATA, and NVMe Storage Controller Conditions, Limitations, and Compatibility for additional details.\nstorage ([]DiskConfig) - Configures a collection of one or more disks to be provisioned along with the VM. See the Storage Configuration.\nNetwork Adapter Configuration\nDefines a Network Adapter If no adapter is defined, network tasks (communicators, most provisioners) won't work, so it's advised to define one.\nExample that creates two network adapters:\n\"network_adapters\": [ { \"network\": \"VM Network\", \"network_card\": \"vmxnet3\" }, { \"network\": \"OtherNetwork\", \"network_card\": \"vmxnet3\" } ], \nnetwork_adapters { network = \"VM Network\" network_card = \"vmxnet3\" } network_adapters { network = \"OtherNetwork\" network_card = \"vmxnet3\" } \nnetwork_card (string) - Specifies the virtual machine network card type. For example vmxnet3.\nOptional\nnetwork (string) - Specifies the network to which the virtual machine will connect. If no network is specified, provide 'host' to allow Packer to search for an available network. For networks placed within a network folder vCenter Server, provider the object path to the network. For example, network = \"/<DatacenterName>/<FolderName>/<NetworkName>\".\nmac_address (string) - Specifies the network card MAC address. For example 00:50:56:00:00:00.\npassthrough (*bool) - Specifies whether to enable DirectPath I/O passthrough for the network device.\nStorage Configuration\nDefines the disk storage for a VM.\nExample that will create a 15GB and a 20GB disk on the VM. The second disk will be thin provisioned:\n\"storage\": [ { \"disk_size\": 15000 }, { \"disk_size\": 20000, \"disk_thin_provisioned\": true } ], \nstorage { disk_size = 15000 } storage { disk_size = 20000 disk_thin_provisioned = true } \nExample that creates 2 pvscsi controllers and adds 2 disks to each one:\n\"disk_controller_type\": [\"pvscsi\", \"pvscsi\"], \"storage\": [ { \"disk_size\": 15000, \"disk_controller_index\": 0 }, { \"disk_size\": 15000, \"disk_controller_index\": 0 }, { \"disk_size\": 15000, \"disk_controller_index\": 1 }, { \"disk_size\": 15000, \"disk_controller_index\": 1 } ], \ndisk_controller_type = [\"pvscsi\", \"pvscsi\"] storage { disk_size = 15000, disk_controller_index = 0 } storage { disk_size = 15000 disk_controller_index = 0 } storage { disk_size = 15000 disk_controller_index = 1 } storage { disk_size = 15000 disk_controller_index = 1 } \ndisk_size (int64) - The size of the disk in MiB.\nOptional\ndisk_thin_provisioned (bool) - Enable VMDK thin provisioning for VM. Defaults to false.\ndisk_eagerly_scrub (bool) - Enable VMDK eager scrubbing for VM. Defaults to false.\ndisk_controller_index (int) - The assigned disk controller. Defaults to the first one (0).\nExport Configuration\nYou can export an image in Open Virtualization Format (OVF) to the Packer host.\nExample usage:\n... \"vm_name\": \"example-ubuntu\", ... \"export\": { \"force\": true, \"output_directory\": \"./output-artifacts\" }, \n# ... vm_name = \"example-ubuntu\" # ... export { force = true output_directory = \"./output-artifacts\" } \nThe above configuration would create the following files:\n./output-artifacts/example-ubuntu-disk-0.vmdk ./output-artifacts/example-ubuntu.mf ./output-artifacts/example-ubuntu.ovf \nname (string) - Name of the exported image in Open Virtualization Format (OVF). The name of the virtual machine with the .ovf extension is used if this option is not specified.\nforce (bool) - Forces the export to overwrite existing files. Defaults to false. If set to false, the export will fail if the files already exists.\nimage_files (bool) - Include additional image files that are that are associated with the virtual machine. Defaults to false. For example, .nvram and .log files.\nmanifest (string) - Generate a manifest file with the specified hash algorithm. Defaults to sha256. Available options include none, sha1, sha256, and sha512. Use none for no manifest.\noptions ([]string) - Advanced image export options. Options can include:\nmac - MAC address is exported for each Ethernet device.\nuuid - UUID is exported for the virtual machine.\nextraconfig - Extra configuration options are exported for the virtual machine.\nnodevicesubtypes - Resource subtypes for CD/DVD drives, floppy drives, and serial and parallel ports are not exported.\nFor example, adding the following export config option outputs the MAC addresses for each Ethernet device in the OVF descriptor:\n... \"export\": { \"options\": [\"mac\"] }, \n... export { options = [\"mac\"] } \nOutput Configuration:\noutput_directory (string) - This setting specifies the directory that artifacts from the build, such as the virtual machine files and disks, will be output to. The path to the directory may be relative or absolute. If relative, the path is relative to the working directory packer is executed from. This directory must not exist or, if created, must be empty prior to running the builder. By default this is \"output-BUILDNAME\" where \"BUILDNAME\" is the name of the build.\ndirectory_permission (os.FileMode) - The permissions to apply to the \"output_directory\", and to any parent directories that get created for output_directory. By default this is \"0750\". You should express the permission as quoted string with a leading zero such as \"0755\" in JSON file, because JSON does not support octal value. In Unix-like OS, the actual permission may differ from this value because of umask.\nContent Library Import Configuration\nWith this configuration Packer creates a library item in a content library whose content is a VM template or an OVF template created from the just built VM. The template is stored in a existing or newly created library item.\nlibrary (string) - Name of the library in which the new library item containing the template should be created/updated. The Content Library should be of type Local to allow deploying virtual machines.\nname (string) - Name of the library item that will be created or updated. For VM templates, the name of the item should be different from vm_name and the default is vm_name + timestamp when not set. VM templates will be always imported to a new library item. For OVF templates, the name defaults to vm_name when not set, and if an item with the same name already exists it will be then updated with the new OVF template, otherwise a new item will be created.\nNote: It's not possible to update existing library items with a new VM template. If updating an existing library item is necessary, use an OVF template instead by setting the ovf option as true.\ndescription (string) - Description of the library item that will be created. Defaults to \"Packer imported vm_name VM template\".\ncluster (string) - Cluster onto which the virtual machine template should be placed. If cluster and resource_pool are both specified, resource_pool must belong to cluster. If cluster and host are both specified, host must be a member of cluster. This option is not used when importing OVF templates. Defaults to cluster.\nfolder (string) - Virtual machine folder into which the virtual machine template should be placed. This option is not used when importing OVF templates. Defaults to the same folder as the source virtual machine.\nhost (string) - Host onto which the virtual machine template should be placed. If host and resource_pool are both specified, resource_pool must belong to host. If host and cluster are both specified, host must be a member of cluster. This option is not used when importing OVF templates. Defaults to host.\nresource_pool (string) - Resource pool into which the virtual machine template should be placed. Defaults to resource_pool. if resource_pool is also unset, the system will attempt to choose a suitable resource pool for the virtual machine template.\ndatastore (string) - The datastore for the virtual machine template's configuration and log files. This option is not used when importing OVF templates. Defaults to the storage backing associated with the library specified by library.\ndestroy (bool) - If set to true, the VM will be destroyed after deploying the template to the Content Library. Defaults to false.\novf (bool) - When set to true, Packer will import and OVF template to the content library item. Defaults to false.\nskip_import (bool) - When set to true, the VM won't be imported to the content library item. Useful for setting to true during a build test stage. Defaults to false.\novf_flags ([]string) - Flags to use for OVF package creation. The supported flags can be obtained using ExportFlag.list. If unset, no flags will be used. Known values: EXTRA_CONFIG, PRESERVE_MAC\nMinimal example of usage to import a VM template:\n\"content_library_destination\" : { \"library\": \"Packer Library Test\" } \ncontent_library_destination { library = \"Packer Library Test\" } \nMinimal example of usage to import a OVF template:\n\"content_library_destination\" : { \"library\": \"Packer Library Test\", \"ovf\": true } \ncontent_library_destination { library = \"Packer Library Test\" ovf = true } \nExtra Configuration Parameters\nconfiguration_parameters (map[string]string) - configuration_parameters is a direct passthrough to the vSphere API's ConfigSpec: https://vdc-download.vmware.com/vmwb-repository/dcr-public/bf660c0a-f060-46e8-a94d-4b5e6ffc77ad/208bc706-e281-49b6-a0ce-b402ec19ef82/SDK/vsphere-ws/docs/ReferenceGuide/vim.vm.ConfigSpec.html\ntools_sync_time (bool) - Enables time synchronization with the host. Defaults to false.\ntools_upgrade_policy (bool) - If sets to true, vSphere will automatically check and upgrade VMware Tools upon a system power cycle. If not set, defaults to manual upgrade.\nCommunicator configuration\nOptional common fields:\ncommunicator (string) - Packer currently supports three kinds of communicators:\nnone - No communicator will be used. If this is set, most provisioners also can't be used.\nssh - An SSH connection will be established to the machine. This is usually the default.\nwinrm - A WinRM connection will be established.\nIn addition to the above, some builders have custom communicators they can use. For example, the Docker builder has a \"docker\" communicator that uses docker exec and docker cp to execute scripts and copy files.\npause_before_connecting (duration string | ex: \"1h5m2s\") - We recommend that you enable SSH or WinRM as the very last step in your guest's bootstrap script, but sometimes you may have a race condition where you need Packer to wait before attempting to connect to your guest.\nIf you end up in this situation, you can use the template option pause_before_connecting. By default, there is no pause. For example if you set pause_before_connecting to 10m Packer will check whether it can connect, as normal. But once a connection attempt is successful, it will disconnect and then wait 10 minutes before connecting to the guest and beginning provisioning.\nOptional SSH fields:\nssh_host (string) - The address to SSH to. This usually is automatically configured by the builder.\nssh_port (int) - The port to connect to SSH. This defaults to 22.\nssh_username (string) - The username to connect to SSH with. Required if using SSH.\nssh_password (string) - A plaintext password to use to authenticate with SSH.\nssh_ciphers ([]string) - This overrides the value of ciphers supported by default by Golang. The default value is [ \"aes128-gcm@openssh.com\", \"chacha20-poly1305@openssh.com\", \"aes128-ctr\", \"aes192-ctr\", \"aes256-ctr\", ]\nValid options for ciphers include: \"aes128-ctr\", \"aes192-ctr\", \"aes256-ctr\", \"aes128-gcm@openssh.com\", \"chacha20-poly1305@openssh.com\", \"arcfour256\", \"arcfour128\", \"arcfour\", \"aes128-cbc\", \"3des-cbc\",\nssh_clear_authorized_keys (bool) - If true, Packer will attempt to remove its temporary key from ~/.ssh/authorized_keys and /root/.ssh/authorized_keys. This is a mostly cosmetic option, since Packer will delete the temporary private key from the host system regardless of whether this is set to true (unless the user has set the -debug flag). Defaults to \"false\"; currently only works on guests with sed installed.\nssh_key_exchange_algorithms ([]string) - If set, Packer will override the value of key exchange (kex) algorithms supported by default by Golang. Acceptable values include: \"curve25519-sha256@libssh.org\", \"ecdh-sha2-nistp256\", \"ecdh-sha2-nistp384\", \"ecdh-sha2-nistp521\", \"diffie-hellman-group14-sha1\", and \"diffie-hellman-group1-sha1\".\nssh_certificate_file (string) - Path to user certificate used to authenticate with SSH. The ~ can be used in path and will be expanded to the home directory of current user.\nssh_pty (bool) - If true, a PTY will be requested for the SSH connection. This defaults to false.\nssh_timeout (duration string | ex: \"1h5m2s\") - The time to wait for SSH to become available. Packer uses this to determine when the machine has booted so this is usually quite long. Example value: 10m. This defaults to 5m, unless ssh_handshake_attempts is set.\nssh_disable_agent_forwarding (bool) - If true, SSH agent forwarding will be disabled. Defaults to false.\nssh_handshake_attempts (int) - The number of handshakes to attempt with SSH once it can connect. This defaults to 10, unless a ssh_timeout is set.\nssh_bastion_host (string) - A bastion host to use for the actual SSH connection.\nssh_bastion_port (int) - The port of the bastion host. Defaults to 22.\nssh_bastion_agent_auth (bool) - If true, the local SSH agent will be used to authenticate with the bastion host. Defaults to false.\nssh_bastion_username (string) - The username to connect to the bastion host.\nssh_bastion_password (string) - The password to use to authenticate with the bastion host.\nssh_bastion_interactive (bool) - If true, the keyboard-interactive used to authenticate with bastion host.\nssh_bastion_private_key_file (string) - Path to a PEM encoded private key file to use to authenticate with the bastion host. The ~ can be used in path and will be expanded to the home directory of current user.\nssh_bastion_certificate_file (string) - Path to user certificate used to authenticate with bastion host. The ~ can be used in path and will be expanded to the home directory of current user.\nssh_file_transfer_method (string) - scp or sftp - How to transfer files, Secure copy (default) or SSH File Transfer Protocol.\nNOTE: Guests using Windows with Win32-OpenSSH v9.1.0.0p1-Beta, scp (the default protocol for copying data) returns a a non-zero error code since the MOTW cannot be set, which cause any file transfer to fail. As a workaround you can override the transfer protocol with SFTP instead ssh_file_transfer_protocol = \"sftp\".\nssh_proxy_host (string) - A SOCKS proxy host to use for SSH connection\nssh_proxy_port (int) - A port of the SOCKS proxy. Defaults to 1080.\nssh_proxy_username (string) - The optional username to authenticate with the proxy server.\nssh_proxy_password (string) - The optional password to use to authenticate with the proxy server.\nssh_keep_alive_interval (duration string | ex: \"1h5m2s\") - How often to send \"keep alive\" messages to the server. Set to a negative value (-1s) to disable. Example value: 10s. Defaults to 5s.\nssh_read_write_timeout (duration string | ex: \"1h5m2s\") - The amount of time to wait for a remote command to end. This might be useful if, for example, packer hangs on a connection after a reboot. Example: 5m. Disabled by default.\nssh_remote_tunnels ([]string) - \nssh_local_tunnels ([]string) - \nssh_private_key_file (string) - Path to a PEM encoded private key file to use to authenticate with SSH. The ~ can be used in path and will be expanded to the home directory of current user.\nOptional WinRM fields:\nwinrm_username (string) - The username to use to connect to WinRM.\nwinrm_password (string) - The password to use to connect to WinRM.\nwinrm_host (string) - The address for WinRM to connect to.\nNOTE: If using an Amazon EBS builder, you can specify the interface WinRM connects to via ssh_interface\nwinrm_no_proxy (bool) - Setting this to true adds the remote host:port to the NO_PROXY environment variable. This has the effect of bypassing any configured proxies when connecting to the remote host. Default to false.\nwinrm_port (int) - The WinRM port to connect to. This defaults to 5985 for plain unencrypted connection and 5986 for SSL when winrm_use_ssl is set to true.\nwinrm_timeout (duration string | ex: \"1h5m2s\") - The amount of time to wait for WinRM to become available. This defaults to 30m since setting up a Windows machine generally takes a long time.\nwinrm_use_ssl (bool) - If true, use HTTPS for WinRM.\nwinrm_insecure (bool) - If true, do not check server certificate chain and host name.\nwinrm_use_ntlm (bool) - If true, NTLMv2 authentication (with session security) will be used for WinRM, rather than default (basic authentication), removing the requirement for basic authentication to be enabled within the target guest. Further reading for remote connection authentication can be found here.\nWorking With Clusters And Hosts\nStandalone Hosts\nOnly use the host option. Optionally specify a resource_pool:\n\"host\": \"esxi-01.example.com\", \"resource_pool\": \"pool1\", \nhost = \"\"esxi-01.example.com\"\" resource_pool = \"pool1\" \nClusters Without DRS\nUse the cluster and hostparameters:\n\"cluster\": \"cluster1\", \"host\": \"esxi-02.example.com\", \ncluster = \"cluster1\" host = \"esxi-02.example.com\" \nClusters With DRS\nOnly use the cluster option. Optionally specify a resource_pool:\n\"cluster\": \"cluster2\", \"resource_pool\": \"pool1\", \ncluster = \"cluster2\" resource_pool = \"pool1\" \nRequired vSphere Privileges\nIt is recommended to create a custom vSphere role with the required privileges to integrate Packer with vSphere. Accounts or groups can be added to the role to ensure that Packer has the least privileged access to the infrastructure. For example, a named service account (e.g. svc-packer-vsphere@example.com).\nClone the default Read-Only vSphere role and add the following privileges, which are based on the capabilities of the vsphere-iso plugin:\nCategoryPrivilegeReference\nContent Library\tAdd library item\tContentLibrary.AddLibraryItem\t\n...\tUpdate Library Item\tContentLibrary.UpdateLibraryItem\t\nDatastore\tAllocate space\tDatastore.AllocateSpace\t\n...\tBrowse datastore\tDatastore.Browse\t\n...\tLow level file operations\tDatastore.FileManagement\t\nNetwork\tAssign network\tNetwork.Assign\t\nResource\tAssign virtual machine to resource pool\tResource.AssignVMToPool\t\nvApp\tExport\tvApp.Export\t\nVirtual Machine\tConfiguration > Add new disk\tVirtualMachine.Config.AddNewDisk\t\n...\tConfiguration > Add or remove device\tVirtualMachine.Config.AddRemoveDevice\t\n...\tConfiguration > Advanced configuration\tVirtualMachine.Config.AdvancedConfig\t\n...\tConfiguration > Change CPU count\tVirtualMachine.Config.CPUCount\t\n...\tConfiguration > Change memory\tVirtualMachine.Config.Memory\t\n...\tConfiguration > Change settings\tVirtualMachine.Config.Settings\t\n...\tConfiguration > Change Resource\tVirtualMachine.Config.Resource\t\n...\tConfiguration > Set annotation\tVirtualMachine.Config.Annotation\t\n...\tEdit Inventory > Create from existing\tVirtualMachine.Inventory.CreateFromExisting\t\n...\tEdit Inventory > Create new\tVirtualMachine.Inventory.Create\t\n...\tEdit Inventory > Remove\tVirtualMachine.Inventory.Delete\t\n...\tInteraction > Configure CD media\tVirtualMachine.Interact.SetCDMedia\t\n...\tInteraction > Configure floppy media\tVirtualMachine.Interact.SetFloppyMedia\t\n...\tInteraction > Connect devices\tVirtualMachine.Interact.DeviceConnection\t\n...\tInteraction > Inject USB HID scan codes\tVirtualMachine.Interact.PutUsbScanCodes\t\n...\tInteraction > Power off\tVirtualMachine.Interact.PowerOff\t\n...\tInteraction > Power on\tVirtualMachine.Interact.PowerOn\t\n...\tProvisioning > Create template from virtual machine\tVirtualMachine.Provisioning.CreateTemplateFromVM\t\n...\tProvisioning > Mark as template\tVirtualMachine.Provisioning.MarkAsTemplate\t\n...\tProvisioning > Mark as virtual machine\tVirtualMachine.Provisioning.MarkAsVM\t\n...\tState > Create snapshot\tVirtualMachine.State.CreateSnapshot\t\nGlobal permissions are required for the content library based on the hierarchical inheritance of permissions. Once the custom vSphere role is created, assign Global Permissions in vSphere to the accounts or groups used for the Packer to vSphere integration, if using the content library.\nLog in to the vCenter Server at https://<management_vcenter_server_fqdn>/ui as administrator@vsphere.local.\nSelect Menu > Administration.\nIn the left pane, select Access control > Global permissions and click the Add permissions icon.\nIn the Add permissions dialog box, enter the service account (e.g. svc-packer-vsphere@example.com), select the custom role (e.g. Packer to vSphere Integration Role) and the Propagate to children check box, and click OK.\nIn an environment with many vCenter Server instances, such as management and workload, in enhanced linked-mode, you may wish to further reduce the scope of access across the vSphere infrastructure. For example, if you do not want Packer to have access to the management vCenter Server instance, but only allow access to workload vCenter Server instances:\nFrom the Hosts and clusters inventory, select management vCenter Server to restrict scope, and click the Permissions tab.\nSelect the service account with the custom role assigned and click the Change role icon.\nIn the Change role dialog box, from the Role drop-down menu, select No Access, select the Propagate to children check box, and click OK."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vsphere/v1.2.6/components/builder/vsphere-iso",
  "text": "VMware vSphere Builder (1.2.6) | Integrations | Packer\nType: vsphere-iso Artifact BuilderId: jetbrains.vsphere\nThis builder uses the vSphere API, and creates virtual machines remotely. It starts from an ISO file and creates new VMs from scratch.\nVMware Player is not required.\nIt uses the official vCenter Server API, and does not require ESXi host modification\nThe builder supports versions following the VMware Product Lifecycle Matrix from General Availability to End of General Support. Builds on versions that are end of support may work, but configuration options may throw errors if they do not exist in the vSphere API for those versions.\nSee example templates in the examples folder.\nThere are many configuration options available for this builder. In addition to the items listed here, you will want to look at the general configuration references for HTTP, Floppy, Boot, Hardware, Output, Run, Shutdown, Communicator, Export, configuration references, which are necessary for this build to succeed and can be found further down the page.\ncreate_snapshot (bool) - Specifies to create a snapshot of the virtual machine to use as a base for linked clones. Defaults to false.\nsnapshot_name (string) - Specifies the name of the snapshot when create_snapshot is true. Defaults to Created By Packer.\nconvert_to_template (bool) - Specifies to convert the cloned virtual machine to a template after the build is complete. Defaults to false. If set to true, the virtual machine can not be imported to a content library.\nexport (*common.ExportConfig) - Specifies the configuration for exporting the virtual machine to an OVF. The virtual machine is not exported if export configuration is not specified.\ncontent_library_destination (*common.ContentLibraryDestinationConfig) - Specifies the configuration for importing a VM template or OVF template to a content library. The template will not be imported if no content library import configuration is specified. If set, convert_to_template must be set to false.\nBoot Configuration\nThe boot configuration is very important: boot_command specifies the keys to type when the virtual machine is first booted in order to start the OS installer. This command is typed after boot_wait, which gives the virtual machine some time to actually load.\nThe boot_command is an array of strings. The strings are all typed in sequence. It is an array only to improve readability within the template.\nThere are a set of special keys available. If these are in your boot command, they will be replaced by the proper key:\n<bs> - Backspace\n<del> - Delete\n<enter> <return> - Simulates an actual \"enter\" or \"return\" keypress.\n<esc> - Simulates pressing the escape key.\n<tab> - Simulates pressing the tab key.\n<f1> - <f12> - Simulates pressing a function key.\n<up> <down> <left> <right> - Simulates pressing an arrow key.\n<spacebar> - Simulates pressing the spacebar.\n<insert> - Simulates pressing the insert key.\n<home> <end> - Simulates pressing the home and end keys.\n<pageUp> <pageDown> - Simulates pressing the page up and page down keys.\n<menu> - Simulates pressing the Menu key.\n<leftAlt> <rightAlt> - Simulates pressing the alt key.\n<leftCtrl> <rightCtrl> - Simulates pressing the ctrl key.\n<leftShift> <rightShift> - Simulates pressing the shift key.\n<leftSuper> <rightSuper> - Simulates pressing the  or Windows key.\n<wait> <wait5> <wait10> - Adds a 1, 5 or 10 second pause before sending any additional keys. This is useful if you have to generally wait for the UI to update before typing more.\n<waitXX> - Add an arbitrary pause before sending any additional keys. The format of XX is a sequence of positive decimal numbers, each with optional fraction and a unit suffix, such as 300ms, 1.5h or 2h45m. Valid time units are ns, us (or s), ms, s, m, h. For example <wait10m> or <wait1m20s>.\n<XXXOn> <XXXOff> - Any printable keyboard character, and of these \"special\" expressions, with the exception of the <wait> types, can also be toggled on or off. For example, to simulate ctrl+c, use <leftCtrlOn>c<leftCtrlOff>. Be sure to release them, otherwise they will be held down until the machine reboots. To hold the c key down, you would use <cOn>. Likewise, <cOff> to release.\n{{ .HTTPIP }} {{ .HTTPPort }} - The IP and port, respectively of an HTTP server that is started serving the directory specified by the http_directory configuration parameter. If http_directory isn't specified, these will be blank!\n{{ .Name }} - The name of the VM.\nExample boot command. This is actually a working boot command used to start an CentOS 6.4 installer:\n\"boot_command\": [ \"<tab><wait>\", \" ks=http://{{ .HTTPIP }}:{{ .HTTPPort }}/centos6-ks.cfg<enter>\" ] \nboot_command = [ \"<tab><wait>\", \" ks=http://{{ .HTTPIP }}:{{ .HTTPPort }}/centos6-ks.cfg<enter>\" ] \nThe example shown below is a working boot command used to start an Ubuntu 12.04 installer:\n\"boot_command\": [ \"<esc><esc><enter><wait>\", \"/install/vmlinuz noapic \", \"preseed/url=http://{{ .HTTPIP }}:{{ .HTTPPort }}/preseed.cfg \", \"debian-installer=en_US auto locale=en_US kbd-chooser/method=us \", \"hostname={{ .Name }} \", \"fb=false debconf/frontend=noninteractive \", \"keyboard-configuration/modelcode=SKIP keyboard-configuration/layout=USA \", \"keyboard-configuration/variant=USA console-setup/ask_detect=false \", \"initrd=/install/initrd.gz -- <enter>\" ] \nboot_command = [ \"<esc><esc><enter><wait>\", \"/install/vmlinuz noapic \", \"preseed/url=http://{{ .HTTPIP }}:{{ .HTTPPort }}/preseed.cfg \", \"debian-installer=en_US auto locale=en_US kbd-chooser/method=us \", \"hostname={{ .Name }} \", \"fb=false debconf/frontend=noninteractive \", \"keyboard-configuration/modelcode=SKIP keyboard-configuration/layout=USA \", \"keyboard-configuration/variant=USA console-setup/ask_detect=false \", \"initrd=/install/initrd.gz -- <enter>\" ] \nFor more examples of various boot commands, see the sample projects from our community templates page.\nWe send each character to the VM with a default delay of 100ms between groups. The delay alleviates possible issues with latency and CPU contention. If you notice missing keys, you can tune this delay by specifying \"boot_keygroup_interval\" in your Packer template, for example:\n{ \"builders\": [ { \"type\": \"vsphere-iso\", \"boot_keygroup_interval\": \"500ms\" ... } ] } \nsource \"vsphere-iso\" \"example\" { boot_keygroup_interval = \"500ms\" # ... } \nboot_keygroup_interval (duration string | ex: \"1h5m2s\") - Time to wait after sending a group of key pressses. The value of this should be a duration. Examples are 5s and 1m30s which will cause Packer to wait five seconds and one minute 30 seconds, respectively. If this isn't specified, a sensible default value is picked depending on the builder type.\nboot_wait (duration string | ex: \"1h5m2s\") - The time to wait after booting the initial virtual machine before typing the boot_command. The value of this should be a duration. Examples are 5s and 1m30s which will cause Packer to wait five seconds and one minute 30 seconds, respectively. If this isn't specified, the default is 10s or 10 seconds. To set boot_wait to 0s, use a negative number, such as \"-1s\"\nboot_command ([]string) - This is an array of commands to type when the virtual machine is first booted. The goal of these commands should be to type just enough to initialize the operating system installer. Special keys can be typed as well, and are covered in the section below on the boot command. If this is not specified, it is assumed the installer will start itself.\nhttp_ip (string) - The IP address to use for the HTTP server started to serve the http_directory. If unset, Packer will automatically discover and assign an IP.\nHttp directory configuration\nPacker will create an http server serving http_directory when it is set, a random free port will be selected and the architecture of the directory referenced will be available in your builder.\nExample usage from a builder:\nwget http://{{ .HTTPIP }}:{{ .HTTPPort }}/foo/bar/preseed.cfg \nhttp_directory (string) - Path to a directory to serve using an HTTP server. The files in this directory will be available over HTTP that will be requestable from the virtual machine. This is useful for hosting kickstart files and so on. By default this is an empty string, which means no HTTP server will be started. The address and port of the HTTP server will be available as variables in boot_command. This is covered in more detail below.\nhttp_content (map[string]string) - Key/Values to serve using an HTTP server. http_content works like and conflicts with http_directory. The keys represent the paths and the values contents, the keys must start with a slash, ex: /path/to/file. http_content is useful for hosting kickstart files and so on. By default this is empty, which means no HTTP server will be started. The address and port of the HTTP server will be available as variables in boot_command. This is covered in more detail below. Example:\nhttp_content = { \"/a/b\" = file(\"http/b\") \"/foo/bar\" = templatefile(\"${path.root}/preseed.cfg\", { packages = [\"nginx\"] }) } \nhttp_port_min (int) - These are the minimum and maximum port to use for the HTTP server started to serve the http_directory. Because Packer often runs in parallel, Packer will choose a randomly available port in this range to run the HTTP server. If you want to force the HTTP server to be on one port, make this minimum and maximum port the same. By default the values are 8000 and 9000, respectively.\nhttp_port_max (int) - HTTP Port Max\nhttp_bind_address (string) - This is the bind address for the HTTP server. Defaults to 0.0.0.0 so that it will work with any network interface.\nFloppy configuration\nfloppy_img_path (string) - Datastore path to a floppy image that will be mounted to the VM. Example: [datastore1] ISO/pvscsi-Windows8.flp.\nfloppy_files ([]string) - List of local files to be mounted to the VM floppy drive. Can be used to make Debian preseed or RHEL kickstart files available to the VM.\nfloppy_dirs ([]string) - List of directories to copy files from.\nfloppy_content (map[string]string) - Key/Values to add to the floppy disk. The keys represent the paths, and the values contents. It can be used alongside floppy_files or floppy_dirs, which is useful to add large files without loading them into memory. If any paths are specified by both, the contents in floppy_content will take precedence.\nfloppy_content = { \"meta-data\" = jsonencode(local.instance_data) \"user-data\" = templatefile(\"user-data\", { packages = [\"nginx\"] }) } \nfloppy_label (string) - The label to use for the floppy disk that is attached when the VM is booted. This is most useful for cloud-init, Kickstart or other early initialization tools, which can benefit from labelled floppy disks. By default, the floppy label will be 'packer'.\nConnection Configuration\nvcenter_server (string) - vCenter Server hostname.\nusername (string) - vSphere username.\npassword (string) - vSphere password.\ninsecure_connection (bool) - Do not validate the vCenter Server TLS certificate. Defaults to false.\ndatacenter (string) - vSphere datacenter name. Required if there is more than one datacenter in the vSphere inventory.\nHardware Configuration\nCPUs (int32) - Number of CPU cores.\ncpu_cores (int32) - Number of CPU cores per socket.\nCPU_reservation (int64) - Amount of reserved CPU resources in MHz.\nCPU_limit (int64) - Upper limit of available CPU resources in MHz.\nCPU_hot_plug (bool) - Enable CPU hot plug setting for virtual machine. Defaults to false.\nRAM (int64) - Amount of RAM in MB.\nRAM_reservation (int64) - Amount of reserved RAM in MB.\nRAM_reserve_all (bool) - Reserve all available RAM. Defaults to false. Cannot be used together with RAM_reservation.\nRAM_hot_plug (bool) - Enable RAM hot plug setting for virtual machine. Defaults to false.\nvideo_ram (int64) - Amount of video memory in KB. See vSphere documentation for supported maximums. Defaults to 4096 KB.\ndisplays (int32) - Number of video displays. See vSphere documentation for supported maximums. Defaults to 1.\nvgpu_profile (string) - vGPU profile for accelerated graphics. See NVIDIA GRID vGPU documentation for examples of profile names. Defaults to none.\nNestedHV (bool) - Enable nested hardware virtualization for VM. Defaults to false.\nfirmware (string) - Set the Firmware for virtual machine. Supported values: bios, efi or efi-secure. Defaults to bios.\nforce_bios_setup (bool) - During the boot, force entry into the BIOS setup screen. Defaults to false.\nvTPM (bool) - Add virtual TPM device for virtual machine. Defaults to false.\nprecision_clock (string) - Add a precision clock device for virtual machine. Defaults to none.\nLocation Configuration\nvm_name (string) - Name of the virtual machine.\nfolder (string) - VM folder where the virtual machine is created.\ncluster (string) - vSphere cluster where the virtual machine is created. See the Working With Clusters And Hosts section above for more details.\nhost (string) - ESXi host where the virtual machine is created. A full path must be specified if the host is in a folder. For example folder/host. See the Working With Clusters And Hosts section above for more details.\nresource_pool (string) - vSphere resource pool where the virtual machine is created. If this is not specified, the root resource pool associated with the host or cluster is used. Note that the full path to the resource pool must be provided. For example, a simple resource pool path might resemble rp-packer and a nested path might resemble 'rp-packer/rp-linux-images'.\ndatastore (string) - vSphere datastore where the virtual machine is created. Required if host is a cluster, or if host has multiple datastores.\nset_host_for_datastore_uploads (bool) - Specifies that the host is used for uploading files to the datastore. Defaults to false.\nRun Configuration\nboot_order (string) - Priority of boot devices. Defaults to disk,cdrom\nShutdown Configuration\nshutdown_command (string) - Specify a VM guest shutdown command. This command will be executed using the communicator. Otherwise, the VMware Tools are used to gracefully shutdown the VM.\nshutdown_timeout (duration string | ex: \"1h5m2s\") - Amount of time to wait for graceful VM shutdown. Defaults to 5m or five minutes. This will likely need to be modified if the communicator is 'none'.\ndisable_shutdown (bool) - Packer normally halts the virtual machine after all provisioners have run when no shutdown_command is defined. If this is set to true, Packer will not halt the virtual machine but will assume that you will send the stop signal yourself through a preseed.cfg, a script or the final provisioner. Packer will wait for a default of five minutes until the virtual machine is shutdown. The timeout can be changed using shutdown_timeout option.\nWait Configuration\nip_wait_timeout (duration string | ex: \"1h5m2s\") - Amount of time to wait for VM's IP, similar to 'ssh_timeout'. Defaults to 30m (30 minutes). See the Golang ParseDuration documentation for full details.\nip_settle_timeout (duration string | ex: \"1h5m2s\") - Amount of time to wait for VM's IP to settle down, sometimes VM may report incorrect IP initially, then its recommended to set that parameter to apx. 2 minutes. Examples 45s and 10m. Defaults to 5s(5 seconds). See the Golang ParseDuration documentation for full details.\nip_wait_address (*string) - Set this to a CIDR address to cause the service to wait for an address that is contained in this network range. Defaults to \"0.0.0.0/0\" for any ipv4 address. Examples include:\nempty string (\"\") - remove all filters\n0:0:0:0:0:0:0:0/0 - allow only ipv6 addresses\n192.168.1.0/24 - only allow ipv4 addresses from 192.168.1.1 to 192.168.1.254\nISO Configuration\nBy default, Packer will symlink, download or copy image files to the Packer cache into a \"hash($iso_url+$iso_checksum).$iso_target_extension\" file. Packer uses hashicorp/go-getter in file mode in order to perform a download.\ngo-getter supports the following protocols:\nLocal files\nGit\nMercurial\nAmazon S3\nExamples: go-getter can guess the checksum type based on iso_checksum length, and it is also possible to specify the checksum type.\n\"iso_checksum\": \"946a6077af6f5f95a51f82fdc44051c7aa19f9cfc5f737954845a6050543d7c2\", \"iso_url\": \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \n\"iso_checksum\": \"file:ubuntu.org/..../ubuntu-14.04.1-server-amd64.iso.sum\", \"iso_url\": \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \n\"iso_checksum\": \"file://./shasums.txt\", \"iso_url\": \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \n\"iso_checksum\": \"file:./shasums.txt\", \"iso_url\": \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \niso_checksum = \"946a6077af6f5f95a51f82fdc44051c7aa19f9cfc5f737954845a6050543d7c2\" iso_url = \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \niso_checksum = \"file:ubuntu.org/..../ubuntu-14.04.1-server-amd64.iso.sum\" iso_url = \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \niso_checksum = \"file://./shasums.txt\" iso_url = \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \niso_checksum = \"file:./shasums.txt\", iso_url = \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \niso_checksum (string) - The checksum for the ISO file or virtual hard drive file. The type of the checksum is specified within the checksum field as a prefix, ex: \"md5:{$checksum}\". The type of the checksum can also be omitted and Packer will try to infer it based on string length. Valid values are \"none\", \"{$checksum}\", \"md5:{$checksum}\", \"sha1:{$checksum}\", \"sha256:{$checksum}\", \"sha512:{$checksum}\" or \"file:{$path}\". Here is a list of valid checksum values:\nmd5:090992ba9fd140077b0661cb75f7ce13\n090992ba9fd140077b0661cb75f7ce13\nsha1:ebfb681885ddf1234c18094a45bbeafd91467911\nebfb681885ddf1234c18094a45bbeafd91467911\nsha256:ed363350696a726b7932db864dda019bd2017365c9e299627830f06954643f93\ned363350696a726b7932db864dda019bd2017365c9e299627830f06954643f93\nfile:http://releases.ubuntu.com/20.04/SHA256SUMS\nfile:file://./local/path/file.sum\nfile:./local/path/file.sum\nnone Although the checksum will not be verified when it is set to \"none\", this is not recommended since these files can be very large and corruption does happen from time to time.\niso_url (string) - A URL to the ISO containing the installation image or virtual hard drive (VHD or VHDX) file to clone.\niso_urls ([]string) - Multiple URLs for the ISO to download. Packer will try these in order. If anything goes wrong attempting to download or while downloading a single URL, it will move on to the next. All URLs must point to the same file (same checksum). By default this is empty and iso_url is used. Only one of iso_url or iso_urls can be specified.\niso_target_path (string) - The path where the iso should be saved after download. By default will go in the packer cache, with a hash of the original filename and checksum as its name.\niso_target_extension (string) - The extension of the iso file after download. This defaults to iso.\nCDRom Configuration\nEach iso defined in the CDRom Configuration adds a new drive. If the \"iso_url\" is defined in addition to the \"iso_paths\", the \"iso_url\" is added to the VM first. This keeps the \"iso_url\" first in the boot order by default allowing the boot iso being defined by the iso_url and the vmware tools iso added from the datastore. Example:\n\"iso_urls\": [ \"win10.iso\", \"http://example.org/isos/win10.iso\" ], \"iso_paths\": [ \"[] /usr/lib/vmware/isoimages/windows.iso\" ], \niso_urls = [ \"win10.iso\", \"http://example.org/isos/win10.iso\" ] iso_paths = [ \"[] /usr/lib/vmware/isoimages/windows.iso\" ] \ncdrom_type (string) - Which controller to use. Example: sata. Defaults to ide.\niso_paths ([]string) - A list of paths to ISO files in either a datastore or a content library that will be mounted to the VM.\niso_paths = [ \"[nfs] iso/ubuntu-server-amd64.iso\", \"Packer/ubuntu-server-amd64/ubuntu-server-amd64.iso\" ] \nTwo ISOs are referenced:\nAn ISO in the \"iso\" folder of the \"nfs\" datastore with the file name of \"ubuntu-server-amd64.iso\".\nAn ISO in the \"Packer\" content library with the item name of \"ubuntu-server-amd64\".\nNote: All files in a content library have an associated item name. To determine the file name, view the datastore backing the content library or use the govc vSphere CLI.\nremove_cdrom (bool) - Remove CD-ROM devices from template. Defaults to false.\nAn iso (CD) containing custom files can be made available for your build.\nBy default, no extra CD will be attached. All files listed in this setting get placed into the root directory of the CD and the CD is attached as the second CD device.\nThis config exists to work around modern operating systems that have no way to mount floppy disks, which was our previous go-to for adding files at boot time.\ncd_files ([]string) - A list of files to place onto a CD that is attached when the VM is booted. This can include either files or directories; any directories will be copied onto the CD recursively, preserving directory structure hierarchy. Symlinks will have the link's target copied into the directory tree on the CD where the symlink was. File globbing is allowed.\nUsage example (JSON):\n\"cd_files\": [\"./somedirectory/meta-data\", \"./somedirectory/user-data\"], \"cd_label\": \"cidata\", \ncd_files = [\"./somedirectory/meta-data\", \"./somedirectory/user-data\"] cd_label = \"cidata\" \nThe above will create a CD with two files, user-data and meta-data in the CD root. This specific example is how you would create a CD that can be used for an Ubuntu 20.04 autoinstall.\nSince globbing is also supported,\ncd_files = [\"./somedirectory/*\"] cd_label = \"cidata\" \nWould also be an acceptable way to define the above cd. The difference between providing the directory with or without the glob is whether the directory itself or its contents will be at the CD root.\nUse of this option assumes that you have a command line tool installed that can handle the iso creation. Packer will use one of the following tools:\nxorriso\nmkisofs\nhdiutil (normally found in macOS)\noscdimg (normally found in Windows as part of the Windows ADK)\ncd_content (map[string]string) - Key/Values to add to the CD. The keys represent the paths, and the values contents. It can be used alongside cd_files, which is useful to add large files without loading them into memory. If any paths are specified by both, the contents in cd_content will take precedence.\ncd_files = [\"vendor-data\"] cd_content = { \"meta-data\" = jsonencode(local.instance_data) \"user-data\" = templatefile(\"user-data\", { packages = [\"nginx\"] }) } cd_label = \"cidata\" \ncd_label (string) - CD Label\nCreate Configuration\nvm_version (uint) - Specifies the virtual machine hardware version. Defaults to the most current virtual machine hardware version supported by the ESXi host. Refer to VMware KB article 1003746 for the list of supported virtual machine hardware versions.\nguest_os_type (string) - Specifies the guest operating system identifier for the virtual machine. If not specified, the setting defaults to otherGuest.\nTo get a list of supported guest operating system identifiers for your ESXi host, run the following PowerShell command using VMware.PowerCLI:\nConnect-VIServer -Server \"vc.example.com\" -User \"administrator@vsphere\" -Password \"password\" $esxiHost = Get-VMHost -Name \"esxi.example.com\" $environmentBrowser = Get-View -Id $esxiHost.ExtensionData.Parent.ExtensionData.ConfigManager.EnvironmentBrowser $vmxVersion = ($environmentBrowser.QueryConfigOptionDescriptor() | Where-Object DefaultConfigOption).Key $osDescriptor = $environmentBrowser.QueryConfigOption($vmxVersion, $null).GuestOSDescriptor $osDescriptor | Select-Object Id, Fullname \nnetwork_adapters ([]NIC) - Specifies the network adapters for the virtual machine. If no network adapter is defined, all network-related operations will be skipped.\nusb_controller ([]string) - Specifies the USB controllers for the virtual machine. Use usb for a USB 2.0 controller and `xhci`` for a USB 3.0 controller. -> Note: Maximum of one controller of each type.\nnotes (string) - Specifies the annotations for the virtual machine.\ndestroy (bool) - Specifies whether to destroy the virtual machine after the build is complete.\ndisk_controller_type ([]string) - Set VM disk controller type. Example lsilogic, lsilogic-sas, pvscsi, nvme, or scsi. Use a list to define additional controllers. Defaults to lsilogic. See SCSI, SATA, and NVMe Storage Controller Conditions, Limitations, and Compatibility for additional details.\nstorage ([]DiskConfig) - Configures a collection of one or more disks to be provisioned along with the VM. See the Storage Configuration.\nNetwork Adapter Configuration\nDefines a Network Adapter If no adapter is defined, network tasks (communicators, most provisioners) won't work, so it's advised to define one.\nExample that creates two network adapters:\n\"network_adapters\": [ { \"network\": \"VM Network\", \"network_card\": \"vmxnet3\" }, { \"network\": \"OtherNetwork\", \"network_card\": \"vmxnet3\" } ], \nnetwork_adapters { network = \"VM Network\" network_card = \"vmxnet3\" } network_adapters { network = \"OtherNetwork\" network_card = \"vmxnet3\" } \nnetwork_card (string) - Specifies the virtual machine network card type. For example vmxnet3.\nnetwork (string) - Specifies the network to which the virtual machine will connect. If no network is specified, provide 'host' to allow Packer to search for an available network. For networks placed within a network folder vCenter Server, provider the object path to the network. For example, network = \"/<DatacenterName>/<FolderName>/<NetworkName>\".\nmac_address (string) - Specifies the network card MAC address. For example 00:50:56:00:00:00.\npassthrough (*bool) - Specifies whether to enable DirectPath I/O passthrough for the network device.\nStorage Configuration\nDefines the disk storage for a VM.\nExample that will create a 15GB and a 20GB disk on the VM. The second disk will be thin provisioned:\n\"storage\": [ { \"disk_size\": 15000 }, { \"disk_size\": 20000, \"disk_thin_provisioned\": true } ], \nstorage { disk_size = 15000 } storage { disk_size = 20000 disk_thin_provisioned = true } \nExample that creates 2 pvscsi controllers and adds 2 disks to each one:\n\"disk_controller_type\": [\"pvscsi\", \"pvscsi\"], \"storage\": [ { \"disk_size\": 15000, \"disk_controller_index\": 0 }, { \"disk_size\": 15000, \"disk_controller_index\": 0 }, { \"disk_size\": 15000, \"disk_controller_index\": 1 }, { \"disk_size\": 15000, \"disk_controller_index\": 1 } ], \ndisk_controller_type = [\"pvscsi\", \"pvscsi\"] storage { disk_size = 15000, disk_controller_index = 0 } storage { disk_size = 15000 disk_controller_index = 0 } storage { disk_size = 15000 disk_controller_index = 1 } storage { disk_size = 15000 disk_controller_index = 1 } \ndisk_size (int64) - The size of the disk in MiB.\ndisk_thin_provisioned (bool) - Enable VMDK thin provisioning for VM. Defaults to false.\ndisk_eagerly_scrub (bool) - Enable VMDK eager scrubbing for VM. Defaults to false.\ndisk_controller_index (int) - The assigned disk controller. Defaults to the first one (0).\nExport Configuration\nYou can export an image in Open Virtualization Format (OVF) to the Packer host.\nExample usage:\n... \"vm_name\": \"example-ubuntu\", ... \"export\": { \"force\": true, \"output_directory\": \"./output-artifacts\" }, \n# ... vm_name = \"example-ubuntu\" # ... export { force = true output_directory = \"./output-artifacts\" } \nThe above configuration would create the following files:\n./output-artifacts/example-ubuntu-disk-0.vmdk ./output-artifacts/example-ubuntu.mf ./output-artifacts/example-ubuntu.ovf \nname (string) - Name of the exported image in Open Virtualization Format (OVF). The name of the virtual machine with the .ovf extension is used if this option is not specified.\nforce (bool) - Forces the export to overwrite existing files. Defaults to false. If set to false, the export will fail if the files already exists.\nimage_files (bool) - Include additional image files that are that are associated with the virtual machine. Defaults to false. For example, .nvram and .log files.\nmanifest (string) - Generate a manifest file with the specified hash algorithm. Defaults to sha256. Available options include none, sha1, sha256, and sha512. Use none for no manifest.\noptions ([]string) - Advanced image export options. Options can include:\nmac - MAC address is exported for each Ethernet device.\nuuid - UUID is exported for the virtual machine.\nextraconfig - Extra configuration options are exported for the virtual machine.\nnodevicesubtypes - Resource subtypes for CD/DVD drives, floppy drives, and serial and parallel ports are not exported.\nFor example, adding the following export config option outputs the MAC addresses for each Ethernet device in the OVF descriptor:\n... \"export\": { \"options\": [\"mac\"] }, \n... export { options = [\"mac\"] } \nOutput Configuration:\noutput_directory (string) - This setting specifies the directory that artifacts from the build, such as the virtual machine files and disks, will be output to. The path to the directory may be relative or absolute. If relative, the path is relative to the working directory packer is executed from. This directory must not exist or, if created, must be empty prior to running the builder. By default this is \"output-BUILDNAME\" where \"BUILDNAME\" is the name of the build.\ndirectory_permission (os.FileMode) - The permissions to apply to the \"output_directory\", and to any parent directories that get created for output_directory. By default this is \"0750\". You should express the permission as quoted string with a leading zero such as \"0755\" in JSON file, because JSON does not support octal value. In Unix-like OS, the actual permission may differ from this value because of umask.\nContent Library Import Configuration\nWith this configuration Packer creates a library item in a content library whose content is a VM template or an OVF template created from the just built VM. The template is stored in a existing or newly created library item.\nlibrary (string) - Name of the library in which the new library item containing the template should be created/updated. The Content Library should be of type Local to allow deploying virtual machines.\nname (string) - Name of the library item that will be created or updated. For VM templates, the name of the item should be different from vm_name and the default is vm_name + timestamp when not set. VM templates will be always imported to a new library item. For OVF templates, the name defaults to vm_name when not set, and if an item with the same name already exists it will be then updated with the new OVF template, otherwise a new item will be created.\nNote: It's not possible to update existing library items with a new VM template. If updating an existing library item is necessary, use an OVF template instead by setting the ovf option as true.\ndescription (string) - Description of the library item that will be created. Defaults to \"Packer imported vm_name VM template\".\ncluster (string) - Cluster onto which the virtual machine template should be placed. If cluster and resource_pool are both specified, resource_pool must belong to cluster. If cluster and host are both specified, host must be a member of cluster. This option is not used when importing OVF templates. Defaults to cluster.\nfolder (string) - Virtual machine folder into which the virtual machine template should be placed. This option is not used when importing OVF templates. Defaults to the same folder as the source virtual machine.\nhost (string) - Host onto which the virtual machine template should be placed. If host and resource_pool are both specified, resource_pool must belong to host. If host and cluster are both specified, host must be a member of cluster. This option is not used when importing OVF templates. Defaults to host.\nresource_pool (string) - Resource pool into which the virtual machine template should be placed. Defaults to resource_pool. if resource_pool is also unset, the system will attempt to choose a suitable resource pool for the virtual machine template.\ndatastore (string) - The datastore for the virtual machine template's configuration and log files. This option is not used when importing OVF templates. Defaults to the storage backing associated with the library specified by library.\ndestroy (bool) - If set to true, the VM will be destroyed after deploying the template to the Content Library. Defaults to false.\novf (bool) - When set to true, Packer will import and OVF template to the content library item. Defaults to false.\nskip_import (bool) - When set to true, the VM won't be imported to the content library item. Useful for setting to true during a build test stage. Defaults to false.\novf_flags ([]string) - Flags to use for OVF package creation. The supported flags can be obtained using ExportFlag.list. If unset, no flags will be used. Known values: EXTRA_CONFIG, PRESERVE_MAC\nMinimal example of usage to import a VM template:\n\"content_library_destination\" : { \"library\": \"Packer Library Test\" } \ncontent_library_destination { library = \"Packer Library Test\" } \nMinimal example of usage to import a OVF template:\n\"content_library_destination\" : { \"library\": \"Packer Library Test\", \"ovf\": true } \ncontent_library_destination { library = \"Packer Library Test\" ovf = true } \nExtra Configuration Parameters\nconfiguration_parameters (map[string]string) - configuration_parameters is a direct passthrough to the vSphere API's ConfigSpec: https://vdc-download.vmware.com/vmwb-repository/dcr-public/bf660c0a-f060-46e8-a94d-4b5e6ffc77ad/208bc706-e281-49b6-a0ce-b402ec19ef82/SDK/vsphere-ws/docs/ReferenceGuide/vim.vm.ConfigSpec.html\ntools_sync_time (bool) - Enables time synchronization with the host. Defaults to false.\ntools_upgrade_policy (bool) - If sets to true, vSphere will automatically check and upgrade VMware Tools upon a system power cycle. If not set, defaults to manual upgrade.\nCommunicator configuration\nOptional common fields:\ncommunicator (string) - Packer currently supports three kinds of communicators:\nnone - No communicator will be used. If this is set, most provisioners also can't be used.\nssh - An SSH connection will be established to the machine. This is usually the default.\nwinrm - A WinRM connection will be established.\nIn addition to the above, some builders have custom communicators they can use. For example, the Docker builder has a \"docker\" communicator that uses docker exec and docker cp to execute scripts and copy files.\npause_before_connecting (duration string | ex: \"1h5m2s\") - We recommend that you enable SSH or WinRM as the very last step in your guest's bootstrap script, but sometimes you may have a race condition where you need Packer to wait before attempting to connect to your guest.\nIf you end up in this situation, you can use the template option pause_before_connecting. By default, there is no pause. For example if you set pause_before_connecting to 10m Packer will check whether it can connect, as normal. But once a connection attempt is successful, it will disconnect and then wait 10 minutes before connecting to the guest and beginning provisioning.\nOptional SSH fields:\nNOTE: Guests using Windows with Win32-OpenSSH v9.1.0.0p1-Beta, scp (the default protocol for copying data) returns a a non-zero error code since the MOTW cannot be set, which cause any file transfer to fail. As a workaround you can override the transfer protocol with SFTP instead ssh_file_transfer_protocol = \"sftp\".\nssh_private_key_file (string) - Path to a PEM encoded private key file to use to authenticate with SSH. The ~ can be used in path and will be expanded to the home directory of current user.\nOptional WinRM fields:\nWorking With Clusters And Hosts\nStandalone Hosts\nOnly use the host option. Optionally specify a resource_pool:\n\"host\": \"esxi-01.example.com\", \"resource_pool\": \"pool1\", \nhost = \"\"esxi-01.example.com\"\" resource_pool = \"pool1\" \nClusters Without DRS\nUse the cluster and hostparameters:\n\"cluster\": \"cluster1\", \"host\": \"esxi-02.example.com\", \ncluster = \"cluster1\" host = \"esxi-02.example.com\" \nClusters With DRS\nOnly use the cluster option. Optionally specify a resource_pool:\n\"cluster\": \"cluster2\", \"resource_pool\": \"pool1\", \ncluster = \"cluster2\" resource_pool = \"pool1\" \nRequired vSphere Privileges\nIt is recommended to create a custom vSphere role with the required privileges to integrate Packer with vSphere. Accounts or groups can be added to the role to ensure that Packer has the least privileged access to the infrastructure. For example, a named service account (e.g. svc-packer-vsphere@example.com).\nClone the default Read-Only vSphere role and add the following privileges, which are based on the capabilities of the vsphere-iso plugin:\nCategoryPrivilegeReference\nContent Library\tAdd library item\tContentLibrary.AddLibraryItem\t\n...\tUpdate Library Item\tContentLibrary.UpdateLibraryItem\t\nDatastore\tAllocate space\tDatastore.AllocateSpace\t\n...\tBrowse datastore\tDatastore.Browse\t\n...\tLow level file operations\tDatastore.FileManagement\t\nNetwork\tAssign network\tNetwork.Assign\t\nResource\tAssign virtual machine to resource pool\tResource.AssignVMToPool\t\nvApp\tExport\tvApp.Export\t\nVirtual Machine\tConfiguration > Add new disk\tVirtualMachine.Config.AddNewDisk\t\n...\tConfiguration > Add or remove device\tVirtualMachine.Config.AddRemoveDevice\t\n...\tConfiguration > Advanced configuration\tVirtualMachine.Config.AdvancedConfig\t\n...\tConfiguration > Change CPU count\tVirtualMachine.Config.CPUCount\t\n...\tConfiguration > Change memory\tVirtualMachine.Config.Memory\t\n...\tConfiguration > Change settings\tVirtualMachine.Config.Settings\t\n...\tConfiguration > Change Resource\tVirtualMachine.Config.Resource\t\n...\tConfiguration > Set annotation\tVirtualMachine.Config.Annotation\t\n...\tEdit Inventory > Create from existing\tVirtualMachine.Inventory.CreateFromExisting\t\n...\tEdit Inventory > Create new\tVirtualMachine.Inventory.Create\t\n...\tEdit Inventory > Remove\tVirtualMachine.Inventory.Delete\t\n...\tInteraction > Configure CD media\tVirtualMachine.Interact.SetCDMedia\t\n...\tInteraction > Configure floppy media\tVirtualMachine.Interact.SetFloppyMedia\t\n...\tInteraction > Connect devices\tVirtualMachine.Interact.DeviceConnection\t\n...\tInteraction > Inject USB HID scan codes\tVirtualMachine.Interact.PutUsbScanCodes\t\n...\tInteraction > Power off\tVirtualMachine.Interact.PowerOff\t\n...\tInteraction > Power on\tVirtualMachine.Interact.PowerOn\t\n...\tProvisioning > Create template from virtual machine\tVirtualMachine.Provisioning.CreateTemplateFromVM\t\n...\tProvisioning > Mark as template\tVirtualMachine.Provisioning.MarkAsTemplate\t\n...\tProvisioning > Mark as virtual machine\tVirtualMachine.Provisioning.MarkAsVM\t\n...\tState > Create snapshot\tVirtualMachine.State.CreateSnapshot\t\nGlobal permissions are required for the content library based on the hierarchical inheritance of permissions. Once the custom vSphere role is created, assign Global Permissions in vSphere to the accounts or groups used for the Packer to vSphere integration, if using the content library.\nLog in to the vCenter Server at https://<management_vcenter_server_fqdn>/ui as administrator@vsphere.local.\nSelect Menu > Administration.\nIn the left pane, select Access control > Global permissions and click the Add permissions icon.\nIn the Add permissions dialog box, enter the service account (e.g. svc-packer-vsphere@example.com), select the custom role (e.g. Packer to vSphere Integration Role) and the Propagate to children check box, and click OK.\nIn an environment with many vCenter Server instances, such as management and workload, in enhanced linked-mode, you may wish to further reduce the scope of access across the vSphere infrastructure. For example, if you do not want Packer to have access to the management vCenter Server instance, but only allow access to workload vCenter Server instances:\nFrom the Hosts and clusters inventory, select management vCenter Server to restrict scope, and click the Permissions tab.\nSelect the service account with the custom role assigned and click the Change role icon.\nIn the Change role dialog box, from the Role drop-down menu, select No Access, select the Propagate to children check box, and click OK."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vsphere/v1.2.5/components/builder/vsphere-iso",
  "text": "VMware vSphere Builder (1.2.5) | Integrations | Packer\nType: vsphere-iso Artifact BuilderId: jetbrains.vsphere\nThis builder uses the vSphere API, and creates virtual machines remotely. It starts from an ISO file and creates new VMs from scratch.\nVMware Player is not required.\nIt uses the official vCenter Server API, and does not require ESXi host modification\nThe builder supports versions following the VMware Product Lifecycle Matrix from General Availability to End of General Support. Builds on versions that are end of support may work, but configuration options may throw errors if they do not exist in the vSphere API for those versions.\nSee example templates in the examples folder.\nThere are many configuration options available for this builder. In addition to the items listed here, you will want to look at the general configuration references for HTTP, Floppy, Boot, Hardware, Output, Run, Shutdown, Communicator, Export, configuration references, which are necessary for this build to succeed and can be found further down the page.\ncreate_snapshot (bool) - Specifies to create a snapshot of the virtual machine to use as a base for linked clones. Defaults to false.\nsnapshot_name (string) - Specifies the name of the snapshot when create_snapshot is true. Defaults to Created By Packer.\nconvert_to_template (bool) - Specifies to convert the cloned virtual machine to a template after the build is complete. Defaults to false. If set to true, the virtual machine can not be imported to a content library.\nexport (*common.ExportConfig) - Specifies the configuration for exporting the virtual machine to an OVF. The virtual machine is not exported if export configuration is not specified.\nWe send each character to the VM with a default delay of 100ms between groups. The delay alleviates possible issues with latency and CPU contention. If you notice missing keys, you can tune this delay by specifying \"boot_keygroup_interval\" in your Packer template, for example:\n{ \"builders\": [ { \"type\": \"vsphere-iso\", \"boot_keygroup_interval\": \"500ms\" ... } ] } \nsource \"vsphere-iso\" \"example\" { boot_keygroup_interval = \"500ms\" # ... } \nhttp_ip (string) - The IP address to use for the HTTP server started to serve the http_directory. If unset, Packer will automatically discover and assign an IP.\nHttp directory configuration\nFloppy configuration\nfloppy_img_path (string) - Datastore path to a floppy image that will be mounted to the VM. Example: [datastore1] ISO/pvscsi-Windows8.flp.\nfloppy_files ([]string) - List of local files to be mounted to the VM floppy drive. Can be used to make Debian preseed or RHEL kickstart files available to the VM.\nfloppy_dirs ([]string) - List of directories to copy files from.\nfloppy_label (string) - The label to use for the floppy disk that is attached when the VM is booted. This is most useful for cloud-init, Kickstart or other early initialization tools, which can benefit from labelled floppy disks. By default, the floppy label will be 'packer'.\nvcenter_server (string) - vCenter Server hostname.\nusername (string) - vSphere username.\npassword (string) - vSphere password.\ninsecure_connection (bool) - Do not validate the vCenter Server TLS certificate. Defaults to false.\ndatacenter (string) - vSphere datacenter name. Required if there is more than one datacenter in the vSphere inventory.\nCPUs (int32) - Number of CPU cores.\ncpu_cores (int32) - Number of CPU cores per socket.\nCPU_reservation (int64) - Amount of reserved CPU resources in MHz.\nCPU_limit (int64) - Upper limit of available CPU resources in MHz.\nCPU_hot_plug (bool) - Enable CPU hot plug setting for virtual machine. Defaults to false.\nRAM (int64) - Amount of RAM in MB.\nRAM_reservation (int64) - Amount of reserved RAM in MB.\nRAM_reserve_all (bool) - Reserve all available RAM. Defaults to false. Cannot be used together with RAM_reservation.\nRAM_hot_plug (bool) - Enable RAM hot plug setting for virtual machine. Defaults to false.\nvideo_ram (int64) - Amount of video memory in KB. See vSphere documentation for supported maximums. Defaults to 4096 KB.\ndisplays (int32) - Number of video displays. See vSphere documentation for supported maximums. Defaults to 1.\nvgpu_profile (string) - vGPU profile for accelerated graphics. See NVIDIA GRID vGPU documentation for examples of profile names. Defaults to none.\nNestedHV (bool) - Enable nested hardware virtualization for VM. Defaults to false.\nfirmware (string) - Set the Firmware for virtual machine. Supported values: bios, efi or efi-secure. Defaults to bios.\nforce_bios_setup (bool) - During the boot, force entry into the BIOS setup screen. Defaults to false.\nvTPM (bool) - Add virtual TPM device for virtual machine. Defaults to false.\nprecision_clock (string) - Add a precision clock device for virtual machine. Defaults to none.\nvm_name (string) - Name of the virtual machine.\nfolder (string) - VM folder where the virtual machine is created.\ncluster (string) - vSphere cluster where the virtual machine is created. See the Working With Clusters And Hosts section above for more details.\nhost (string) - ESXi host where the virtual machine is created. A full path must be specified if the host is in a folder. For example folder/host. See the Working With Clusters And Hosts section above for more details.\nresource_pool (string) - vSphere resource pool where the virtual machine is created. If this is not specified, the root resource pool associated with the host or cluster is used. Note that the full path to the resource pool must be provided. For example, a simple resource pool path might resemble rp-packer and a nested path might resemble 'rp-packer/rp-linux-images'.\ndatastore (string) - vSphere datastore where the virtual machine is created. Required if host is a cluster, or if host has multiple datastores.\nset_host_for_datastore_uploads (bool) - Specifies that the host is used for uploading files to the datastore. Defaults to false.\nboot_order (string) - Priority of boot devices. Defaults to disk,cdrom\nip_settle_timeout (duration string | ex: \"1h5m2s\") - Amount of time to wait for VM's IP to settle down, sometimes VM may report incorrect IP initially, then its recommended to set that parameter to apx. 2 minutes. Examples 45s and 10m. Defaults to 5s(5 seconds). See the Golang ParseDuration documentation for full details.\nISO Configuration\nBy default, Packer will symlink, download or copy image files to the Packer cache into a \"hash($iso_url+$iso_checksum).$iso_target_extension\" file. Packer uses hashicorp/go-getter in file mode in order to perform a download.\ngo-getter supports the following protocols:\nLocal files\nGit\nMercurial\nAmazon S3\nExamples: go-getter can guess the checksum type based on iso_checksum length, and it is also possible to specify the checksum type.\n\"iso_checksum\": \"946a6077af6f5f95a51f82fdc44051c7aa19f9cfc5f737954845a6050543d7c2\", \"iso_url\": \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \n\"iso_checksum\": \"file:ubuntu.org/..../ubuntu-14.04.1-server-amd64.iso.sum\", \"iso_url\": \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \n\"iso_checksum\": \"file://./shasums.txt\", \"iso_url\": \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \n\"iso_checksum\": \"file:./shasums.txt\", \"iso_url\": \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \niso_checksum = \"946a6077af6f5f95a51f82fdc44051c7aa19f9cfc5f737954845a6050543d7c2\" iso_url = \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \niso_checksum = \"file:ubuntu.org/..../ubuntu-14.04.1-server-amd64.iso.sum\" iso_url = \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \niso_checksum = \"file://./shasums.txt\" iso_url = \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \niso_checksum = \"file:./shasums.txt\", iso_url = \"ubuntu.org/.../ubuntu-14.04.1-server-amd64.iso\" \niso_checksum (string) - The checksum for the ISO file or virtual hard drive file. The type of the checksum is specified within the checksum field as a prefix, ex: \"md5:{$checksum}\". The type of the checksum can also be omitted and Packer will try to infer it based on string length. Valid values are \"none\", \"{$checksum}\", \"md5:{$checksum}\", \"sha1:{$checksum}\", \"sha256:{$checksum}\", \"sha512:{$checksum}\" or \"file:{$path}\". Here is a list of valid checksum values:\nmd5:090992ba9fd140077b0661cb75f7ce13\n090992ba9fd140077b0661cb75f7ce13\nsha1:ebfb681885ddf1234c18094a45bbeafd91467911\nebfb681885ddf1234c18094a45bbeafd91467911\nsha256:ed363350696a726b7932db864dda019bd2017365c9e299627830f06954643f93\ned363350696a726b7932db864dda019bd2017365c9e299627830f06954643f93\nfile:http://releases.ubuntu.com/20.04/SHA256SUMS\nfile:file://./local/path/file.sum\nfile:./local/path/file.sum\nnone Although the checksum will not be verified when it is set to \"none\", this is not recommended since these files can be very large and corruption does happen from time to time.\niso_url (string) - A URL to the ISO containing the installation image or virtual hard drive (VHD or VHDX) file to clone.\niso_urls ([]string) - Multiple URLs for the ISO to download. Packer will try these in order. If anything goes wrong attempting to download or while downloading a single URL, it will move on to the next. All URLs must point to the same file (same checksum). By default this is empty and iso_url is used. Only one of iso_url or iso_urls can be specified.\niso_target_path (string) - The path where the iso should be saved after download. By default will go in the packer cache, with a hash of the original filename and checksum as its name.\niso_target_extension (string) - The extension of the iso file after download. This defaults to iso.\nCDRom Configuration\nEach iso defined in the CDRom Configuration adds a new drive. If the \"iso_url\" is defined in addition to the \"iso_paths\", the \"iso_url\" is added to the VM first. This keeps the \"iso_url\" first in the boot order by default allowing the boot iso being defined by the iso_url and the vmware tools iso added from the datastore. Example:\n\"iso_urls\": [ \"win10.iso\", \"http://example.org/isos/win10.iso\" ], \"iso_paths\": [ \"[] /usr/lib/vmware/isoimages/windows.iso\" ], \niso_urls = [ \"win10.iso\", \"http://example.org/isos/win10.iso\" ] iso_paths = [ \"[] /usr/lib/vmware/isoimages/windows.iso\" ] \ncdrom_type (string) - Which controller to use. Example: sata. Defaults to ide.\niso_paths ([]string) - A list of paths to ISO files in either a datastore or a content library that will be mounted to the VM.\niso_paths = [ \"[nfs] iso/ubuntu-server-amd64.iso\", \"Packer/ubuntu-server-amd64/ubuntu-server-amd64.iso\" ] \nAn ISO in the \"iso\" folder of the \"nfs\" datastore with the file name of \"ubuntu-server-amd64.iso\".\nAn ISO in the \"Packer\" content library with the item name of \"ubuntu-server-amd64\".\nremove_cdrom (bool) - Remove CD-ROM devices from template. Defaults to false.\nCreate Configuration\nvm_version (uint) - Specifies the virtual machine hardware version. Defaults to the most current virtual machine hardware version supported by the ESXi host. Refer to VMware KB article 1003746 for the list of supported virtual machine hardware versions.\nguest_os_type (string) - Specifies the guest operating system identifier for the virtual machine. If not specified, the setting defaults to otherGuest.\nTo get a list of supported guest operating system identifiers for your ESXi host, run the following PowerShell command using VMware.PowerCLI:\nConnect-VIServer -Server \"vc.example.com\" -User \"administrator@vsphere\" -Password \"password\" $esxiHost = Get-VMHost -Name \"esxi.example.com\" $environmentBrowser = Get-View -Id $esxiHost.ExtensionData.Parent.ExtensionData.ConfigManager.EnvironmentBrowser $vmxVersion = ($environmentBrowser.QueryConfigOptionDescriptor() | Where-Object DefaultConfigOption).Key $osDescriptor = $environmentBrowser.QueryConfigOption($vmxVersion, $null).GuestOSDescriptor $osDescriptor | Select-Object Id, Fullname \nnetwork_adapters ([]NIC) - Specifies the network adapters for the virtual machine. If no network adapter is defined, all network-related operations will be skipped.\nusb_controller ([]string) - Specifies the USB controllers for the virtual machine. Use usb for a USB 2.0 controller and `xhci`` for a USB 3.0 controller. -> Note: Maximum of one controller of each type.\nnotes (string) - Specifies the annotations for the virtual machine.\ndestroy (bool) - Specifies whether to destroy the virtual machine after the build is complete.\ndisk_controller_type ([]string) - Set VM disk controller type. Example lsilogic, lsilogic-sas, pvscsi, nvme, or scsi. Use a list to define additional controllers. Defaults to lsilogic. See SCSI, SATA, and NVMe Storage Controller Conditions, Limitations, and Compatibility for additional details.\nstorage ([]DiskConfig) - Configures a collection of one or more disks to be provisioned along with the VM. See the Storage Configuration.\nNetwork Adapter Configuration\nDefines a Network Adapter If no adapter is defined, network tasks (communicators, most provisioners) won't work, so it's advised to define one.\nExample that creates two network adapters:\n\"network_adapters\": [ { \"network\": \"VM Network\", \"network_card\": \"vmxnet3\" }, { \"network\": \"OtherNetwork\", \"network_card\": \"vmxnet3\" } ], \nnetwork_adapters { network = \"VM Network\" network_card = \"vmxnet3\" } network_adapters { network = \"OtherNetwork\" network_card = \"vmxnet3\" } \nnetwork_card (string) - Specifies the virtual machine network card type. For example vmxnet3.\nnetwork (string) - Specifies the network to which the virtual machine will connect. If no network is specified, provide 'host' to allow Packer to search for an available network. For networks placed within a network folder vCenter Server, provider the object path to the network. For example, network = \"/<DatacenterName>/<FolderName>/<NetworkName>\".\nmac_address (string) - Specifies the network card MAC address. For example 00:50:56:00:00:00.\npassthrough (*bool) - Specifies whether to enable DirectPath I/O passthrough for the network device.\nDefines the disk storage for a VM.\nExample that will create a 15GB and a 20GB disk on the VM. The second disk will be thin provisioned:\nExample that creates 2 pvscsi controllers and adds 2 disks to each one:\ndisk_thin_provisioned (bool) - Enable VMDK thin provisioning for VM. Defaults to false.\ndisk_eagerly_scrub (bool) - Enable VMDK eager scrubbing for VM. Defaults to false.\ndisk_controller_index (int) - The assigned disk controller. Defaults to the first one (0).\nExample usage:\nname (string) - Name of the exported image in Open Virtualization Format (OVF). The name of the virtual machine with the .ovf extension is used if this option is not specified.\nforce (bool) - Forces the export to overwrite existing files. Defaults to false. If set to false, the export will fail if the files already exists.\nmanifest (string) - Generate a manifest file with the specified hash algorithm. Defaults to sha256. Available options include none, sha1, sha256, and sha512. Use none for no manifest.\noptions ([]string) - Advanced image export options. Options can include:\nnodevicesubtypes - Resource subtypes for CD/DVD drives, floppy drives, and serial and parallel ports are not exported.\nFor example, adding the following export config option outputs the MAC addresses for each Ethernet device in the OVF descriptor:\nOutput Configuration:\noutput_directory (string) - This setting specifies the directory that artifacts from the build, such as the virtual machine files and disks, will be output to. The path to the directory may be relative or absolute. If relative, the path is relative to the working directory packer is executed from. This directory must not exist or, if created, must be empty prior to running the builder. By default this is \"output-BUILDNAME\" where \"BUILDNAME\" is the name of the build.\nContent Library Import Configuration\nWith this configuration Packer creates a library item in a content library whose content is a VM template or an OVF template created from the just built VM. The template is stored in a existing or newly created library item.\nlibrary (string) - Name of the library in which the new library item containing the template should be created/updated. The Content Library should be of type Local to allow deploying virtual machines.\nname (string) - Name of the library item that will be created or updated. For VM templates, the name of the item should be different from vm_name and the default is vm_name + timestamp when not set. VM templates will be always imported to a new library item. For OVF templates, the name defaults to vm_name when not set, and if an item with the same name already exists it will be then updated with the new OVF template, otherwise a new item will be created.\nNote: It's not possible to update existing library items with a new VM template. If updating an existing library item is necessary, use an OVF template instead by setting the ovf option as true.\ndescription (string) - Description of the library item that will be created. Defaults to \"Packer imported vm_name VM template\".\ncluster (string) - Cluster onto which the virtual machine template should be placed. If cluster and resource_pool are both specified, resource_pool must belong to cluster. If cluster and host are both specified, host must be a member of cluster. This option is not used when importing OVF templates. Defaults to cluster.\nfolder (string) - Virtual machine folder into which the virtual machine template should be placed. This option is not used when importing OVF templates. Defaults to the same folder as the source virtual machine.\nhost (string) - Host onto which the virtual machine template should be placed. If host and resource_pool are both specified, resource_pool must belong to host. If host and cluster are both specified, host must be a member of cluster. This option is not used when importing OVF templates. Defaults to host.\nresource_pool (string) - Resource pool into which the virtual machine template should be placed. Defaults to resource_pool. if resource_pool is also unset, the system will attempt to choose a suitable resource pool for the virtual machine template.\ndatastore (string) - The datastore for the virtual machine template's configuration and log files. This option is not used when importing OVF templates. Defaults to the storage backing associated with the library specified by library.\ndestroy (bool) - If set to true, the VM will be destroyed after deploying the template to the Content Library. Defaults to false.\novf (bool) - When set to true, Packer will import and OVF template to the content library item. Defaults to false.\nskip_import (bool) - When set to true, the VM won't be imported to the content library item. Useful for setting to true during a build test stage. Defaults to false.\novf_flags ([]string) - Flags to use for OVF package creation. The supported flags can be obtained using ExportFlag.list. If unset, no flags will be used. Known values: EXTRA_CONFIG, PRESERVE_MAC\nMinimal example of usage to import a VM template:\n\"content_library_destination\" : { \"library\": \"Packer Library Test\" } \ncontent_library_destination { library = \"Packer Library Test\" } \nMinimal example of usage to import a OVF template:\n\"content_library_destination\" : { \"library\": \"Packer Library Test\", \"ovf\": true } \ncontent_library_destination { library = \"Packer Library Test\" ovf = true } \nconfiguration_parameters (map[string]string) - configuration_parameters is a direct passthrough to the vSphere API's ConfigSpec: https://vdc-download.vmware.com/vmwb-repository/dcr-public/bf660c0a-f060-46e8-a94d-4b5e6ffc77ad/208bc706-e281-49b6-a0ce-b402ec19ef82/SDK/vsphere-ws/docs/ReferenceGuide/vim.vm.ConfigSpec.html\ntools_sync_time (bool) - Enables time synchronization with the host. Defaults to false.\ntools_upgrade_policy (bool) - If sets to true, vSphere will automatically check and upgrade VMware Tools upon a system power cycle. If not set, defaults to manual upgrade.\nCommunicator configuration\nOptional common fields:\nOptional SSH fields:\nNOTE: Guests using Windows with Win32-OpenSSH v9.1.0.0p1-Beta, scp (the default protocol for copying data) returns a a non-zero error code since the MOTW cannot be set, which cause any file transfer to fail. As a workaround you can override the transfer protocol with SFTP instead ssh_file_transfer_protocol = \"sftp\".\nOptional WinRM fields:\nWorking With Clusters And Hosts\nStandalone Hosts\nOnly use the host option. Optionally specify a resource_pool:\n\"host\": \"esxi-01.example.com\", \"resource_pool\": \"pool1\", \nhost = \"\"esxi-01.example.com\"\" resource_pool = \"pool1\" \nClusters Without DRS\nUse the cluster and hostparameters:\n\"cluster\": \"cluster1\", \"host\": \"esxi-02.example.com\", \ncluster = \"cluster1\" host = \"esxi-02.example.com\" \nClusters With DRS\nOnly use the cluster option. Optionally specify a resource_pool:\n\"cluster\": \"cluster2\", \"resource_pool\": \"pool1\", \ncluster = \"cluster2\" resource_pool = \"pool1\" \nRequired vSphere Privileges\nIt is recommended to create a custom vSphere role with the required privileges to integrate Packer with vSphere. Accounts or groups can be added to the role to ensure that Packer has the least privileged access to the infrastructure. For example, a named service account (e.g. svc-packer-vsphere@example.com).\nClone the default Read-Only vSphere role and add the following privileges, which are based on the capabilities of the vsphere-iso plugin:\nCategoryPrivilegeReference\nContent Library\tAdd library item\tContentLibrary.AddLibraryItem\t\n...\tUpdate Library Item\tContentLibrary.UpdateLibraryItem\t\nDatastore\tAllocate space\tDatastore.AllocateSpace\t\n...\tBrowse datastore\tDatastore.Browse\t\n...\tLow level file operations\tDatastore.FileManagement\t\nNetwork\tAssign network\tNetwork.Assign\t\nResource\tAssign virtual machine to resource pool\tResource.AssignVMToPool\t\nvApp\tExport\tvApp.Export\t\nVirtual Machine\tConfiguration > Add new disk\tVirtualMachine.Config.AddNewDisk\t\n...\tConfiguration > Add or remove device\tVirtualMachine.Config.AddRemoveDevice\t\n...\tConfiguration > Advanced configuration\tVirtualMachine.Config.AdvancedConfig\t\n...\tConfiguration > Change CPU count\tVirtualMachine.Config.CPUCount\t\n...\tConfiguration > Change memory\tVirtualMachine.Config.Memory\t\n...\tConfiguration > Change settings\tVirtualMachine.Config.Settings\t\n...\tConfiguration > Change Resource\tVirtualMachine.Config.Resource\t\n...\tConfiguration > Set annotation\tVirtualMachine.Config.Annotation\t\n...\tEdit Inventory > Create from existing\tVirtualMachine.Inventory.CreateFromExisting\t\n...\tEdit Inventory > Create new\tVirtualMachine.Inventory.Create\t\n...\tEdit Inventory > Remove\tVirtualMachine.Inventory.Delete\t\n...\tInteraction > Configure CD media\tVirtualMachine.Interact.SetCDMedia\t\n...\tInteraction > Configure floppy media\tVirtualMachine.Interact.SetFloppyMedia\t\n...\tInteraction > Connect devices\tVirtualMachine.Interact.DeviceConnection\t\n...\tInteraction > Inject USB HID scan codes\tVirtualMachine.Interact.PutUsbScanCodes\t\n...\tInteraction > Power off\tVirtualMachine.Interact.PowerOff\t\n...\tInteraction > Power on\tVirtualMachine.Interact.PowerOn\t\n...\tProvisioning > Create template from virtual machine\tVirtualMachine.Provisioning.CreateTemplateFromVM\t\n...\tProvisioning > Mark as template\tVirtualMachine.Provisioning.MarkAsTemplate\t\n...\tProvisioning > Mark as virtual machine\tVirtualMachine.Provisioning.MarkAsVM\t\n...\tState > Create snapshot\tVirtualMachine.State.CreateSnapshot\t\nGlobal permissions are required for the content library based on the hierarchical inheritance of permissions. Once the custom vSphere role is created, assign Global Permissions in vSphere to the accounts or groups used for the Packer to vSphere integration, if using the content library.\nLog in to the vCenter Server at https://<management_vcenter_server_fqdn>/ui as administrator@vsphere.local.\nSelect Menu > Administration.\nIn the left pane, select Access control > Global permissions and click the Add permissions icon.\nIn the Add permissions dialog box, enter the service account (e.g. svc-packer-vsphere@example.com), select the custom role (e.g. Packer to vSphere Integration Role) and the Propagate to children check box, and click OK.\nIn an environment with many vCenter Server instances, such as management and workload, in enhanced linked-mode, you may wish to further reduce the scope of access across the vSphere infrastructure. For example, if you do not want Packer to have access to the management vCenter Server instance, but only allow access to workload vCenter Server instances:\nFrom the Hosts and clusters inventory, select management vCenter Server to restrict scope, and click the Permissions tab.\nSelect the service account with the custom role assigned and click the Change role icon.\nIn the Change role dialog box, from the Role drop-down menu, select No Access, select the Propagate to children check box, and click OK."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vsphere/v1.2.2/components/builder/vsphere-iso",
  "text": "VMware vSphere Builder (1.2.2) | Integrations | Packer\nSee complete Ubuntu, Windows, and macOS templates in the examples folder.\ncreate_snapshot (bool) - Create a snapshot when set to true, so the VM can be used as a base for linked clones. Defaults to false.\nsnapshot_name (string) - When create_snapshot is true, snapshot_name determines the name of the snapshot. Defaults to Created By Packer.\nconvert_to_template (bool) - Convert VM to a template. Defaults to false.\nexport (*common.ExportConfig) - Configuration for exporting VM to an ovf file. The VM will not be exported if no Export Configuration is specified.\ncontent_library_destination (*common.ContentLibraryDestinationConfig) - Configuration for importing the VM template to a Content Library. The VM template will not be imported if no Content Library Import Configuration is specified. The import doesn't work if convert_to_template is set to true.\nvgpu_profile (string) - vGPU profile for accelerated graphics. See NVIDIA GRID vGPU documentation for examples of profile names. Defaults to none.\nvm_name (string) - Name of the new VM to create.\nfolder (string) - VM folder to create the VM in.\ncluster (string) - vSphere cluster where target VM is created. See the Working With Clusters And Hosts section above for more details.\nhost (string) - ESXi host where target VM is created. A full path must be specified if the host is in a folder. For example folder/host. See the Working With Clusters And Hosts section above for more details.\nresource_pool (string) - vSphere resource pool. If not set, it will look for the root resource pool of the host or cluster. If a root resource is not found, it will then look for a default resource pool.\ndatastore (string) - vSphere datastore. Required if host is a cluster, or if host has multiple datastores.\nset_host_for_datastore_uploads (bool) - Set this to true if packer should use the host for uploading files to the datastore. Defaults to false.\niso_paths ([]string) - List of Datastore or Content Library paths to ISO files that will be mounted to the VM. Here's an HCL2 example:\niso_paths = [ \"[datastore1] ISO/ubuntu.iso\", \"Packer Library Test/ubuntu-16.04.6-server-amd64/ubuntu-16.04.6-server-amd64.iso\" ] \nvm_version (uint) - Set VM hardware version. Defaults to the most current VM hardware version supported by the vCenter Server version. See VMware KB article 1003746 for the full list of supported VM hardware versions.\nguest_os_type (string) - Set VM OS type. Defaults to otherGuest. See here for a full list of possible values.\nnetwork_adapters ([]NIC) - Sets the network adapters, if no adapter is specified, every network related task is not applicable.\nusb_controller ([]string) - Create USB controllers for the virtual machine. \"usb\" for a usb 2.0 controller. \"xhci\" for a usb 3.0 controller. There can only be at most one of each.\nnotes (string) - VM notes.\ndestroy (bool) - If set to true, the VM will be destroyed after the builder completes\nnetwork_card (string) - Set VM network card type. Example vmxnet3.\nnetwork (string) - Set the network in which the VM will be connected to. If no network is specified, host must be specified to allow Packer to look for the available network. If the network is inside a network folder in vCenter, you need to provide the full path to the network.\nmac_address (string) - Set network card MAC address\npassthrough (*bool) - Enable DirectPath I/O passthrough\ndisk_size (int64) - The size of the disk in MB.\ndisk_controller_index (int) - The assigned disk controller. Defaults to the first one (0)\nYou may optionally export an ovf from vSphere to the instance running Packer.\n... \"vm_name\": \"example-ubuntu\", ... \"export\": { \"force\": true, \"output_directory\": \"./output_vsphere\" }, \n# ... vm_name = \"example-ubuntu\" # ... export { force = true output_directory = \"./output_vsphere\" } \n./output_vsphere/example-ubuntu-disk-0.vmdk ./output_vsphere/example-ubuntu.mf ./output_vsphere/example-ubuntu.ovf \nname (string) - Name of the ovf. defaults to the name of the VM\nforce (bool) - Overwrite ovf if it exists\nimages (bool) - Deprecated: Images will be removed in a future release. Please see image_files for more details on this argument.\nimage_files (bool) - In exported files, include additional image files that are attached to the VM, such as nvram, iso, img.\nmanifest (string) - Generate manifest using sha1, sha256, sha512. Defaults to 'sha256'. Use 'none' for no manifest.\noptions ([]string) - Advanced ovf export options. Options can include:\nmac - MAC address is exported for all ethernet devices\nuuid - UUID is exported for all virtual machines\nextraconfig - all extra configuration options are exported for a virtual machine\nnodevicesubtypes - resource subtypes for CD/DVD drives, floppy drives, and serial and parallel ports are not exported\nFor example, adding the following export config option would output the mac addresses for all Ethernet devices in the ovf file:\nconfiguration_parameters (map[string]string) - configuration_parameters is a direct passthrough to the vSphere API's ConfigSpec: https://vdc-download.vmware.com/vmwb-repository/dcr-public/bf660c0a-f060-46e8-a94d-4b5e6ffc77ad/208bc706-e281-49b6-a0ce-b402ec19ef82/SDK/vsphere-ws/docs/ReferenceGuide/vim.vm.ConfigSpec.html\n\"host\": \"esxi-1.vsphere65.test\", \"resource_pool\": \"pool1\", \nhost = \"\"esxi-1.vsphere65.test\"\" resource_pool = \"pool1\" \n\"cluster\": \"cluster1\", \"host\": \"esxi-2.vsphere65.test\", \ncluster = \"cluster1\" host = \"esxi-2.vsphere65.test\" \nIt is recommended to create a custom vSphere role with the required privileges to integrate Packer with vSphere. Accounts or groups can be added to the role to ensure that Packer has the least privileged access to the infrastructure. For example, a named service account (e.g. svc-packer-vsphere@example.com). "
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vsphere/v1.2.3/components/builder/vsphere-iso",
  "text": "VMware vSphere Builder (1.2.3) | Integrations | Packer\nSee complete Ubuntu, Windows, and macOS templates in the examples folder.\ncreate_snapshot (bool) - Create a snapshot when set to true, so the VM can be used as a base for linked clones. Defaults to false.\nsnapshot_name (string) - When create_snapshot is true, snapshot_name determines the name of the snapshot. Defaults to Created By Packer.\nconvert_to_template (bool) - Convert VM to a template. Defaults to false.\nexport (*common.ExportConfig) - Configuration for exporting VM to an ovf file. The VM will not be exported if no Export Configuration is specified.\ncontent_library_destination (*common.ContentLibraryDestinationConfig) - Configuration for importing the VM template to a Content Library. The VM template will not be imported if no Content Library Import Configuration is specified. The import doesn't work if convert_to_template is set to true.\nvm_name (string) - Name of the new VM to create.\nfolder (string) - VM folder to create the VM in.\ncluster (string) - vSphere cluster where target VM is created. See the Working With Clusters And Hosts section above for more details.\nhost (string) - ESXi host where target VM is created. A full path must be specified if the host is in a folder. For example folder/host. See the Working With Clusters And Hosts section above for more details.\nresource_pool (string) - vSphere resource pool. If not set, it will look for the root resource pool of the host or cluster. If a root resource is not found, it will then look for a default resource pool.\ndatastore (string) - vSphere datastore. Required if host is a cluster, or if host has multiple datastores.\nset_host_for_datastore_uploads (bool) - Set this to true if packer should use the host for uploading files to the datastore. Defaults to false.\niso_paths ([]string) - List of Datastore or Content Library paths to ISO files that will be mounted to the VM. Here's an HCL2 example:\niso_paths = [ \"[datastore1] ISO/ubuntu.iso\", \"Packer Library Test/ubuntu-16.04.6-server-amd64/ubuntu-16.04.6-server-amd64.iso\" ] \nvm_version (uint) - Set VM hardware version. Defaults to the most current VM hardware version supported by the vCenter Server version. See VMware KB article 1003746 for the full list of supported VM hardware versions.\nguest_os_type (string) - Set VM OS type. Defaults to otherGuest. See here for a full list of possible values.\nnetwork_adapters ([]NIC) - Sets the network adapters, if no adapter is specified, every network related task is not applicable.\nusb_controller ([]string) - Create USB controllers for the virtual machine. \"usb\" for a usb 2.0 controller. \"xhci\" for a usb 3.0 controller. There can only be at most one of each.\nnotes (string) - VM notes.\ndestroy (bool) - If set to true, the VM will be destroyed after the builder completes\nnetwork_card (string) - Set VM network card type. Example vmxnet3.\nnetwork (string) - Set the network in which the VM will be connected to. If no network is specified, host must be specified to allow Packer to look for the available network. If the network is inside a network folder in vCenter, you need to provide the full path to the network.\nmac_address (string) - Set network card MAC address\npassthrough (*bool) - Enable DirectPath I/O passthrough\ndisk_size (int64) - The size of the disk in MB.\ndisk_controller_index (int) - The assigned disk controller. Defaults to the first one (0)\nYou may optionally export an ovf from vSphere to the instance running Packer.\n... \"vm_name\": \"example-ubuntu\", ... \"export\": { \"force\": true, \"output_directory\": \"./output_vsphere\" }, \n# ... vm_name = \"example-ubuntu\" # ... export { force = true output_directory = \"./output_vsphere\" } \n./output_vsphere/example-ubuntu-disk-0.vmdk ./output_vsphere/example-ubuntu.mf ./output_vsphere/example-ubuntu.ovf \nname (string) - Name of the ovf. defaults to the name of the VM\nforce (bool) - Overwrite ovf if it exists\nimages (bool) - Deprecated: Images will be removed in a future release. Please see image_files for more details on this argument.\nimage_files (bool) - In exported files, include additional image files that are attached to the VM, such as nvram, iso, img.\nmanifest (string) - Generate manifest using sha1, sha256, sha512. Defaults to 'sha256'. Use 'none' for no manifest.\noptions ([]string) - Advanced ovf export options. Options can include:\nmac - MAC address is exported for all ethernet devices\nuuid - UUID is exported for all virtual machines\nextraconfig - all extra configuration options are exported for a virtual machine\nnodevicesubtypes - resource subtypes for CD/DVD drives, floppy drives, and serial and parallel ports are not exported\nFor example, adding the following export config option would output the mac addresses for all Ethernet devices in the ovf file:\n\"host\": \"esxi-1.vsphere65.test\", \"resource_pool\": \"pool1\", \nhost = \"\"esxi-1.vsphere65.test\"\" resource_pool = \"pool1\" \n\"cluster\": \"cluster1\", \"host\": \"esxi-2.vsphere65.test\", \ncluster = \"cluster1\" host = \"esxi-2.vsphere65.test\" \nIt is recommended to create a custom vSphere role with the required privileges to integrate Packer with vSphere. Accounts or groups can be added to the role to ensure that Packer has the least privileged access to the infrastructure. For example, a named service account (e.g. svc-packer-vsphere@example.com). "
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vsphere/v1.2.4/components/builder/vsphere-iso",
  "text": "VMware vSphere Builder (1.2.4) | Integrations | Packer\nSee complete Ubuntu, Windows, and macOS templates in the examples folder.\ncreate_snapshot (bool) - Create a snapshot when set to true, so the VM can be used as a base for linked clones. Defaults to false.\nsnapshot_name (string) - When create_snapshot is true, snapshot_name determines the name of the snapshot. Defaults to Created By Packer.\nconvert_to_template (bool) - Convert VM to a template. Defaults to false.\nexport (*common.ExportConfig) - Configuration for exporting VM to an ovf file. The VM will not be exported if no Export Configuration is specified.\ncontent_library_destination (*common.ContentLibraryDestinationConfig) - Configuration for importing the VM template to a Content Library. The VM template will not be imported if no Content Library Import Configuration is specified. The import doesn't work if convert_to_template is set to true.\nvm_version (uint) - Set VM hardware version. Defaults to the most current VM hardware version supported by the vCenter Server version. See VMware KB article 1003746 for the full list of supported VM hardware versions.\nguest_os_type (string) - Set VM OS type. Defaults to otherGuest. See here for a full list of possible values.\nnetwork_adapters ([]NIC) - Sets the network adapters, if no adapter is specified, every network related task is not applicable.\nusb_controller ([]string) - Create USB controllers for the virtual machine. \"usb\" for a usb 2.0 controller. \"xhci\" for a usb 3.0 controller. There can only be at most one of each.\nnotes (string) - VM notes.\ndestroy (bool) - If set to true, the VM will be destroyed after the builder completes\nnetwork_card (string) - Set VM network card type. Example vmxnet3.\nnetwork (string) - Set the network in which the VM will be connected to. If no network is specified, host must be specified to allow Packer to look for the available network. If the network is inside a network folder in vCenter, you need to provide the full path to the network.\nmac_address (string) - Set network card MAC address\npassthrough (*bool) - Enable DirectPath I/O passthrough\nforce (bool) - Forces the export to overwrite existing files. Defaults to false. If set to false, the export will fail if the files already exists.\noptions ([]string) - Advanced image export options. Options can include:\n\"host\": \"esxi-1.vsphere65.test\", \"resource_pool\": \"pool1\", \nhost = \"\"esxi-1.vsphere65.test\"\" resource_pool = \"pool1\" \n\"cluster\": \"cluster1\", \"host\": \"esxi-2.vsphere65.test\", \ncluster = \"cluster1\" host = \"esxi-2.vsphere65.test\" \nIt is recommended to create a custom vSphere role with the required privileges to integrate Packer with vSphere. Accounts or groups can be added to the role to ensure that Packer has the least privileged access to the infrastructure. For example, a named service account (e.g. svc-packer-vsphere@example.com). "
},
{
  "url": "https://developer.hashicorp.com/vault/docs/v1.13.x/platform/k8s/helm/enterprise",
  "text": "Vault Enterprise License Management - Kubernetes | Vault\nYou can use this Helm chart to deploy Vault Enterprise by following a few extra steps around licensing.\nNote: As of Vault Enterprise 1.8, the license must be specified via HCL configuration or environment variables on startup, unless the Vault cluster was created with an older Vault version and the license was stored. More information is available in the Vault Enterprise License docs.\nImportant Note: This chart is not compatible with Helm 2. Please use Helm 3.6+ with this chart.\nLicense install\nFirst create a Kubernetes secret using the contents of your license file. For example, the following commands create a secret with the name vault-ent-license and key license:\nsecret=$(cat 1931d1f4-bdfd-6881-f3f5-19349374841f.hclic) kubectl create secret generic vault-ent-license --from-literal=\"license=${secret}\" \nNote: If you cannot find your .hclic file, please contact your sales team or Technical Account Manager.\nIn your chart overrides, set the values of server.image to one of the enterprise release tags. Also set the name of the secret you just created in server.enterpriseLicense.\n# config.yaml server: image: repository: hashicorp/vault-enterprise tag: 1.13.1-ent enterpriseLicense: secretName: vault-ent-license \nNow run helm install:\n$ helm install hashicorp hashicorp/vault -f config.yaml \nOnce the cluster is initialized and unsealed, you may check the license status using the vault license get command:\nkubectl exec -ti vault-0 -- vault license get \nLicense update\nTo update the autoloaded license in Vault, you may do the following:\nUpdate your license secret with the new license data\nnew_secret=$(base64 < ./new-license.hclic | tr -d '\\n') cat > patch-license.yaml <<EOF data: license: ${new_secret} EOF kubectl patch secret vault-ent-license --patch \"$(cat patch-license.yaml)\" \nWait until vault license inspect shows the updated license\nSince the inspect command is reading the license file from the mounted secret, this tells you when the updated secret has been propagated to the mount on the Vault pod.\nkubectl exec vault-0 -- vault license inspect \nReload Vault's license config\nYou may use the sys/config/reload/license API endpoint:\nkubectl exec vault-0 -- vault write -f sys/config/reload/license \nOr you may issue an HUP signal directly to Vault:\nkubectl exec vault-0 -- pkill -HUP vault \nVerify that vault license get shows the updated license\nkubectl exec vault-0 -- vault license get \nIn your chart overrides, set the values of server.image to one of the enterprise release tags. Install the chart, and initialize and unseal vault as described in Running Vault.\nAfter Vault has been initialized and unsealed, setup a port-forward tunnel to the Vault Enterprise cluster:\nkubectl port-forward vault-0 8200:8200 \nNext, in a separate terminal, create a payload.json file that contains the license key like this example:\n{ \"text\": \"01ABCDEFG...\" } \nFinally, using curl, apply the license key to the Vault API:\ncurl \\ --header \"X-Vault-Token: VAULT_LOGIN_TOKEN_HERE\" \\ --request POST \\ --data @payload.json \\ http://127.0.0.1:8200/v1/sys/license \nTo verify that the license installation worked correctly, using curl, run the following:\ncurl \\ --header \"X-Vault-Token: VAULT_LOGIN_TOKEN_HERE\" \\ http://127.0.0.1:8200/v1/sys/license"
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vsphere/v1.2.0/components/builder/vsphere-iso",
  "text": "VMware vSphere Builder (1.2.0) | Integrations | Packer\nSee complete Ubuntu, Windows, and macOS templates in the examples folder.\ncreate_snapshot (bool) - Create a snapshot when set to true, so the VM can be used as a base for linked clones. Defaults to false.\nsnapshot_name (string) - When create_snapshot is true, snapshot_name determines the name of the snapshot. Defaults to Created By Packer.\nconvert_to_template (bool) - Convert VM to a template. Defaults to false.\nexport (*common.ExportConfig) - Configuration for exporting VM to an ovf file. The VM will not be exported if no Export Configuration is specified.\ncontent_library_destination (*common.ContentLibraryDestinationConfig) - Configuration for importing the VM template to a Content Library. The VM template will not be imported if no Content Library Import Configuration is specified. The import doesn't work if convert_to_template is set to true.\n`wget http://{{ .HTTPIP }}:{{ .HTTPPort }}/foo/bar/preseed.cfg` \nvm_name (string) - Name of the new VM to create.\nfolder (string) - VM folder to create the VM in.\ncluster (string) - vSphere cluster where target VM is created. See the Working With Clusters And Hosts section above for more details.\nhost (string) - ESXi host where target VM is created. A full path must be specified if the host is in a folder. For example folder/host. See the Working With Clusters And Hosts section above for more details.\nresource_pool (string) - vSphere resource pool. If not set, it will look for the root resource pool of the host or cluster. If a root resource is not found, it will then look for a default resource pool.\ndatastore (string) - vSphere datastore. Required if host is a cluster, or if host has multiple datastores.\nset_host_for_datastore_uploads (bool) - Set this to true if packer should use the host for uploading files to the datastore. Defaults to false.\niso_paths ([]string) - List of Datastore or Content Library paths to ISO files that will be mounted to the VM. Here's an HCL2 example:\niso_paths = [ \"[datastore1] ISO/ubuntu.iso\", \"Packer Library Test/ubuntu-16.04.6-server-amd64/ubuntu-16.04.6-server-amd64.iso\" ] \nvm_version (uint) - Set VM hardware version. Defaults to the most current VM hardware version supported by the vCenter Server version. See VMware KB article 1003746 for the full list of supported VM hardware versions.\nguest_os_type (string) - Set VM OS type. Defaults to otherGuest. See here for a full list of possible values.\nnetwork_adapters ([]NIC) - Sets the network adapters, if no adapter is specified, every network related task is not applicable.\nusb_controller ([]string) - Create USB controllers for the virtual machine. \"usb\" for a usb 2.0 controller. \"xhci\" for a usb 3.0 controller. There can only be at most one of each.\nnotes (string) - VM notes.\ndestroy (bool) - If set to true, the VM will be destroyed after the builder completes\nnetwork_card (string) - Set VM network card type. Example vmxnet3.\nnetwork (string) - Set the network in which the VM will be connected to. If no network is specified, host must be specified to allow Packer to look for the available network. If the network is inside a network folder in vCenter, you need to provide the full path to the network.\nmac_address (string) - Set network card MAC address\npassthrough (*bool) - Enable DirectPath I/O passthrough\ndisk_size (int64) - The size of the disk in MB.\ndisk_controller_index (int) - The assigned disk controller. Defaults to the first one (0)\nYou may optionally export an ovf from vSphere to the instance running Packer.\n... \"vm_name\": \"example-ubuntu\", ... \"export\": { \"force\": true, \"output_directory\": \"./output_vsphere\" }, \n# ... vm_name = \"example-ubuntu\" # ... export { force = true output_directory = \"./output_vsphere\" } \n./output_vsphere/example-ubuntu-disk-0.vmdk ./output_vsphere/example-ubuntu.mf ./output_vsphere/example-ubuntu.ovf \nname (string) - Name of the ovf. defaults to the name of the VM\nforce (bool) - Overwrite ovf if it exists\nimages (bool) - Deprecated: Images will be removed in a future release. Please see image_files for more details on this argument.\nimage_files (bool) - In exported files, include additional image files that are attached to the VM, such as nvram, iso, img.\nmanifest (string) - Generate manifest using sha1, sha256, sha512. Defaults to 'sha256'. Use 'none' for no manifest.\noptions ([]string) - Advanced ovf export options. Options can include:\nmac - MAC address is exported for all ethernet devices\nuuid - UUID is exported for all virtual machines\nextraconfig - all extra configuration options are exported for a virtual machine\nnodevicesubtypes - resource subtypes for CD/DVD drives, floppy drives, and serial and parallel ports are not exported\nFor example, adding the following export config option would output the mac addresses for all Ethernet devices in the ovf file:\n\"host\": \"esxi-1.vsphere65.test\", \"resource_pool\": \"pool1\", \nhost = \"\"esxi-1.vsphere65.test\"\" resource_pool = \"pool1\" \n\"cluster\": \"cluster1\", \"host\": \"esxi-2.vsphere65.test\", \ncluster = \"cluster1\" host = \"esxi-2.vsphere65.test\" \nIt is recommended to create a custom vSphere role with the required privileges to integrate Packer with vSphere. Accounts or groups can be added to the role to ensure that Packer has the least privileged access to the infrastructure. For example, a named service account (e.g. svc-packer-vsphere@example.com). \n...\tLow level file operations\tDatastore.Browse\t"
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vsphere/v1.2.1/components/builder/vsphere-iso",
  "text": "VMware vSphere Builder (1.2.1) | Integrations | Packer\nvm_name (string) - Name of the new VM to create.\nfolder (string) - VM folder to create the VM in.\ncluster (string) - vSphere cluster where target VM is created. See the Working With Clusters And Hosts section above for more details.\nhost (string) - ESXi host where target VM is created. A full path must be specified if the host is in a folder. For example folder/host. See the Working With Clusters And Hosts section above for more details.\nresource_pool (string) - vSphere resource pool. If not set, it will look for the root resource pool of the host or cluster. If a root resource is not found, it will then look for a default resource pool.\ndatastore (string) - vSphere datastore. Required if host is a cluster, or if host has multiple datastores.\nset_host_for_datastore_uploads (bool) - Set this to true if packer should use the host for uploading files to the datastore. Defaults to false.\niso_paths ([]string) - List of Datastore or Content Library paths to ISO files that will be mounted to the VM. Here's an HCL2 example:\niso_paths = [ \"[datastore1] ISO/ubuntu.iso\", \"Packer Library Test/ubuntu-16.04.6-server-amd64/ubuntu-16.04.6-server-amd64.iso\" ] \ndisk_size (int64) - The size of the disk in MB.\ndisk_controller_index (int) - The assigned disk controller. Defaults to the first one (0)\nYou may optionally export an ovf from vSphere to the instance running Packer.\n... \"vm_name\": \"example-ubuntu\", ... \"export\": { \"force\": true, \"output_directory\": \"./output_vsphere\" }, \n# ... vm_name = \"example-ubuntu\" # ... export { force = true output_directory = \"./output_vsphere\" } \n./output_vsphere/example-ubuntu-disk-0.vmdk ./output_vsphere/example-ubuntu.mf ./output_vsphere/example-ubuntu.ovf \nname (string) - Name of the ovf. defaults to the name of the VM\nforce (bool) - Overwrite ovf if it exists\nimages (bool) - Deprecated: Images will be removed in a future release. Please see image_files for more details on this argument.\nimage_files (bool) - In exported files, include additional image files that are attached to the VM, such as nvram, iso, img.\nmanifest (string) - Generate manifest using sha1, sha256, sha512. Defaults to 'sha256'. Use 'none' for no manifest.\noptions ([]string) - Advanced ovf export options. Options can include:\nmac - MAC address is exported for all ethernet devices\nuuid - UUID is exported for all virtual machines\nextraconfig - all extra configuration options are exported for a virtual machine\nnodevicesubtypes - resource subtypes for CD/DVD drives, floppy drives, and serial and parallel ports are not exported\nFor example, adding the following export config option would output the mac addresses for all Ethernet devices in the ovf file:"
},
{
  "url": "https://developer.hashicorp.com/vault/docs/v1.13.x/platform/k8s/helm/openshift",
  "text": "Running Vault - OpenShift | Vault\nImportant Note: This chart is not compatible with Helm 2. Please use Helm 3.6+ with this chart.\nThe following documentation describes installing, running, and using Vault and Vault Agent Injector on OpenShift.\nNote: We recommend using the Vault agent injector on Openshift instead of the Secrets Store CSI driver. OpenShift does not recommend using hostPath mounting in production or certify Helm charts using CSI objects because pods must run as privileged. If you would like to run the Secrets Store CSI driver on a development or testing cluster, refer to installation instructions for the Vault CSI provider.\nThe following are required to install Vault and Vault Agent Injector on OpenShift:\nCluster Admin privileges to bind the auth-delegator role to Vault's service account\nHelm v3.6+\nOpenShift 4.3+\nVault Helm v0.6.0+\nVault K8s v0.4.0+\nNote: Support for Consul on OpenShift is available since Consul 1.9. However, for highly available deployments, Raft integrated storage is recommended.\nThe documentation, configuration and examples for Vault Helm and Vault K8s Agent Injector are applicable to OpenShift installations. For more examples see the existing documentation:\nVault Helm documentation\nVault K8s documentation\nThe Vault Helm chart is the recommended way to install and configure Vault on OpenShift. In addition to running Vault itself, the Helm chart is the primary method for installing and configuring Vault Agent Injection Mutating Webhook.\nWhile the Helm chart automatically sets up complex resources and exposes the configuration to meet your requirements, it does not automatically operate Vault. You are still responsible for learning how to monitor, backup, upgrade, etc. the Vault cluster.\nSecurity Warning: By default, the chart runs in standalone mode. This mode uses a single Vault server with a file storage backend. This is a less secure and less resilient installation that is NOT appropriate for a production setup. It is highly recommended to use a properly secured Kubernetes cluster, learn the available configuration options, and read the production deployment checklist.\nInstall Vault\nTo use the Helm chart, add the Hashicorp helm repository and check that you have access to the chart:\n$ helm repo add hashicorp https://helm.releases.hashicorp.com \"hashicorp\" has been added to your repositories $ helm search repo hashicorp/vault NAME CHART VERSION APP VERSION DESCRIPTION hashicorp/vault 0.24.0 1.13.1 Official HashiCorp Vault Chart \nImportant: The Helm chart is new and under significant development. Please always run Helm with --dry-run before any install or upgrade to verify changes.\nUse helm install to install the latest release of the Vault Helm chart.\n$ helm install vault hashicorp/vault \nOr install a specific version of the chart.\n# List the available releases $ helm search repo hashicorp/vault -l NAME CHART VERSION APP VERSION DESCRIPTION hashicorp/vault 0.24.0 1.13.1 Official HashiCorp Vault Chart hashicorp/vault 0.23.0 1.12.1 Official HashiCorp Vault Chart hashicorp/vault 0.22.1 1.12.0 Official HashiCorp Vault Chart hashicorp/vault 0.22.0 1.11.3 Official HashiCorp Vault Chart hashicorp/vault 0.21.0 1.11.2 Official HashiCorp Vault Chart hashicorp/vault 0.20.1 1.10.3 Official HashiCorp Vault Chart hashicorp/vault 0.20.0 1.10.3 Official HashiCorp Vault Chart ... # Install version 0.24.0 $ helm install vault hashicorp/vault --version 0.24.0 \nThe helm install command accepts parameters to override default configuration values inline or defined in a file. For all OpenShift deployments, global.openshift should be set to true.\nOverride the server.dev.enabled configuration value:\n$ helm install vault hashicorp/vault \\ --set \"global.openshift=true\" \\ --set \"server.dev.enabled=true\" \nOverride all the configuration found in a file:\n$ cat override-values.yml global: openshift: true server: ha: enabled: true replicas: 5 ## $ helm install vault hashicorp/vault \\ --values override-values.yml \nDev mode\nThe Helm chart may run a Vault server in development. This installs a single Vault server with a memory storage backend.\nDev mode: This is ideal for learning and demonstration environments but NOT recommended for a production environment.\nInstall the latest Vault Helm chart in development mode.\n$ helm install vault hashicorp/vault \\ --set \"global.openshift=true\" \\ --set \"server.dev.enabled=true\" \nHighly available raft mode\nThe following creates a Vault cluster using the Raft integrated storage backend.\nInstall the latest Vault Helm chart in HA Raft mode:\n$ helm install vault hashicorp/vault \\ --set='global.openshift=true' \\ --set='server.ha.enabled=true' \\ --set='server.ha.raft.enabled=true' \nNext, initialize and unseal vault-0 pod:\n$ oc exec -ti vault-0 -- vault operator init $ oc exec -ti vault-0 -- vault operator unseal \nFinally, join the remaining pods to the Raft cluster and unseal them. The pods will need to communicate directly so we'll configure the pods to use the internal service provided by the Helm chart:\n$ oc exec -ti vault-1 -- vault operator raft join http://vault-0.vault-internal:8200 $ oc exec -ti vault-1 -- vault operator unseal $ oc exec -ti vault-2 -- vault operator raft join http://vault-0.vault-internal:8200 $ oc exec -ti vault-2 -- vault operator unseal \nTo verify if the Raft cluster has successfully been initialized, run the following.\nFirst, login using the root token on the vault-0 pod:\n$ oc exec -ti vault-0 -- vault login \nNext, list all the raft peers:\n$ oc exec -ti vault-0 -- vault operator raft list-peers Node Address State Voter ---- ------- ----- ----- a1799962-8711-7f28-23f0-cea05c8a527d vault-0.vault-internal:8201 leader true e6876c97-aaaa-a92e-b99a-0aafab105745 vault-1.vault-internal:8201 follower true 4b5d7383-ff31-44df-e008-6a606828823b vault-2.vault-internal:8201 follower true \nVault with integrated storage (Raft) is now ready to use!\nExternal mode\nThe Helm chart may be run in external mode. This installs no Vault server and relies on a network addressable Vault server to exist.\nInstall the latest Vault Helm chart in external mode.\n$ helm install vault hashicorp/vault \\ --set \"global.openshift=true\" \\ --set \"injector.externalVaultAddr=http://external-vault:8200\" \nRefer to the Integrate a Kubernetes Cluster with an External Vault tutorial to learn how to use an external Vault within a Kubernetes cluster."
},
{
  "url": "https://developer.hashicorp.com/vault/docs/v1.13.x/platform/k8s/helm/terraform",
  "text": "Configure Vault Helm using Terraform | Vault\nConfiguring Vault helm with terraform\nTerraform may also be used to configure and deploy the Vault Helm chart, by using the Helm provider.\nFor example, to configure the chart to deploy HA Vault with integrated storage (raft), the values overrides can be set on the command-line, in a values yaml file, or with a Terraform configuration:\n$ helm install vault hashicorp/vault \\ --set='server.ha.enabled=true' \\ --set='server.ha.raft.enabled=true' \nThe values file can also be used directly in the Terraform configuration with the values directive.\nVault config as a multi-line string\nserver: ha: enabled: true raft: enabled: true setNodeId: true config: | ui = false listener \"tcp\" { tls_disable = 1 address = \"[::]:8200\" cluster_address = \"[::]:8201\" } storage \"raft\" { path = \"/vault/data\" } service_registration \"kubernetes\" {} seal \"awskms\" { region = \"us-west-2\" kms_key_id = \"alias/my-kms-key\" } \nLists of volumes and volumeMounts\nserver: volumes: - name: userconfig-my-gcp-iam secret: defaultMode: 420 secretName: my-gcp-iam volumeMounts: - mountPath: /vault/userconfig/my-gcp-iam name: userconfig-my-gcp-iam readOnly: true \nAnnotations\nAnnotations can be set as a YAML map:\nserver: ingress: annotations: service.beta.kubernetes.io/azure-load-balancer-internal: true service.beta.kubernetes.io/azure-load-balancer-internal-subnet: apps-subnet \nor as a multi-line string:\nserver: ingress: annotations: | service.beta.kubernetes.io/azure-load-balancer-internal: true service.beta.kubernetes.io/azure-load-balancer-internal-subnet: apps-subnet"
},
{
  "url": "https://developer.hashicorp.com/vault/docs/v1.12.x/platform/k8s/helm/enterprise",
  "text": "Vault Enterprise License Management - Kubernetes | Vault\nYou can use this Helm chart to deploy Vault Enterprise by following a few extra steps around licensing.\nNote: As of Vault Enterprise 1.8, the license must be specified via HCL configuration or environment variables on startup, unless the Vault cluster was created with an older Vault version and the license was stored. More information is available in the Vault Enterprise License docs.\nImportant Note: This chart is not compatible with Helm 2. Please use Helm 3.6+ with this chart.\nLicense install\nFirst create a Kubernetes secret using the contents of your license file. For example, the following commands create a secret with the name vault-ent-license and key license:\nsecret=$(cat 1931d1f4-bdfd-6881-f3f5-19349374841f.hclic) kubectl create secret generic vault-ent-license --from-literal=\"license=${secret}\" \nNote: If you cannot find your .hclic file, please contact your sales team or Technical Account Manager.\nIn your chart overrides, set the values of server.image to one of the enterprise release tags. Also set the name of the secret you just created in server.enterpriseLicense.\n# config.yaml server: image: repository: hashicorp/vault-enterprise tag: 1.12.1-ent enterpriseLicense: secretName: vault-ent-license \nNow run helm install:\n$ helm install hashicorp hashicorp/vault -f config.yaml \nOnce the cluster is initialized and unsealed, you may check the license status using the vault license get command:\nkubectl exec -ti vault-0 -- vault license get \nLicense update\nTo update the autoloaded license in Vault, you may do the following:\nUpdate your license secret with the new license data\nnew_secret=$(base64 < ./new-license.hclic | tr -d '\\n') cat > patch-license.yaml <<EOF data: license: ${new_secret} EOF kubectl patch secret vault-ent-license --patch \"$(cat patch-license.yaml)\" \nWait until vault license inspect shows the updated license\nSince the inspect command is reading the license file from the mounted secret, this tells you when the updated secret has been propagated to the mount on the Vault pod.\nkubectl exec vault-0 -- vault license inspect \nReload Vault's license config\nYou may use the sys/config/reload/license API endpoint:\nkubectl exec vault-0 -- vault write -f sys/config/reload/license \nOr you may issue an HUP signal directly to Vault:\nkubectl exec vault-0 -- pkill -HUP vault \nVerify that vault license get shows the updated license\nkubectl exec vault-0 -- vault license get \nIn your chart overrides, set the values of server.image to one of the enterprise release tags. Install the chart, and initialize and unseal vault as described in Running Vault.\nAfter Vault has been initialized and unsealed, setup a port-forward tunnel to the Vault Enterprise cluster:\nkubectl port-forward vault-0 8200:8200 \nNext, in a separate terminal, create a payload.json file that contains the license key like this example:\n{ \"text\": \"01ABCDEFG...\" } \nFinally, using curl, apply the license key to the Vault API:\ncurl \\ --header \"X-Vault-Token: VAULT_LOGIN_TOKEN_HERE\" \\ --request POST \\ --data @payload.json \\ http://127.0.0.1:8200/v1/sys/license \nTo verify that the license installation worked correctly, using curl, run the following:\ncurl \\ --header \"X-Vault-Token: VAULT_LOGIN_TOKEN_HERE\" \\ http://127.0.0.1:8200/v1/sys/license"
},
{
  "url": "https://developer.hashicorp.com/vault/docs/v1.12.x/platform/k8s/helm/openshift",
  "text": "Running Vault - OpenShift | Vault\nImportant Note: This chart is not compatible with Helm 2. Please use Helm 3.6+ with this chart.\nThe following documentation describes installing, running, and using Vault and Vault Agent Injector on OpenShift.\nNote: We recommend using the Vault agent injector on Openshift instead of the Secrets Store CSI driver. OpenShift does not recommend using hostPath mounting in production or certify Helm charts using CSI objects because pods must run as privileged. If you would like to run the Secrets Store CSI driver on a development or testing cluster, refer to installation instructions for the Vault CSI provider.\nThe following are required to install Vault and Vault Agent Injector on OpenShift:\nCluster Admin privileges to bind the auth-delegator role to Vault's service account\nHelm v3.6+\nOpenShift 4.3+\nVault Helm v0.6.0+\nVault K8s v0.4.0+\nNote: Support for Consul on OpenShift is available since Consul 1.9. However, for highly available deployments, Raft integrated storage is recommended.\nThe documentation, configuration and examples for Vault Helm and Vault K8s Agent Injector are applicable to OpenShift installations. For more examples see the existing documentation:\nVault Helm documentation\nVault K8s documentation\nThe Vault Helm chart is the recommended way to install and configure Vault on OpenShift. In addition to running Vault itself, the Helm chart is the primary method for installing and configuring Vault Agent Injection Mutating Webhook.\nWhile the Helm chart automatically sets up complex resources and exposes the configuration to meet your requirements, it does not automatically operate Vault. You are still responsible for learning how to monitor, backup, upgrade, etc. the Vault cluster.\nSecurity Warning: By default, the chart runs in standalone mode. This mode uses a single Vault server with a file storage backend. This is a less secure and less resilient installation that is NOT appropriate for a production setup. It is highly recommended to use a properly secured Kubernetes cluster, learn the available configuration options, and read the production deployment checklist.\nInstall Vault\nTo use the Helm chart, add the Hashicorp helm repository and check that you have access to the chart:\n$ helm repo add hashicorp https://helm.releases.hashicorp.com \"hashicorp\" has been added to your repositories $ helm search repo hashicorp/vault NAME CHART VERSION APP VERSION DESCRIPTION hashicorp/vault 0.23.0 1.12.1 Official HashiCorp Vault Chart \nImportant: The Helm chart is new and under significant development. Please always run Helm with --dry-run before any install or upgrade to verify changes.\nUse helm install to install the latest release of the Vault Helm chart.\n$ helm install vault hashicorp/vault \nOr install a specific version of the chart.\n# List the available releases $ helm search repo hashicorp/vault -l NAME CHART VERSION APP VERSION DESCRIPTION hashicorp/vault 0.23.0 1.12.1 Official HashiCorp Vault Chart hashicorp/vault 0.22.1 1.12.0 Official HashiCorp Vault Chart hashicorp/vault 0.22.0 1.11.3 Official HashiCorp Vault Chart hashicorp/vault 0.21.0 1.11.2 Official HashiCorp Vault Chart hashicorp/vault 0.20.1 1.10.3 Official HashiCorp Vault Chart hashicorp/vault 0.20.0 1.10.3 Official HashiCorp Vault Chart hashicorp/vault 0.19.0 1.9.2 Official HashiCorp Vault Chart ... # Install version 0.23.0 $ helm install vault hashicorp/vault --version 0.23.0 \nThe helm install command accepts parameters to override default configuration values inline or defined in a file. For all OpenShift deployments, global.openshift should be set to true.\nOverride the server.dev.enabled configuration value:\n$ helm install vault hashicorp/vault \\ --set \"global.openshift=true\" \\ --set \"server.dev.enabled=true\" \nOverride all the configuration found in a file:\n$ cat override-values.yml global: openshift: true server: ha: enabled: true replicas: 5 ## $ helm install vault hashicorp/vault \\ --values override-values.yml \nDev mode\nThe Helm chart may run a Vault server in development. This installs a single Vault server with a memory storage backend.\nDev mode: This is ideal for learning and demonstration environments but NOT recommended for a production environment.\nInstall the latest Vault Helm chart in development mode.\n$ helm install vault hashicorp/vault \\ --set \"global.openshift=true\" \\ --set \"server.dev.enabled=true\" \nHighly available raft mode\nThe following creates a Vault cluster using the Raft integrated storage backend.\nInstall the latest Vault Helm chart in HA Raft mode:\n$ helm install vault hashicorp/vault \\ --set='global.openshift=true' \\ --set='server.ha.enabled=true' \\ --set='server.ha.raft.enabled=true' \nNext, initialize and unseal vault-0 pod:\n$ oc exec -ti vault-0 -- vault operator init $ oc exec -ti vault-0 -- vault operator unseal \nFinally, join the remaining pods to the Raft cluster and unseal them. The pods will need to communicate directly so we'll configure the pods to use the internal service provided by the Helm chart:\n$ oc exec -ti vault-1 -- vault operator raft join http://vault-0.vault-internal:8200 $ oc exec -ti vault-1 -- vault operator unseal $ oc exec -ti vault-2 -- vault operator raft join http://vault-0.vault-internal:8200 $ oc exec -ti vault-2 -- vault operator unseal \nTo verify if the Raft cluster has successfully been initialized, run the following.\nFirst, login using the root token on the vault-0 pod:\n$ oc exec -ti vault-0 -- vault login \nNext, list all the raft peers:\n$ oc exec -ti vault-0 -- vault operator raft list-peers Node Address State Voter ---- ------- ----- ----- a1799962-8711-7f28-23f0-cea05c8a527d vault-0.vault-internal:8201 leader true e6876c97-aaaa-a92e-b99a-0aafab105745 vault-1.vault-internal:8201 follower true 4b5d7383-ff31-44df-e008-6a606828823b vault-2.vault-internal:8201 follower true \nVault with integrated storage (Raft) is now ready to use!\nExternal mode\nThe Helm chart may be run in external mode. This installs no Vault server and relies on a network addressable Vault server to exist.\nInstall the latest Vault Helm chart in external mode.\n$ helm install vault hashicorp/vault \\ --set \"global.openshift=true\" \\ --set \"injector.externalVaultAddr=http://external-vault:8200\" \nRefer to the Integrate a Kubernetes Cluster with an External Vault tutorial to learn how to use an external Vault within a Kubernetes cluster."
},
{
  "url": "https://developer.hashicorp.com/vault/docs/v1.12.x/platform/k8s/helm/terraform",
  "text": "Configure Vault Helm using Terraform | Vault\nConfiguring Vault helm with terraform\nTerraform may also be used to configure and deploy the Vault Helm chart, by using the Helm provider.\nFor example, to configure the chart to deploy HA Vault with integrated storage (raft), the values overrides can be set on the command-line, in a values yaml file, or with a Terraform configuration:\n$ helm install vault hashicorp/vault \\ --set='server.ha.enabled=true' \\ --set='server.ha.raft.enabled=true' \nThe values file can also be used directly in the Terraform configuration with the values directive.\nVault config as a multi-line string\nserver: ha: enabled: true raft: enabled: true setNodeId: true config: | ui = false listener \"tcp\" { tls_disable = 1 address = \"[::]:8200\" cluster_address = \"[::]:8201\" } storage \"raft\" { path = \"/vault/data\" } service_registration \"kubernetes\" {} seal \"awskms\" { region = \"us-west-2\" kms_key_id = \"alias/my-kms-key\" } \nLists of volumes and volumeMounts\nserver: volumes: - name: userconfig-my-gcp-iam secret: defaultMode: 420 secretName: my-gcp-iam volumeMounts: - mountPath: /vault/userconfig/my-gcp-iam name: userconfig-my-gcp-iam readOnly: true \nAnnotations\nAnnotations can be set as a YAML map:\nserver: ingress: annotations: service.beta.kubernetes.io/azure-load-balancer-internal: true service.beta.kubernetes.io/azure-load-balancer-internal-subnet: apps-subnet \nor as a multi-line string:\nserver: ingress: annotations: | service.beta.kubernetes.io/azure-load-balancer-internal: true service.beta.kubernetes.io/azure-load-balancer-internal-subnet: apps-subnet"
},
{
  "url": "https://developer.hashicorp.com/vault/docs/v1.11.x/platform/k8s/helm/enterprise",
  "text": "Vault Enterprise License Management - Kubernetes | Vault\nYou can use this Helm chart to deploy Vault Enterprise by following a few extra steps around licensing.\nNote: As of Vault Enterprise 1.8, the license must be specified via HCL configuration or environment variables on startup, unless the Vault cluster was created with an older Vault version and the license was stored. More information is available in the Vault Enterprise License docs.\nLicense install\nFirst create a Kubernetes secret using the contents of your license file. For example, the following commands create a secret with the name vault-ent-license and key license:\nsecret=$(cat 1931d1f4-bdfd-6881-f3f5-19349374841f.hclic) kubectl create secret generic vault-ent-license --from-literal=\"license=${secret}\" \nNote: If you cannot find your .hclic file, please contact your sales team or Technical Account Manager.\nIn your chart overrides, set the values of server.image to one of the enterprise release tags. Also set the name of the secret you just created in server.enterpriseLicense.\n# config.yaml server: image: repository: hashicorp/vault-enterprise tag: 1.11.3-ent enterpriseLicense: secretName: vault-ent-license \nNow run helm install:\n$ helm install hashicorp hashicorp/vault -f config.yaml \nOnce the cluster is initialized and unsealed, you may check the license status using the vault license get command:\nkubectl exec -ti vault-0 -- vault license get \nLicense update\nTo update the autoloaded license in Vault, you may do the following:\nUpdate your license secret with the new license data\nnew_secret=$(base64 < ./new-license.hclic | tr -d '\\n') cat > patch-license.yaml <<EOF data: license: ${new_secret} EOF kubectl patch secret vault-ent-license --patch \"$(cat patch-license.yaml)\" \nWait until vault license inspect shows the updated license\nSince the inspect command is reading the license file from the mounted secret, this tells you when the updated secret has been propagated to the mount on the Vault pod.\nkubectl exec vault-0 -- vault license inspect \nReload Vault's license config\nYou may use the sys/config/reload/license API endpoint:\nkubectl exec vault-0 -- vault write -f sys/config/reload/license \nOr you may issue an HUP signal directly to Vault:\nkubectl exec vault-0 -- pkill -HUP vault \nVerify that vault license get shows the updated license\nkubectl exec vault-0 -- vault license get \nIn your chart overrides, set the values of server.image to one of the enterprise release tags. Install the chart, and initialize and unseal vault as described in Running Vault.\nAfter Vault has been initialized and unsealed, setup a port-forward tunnel to the Vault Enterprise cluster:\nkubectl port-forward vault-0 8200:8200 \nNext, in a separate terminal, create a payload.json file that contains the license key like this example:\n{ \"text\": \"01ABCDEFG...\" } \nFinally, using curl, apply the license key to the Vault API:\ncurl \\ --header \"X-Vault-Token: VAULT_LOGIN_TOKEN_HERE\" \\ --request POST \\ --data @payload.json \\ http://127.0.0.1:8200/v1/sys/license \nTo verify that the license installation worked correctly, using curl, run the following:\ncurl \\ --header \"X-Vault-Token: VAULT_LOGIN_TOKEN_HERE\" \\ http://127.0.0.1:8200/v1/sys/license"
},
{
  "url": "https://developer.hashicorp.com/vault/docs/v1.11.x/platform/k8s/helm/openshift",
  "text": "Running Vault - OpenShift | Vault\nThe following documentation describes installing, running, and using Vault and Vault Agent Injector on OpenShift.\nNote: We recommend using the Vault agent injector on Openshift instead of the Secrets Store CSI driver. OpenShift does not recommend using hostPath mounting in production or certify Helm charts using CSI objects because pods must run as privileged. If you would like to run the Secrets Store CSI driver on a development or testing cluster, refer to installation instructions for the Vault CSI provider.\nThe following are required to install Vault and Vault Agent Injector on OpenShift:\nCluster Admin privileges to bind the auth-delegator role to Vault's service account\nHelm v3.6+\nOpenShift 4.3+\nVault Helm v0.6.0+\nVault K8s v0.4.0+\nNote: Support for Consul on OpenShift is available since Consul 1.9. However, for highly available deployments, Raft integrated storage is recommended.\nThe documentation, configuration and examples for Vault Helm and Vault K8s Agent Injector are applicable to OpenShift installations. For more examples see the existing documentation:\nVault Helm documentation\nVault K8s documentation\nThe Vault Helm chart is the recommended way to install and configure Vault on OpenShift. In addition to running Vault itself, the Helm chart is the primary method for installing and configuring Vault Agent Injection Mutating Webhook.\nWhile the Helm chart automatically sets up complex resources and exposes the configuration to meet your requirements, it does not automatically operate Vault. You are still responsible for learning how to monitor, backup, upgrade, etc. the Vault cluster.\nSecurity Warning: By default, the chart runs in standalone mode. This mode uses a single Vault server with a file storage backend. This is a less secure and less resilient installation that is NOT appropriate for a production setup. It is highly recommended to use a properly secured Kubernetes cluster, learn the available configuration options, and read the production deployment checklist.\nInstall Vault\nTo use the Helm chart, add the Hashicorp helm repository and check that you have access to the chart:\n$ helm repo add hashicorp https://helm.releases.hashicorp.com \"hashicorp\" has been added to your repositories $ helm search repo hashicorp/vault NAME CHART VERSION APP VERSION DESCRIPTION hashicorp/vault 0.22.0 1.11.3 Official HashiCorp Vault Chart \nImportant: The Helm chart is new and under significant development. Please always run Helm with --dry-run before any install or upgrade to verify changes.\nUse helm install to install the latest release of the Vault Helm chart.\n$ helm install vault hashicorp/vault \nOr install a specific version of the chart.\n# List the available releases $ helm search repo hashicorp/vault -l NAME CHART VERSION APP VERSION DESCRIPTION hashicorp/vault 0.22.0 1.11.3 Official HashiCorp Vault Chart hashicorp/vault 0.21.0 1.11.2 Official HashiCorp Vault Chart hashicorp/vault 0.20.1 1.10.3 Official HashiCorp Vault Chart hashicorp/vault 0.20.0 1.10.3 Official HashiCorp Vault Chart hashicorp/vault 0.19.0 1.9.2 Official HashiCorp Vault Chart hashicorp/vault 0.18.0 1.9.0 Official HashiCorp Vault Chart hashicorp/vault 0.17.1 1.8.4 Official HashiCorp Vault Chart ... # Install version 0.22.0 $ helm install vault hashicorp/vault --version 0.22.0 \nThe helm install command accepts parameters to override default configuration values inline or defined in a file. For all OpenShift deployments, global.openshift should be set to true.\nOverride the server.dev.enabled configuration value:\nOverride all the configuration found in a file:\n$ cat override-values.yml global: openshift: true server: ha: enabled: true replicas: 5 ## $ helm install vault hashicorp/vault \\ --values override-values.yml \nDev mode\nThe Helm chart may run a Vault server in development. This installs a single Vault server with a memory storage backend.\nDev mode: This is ideal for learning and demonstration environments but NOT recommended for a production environment.\nInstall the latest Vault Helm chart in development mode.\nHighly available raft mode\nThe following creates a Vault cluster using the Raft integrated storage backend.\nInstall the latest Vault Helm chart in HA Raft mode:\n$ helm install vault hashicorp/vault \\ --set='global.openshift=true' \\ --set='server.ha.enabled=true' \\ --set='server.ha.raft.enabled=true' \nNext, initialize and unseal vault-0 pod:\n$ oc exec -ti vault-0 -- vault operator init $ oc exec -ti vault-0 -- vault operator unseal \nFinally, join the remaining pods to the Raft cluster and unseal them. The pods will need to communicate directly so we'll configure the pods to use the internal service provided by the Helm chart:\n$ oc exec -ti vault-1 -- vault operator raft join http://vault-0.vault-internal:8200 $ oc exec -ti vault-1 -- vault operator unseal $ oc exec -ti vault-2 -- vault operator raft join http://vault-0.vault-internal:8200 $ oc exec -ti vault-2 -- vault operator unseal \nTo verify if the Raft cluster has successfully been initialized, run the following.\nFirst, login using the root token on the vault-0 pod:\n$ oc exec -ti vault-0 -- vault login \nNext, list all the raft peers:\n$ oc exec -ti vault-0 -- vault operator raft list-peers Node Address State Voter ---- ------- ----- ----- a1799962-8711-7f28-23f0-cea05c8a527d vault-0.vault-internal:8201 leader true e6876c97-aaaa-a92e-b99a-0aafab105745 vault-1.vault-internal:8201 follower true 4b5d7383-ff31-44df-e008-6a606828823b vault-2.vault-internal:8201 follower true \nVault with integrated storage (Raft) is now ready to use!\nExternal mode\nThe Helm chart may be run in external mode. This installs no Vault server and relies on a network addressable Vault server to exist.\nInstall the latest Vault Helm chart in external mode.\n$ helm install vault hashicorp/vault \\ --set \"global.openshift=true\" \\ --set \"injector.externalVaultAddr=http://external-vault:8200\" \nRefer to the Integrate a Kubernetes Cluster with an External Vault tutorial to learn how to use an external Vault within a Kubernetes cluster."
},
{
  "url": "https://developer.hashicorp.com/vault/docs/v1.10.x/platform/k8s/helm/enterprise",
  "text": "Vault Enterprise License Management - Kubernetes | Vault\nYou can use this Helm chart to deploy Vault Enterprise by following a few extra steps around licensing.\nNote: As of Vault Enterprise 1.8, the license must be specified via HCL configuration or environment variables on startup, unless the Vault cluster was created with an older Vault version and the license was stored, or the Vault Enterprise binary has the license baked in (prem or pro version tags). More information is available in the Vault Enterprise License docs.\nLicense Install\nFirst create a Kubernetes secret using the contents of your license file. For example, the following commands create a secret with the name vault-ent-license and key license:\nsecret=$(cat 1931d1f4-bdfd-6881-f3f5-19349374841f.hclic) kubectl create secret generic vault-ent-license --from-literal=\"license=${secret}\" \nNote: If you cannot find your .hclic file, please contact your sales team or Technical Account Manager.\nIn your chart overrides, set the values of server.image to one of the enterprise release tags. Also set the name of the secret you just created in server.enterpriseLicense.\n# config.yaml server: image: repository: hashicorp/vault-enterprise tag: 1.10.3-ent enterpriseLicense: secretName: vault-ent-license \nNow run helm install:\n$ helm install hashicorp hashicorp/vault -f config.yaml \nOnce the cluster is initialized and unsealed, you may check the license status using the vault license get command:\nkubectl exec -ti vault-0 -- vault license get \nLicense Update\nTo update the autoloaded license in Vault, you may do the following:\nUpdate your license secret with the new license data\nnew_secret=$(base64 < ./new-license.hclic | tr -d '\\n') cat > patch-license.yaml <<EOF data: license: ${new_secret} EOF kubectl patch secret vault-ent-license --patch \"$(cat patch-license.yaml)\" \nWait until vault license inspect shows the updated license\nSince the inspect command is reading the license file from the mounted secret, this tells you when the updated secret has been propagated to the mount on the Vault pod.\nkubectl exec vault-0 -- vault license inspect \nReload Vault's license config\nYou may use the sys/config/reload/license API endpoint:\nkubectl exec vault-0 -- vault write -f sys/config/reload/license \nOr you may issue an HUP signal directly to Vault:\nkubectl exec vault-0 -- pkill -HUP vault \nVerify that vault license get shows the updated license\nkubectl exec vault-0 -- vault license get \nIn your chart overrides, set the values of server.image to one of the enterprise release tags. Install the chart, and initialize and unseal vault as described in Running Vault.\nAfter Vault has been initialized and unsealed, setup a port-forward tunnel to the Vault Enterprise cluster:\nkubectl port-forward vault-0 8200:8200 \nNext, in a separate terminal, create a payload.json file that contains the license key like this example:\n{ \"text\": \"01ABCDEFG...\" } \nFinally, using curl, apply the license key to the Vault API:\ncurl \\ --header \"X-Vault-Token: VAULT_LOGIN_TOKEN_HERE\" \\ --request POST \\ --data @payload.json \\ http://127.0.0.1:8200/v1/sys/license \nTo verify that the license installation worked correctly, using curl, run the following:\ncurl \\ --header \"X-Vault-Token: VAULT_LOGIN_TOKEN_HERE\" \\ http://127.0.0.1:8200/v1/sys/license"
},
{
  "url": "https://developer.hashicorp.com/vault/docs/v1.10.x/platform/k8s/helm/openshift",
  "text": "Running Vault - OpenShift | Vault\nThe following documentation describes installing, running, and using Vault and Vault Agent Injector on OpenShift.\nNote: We recommend using the Vault agent injector on Openshift instead of the Secrets Store CSI driver. OpenShift does not recommend using hostPath mounting in production or certify Helm charts using CSI objects because pods must run as privileged. If you would like to run the Secrets Store CSI driver on a development or testing cluster, refer to installation instructions for the Vault CSI provider.\nThe following are required to install Vault and Vault Agent Injector on OpenShift:\nCluster Admin privileges to bind the auth-delegator role to Vault's service account\nHelm v3.6+\nOpenShift 4.X\nVault Helm v0.6.0+\nVault K8s v0.4.0+\nNote: Support for Consul on OpenShift is available since Consul 1.9. However, for highly available deployments, Raft integrated storage is recommended.\nThe documentation, configuration and examples for Vault Helm and Vault K8s Agent Injector are applicable to OpenShift installations. For more examples see the existing documentation:\nVault Helm documentation\nVault K8s documentation\nThe Vault Helm chart is the recommended way to install and configure Vault on OpenShift. In addition to running Vault itself, the Helm chart is the primary method for installing and configuring Vault Agent Injection Mutating Webhook.\nWhile the Helm chart automatically sets up complex resources and exposes the configuration to meet your requirements, it does not automatically operate Vault. You are still responsible for learning how to monitor, backup, upgrade, etc. the Vault cluster.\nSecurity Warning: By default, the chart runs in standalone mode. This mode uses a single Vault server with a file storage backend. This is a less secure and less resilient installation that is NOT appropriate for a production setup. It is highly recommended to use a properly secured Kubernetes cluster, learn the available configuration options, and read the production deployment checklist.\nInstall Vault\nTo use the Helm chart, add the Hashicorp helm repository and check that you have access to the chart:\n$ helm repo add hashicorp https://helm.releases.hashicorp.com \"hashicorp\" has been added to your repositories $ helm search repo hashicorp/vault NAME CHART VERSION APP VERSION DESCRIPTION hashicorp/vault 0.20.1 1.10.3 Official HashiCorp Vault Chart \nImportant: The Helm chart is new and under significant development. Please always run Helm with --dry-run before any install or upgrade to verify changes.\nUse helm install to install the latest release of the Vault Helm chart.\n$ helm install vault hashicorp/vault \nOr install a specific version of the chart.\n# List the available releases $ helm search repo hashicorp/vault -l NAME CHART VERSION APP VERSION DESCRIPTION hashicorp/vault 0.20.1 1.10.3 Official HashiCorp Vault Chart hashicorp/vault 0.20.0 1.10.3 Official HashiCorp Vault Chart hashicorp/vault 0.19.0 1.9.2 Official HashiCorp Vault Chart hashicorp/vault 0.18.0 1.9.0 Official HashiCorp Vault Chart hashicorp/vault 0.17.1 1.8.4 Official HashiCorp Vault Chart hashicorp/vault 0.17.0 1.8.4 Official HashiCorp Vault Chart hashicorp/vault 0.16.1 1.8.3 Official HashiCorp Vault Chart hashicorp/vault 0.16.0 1.8.2 Official HashiCorp Vault Chart hashicorp/vault 0.15.0 1.8.1 Official HashiCorp Vault Chart hashicorp/vault 0.14.0 1.8.0 Official HashiCorp Vault Chart # Install version 0.20.1 $ helm install vault hashicorp/vault --version 0.20.1 \nThe helm install command accepts parameters to override default configuration values inline or defined in a file. For all OpenShift deployments, global.openshift should be set to true.\nOverride the server.dev.enabled configuration value:\nOverride all the configuration found in a file:\n$ cat override-values.yml global: openshift: true server: ha: enabled: true replicas: 5 ## $ helm install vault hashicorp/vault \\ --values override-values.yml \nDev mode\nThe Helm chart may run a Vault server in development. This installs a single Vault server with a memory storage backend.\nDev mode: This is ideal for learning and demonstration environments but NOT recommended for a production environment.\nInstall the latest Vault Helm chart in development mode.\nHighly Available Raft Mode\nThe following creates a Vault cluster using the Raft integrated storage backend.\nInstall the latest Vault Helm chart in HA Raft mode:\n$ helm install vault hashicorp/vault \\ --set='global.openshift=true' \\ --set='server.ha.enabled=true' \\ --set='server.ha.raft.enabled=true' \nNext, initialize and unseal vault-0 pod:\n$ oc exec -ti vault-0 -- vault operator init $ oc exec -ti vault-0 -- vault operator unseal \nFinally, join the remaining pods to the Raft cluster and unseal them. The pods will need to communicate directly so we'll configure the pods to use the internal service provided by the Helm chart:\n$ oc exec -ti vault-1 -- vault operator raft join http://vault-0.vault-internal:8200 $ oc exec -ti vault-1 -- vault operator unseal $ oc exec -ti vault-2 -- vault operator raft join http://vault-0.vault-internal:8200 $ oc exec -ti vault-2 -- vault operator unseal \nTo verify if the Raft cluster has successfully been initialized, run the following.\nFirst, login using the root token on the vault-0 pod:\n$ oc exec -ti vault-0 -- vault login \nNext, list all the raft peers:\n$ oc exec -ti vault-0 -- vault operator raft list-peers Node Address State Voter ---- ------- ----- ----- a1799962-8711-7f28-23f0-cea05c8a527d vault-0.vault-internal:8201 leader true e6876c97-aaaa-a92e-b99a-0aafab105745 vault-1.vault-internal:8201 follower true 4b5d7383-ff31-44df-e008-6a606828823b vault-2.vault-internal:8201 follower true \nVault with integrated storage (Raft) is now ready to use!\nExternal mode\nThe Helm chart may be run in external mode. This installs no Vault server and relies on a network addressable Vault server to exist.\nInstall the latest Vault Helm chart in external mode.\n$ helm install vault hashicorp/vault \\ --set \"global.openshift=true\" \\ --set \"injector.externalVaultAddr=http://external-vault:8200\" \nRefer to the Integrate a Kubernetes Cluster with an External Vault tutorial to learn how to use an external Vault within a Kubernetes cluster."
},
{
  "url": "https://developer.hashicorp.com/vault/docs/v1.11.x/platform/k8s/helm/terraform",
  "text": "Configure Vault Helm using Terraform | Vault\nConfiguring Vault helm with terraform\nTerraform may also be used to configure and deploy the Vault Helm chart, by using the Helm provider.\nFor example, to configure the chart to deploy HA Vault with integrated storage (raft), the values overrides can be set on the command-line, in a values yaml file, or with a Terraform configuration:\n$ helm install vault hashicorp/vault \\ --set='server.ha.enabled=true' \\ --set='server.ha.raft.enabled=true' \nThe values file can also be used directly in the Terraform configuration with the values directive.\nVault config as a multi-line string\nserver: ha: enabled: true raft: enabled: true setNodeId: true config: | ui = false listener \"tcp\" { tls_disable = 1 address = \"[::]:8200\" cluster_address = \"[::]:8201\" } storage \"raft\" { path = \"/vault/data\" } service_registration \"kubernetes\" {} seal \"awskms\" { region = \"us-west-2\" kms_key_id = \"alias/my-kms-key\" } \nLists of volumes and volumeMounts\nserver: volumes: - name: userconfig-my-gcp-iam secret: defaultMode: 420 secretName: my-gcp-iam volumeMounts: - mountPath: /vault/userconfig/my-gcp-iam name: userconfig-my-gcp-iam readOnly: true \nAnnotations\nAnnotations can be set as a YAML map:\nserver: ingress: annotations: service.beta.kubernetes.io/azure-load-balancer-internal: true service.beta.kubernetes.io/azure-load-balancer-internal-subnet: apps-subnet \nor as a multi-line string:\nserver: ingress: annotations: | service.beta.kubernetes.io/azure-load-balancer-internal: true service.beta.kubernetes.io/azure-load-balancer-internal-subnet: apps-subnet"
},
{
  "url": "https://developer.hashicorp.com/vault/docs/v1.7.x/platform/k8s/helm/openshift",
  "text": "The following documentation describes installing, running and using Vault and Vault Agent Injector on OpenShift.\nHelm v3\nOpenShift 4.X\nNote: At this time, Consul does not support OpenShift. For highly available deployments, Raft integrated storage is recommended.\n$ helm repo add hashicorp https://helm.releases.hashicorp.com \"hashicorp\" has been added to your repositories $ helm search repo hashicorp/vault NAME CHART VERSION APP VERSION DESCRIPTION hashicorp/vault 0.9.1 1.6.2 Official HashiCorp Vault Chart \n# List the available releases $ helm search repo hashicorp/vault -l NAME CHART VERSION APP VERSION DESCRIPTION hashicorp/vault 0.9.1 1.6.2 Official HashiCorp Vault Chart hashicorp/vault 0.9.0 1.6.1 Official HashiCorp Vault Chart hashicorp/vault 0.8.0 1.5.4 Official HashiCorp Vault Chart hashicorp/vault 0.7.0 1.5.2 Official HashiCorp Vault Chart hashicorp/vault 0.6.0 1.4.2 Official HashiCorp Vault Chart # Install version 0.9.1 $ helm install vault hashicorp/vault --version 0.9.1 \nHighly Available Raft Mode\nStep-by-step instructions: The Integrate a Kubernetes Cluster with an External Vault guide demonstrates using an external Vault within a Kubernetes cluster."
},
{
  "url": "https://developer.hashicorp.com/vault/docs/v1.8.x/platform/k8s/helm/enterprise",
  "text": "Note: As of Vault Enterprise 1.8, the license must be specified via HCL configuration or environment variables on startup, unless the Vault cluster was created with an older Vault version and the license was stored, or the Vault Enterprise binary has the license baked in (prem or pro version tags). More information is available in the Vault Enterprise License docs.\nLicense Install\n# config.yaml server: image: repository: hashicorp/vault-enterprise tag: 1.8.0_ent enterpriseLicense: secretName: vault-ent-license \nLicense Update\ncurl \\ --header \"X-Vault-Token: VAULT_LOGIN_TOKEN_HERE\" \\ --request PUT \\ --data @payload.json \\ http://127.0.0.1:8200/v1/sys/license "
},
{
  "url": "https://developer.hashicorp.com/vault/docs/v1.6.x/platform/k8s/helm/openshift",
  "text": "The following documentation describes installing, running and using Vault and Vault Agent Injector on OpenShift.\nHelm v3\nOpenShift 4.X\nNote: At this time, Consul does not support OpenShift. For highly available deployments, Raft integrated storage is recommended.\n$ helm repo add hashicorp https://helm.releases.hashicorp.com \"hashicorp\" has been added to your repositories $ helm search repo hashicorp/vault NAME CHART VERSION APP VERSION DESCRIPTION hashicorp/vault 0.8.0 1.5.4 Official HashiCorp Vault Chart \n# List the available releases $ helm search repo hashicorp/vault -l NAME CHART VERSION APP VERSION DESCRIPTION hashicorp/vault 0.8.0 1.5.4 Official HashiCorp Vault Chart hashicorp/vault 0.7.0 1.5.2 Official HashiCorp Vault Chart hashicorp/vault 0.6.0 1.4.2 Official HashiCorp Vault Chart # Install version 0.8.0 $ helm install vault hashicorp/vault --version 0.8.0 \nHighly Available Raft Mode\nStep-by-step instructions: The Integrate a Kubernetes Cluster with an External Vault guide demonstrates using an external Vault within a Kubernetes cluster."
},
{
  "url": "https://developer.hashicorp.com/vault/docs/v1.15.x/platform/k8s/helm/enterprise",
  "text": "Note: As of Vault Enterprise 1.8, the license must be specified via HCL configuration or environment variables on startup, unless the Vault cluster was created with an older Vault version and the license was stored. More information is available in the Vault Enterprise License docs.\nLicense install\n# config.yaml server: image: repository: hashicorp/vault-enterprise tag: 1.15.2-ent enterpriseLicense: secretName: vault-ent-license \nLicense update"
},
{
  "url": "https://developer.hashicorp.com/vault/docs/v1.16.x/platform/k8s/helm/enterprise",
  "text": "# config.yaml server: image: repository: hashicorp/vault-enterprise tag: 1.16.1-ent enterpriseLicense: secretName: vault-ent-license "
},
{
  "url": "https://developer.hashicorp.com/vault/docs/v1.14.x/platform/k8s/helm/enterprise",
  "text": "# config.yaml server: image: repository: hashicorp/vault-enterprise tag: 1.14.0-ent enterpriseLicense: secretName: vault-ent-license "
},
{
  "url": "https://developer.hashicorp.com/vault/docs/v1.8.x/platform/k8s/helm/openshift",
  "text": "The following documentation describes installing, running and using Vault and Vault Agent Injector on OpenShift.\nHelm v3\nOpenShift 4.X\nNote: At this time, Consul does not support OpenShift. For highly available deployments, Raft integrated storage is recommended.\n$ helm repo add hashicorp https://helm.releases.hashicorp.com \"hashicorp\" has been added to your repositories $ helm search repo hashicorp/vault NAME CHART VERSION APP VERSION DESCRIPTION hashicorp/vault 0.13.0 1.7.3 Official HashiCorp Vault Chart \n# List the available releases $ helm search repo hashicorp/vault -l NAME CHART VERSION APP VERSION DESCRIPTION hashicorp/vault 0.13.0 1.7.3 Official HashiCorp Vault Chart hashicorp/vault 0.12.0 1.7.2 Official HashiCorp Vault Chart hashicorp/vault 0.11.0 1.7.0 Official HashiCorp Vault Chart hashicorp/vault 0.10.0 1.7.0 Official HashiCorp Vault Chart hashicorp/vault 0.9.1 1.6.2 Official HashiCorp Vault Chart hashicorp/vault 0.9.0 1.6.1 Official HashiCorp Vault Chart hashicorp/vault 0.8.0 1.5.4 Official HashiCorp Vault Chart hashicorp/vault 0.7.0 1.5.2 Official HashiCorp Vault Chart hashicorp/vault 0.6.0 1.4.2 Official HashiCorp Vault Chart # Install version 0.13.0 $ helm install vault hashicorp/vault --version 0.13.0 \nHighly Available Raft Mode\nStep-by-step instructions: The Integrate a Kubernetes Cluster with an External Vault guide demonstrates using an external Vault within a Kubernetes cluster."
},
{
  "url": "https://developer.hashicorp.com/vault/docs/v1.5.x/platform/k8s/helm/openshift",
  "text": "Note: OpenShift support is a beta feature.\nThe following documentation describes installing, running and using Vault and Vault Agent Injector on OpenShift.\nHelm v3\nNote: At this time, Consul does not support OpenShift. For highly available deployments, Raft integrated storage is recommended.\n$ helm repo add hashicorp https://helm.releases.hashicorp.com \"hashicorp\" has been added to your repositories $ helm search repo hashicorp/vault NAME CHART VERSION APP VERSION DESCRIPTION hashicorp/vault 0.6.0 1.4.2 Official HashiCorp Vault Chart \n# List the available releases $ helm search repo hashicorp/vault -l NAME CHART VERSION APP VERSION DESCRIPTION hashicorp/vault 0.6.0 1.4.2 Official HashiCorp Vault Chart hashicorp/vault 0.5.0 Install and configure Vault on Kubernetes. hashicorp/vault 0.4.0 Install and configure Vault on Kubernetes. # Install version 0.5.0 $ helm install vault hashicorp/vault --version 0.5.0 \nStep-by-step instructions: The Integrate a Kubernetes Cluster with an External Vault guide demonstrates using an external Vault within a Kubernetes cluster."
},
{
  "url": "https://developer.hashicorp.com/vault/docs/v1.9.x/platform/k8s/helm/enterprise",
  "text": "Note: As of Vault Enterprise 1.8, the license must be specified via HCL configuration or environment variables on startup, unless the Vault cluster was created with an older Vault version and the license was stored, or the Vault Enterprise binary has the license baked in (prem or pro version tags). More information is available in the Vault Enterprise License docs.\nLicense Install\n# config.yaml server: image: repository: hashicorp/vault-enterprise tag: 1.9.2-ent enterpriseLicense: secretName: vault-ent-license \nLicense Update"
},
{
  "url": "https://developer.hashicorp.com/vault/docs/v1.4.x/platform/k8s/run",
  "text": "Vault | HashiCorp Developer\nThis page does not exist for version v1.4.x."
},
{
  "url": "https://developer.hashicorp.com/vault/docs/v1.9.x/platform/k8s/helm/openshift",
  "text": "$ helm repo add hashicorp https://helm.releases.hashicorp.com \"hashicorp\" has been added to your repositories $ helm search repo hashicorp/vault NAME CHART VERSION APP VERSION DESCRIPTION hashicorp/vault 0.19.0 1.9.2 Official HashiCorp Vault Chart \n# List the available releases $ helm search repo hashicorp/vault -l NAME CHART VERSION APP VERSION DESCRIPTION hashicorp/vault 0.19.0 1.9.2 Official HashiCorp Vault Chart hashicorp/vault 0.18.0 1.9.0 Official HashiCorp Vault Chart hashicorp/vault 0.17.1 1.8.4 Official HashiCorp Vault Chart hashicorp/vault 0.17.0 1.8.4 Official HashiCorp Vault Chart hashicorp/vault 0.16.1 1.8.3 Official HashiCorp Vault Chart hashicorp/vault 0.16.0 1.8.2 Official HashiCorp Vault Chart hashicorp/vault 0.15.0 1.8.1 Official HashiCorp Vault Chart hashicorp/vault 0.14.0 1.8.0 Official HashiCorp Vault Chart # Install version 0.19.0 $ helm install vault hashicorp/vault --version 0.19.0 "
},
{
  "url": "https://developer.hashicorp.com/vault/docs/v1.15.x/platform/k8s/helm/openshift",
  "text": "OpenShift 4.3+\n$ helm repo add hashicorp https://helm.releases.hashicorp.com \"hashicorp\" has been added to your repositories $ helm search repo hashicorp/vault NAME CHART VERSION APP VERSION DESCRIPTION hashicorp/vault 0.27.0 1.15.2 Official HashiCorp Vault Chart \n# List the available releases $ helm search repo hashicorp/vault -l NAME CHART VERSION APP VERSION DESCRIPTION hashicorp/vault 0.27.0 1.15.2 Official HashiCorp Vault Chart hashicorp/vault 0.26.1 1.15.1 Official HashiCorp Vault Chart hashicorp/vault 0.26.0 1.15.1 Official HashiCorp Vault Chart hashicorp/vault 0.25.0 1.14.0 Official HashiCorp Vault Chart hashicorp/vault 0.24.0 1.13.1 Official HashiCorp Vault Chart hashicorp/vault 0.23.0 1.12.1 Official HashiCorp Vault Chart hashicorp/vault 0.22.1 1.12.0 Official HashiCorp Vault Chart hashicorp/vault 0.22.0 1.11.3 Official HashiCorp Vault Chart ... # Install version 0.27.0 $ helm install vault hashicorp/vault --version 0.27.0 \nHighly available raft mode"
},
{
  "url": "https://developer.hashicorp.com/vault/docs/v1.16.x/platform/k8s/helm/openshift",
  "text": "$ helm repo add hashicorp https://helm.releases.hashicorp.com \"hashicorp\" has been added to your repositories $ helm search repo hashicorp/vault NAME CHART VERSION APP VERSION DESCRIPTION hashicorp/vault 0.28.0 1.16.1 Official HashiCorp Vault Chart \n# List the available releases $ helm search repo hashicorp/vault -l NAME CHART VERSION APP VERSION DESCRIPTION hashicorp/vault 0.28.0 1.16.1 Official HashiCorp Vault Chart hashicorp/vault 0.27.0 1.15.2 Official HashiCorp Vault Chart hashicorp/vault 0.26.1 1.15.1 Official HashiCorp Vault Chart hashicorp/vault 0.26.0 1.15.1 Official HashiCorp Vault Chart hashicorp/vault 0.25.0 1.14.0 Official HashiCorp Vault Chart hashicorp/vault 0.24.0 1.13.1 Official HashiCorp Vault Chart hashicorp/vault 0.23.0 1.12.1 Official HashiCorp Vault Chart hashicorp/vault 0.22.1 1.12.0 Official HashiCorp Vault Chart ... # Install version 0.28.0 $ helm install vault hashicorp/vault --version 0.28.0 "
},
{
  "url": "https://developer.hashicorp.com/vault/docs/v1.16.x/platform/k8s/helm/terraform",
  "text": "Configure Vault Helm using Terraform | Vault\nConfiguring Vault helm with terraform\nTerraform may also be used to configure and deploy the Vault Helm chart, by using the Helm provider.\nFor example, to configure the chart to deploy HA Vault with integrated storage (raft), the values overrides can be set on the command-line, in a values yaml file, or with a Terraform configuration:\n$ helm install vault hashicorp/vault \\ --set='server.ha.enabled=true' \\ --set='server.ha.raft.enabled=true' \nThe values file can also be used directly in the Terraform configuration with the values directive.\nVault config as a multi-line string\nserver: ha: enabled: true raft: enabled: true setNodeId: true config: | ui = false listener \"tcp\" { tls_disable = 1 address = \"[::]:8200\" cluster_address = \"[::]:8201\" } storage \"raft\" { path = \"/vault/data\" } service_registration \"kubernetes\" {} seal \"awskms\" { region = \"us-west-2\" kms_key_id = \"alias/my-kms-key\" } \nLists of volumes and volumeMounts\nserver: volumes: - name: userconfig-my-gcp-iam secret: defaultMode: 420 secretName: my-gcp-iam volumeMounts: - mountPath: /vault/userconfig/my-gcp-iam name: userconfig-my-gcp-iam readOnly: true \nAnnotations\nAnnotations can be set as a YAML map:\nserver: ingress: annotations: service.beta.kubernetes.io/azure-load-balancer-internal: true service.beta.kubernetes.io/azure-load-balancer-internal-subnet: apps-subnet \nor as a multi-line string:\nserver: ingress: annotations: | service.beta.kubernetes.io/azure-load-balancer-internal: true service.beta.kubernetes.io/azure-load-balancer-internal-subnet: apps-subnet"
},
{
  "url": "https://developer.hashicorp.com/vault/docs/v1.14.x/platform/k8s/helm/openshift",
  "text": "$ helm repo add hashicorp https://helm.releases.hashicorp.com \"hashicorp\" has been added to your repositories $ helm search repo hashicorp/vault NAME CHART VERSION APP VERSION DESCRIPTION hashicorp/vault 0.25.0 1.14.0 Official HashiCorp Vault Chart \n# List the available releases $ helm search repo hashicorp/vault -l NAME CHART VERSION APP VERSION DESCRIPTION hashicorp/vault 0.25.0 1.14.0 Official HashiCorp Vault Chart hashicorp/vault 0.24.0 1.13.1 Official HashiCorp Vault Chart hashicorp/vault 0.23.0 1.12.1 Official HashiCorp Vault Chart hashicorp/vault 0.22.1 1.12.0 Official HashiCorp Vault Chart hashicorp/vault 0.22.0 1.11.3 Official HashiCorp Vault Chart hashicorp/vault 0.21.0 1.11.2 Official HashiCorp Vault Chart hashicorp/vault 0.20.1 1.10.3 Official HashiCorp Vault Chart ... # Install version 0.25.0 $ helm install vault hashicorp/vault --version 0.25.0 "
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/urlencode",
  "text": "urlencode - Functions - Configuration Language | Terraform\nurlencode applies URL encoding to a given string.\nThis function identifies characters in the given string that would have a special meaning when included as a query string argument in a URL and escapes them using RFC 3986 \"percent encoding\".\nThe exact set of characters escaped may change over time, but the result is guaranteed to be interpolatable into a query string argument without inadvertently introducing additional delimiters.\nIf the given string contains non-ASCII characters, these are first encoded as UTF-8 and then percent encoding is applied separately to each UTF-8 byte.\n> urlencode(\"Hello World!\") Hello+World%21 > urlencode(\"\") %E2%98%83 > \"http://example.com/search?q=${urlencode(\"terraform urlencode\")}\" http://example.com/search?q=terraform+urlencode"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/yamldecode",
  "text": "yamldecode - Functions - Configuration Language | Terraform\nyamldecode parses a string as a subset of YAML, and produces a representation of its value.\nThis function supports a subset of YAML 1.2, as described below.\nThis function maps YAML values to Terraform language values in the following way:\nYAML typeTerraform type\n!!str\tstring\t\n!!float\tnumber\t\n!!int\tnumber\t\n!!bool\tbool\t\n!!map\tobject(...) with attribute types determined per this table\t\n!!seq\ttuple(...) with element types determined per this table\t\n!!null\tThe Terraform language null value\t\n!!timestamp\tstring in RFC 3339 format\t\n!!binary\tstring containing base64-encoded representation\t\nThe Terraform language automatic type conversion rules mean that you don't usually need to worry about exactly what type is produced for a given value, and can just use the result in an intuitive way.\nNote though that the mapping above is ambiguous -- several different source types map to the same target type -- and so round-tripping through yamldecode and then yamlencode cannot produce an identical result.\nYAML is a complex language and it supports a number of possibilities that the Terraform language's type system cannot represent. Therefore this YAML decoder supports only a subset of YAML 1.2, with restrictions including the following:\nAlthough aliases to earlier anchors are supported, cyclic data structures (where a reference to a collection appears inside that collection) are not. If yamldecode detects such a structure then it will return an error.\nOnly the type tags shown in the above table (or equivalent alternative representations of those same tags) are supported. Any other tags will result in an error.\nOnly one YAML document is permitted. If multiple documents are present in the given string then this function will return an error.\n> yamldecode(\"hello: world\") { \"hello\" = \"world\" } > yamldecode(\"true\") true > yamldecode(\"{a: &foo [1, 2, 3], b: *foo}\") { \"a\" = [ 1, 2, 3, ] \"b\" = [ 1, 2, 3, ] } > yamldecode(\"{a: &foo [1, *foo, 3]}\") Error: Error in function call Call to function \"yamldecode\" failed: cannot refer to anchor \"foo\" from inside its own definition. > yamldecode(\"{a: !not-supported foo}\") Error: Error in function call Call to function \"yamldecode\" failed: unsupported tag \"!not-supported\". \njsondecode is a similar operation using JSON instead of YAML.\nyamlencode performs the opposite operation, encoding a value as YAML."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/base64decode",
  "text": "base64decode - Functions - Configuration Language | Terraform\nbase64decode takes a string containing a Base64 character sequence and returns the original string.\nTerraform uses the \"standard\" Base64 alphabet as defined in RFC 4648 section 4.\nStrings in the Terraform language are sequences of unicode characters rather than bytes, so this function will also interpret the resulting bytes as UTF-8. If the bytes after Base64 decoding are not valid UTF-8, this function produces an error.\nWhile we do not recommend manipulating large, raw binary data in the Terraform language, Base64 encoding is the standard way to represent arbitrary byte sequences, and so resource types that accept or return binary data will use Base64 themselves, which avoids the need to encode or decode it directly in most cases. Various other functions with names containing \"base64\" can generate or manipulate Base64 data directly.\nbase64decode is, in effect, a shorthand for calling textdecodebase64 with the encoding name set to UTF-8.\n> base64decode(\"SGVsbG8gV29ybGQ=\") Hello World \nbase64encode performs the opposite operation, encoding the UTF-8 bytes for a string as Base64.\ntextdecodebase64 is a more general function that supports character encodings other than UTF-8.\nbase64gzip applies gzip compression to a string and returns the result with Base64 encoding.\nfilebase64 reads a file from the local filesystem and returns its raw bytes with Base64 encoding."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/base64encode",
  "text": "base64encode - Functions - Configuration Language | Terraform\nbase64encode applies Base64 encoding to a string.\nTerraform uses the \"standard\" Base64 alphabet as defined in RFC 4648 section 4.\nStrings in the Terraform language are sequences of unicode characters rather than bytes, so this function will first encode the characters from the string as UTF-8, and then apply Base64 encoding to the result.\nThe Terraform language applies Unicode normalization to all strings, and so passing a string through base64decode and then base64encode may not yield the original result exactly.\nWhile we do not recommend manipulating large, raw binary data in the Terraform language, Base64 encoding is the standard way to represent arbitrary byte sequences, and so resource types that accept or return binary data will use Base64 themselves, and so this function exists primarily to allow string data to be easily provided to resource types that expect Base64 bytes.\nbase64encode is, in effect, a shorthand for calling textencodebase64 with the encoding name set to UTF-8.\n> base64encode(\"Hello World\") SGVsbG8gV29ybGQ= \nbase64decode performs the opposite operation, decoding Base64 data and interpreting it as a UTF-8 string.\ntextencodebase64 is a more general function that supports character encodings other than UTF-8.\nbase64gzip applies gzip compression to a string and returns the result with Base64 encoding all in one operation.\nfilebase64 reads a file from the local filesystem and returns its raw bytes with Base64 encoding, without creating an intermediate Unicode string."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/base64gzip",
  "text": "base64gzip - Functions - Configuration Language | Terraform\nbase64gzip compresses a string with gzip and then encodes the result in Base64 encoding.\nTerraform uses the \"standard\" Base64 alphabet as defined in RFC 4648 section 4.\nStrings in the Terraform language are sequences of unicode characters rather than bytes, so this function will first encode the characters from the string as UTF-8, then apply gzip compression, and then finally apply Base64 encoding.\nWhile we do not recommend manipulating large, raw binary data in the Terraform language, this function can be used to compress reasonably sized text strings generated within the Terraform language. For example, the result of this function can be used to create a compressed object in Amazon S3 as part of an S3 website.\nbase64encode applies Base64 encoding without gzip compression.\nfilebase64 reads a file from the local filesystem and returns its raw bytes with Base64 encoding."
},
{
  "url": "https://developer.hashicorp.com/vault/docs/v1.15.x/platform/k8s/helm/terraform",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/vault/docs/v1.14.x/platform/k8s/helm/terraform",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/textdecodebase64",
  "text": "textdecodebase64 - Functions - Configuration Language | Terraform\ntextdecodebase64 Function\nNote: This function is supported only in Terraform v0.14 and later.\ntextdecodebase64 function decodes a string that was previously Base64-encoded, and then interprets the result as characters in a specified character encoding.\nTerraform uses the \"standard\" Base64 alphabet as defined in RFC 4648 section 4.\nThe encoding_name argument must contain one of the encoding names or aliases recorded in the IANA character encoding registry. Terraform supports only a subset of the registered encodings, and the encoding support may vary between Terraform versions.\nTerraform accepts the encoding name UTF-8, which will produce the same result as base64decode.\n> textdecodebase64(\"SABlAGwAbABvACAAVwBvAHIAbABkAA==\", \"UTF-16LE\") Hello World \ntextencodebase64 performs the opposite operation, applying target encoding and then Base64 to a string.\nbase64decode is effectively a shorthand for textdecodebase64 where the character encoding is fixed as UTF-8."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/base64gzip",
  "text": "base64gzip - Functions - Configuration Language | Terraform\nbase64gzip compresses a string with gzip and then encodes the result in Base64 encoding.\nStrings in the Terraform language are sequences of unicode characters rather than bytes, so this function will first encode the characters from the string as UTF-8, then apply gzip compression, and then finally apply Base64 encoding.\nWhile we do not recommend manipulating large, raw binary data in the Terraform language, this function can be used to compress reasonably sized text strings generated within the Terraform language. For example, the result of this function can be used to create a compressed object in Amazon S3 as part of an S3 website.\nbase64encode applies Base64 encoding without gzip compression.\nfilebase64 reads a file from the local filesystem and returns its raw bytes with Base64 encoding."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/textencodebase64",
  "text": "textencodebase64 - Functions - Configuration Language | Terraform\ntextencodebase64 Function\nNote: This function is supported only in Terraform v0.14 and later.\ntextencodebase64 encodes the unicode characters in a given string using a specified character encoding, returning the result base64 encoded because Terraform language strings are always sequences of unicode characters.\nsubstr(string, encoding_name) \nThe encoding_name argument must contain one of the encoding names or aliases recorded in the IANA character encoding registry. Terraform supports only a subset of the registered encodings, and the encoding support may vary between Terraform versions. In particular Terraform supports UTF-16LE, which is the native character encoding for the Windows API and therefore sometimes expected by Windows-originated software such as PowerShell.\nTerraform also accepts the encoding name UTF-8, which will produce the same result as base64encode.\n> textencodebase64(\"Hello World\", \"UTF-16LE\") SABlAGwAbABvACAAVwBvAHIAbABkAA== \ntextdecodebase64 performs the opposite operation, decoding Base64 data and interpreting it as a particular character encoding.\nbase64encode applies Base64 encoding of the UTF-8 encoding of a string.\nfilebase64 reads a file from the local filesystem and returns its raw bytes with Base64 encoding, without creating an intermediate Unicode string."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/base64decode",
  "text": "base64decode - Functions - Configuration Language | Terraform\nbase64decode takes a string containing a Base64 character sequence and returns the original string.\nStrings in the Terraform language are sequences of unicode characters rather than bytes, so this function will also interpret the resulting bytes as UTF-8. If the bytes after Base64 decoding are not valid UTF-8, this function produces an error.\nWhile we do not recommend manipulating large, raw binary data in the Terraform language, Base64 encoding is the standard way to represent arbitrary byte sequences, and so resource types that accept or return binary data will use Base64 themselves, which avoids the need to encode or decode it directly in most cases. Various other functions with names containing \"base64\" can generate or manipulate Base64 data directly.\nbase64decode is, in effect, a shorthand for calling textdecodebase64 with the encoding name set to UTF-8.\n> base64decode(\"SGVsbG8gV29ybGQ=\") Hello World \nbase64encode performs the opposite operation, encoding the UTF-8 bytes for a string as Base64.\ntextdecodebase64 is a more general function that supports character encodings other than UTF-8.\nbase64gzip applies gzip compression to a string and returns the result with Base64 encoding.\nfilebase64 reads a file from the local filesystem and returns its raw bytes with Base64 encoding."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/base64encode",
  "text": "base64encode - Functions - Configuration Language | Terraform\nbase64encode applies Base64 encoding to a string.\nStrings in the Terraform language are sequences of unicode characters rather than bytes, so this function will first encode the characters from the string as UTF-8, and then apply Base64 encoding to the result.\nThe Terraform language applies Unicode normalization to all strings, and so passing a string through base64decode and then base64encode may not yield the original result exactly.\nWhile we do not recommend manipulating large, raw binary data in the Terraform language, Base64 encoding is the standard way to represent arbitrary byte sequences, and so resource types that accept or return binary data will use Base64 themselves, and so this function exists primarily to allow string data to be easily provided to resource types that expect Base64 bytes.\nbase64encode is, in effect, a shorthand for calling textencodebase64 with the encoding name set to UTF-8.\n> base64encode(\"Hello World\") SGVsbG8gV29ybGQ= \nbase64decode performs the opposite operation, decoding Base64 data and interpreting it as a UTF-8 string.\ntextencodebase64 is a more general function that supports character encodings other than UTF-8.\nbase64gzip applies gzip compression to a string and returns the result with Base64 encoding all in one operation.\nfilebase64 reads a file from the local filesystem and returns its raw bytes with Base64 encoding, without creating an intermediate Unicode string."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/urlencode",
  "text": "urlencode - Functions - Configuration Language | Terraform\nurlencode applies URL encoding to a given string.\nThis function identifies characters in the given string that would have a special meaning when included as a query string argument in a URL and escapes them using RFC 3986 \"percent encoding\".\nThe exact set of characters escaped may change over time, but the result is guaranteed to be interpolatable into a query string argument without inadvertently introducing additional delimiters.\nIf the given string contains non-ASCII characters, these are first encoded as UTF-8 and then percent encoding is applied separately to each UTF-8 byte.\n> urlencode(\"Hello World!\") Hello+World%21 > urlencode(\"\") %E2%98%83 > \"http://example.com/search?q=${urlencode(\"terraform urlencode\")}\" http://example.com/search?q=terraform+urlencode"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/textencodebase64",
  "text": "textencodebase64 - Functions - Configuration Language | Terraform\ntextencodebase64 Function\nNote: This function is supported only in Terraform v0.14 and later.\ntextencodebase64 encodes the unicode characters in a given string using a specified character encoding, returning the result base64 encoded because Terraform language strings are always sequences of unicode characters.\nsubstr(string, encoding_name) \nThe encoding_name argument must contain one of the encoding names or aliases recorded in the IANA character encoding registry. Terraform supports only a subset of the registered encodings, and the encoding support may vary between Terraform versions. In particular Terraform supports UTF-16LE, which is the native character encoding for the Windows API and therefore sometimes expected by Windows-originated software such as PowerShell.\nTerraform also accepts the encoding name UTF-8, which will produce the same result as base64encode.\n> textencodebase64(\"Hello World\", \"UTF-16LE\") SABlAGwAbABvACAAVwBvAHIAbABkAA== \ntextdecodebase64 performs the opposite operation, decoding Base64 data and interpreting it as a particular character encoding.\nbase64encode applies Base64 encoding of the UTF-8 encoding of a string.\nfilebase64 reads a file from the local filesystem and returns its raw bytes with Base64 encoding, without creating an intermediate Unicode string."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/textdecodebase64",
  "text": "textdecodebase64 - Functions - Configuration Language | Terraform\ntextdecodebase64 Function\nNote: This function is supported only in Terraform v0.14 and later.\ntextdecodebase64 function decodes a string that was previously Base64-encoded, and then interprets the result as characters in a specified character encoding.\nThe encoding_name argument must contain one of the encoding names or aliases recorded in the IANA character encoding registry. Terraform supports only a subset of the registered encodings, and the encoding support may vary between Terraform versions.\nTerraform accepts the encoding name UTF-8, which will produce the same result as base64decode.\n> textdecodebase64(\"SABlAGwAbABvACAAVwBvAHIAbABkAA==\", \"UTF-16LE\") Hello World \ntextencodebase64 performs the opposite operation, applying target encoding and then Base64 to a string.\nbase64decode is effectively a shorthand for textdecodebase64 where the character encoding is fixed as UTF-8."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/yamldecode",
  "text": "yamldecode - Functions - Configuration Language | Terraform\nyamldecode parses a string as a subset of YAML, and produces a representation of its value.\nThis function supports a subset of YAML 1.2, as described below.\nThis function maps YAML values to Terraform language values in the following way:\nYAML typeTerraform type\n!!str\tstring\t\n!!float\tnumber\t\n!!int\tnumber\t\n!!bool\tbool\t\n!!map\tobject(...) with attribute types determined per this table\t\n!!seq\ttuple(...) with element types determined per this table\t\n!!null\tThe Terraform language null value\t\n!!timestamp\tstring in RFC 3339 format\t\n!!binary\tstring containing base64-encoded representation\t\nThe Terraform language automatic type conversion rules mean that you don't usually need to worry about exactly what type is produced for a given value, and can just use the result in an intuitive way.\nNote though that the mapping above is ambiguous -- several different source types map to the same target type -- and so round-tripping through yamldecode and then yamlencode cannot produce an identical result.\nYAML is a complex language and it supports a number of possibilities that the Terraform language's type system cannot represent. Therefore this YAML decoder supports only a subset of YAML 1.2, with restrictions including the following:\nAlthough aliases to earlier anchors are supported, cyclic data structures (where a reference to a collection appears inside that collection) are not. If yamldecode detects such a structure then it will return an error.\nOnly the type tags shown in the above table (or equivalent alternative representations of those same tags) are supported. Any other tags will result in an error.\nOnly one YAML document is permitted. If multiple documents are present in the given string then this function will return an error.\n> yamldecode(\"hello: world\") { \"hello\" = \"world\" } > yamldecode(\"true\") true > yamldecode(\"{a: &foo [1, 2, 3], b: *foo}\") { \"a\" = [ 1, 2, 3, ] \"b\" = [ 1, 2, 3, ] } > yamldecode(\"{a: &foo [1, *foo, 3]}\") Error: Error in function call Call to function \"yamldecode\" failed: cannot refer to anchor \"foo\" from inside its own definition. > yamldecode(\"{a: !not-supported foo}\") Error: Error in function call Call to function \"yamldecode\" failed: unsupported tag \"!not-supported\". \njsondecode is a similar operation using JSON instead of YAML.\nyamlencode performs the opposite operation, encoding a value as YAML."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/base64encode",
  "text": "base64encode - Functions - Configuration Language | Terraform\nbase64encode applies Base64 encoding to a string.\nStrings in the Terraform language are sequences of unicode characters rather than bytes, so this function will first encode the characters from the string as UTF-8, and then apply Base64 encoding to the result.\nThe Terraform language applies Unicode normalization to all strings, and so passing a string through base64decode and then base64encode may not yield the original result exactly.\nWhile we do not recommend manipulating large, raw binary data in the Terraform language, Base64 encoding is the standard way to represent arbitrary byte sequences, and so resource types that accept or return binary data will use Base64 themselves, and so this function exists primarily to allow string data to be easily provided to resource types that expect Base64 bytes.\nbase64encode is, in effect, a shorthand for calling textencodebase64 with the encoding name set to UTF-8.\n> base64encode(\"Hello World\") SGVsbG8gV29ybGQ= \nbase64decode performs the opposite operation, decoding Base64 data and interpreting it as a UTF-8 string.\ntextencodebase64 is a more general function that supports character encodings other than UTF-8.\nbase64gzip applies gzip compression to a string and returns the result with Base64 encoding all in one operation."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/base64gzip",
  "text": "base64gzip - Functions - Configuration Language | Terraform\nbase64gzip compresses a string with gzip and then encodes the result in Base64 encoding.\nStrings in the Terraform language are sequences of unicode characters rather than bytes, so this function will first encode the characters from the string as UTF-8, then apply gzip compression, and then finally apply Base64 encoding.\nWhile we do not recommend manipulating large, raw binary data in the Terraform language, this function can be used to compress reasonably sized text strings generated within the Terraform language. For example, the result of this function can be used to create a compressed object in Amazon S3 as part of an S3 website.\nbase64encode applies Base64 encoding without gzip compression."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/textdecodebase64",
  "text": "textdecodebase64 - Functions - Configuration Language | Terraform\ntextdecodebase64 Function\ntextdecodebase64 function decodes a string that was previously Base64-encoded, and then interprets the result as characters in a specified character encoding.\nThe encoding_name argument must contain one of the encoding names or aliases recorded in the IANA character encoding registry. Terraform supports only a subset of the registered encodings, and the encoding support may vary between Terraform versions.\nTerraform accepts the encoding name UTF-8, which will produce the same result as base64decode.\n> textdecodebase64(\"SABlAGwAbABvACAAVwBvAHIAbABkAA==\", \"UTF-16LE\") Hello World \ntextencodebase64 performs the opposite operation, applying target encoding and then Base64 to a string.\nbase64decode is effectively a shorthand for textdecodebase64 where the character encoding is fixed as UTF-8."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/textencodebase64",
  "text": "textencodebase64 - Functions - Configuration Language | Terraform\ntextencodebase64 Function\ntextencodebase64 encodes the unicode characters in a given string using a specified character encoding, returning the result base64 encoded because Terraform language strings are always sequences of unicode characters.\nsubstr(string, encoding_name) \nThe encoding_name argument must contain one of the encoding names or aliases recorded in the IANA character encoding registry. Terraform supports only a subset of the registered encodings, and the encoding support may vary between Terraform versions. In particular Terraform supports UTF-16LE, which is the native character encoding for the Windows API and therefore sometimes expected by Windows-originated software such as PowerShell.\nTerraform also accepts the encoding name UTF-8, which will produce the same result as base64encode.\n> textencodebase64(\"Hello World\", \"UTF-16LE\") SABlAGwAbABvACAAVwBvAHIAbABkAA== \ntextdecodebase64 performs the opposite operation, decoding Base64 data and interpreting it as a particular character encoding.\nbase64encode applies Base64 encoding of the UTF-8 encoding of a string."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/urlencode",
  "text": "urlencode - Functions - Configuration Language | Terraform\nurlencode applies URL encoding to a given string.\nThis function identifies characters in the given string that would have a special meaning when included as a query string argument in a URL and escapes them using RFC 3986 \"percent encoding\".\nThe exact set of characters escaped may change over time, but the result is guaranteed to be interpolatable into a query string argument without inadvertently introducing additional delimiters.\nIf the given string contains non-ASCII characters, these are first encoded as UTF-8 and then percent encoding is applied separately to each UTF-8 byte.\n> urlencode(\"Hello World!\") Hello+World%21 > urlencode(\"\") %E2%98%83 > \"http://example.com/search?q=${urlencode(\"terraform urlencode\")}\" http://example.com/search?q=terraform+urlencode"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/yamldecode",
  "text": "yamldecode - Functions - Configuration Language | Terraform\nyamldecode parses a string as a subset of YAML, and produces a representation of its value.\nThis function supports a subset of YAML 1.2, as described below.\nThis function maps YAML values to Terraform language values in the following way:\nYAML typeTerraform type\n!!str\tstring\t\n!!float\tnumber\t\n!!int\tnumber\t\n!!bool\tbool\t\n!!map\tobject(...) with attribute types determined per this table\t\n!!seq\ttuple(...) with element types determined per this table\t\n!!null\tThe Terraform language null value\t\n!!timestamp\tstring in RFC 3339 format\t\n!!binary\tstring containing base64-encoded representation\t\nThe Terraform language automatic type conversion rules mean that you don't usually need to worry about exactly what type is produced for a given value, and can just use the result in an intuitive way.\nNote though that the mapping above is ambiguous -- several different source types map to the same target type -- and so round-tripping through yamldecode and then yamlencode cannot produce an identical result.\nYAML is a complex language and it supports a number of possibilities that the Terraform language's type system cannot represent. Therefore this YAML decoder supports only a subset of YAML 1.2, with restrictions including the following:\nAlthough aliases to earlier anchors are supported, cyclic data structures (where a reference to a collection appears inside that collection) are not. If yamldecode detects such a structure then it will return an error.\nOnly the type tags shown in the above table (or equivalent alternative representations of those same tags) are supported. Any other tags will result in an error.\nOnly one YAML document is permitted. If multiple documents are present in the given string then this function will return an error.\n> yamldecode(\"hello: world\") { \"hello\" = \"world\" } > yamldecode(\"true\") true > yamldecode(\"{a: &foo [1, 2, 3], b: *foo}\") { \"a\" = [ 1, 2, 3, ] \"b\" = [ 1, 2, 3, ] } > yamldecode(\"{a: &foo [1, *foo, 3]}\") Error: Error in function call Call to function \"yamldecode\" failed: cannot refer to anchor \"foo\" from inside its own definition. > yamldecode(\"{a: !not-supported foo}\") Error: Error in function call Call to function \"yamldecode\" failed: unsupported tag \"!not-supported\". \njsondecode is a similar operation using JSON instead of YAML.\nyamlencode performs the opposite operation, encoding a value as YAML."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/base64decode",
  "text": "base64decode - Functions - Configuration Language | Terraform\nbase64decode takes a string containing a Base64 character sequence and returns the original string.\nStrings in the Terraform language are sequences of unicode characters rather than bytes, so this function will also interpret the resulting bytes as UTF-8. If the bytes after Base64 decoding are not valid UTF-8, this function produces an error.\nWhile we do not recommend manipulating large, raw binary data in the Terraform language, Base64 encoding is the standard way to represent arbitrary byte sequences, and so resource types that accept or return binary data will use Base64 themselves, which avoids the need to encode or decode it directly in most cases. Various other functions with names containing \"base64\" can generate or manipulate Base64 data directly.\nbase64decode is, in effect, a shorthand for calling textdecodebase64 with the encoding name set to UTF-8.\n> base64decode(\"SGVsbG8gV29ybGQ=\") Hello World \nbase64encode performs the opposite operation, encoding the UTF-8 bytes for a string as Base64.\ntextdecodebase64 is a more general function that supports character encodings other than UTF-8.\nbase64gzip applies gzip compression to a string and returns the result with Base64 encoding."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/base64decode",
  "text": "base64decode - Functions - Configuration Language | Terraform\nbase64decode takes a string containing a Base64 character sequence and returns the original string.\nStrings in the Terraform language are sequences of unicode characters rather than bytes, so this function will also interpret the resulting bytes as UTF-8. If the bytes after Base64 decoding are not valid UTF-8, this function produces an error.\nWhile we do not recommend manipulating large, raw binary data in the Terraform language, Base64 encoding is the standard way to represent arbitrary byte sequences, and so resource types that accept or return binary data will use Base64 themselves, which avoids the need to encode or decode it directly in most cases. Various other functions with names containing \"base64\" can generate or manipulate Base64 data directly.\nbase64decode is, in effect, a shorthand for calling textdecodebase64 with the encoding name set to UTF-8.\n> base64decode(\"SGVsbG8gV29ybGQ=\") Hello World \nbase64encode performs the opposite operation, encoding the UTF-8 bytes for a string as Base64.\ntextdecodebase64 is a more general function that supports character encodings other than UTF-8.\nbase64gzip applies gzip compression to a string and returns the result with Base64 encoding."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/base64encode",
  "text": "base64encode - Functions - Configuration Language | Terraform\nbase64encode applies Base64 encoding to a string.\nStrings in the Terraform language are sequences of unicode characters rather than bytes, so this function will first encode the characters from the string as UTF-8, and then apply Base64 encoding to the result.\nThe Terraform language applies Unicode normalization to all strings, and so passing a string through base64decode and then base64encode may not yield the original result exactly.\nWhile we do not recommend manipulating large, raw binary data in the Terraform language, Base64 encoding is the standard way to represent arbitrary byte sequences, and so resource types that accept or return binary data will use Base64 themselves, and so this function exists primarily to allow string data to be easily provided to resource types that expect Base64 bytes.\nbase64encode is, in effect, a shorthand for calling textencodebase64 with the encoding name set to UTF-8.\n> base64encode(\"Hello World\") SGVsbG8gV29ybGQ= \nbase64decode performs the opposite operation, decoding Base64 data and interpreting it as a UTF-8 string.\ntextencodebase64 is a more general function that supports character encodings other than UTF-8.\nbase64gzip applies gzip compression to a string and returns the result with Base64 encoding all in one operation."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/base64gzip",
  "text": "base64gzip function reference - Functions - Configuration Language | Terraform\nThis topic provides reference information about the base64gzip function. The base64gzip function compresses an HCL string using gzip and then encodes the string using Base64 encoding.\nYou can use the base64gzip function to compress an HCL string and then encode it in the Base64 format. Terraform uses the standard Base64 alphabet that is defined in RFC 4648.\nWhile HashiCorp does not recommend manipulating large, raw binary data in HCL, Base64 encoding can be an effective way to represent small binary objects in memory when you need to pass them as values, rather than referring to files on disk. For example, you could use the base64gzip function to compress a large JSON string so that you can upload it to S3.\nBecause HCL strings are sequences of unicode characters rather than bytes, base64gzip first encodes the characters in the string as UTF-8. Then it applies gzip compression and encodes the string using Base64 format.\nUse the base64gzip function with the following syntax:\nThe argument is the string that you want to compress and encode.\nIn the following example, the function compresses the string at local.my_data and encodes it using the Base64 format.\nbase64gzip(local.my_data) \nThe following example defines a local value my_data that contains the string you want to compress and encode. The base64gzip function compresses and encodes the string, and then it is used to populate an S3 bucket.\nresource \"aws_s3_object\" \"example\" { bucket = \"my_bucket\" key = \"example.txt\" content_base64 = base64gzip(local.my_data) content_encoding = \"gzip\" } \nbase64encode applies Base64 encoding to an HCL string without using gzip compression.\nfilebase64 reads a file from the local filesystem and encodes its raw bits using the Base64 format."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/urlencode",
  "text": "urlencode - Functions - Configuration Language | Terraform\nurlencode applies URL encoding to a given string.\nThis function identifies characters in the given string that would have a special meaning when included as a query string argument in a URL and escapes them using RFC 3986 \"percent encoding\".\nThe exact set of characters escaped may change over time, but the result is guaranteed to be interpolatable into a query string argument without inadvertently introducing additional delimiters.\nIf the given string contains non-ASCII characters, these are first encoded as UTF-8 and then percent encoding is applied separately to each UTF-8 byte.\n> urlencode(\"Hello World!\") Hello+World%21 > urlencode(\"\") %E2%98%83 > \"http://example.com/search?q=${urlencode(\"terraform urlencode\")}\" http://example.com/search?q=terraform+urlencode"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/textencodebase64",
  "text": "textencodebase64 - Functions - Configuration Language | Terraform\ntextencodebase64 Function\ntextencodebase64 encodes the unicode characters in a given string using a specified character encoding, returning the result base64 encoded because Terraform language strings are always sequences of unicode characters.\ntextencodebase64(string, encoding_name) \nThe encoding_name argument must contain one of the encoding names or aliases recorded in the IANA character encoding registry. Terraform supports only a subset of the registered encodings, and the encoding support may vary between Terraform versions. In particular Terraform supports UTF-16LE, which is the native character encoding for the Windows API and therefore sometimes expected by Windows-originated software such as PowerShell.\nTerraform also accepts the encoding name UTF-8, which will produce the same result as base64encode.\n> textencodebase64(\"Hello World\", \"UTF-16LE\") SABlAGwAbABvACAAVwBvAHIAbABkAA== \ntextdecodebase64 performs the opposite operation, decoding Base64 data and interpreting it as a particular character encoding.\nbase64encode applies Base64 encoding of the UTF-8 encoding of a string."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/yamldecode",
  "text": "yamldecode - Functions - Configuration Language | Terraform\nyamldecode parses a string as a subset of YAML, and produces a representation of its value.\nThis function supports a subset of YAML 1.2, as described below.\nThis function maps YAML values to Terraform language values in the following way:\nYAML typeTerraform type\n!!str\tstring\t\n!!float\tnumber\t\n!!int\tnumber\t\n!!bool\tbool\t\n!!map\tobject(...) with attribute types determined per this table\t\n!!seq\ttuple(...) with element types determined per this table\t\n!!null\tThe Terraform language null value\t\n!!timestamp\tstring in RFC 3339 format\t\n!!binary\tstring containing base64-encoded representation\t\nThe Terraform language automatic type conversion rules mean that you don't usually need to worry about exactly what type is produced for a given value, and can just use the result in an intuitive way.\nNote though that the mapping above is ambiguous -- several different source types map to the same target type -- and so round-tripping through yamldecode and then yamlencode cannot produce an identical result.\nYAML is a complex language and it supports a number of possibilities that the Terraform language's type system cannot represent. Therefore this YAML decoder supports only a subset of YAML 1.2, with restrictions including the following:\nAlthough aliases to earlier anchors are supported, cyclic data structures (where a reference to a collection appears inside that collection) are not. If yamldecode detects such a structure then it will return an error.\nOnly the type tags shown in the above table (or equivalent alternative representations of those same tags) are supported. Any other tags will result in an error.\nOnly one YAML document is permitted. If multiple documents are present in the given string then this function will return an error.\n> yamldecode(\"hello: world\") { \"hello\" = \"world\" } > yamldecode(\"true\") true > yamldecode(\"{a: &foo [1, 2, 3], b: *foo}\") { \"a\" = [ 1, 2, 3, ] \"b\" = [ 1, 2, 3, ] } > yamldecode(\"{a: &foo [1, *foo, 3]}\") Error: Error in function call Call to function \"yamldecode\" failed: cannot refer to anchor \"foo\" from inside its own definition. > yamldecode(\"{a: !not-supported foo}\") Error: Error in function call Call to function \"yamldecode\" failed: unsupported tag \"!not-supported\". \njsondecode is a similar operation using JSON instead of YAML.\nyamlencode performs the opposite operation, encoding a value as YAML."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/textdecodebase64",
  "text": "textdecodebase64 - Functions - Configuration Language | Terraform\ntextdecodebase64 Function\ntextdecodebase64 function decodes a string that was previously Base64-encoded, and then interprets the result as characters in a specified character encoding.\nThe encoding_name argument must contain one of the encoding names or aliases recorded in the IANA character encoding registry. Terraform supports only a subset of the registered encodings, and the encoding support may vary between Terraform versions.\nTerraform accepts the encoding name UTF-8, which will produce the same result as base64decode.\n> textdecodebase64(\"SABlAGwAbABvACAAVwBvAHIAbABkAA==\", \"UTF-16LE\") Hello World \ntextencodebase64 performs the opposite operation, applying target encoding and then Base64 to a string.\nbase64decode is effectively a shorthand for textdecodebase64 where the character encoding is fixed as UTF-8."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/base64decode",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/base64encode",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/base64gzip",
  "text": "base64gzip - Functions - Configuration Language | Terraform\nbase64gzip compresses a string with gzip and then encodes the result in Base64 encoding.\nStrings in the Terraform language are sequences of unicode characters rather than bytes, so this function will first encode the characters from the string as UTF-8, then apply gzip compression, and then finally apply Base64 encoding.\nWhile we do not recommend manipulating large, raw binary data in the Terraform language, this function can be used to compress reasonably sized text strings generated within the Terraform language. For example, the result of this function can be used to create a compressed object in Amazon S3 as part of an S3 website.\nbase64encode applies Base64 encoding without gzip compression."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/textencodebase64",
  "text": "substr(string, encoding_name) "
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/textdecodebase64",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/urlencode",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/yamldecode",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/base64decode",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/base64encode",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/base64gzip",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/textdecodebase64",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/textencodebase64",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/yamldecode",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/urlencode",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/base64decode",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/base64encode",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/base64gzip",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/textdecodebase64",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/textencodebase64",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/urlencode",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/yamldecode",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/base64encode",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/base64decode",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/base64gzip",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/textencodebase64",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/textdecodebase64",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/urlencode",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/yamldecode",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.1.x/functions/base64decode",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.1.x/functions/base64encode",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.1.x/functions/urlencode",
  "text": "> urlencode(\"Hello World\") Hello%20World > urlencode(\"\") %E2%98%83 > \"http://example.com/search?q=${urlencode(\"terraform urlencode\")}\" http://example.com/search?q=terraform+urlencode"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.1.x/functions/textdecodebase64",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.1.x/functions/base64gzip",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.1.x/functions/yamldecode",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.1.x/functions/textencodebase64",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.10.x/builders/file",
  "text": "File - Builders | Packer\nType: file Artifact BuilderId: packer.file\nThe file Packer builder is not really a builder, it just creates an artifact from a file. It can be used to debug post-processors without incurring high wait times.\nBelow is a fully functioning example. It create a file at target with the specified content.\n{ \"type\": \"file\", \"content\": \"Lorem ipsum dolor sit amet\", \"target\": \"dummy_artifact\" } \nConfiguration options are organized below into two categories: required and optional. Within each category, the available options are alphabetized and described.\nAny communicator defined is ignored.\ntarget (string) - The path for the artifact file that will be created. If the path contains directories that don't exist, Packer will create them, too.\nYou can only define one of source or content. If none of them is defined the artifact will be empty.\nsource (string) - The path for a file which will be copied as the artifact.\ncontent (string) - The content that will be put into the artifact."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.10.x/builders/null",
  "text": "Null - Builders | Packer\nType: null\nThe null Packer builder is not really a builder, it just sets up an SSH connection and runs the provisioners. It can be used to debug provisioners without incurring high wait times. It does not create any kind of image or artifact.\nBelow is a fully functioning example. It doesn't do anything useful, since no provisioners are defined, but it will connect to the specified host via ssh.\n{ \"type\": \"null\", \"ssh_host\": \"127.0.0.1\", \"ssh_username\": \"foo\", \"ssh_password\": \"bar\" } \nThe null builder has no configuration parameters other than the communicator settings."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.10.x/builders/custom",
  "text": "Custom - Builders | Packer\nPacker is extensible, allowing you to write new builders without having to modify the core source code of Packer itself. Documentation for creating new builders is covered in the custom builders page of the Packer plugin section."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.10.x/builders/community-supported",
  "text": "Community - Builders | Packer\nThe following builders are developed and maintained by various members of the Packer community, not by HashiCorp. For more information on how to use community builders, see our docs on extending Packer.\nCommunity Builders\nARM builders\npacker-plugin-arm-image - simple builder lets you extend on existing system images.\npacker-builder-arm - flexible builder lets you extend or build images from scratch with variety of options (ie. custom partition table).\nExoscale builder - A builder to create Exoscale custom templates based on a Compute instance snapshot.\nCitrix XenServer/Citrix Hypervisor - Plugin for creating Citrix XenServer/Citrix Hypervisor images from an iso image or from an existing template.\nXCP-NG/Citrix XenServer/Citrix Hypervisor/Updated Fork - Plugin for creating XCP-NG/Citrix XenServer/Citrix Hypervisor images from an iso image or from an existing template. This is a fork of the orginal, and reccomended by the developers of XCP-NG."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.10.x/datasources/http",
  "text": "HTTP - Data Sources | Packer\nType: http\nThe http data source makes an HTTP GET request to the given URL and exports information about the response.\ndata \"http\" \"example\" { url = \"https://checkpoint-api.hashicorp.com/v1/check/terraform\" # Optional request headers request_headers = { Accept = \"application/json\" } } \nConfiguration options are organized below into two categories: required and optional. Within each category, the available options are alphabetized and described.\nurl (string) - The URL to request data from. This URL must respond with a 200 OK response and a text/* or application/json Content-Type.\nNot Required:\nrequest_headers (map[string]string) - A map of strings representing additional HTTP headers to include in the request.\nThe outputs for this datasource are as follows:\nurl (string) - The URL the data was requested from.\nbody (string) - The raw body of the HTTP response.\nrequest_headers (map[string]string) - A map of strings representing the response HTTP headers. Duplicate headers are concatenated with, according to RFC2616."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.10.x/provisioners/breakpoint",
  "text": "breakpoint - Provisioners | Packer\nBreakpoint Provisioner\nType: breakpoint\nThe breakpoint provisioner will pause until the user presses \"enter\" to resume the build. This is intended for debugging purposes, and allows you to halt at a particular part of the provisioning process.\nThis is independent of the -debug flag, which will instead halt at every step and between every provisioner.\n{ \"builders\": [ { \"type\": \"null\", \"communicator\": \"none\" } ], \"provisioners\": [ { \"type\": \"shell-local\", \"inline\": \"echo hi\" }, { \"type\": \"breakpoint\", \"disable\": false, \"note\": \"this is a breakpoint\" }, { \"type\": \"shell-local\", \"inline\": \"echo hi 2\" } ] } \ndisable (boolean) - If true, skip the breakpoint. Useful for when you have set multiple breakpoints and want to toggle them off or on. Default: false\nnote (string) - a string to include explaining the purpose or location of the breakpoint. For example, you may find it useful to number your breakpoints or label them with information about where in the build they occur\nParameters common to all provisioners:\npause_before (duration) - Sleep for duration before execution.\nmax_retries (int) - Max times the provisioner will retry in case of failure. Defaults to zero (0). Zero means an error will not be retried.\nonly (array of string) - Only run the provisioner for listed builder(s) by name.\noverride (object) - Override the builder with different settings for a specific builder, eg :\nsource \"null\" \"example1\" { communicator = \"none\" } source \"null\" \"example2\" { communicator = \"none\" } build { sources = [\"source.null.example1\", \"source.null.example2\"] provisioner \"shell-local\" { inline = [\"echo not overridden\"] override = { example1 = { inline = [\"echo yes overridden\"] } } } } \n{ \"builders\": [ { \"type\": \"null\", \"name\": \"example1\", \"communicator\": \"none\" }, { \"type\": \"null\", \"name\": \"example2\", \"communicator\": \"none\" } ], \"provisioners\": [ { \"type\": \"shell-local\", \"inline\": [\"echo not overridden\"], \"override\": { \"example1\": { \"inline\": [\"echo yes overridden\"] } } } ] } \ntimeout (duration) - If the provisioner takes more than for example 1h10m1s or 10m to finish, the provisioner will timeout and fail.\nInsert this provisioner wherever you want the build to pause. You'll see ui output prompting you to press \"enter\" to continue the build when you are ready.\n==> docker: Pausing at breakpoint provisioner with note \"foo bar baz\". ==> docker: Press enter to continue. \nOnce you press enter, the build will resume and run normally until it either completes or errors."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.10.x/provisioners/file",
  "text": "File - Provisioners | Packer\nType: file\nThe file Packer provisioner uploads files to machines built by Packer. The recommended usage of the file provisioner is to use it to upload files, and then use shell provisioner to move them to the proper place, set permissions, etc.\nWarning: You can only upload files to locations that the provisioning user (generally not root) has permission to access. Creating files in /tmp and using a shell provisioner to move them into the final location is the only way to upload files to root owned locations.\nThe file provisioner can upload both single files and complete directories.\n{ \"type\": \"file\", \"source\": \"app.tar.gz\", \"destination\": \"/tmp/app.tar.gz\" } \nThe available configuration options are listed below.\nRequired Parameters:\ncontent (string) - This is the content to copy to destination. If destination is a file, content will be written to that file, in case of a directory a file named pkr-file-content is created. It's recommended to use a file as the destination. The templatefile function might be used here, or any interpolation syntax. This attribute cannot be specified with source or sources.\nsource (string) - The path to a local file or directory to upload to the machine. The path can be absolute or relative. If it is relative, it is relative to the working directory when Packer is executed. If this is a directory, the existence of a trailing slash is important. Read below on uploading directories. Mandatory unless sources is set.\ndestination (string) - The path where the file will be uploaded to in the machine. This value must be a writable location and any parent directories must already exist. If the provisioning user (generally not root) cannot write to this directory, you will receive a \"Permission Denied\" error. If the source is a file, it's a good idea to make the destination a file as well, but if you set your destination as a directory, at least make sure that the destination ends in a trailing slash so that Packer knows to use the source's basename in the final upload path. Failure to do so may cause Packer to fail on file uploads. If the destination file already exists, it will be overwritten.\nOptional Parameters:\nsources ([]string) - A list of sources to upload. This can be used in place of the source option if you have several files that you want to upload to the same place. Note that the destination must be a directory with a trailing slash, and that all files listed in sources will be uploaded to the same directory with their file names preserved.\ndirection (string) - The direction of the file transfer. This defaults to \"upload\". If it is set to \"download\" then the file \"source\" in the machine will be downloaded locally to \"destination\"\ngenerated (bool) - For advanced users only. If true, check the file existence only before uploading, rather than upon pre-build validation. This allows users to upload files created on-the-fly. This defaults to false. We don't recommend using this feature, since it can cause Packer to become dependent on system state. We would prefer you generate your files before the Packer run, but realize that there are situations where this may be unavoidable.\nParameters common to all provisioners:\npause_before (duration) - Sleep for duration before execution.\nmax_retries (int) - Max times the provisioner will retry in case of failure. Defaults to zero (0). Zero means an error will not be retried.\nonly (array of string) - Only run the provisioner for listed builder(s) by name.\noverride (object) - Override the builder with different settings for a specific builder, eg :\nsource \"null\" \"example1\" { communicator = \"none\" } source \"null\" \"example2\" { communicator = \"none\" } build { sources = [\"source.null.example1\", \"source.null.example2\"] provisioner \"shell-local\" { inline = [\"echo not overridden\"] override = { example1 = { inline = [\"echo yes overridden\"] } } } } \n{ \"builders\": [ { \"type\": \"null\", \"name\": \"example1\", \"communicator\": \"none\" }, { \"type\": \"null\", \"name\": \"example2\", \"communicator\": \"none\" } ], \"provisioners\": [ { \"type\": \"shell-local\", \"inline\": [\"echo not overridden\"], \"override\": { \"example1\": { \"inline\": [\"echo yes overridden\"] } } } ] } \ntimeout (duration) - If the provisioner takes more than for example 1h10m1s or 10m to finish, the provisioner will timeout and fail.\nThe file provisioner is also able to upload a complete directory to the remote machine. When uploading a directory, there are a few important things you should know.\nFirst, the destination directory must already exist. If you need to create it, use a shell provisioner just prior to the file provisioner in order to create the directory. If the destination directory does not exist, the file provisioner may succeed, but it will have undefined results.\nNext, the existence of a trailing slash on the source path will determine whether the directory name will be embedded within the destination, or whether the destination will be created. An example explains this best:\nIf the source is /foo (no trailing slash), and the destination is /tmp, then the contents of /foo on the local machine will be uploaded to /tmp/foo on the remote machine. The foo directory on the remote machine will be created by Packer.\nIf the source, however, is /foo/ (a trailing slash is present), and the destination is /tmp, then the contents of /foo will be uploaded into /tmp directly.\nThis behavior was adopted from the standard behavior of rsync. Note that under the covers, rsync may or may not be used.\nIn general, local files used as the source must exist before Packer is run. This is great for catching typos and ensuring that once a build is started, that it will succeed. However, this also means that you can't generate a file during your build and then upload it using the file provisioner later. A convenient workaround is to upload a directory instead of a file. The directory still must exist, but its contents don't. You can write your generated file to the directory during the Packer run, and have it be uploaded later.\nThe behavior when uploading symbolic links depends on the communicator. The Docker communicator will preserve symlinks, but all other communicators will treat local symlinks as regular files. If you wish to preserve symlinks when uploading, it's recommended that you use tar. Below is an example of what that might look like:\n$ ls -l files total 16 drwxr-xr-x 3 mwhooker staff 102 Jan 27 17:10 a lrwxr-xr-x 1 mwhooker staff 1 Jan 27 17:10 b -> a -rw-r--r-- 1 mwhooker staff 0 Jan 27 17:10 file1 lrwxr-xr-x 1 mwhooker staff 5 Jan 27 17:10 file1link -> file1 $ ls -l toupload total 0 -rw-r--r-- 1 mwhooker staff 0 Jan 27 17:10 files.tar \n{ \"provisioners\": [ { \"type\": \"shell-local\", \"command\": \"tar cf toupload/files.tar files\" }, { \"destination\": \"/tmp/\", \"source\": \"./toupload\", \"type\": \"file\" }, { \"inline\": [ \"cd /tmp && tar xf toupload/files.tar\", \"rm toupload/files.tar\" ], \"type\": \"shell\" } ] } \nBecause of the way our WinRM transfers works, it can take a very long time to upload and download even moderately sized files. If you're experiencing slowness using the file provisioner on Windows, it's suggested that you set up an SSH server and use the ssh communicator. If you only want to transfer files to your guest, and if your builder supports it, you may also use the http_directory or http_content directives. This will cause that directory to be available to the guest over HTTP, and set the environment variable PACKER_HTTP_ADDR to the address."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.10.x/provisioners/shell-local",
  "text": "Shell (Local) - Provisioners | Packer\nType: shell-local\nshell-local will run a shell script of your choosing on the machine where Packer is being run - in other words, shell-local will run the shell script on your build server, or your desktop, etc., rather than the remote/guest machine being provisioned by Packer.\nThe remote shell provisioner executes shell scripts on a remote machine.\nThe example below is fully functional.\nsource \"file\" \"example\" { content = \"example content\" } build { source \"source.file.example\" { target = \"./test_artifact.txt\" } provisioner \"shell-local\" { inline = [\"echo foo\"] } } \nThe reference of available configuration options is listed below. The only required element is command.\nExactly one of the following is required:\ncommand (string) - This is a single command to execute. It will be written to a temporary file and run using the execute_command call below. If you are building a Windows VM on AWS, Azure, Google Compute, or OpenStack and would like to access the generated password that Packer uses to connect to the instance via WinRM, you can use the template variable {{.WinRMPassword}} to set this as an environment variable.\ninline (array of strings) - This is an array of commands to execute. The commands are concatenated by newlines and turned into a single file, so they are all executed within the same context. This allows you to change directories in one command and use something in the directory in the next and so on. Inline scripts are the easiest way to pull off simple tasks within the machine in which Packer is running.\nscript (string) - The path to a script to execute. This path can be absolute or relative. If it is relative, it is relative to the working directory when Packer is executed.\nscripts (array of strings) - An array of scripts to execute. The scripts will be executed in the order specified. Each script is executed in isolation, so state such as variables from one script won't carry on to the next.\nOptional parameters:\nenv (map of strings) - A map of key/value pairs to inject prior to the execute_command. Packer injects some environmental variables by default into the environment, as well, which are covered in the section below. Duplicate env settings override environment_vars settings.\nenvironment_vars (array of strings) - An array of key/value pairs to inject prior to the execute_command. The format should be key=value. Packer injects some environmental variables by default into the environment, as well, which are covered in the section below. If you are building a Windows VM on AWS, Azure, Google Compute, or OpenStack and would like to access the generated password that Packer uses to connect to the instance via WinRM, you can use the template variable {{.WinRMPassword}} to set this as an environment variable. For example: \"environment_vars\": \"WINRMPASS={{.WinRMPassword}}\"\nenv_var_format (string) - When we parse the environment_vars that you provide, this gives us a string template to use in order to make sure that we are setting the environment vars correctly. By default on Windows hosts this format is set %s=%s && and on Unix, it is %s='%s'. You probably won't need to change this format, but you can see usage examples for where it is necessary below.\nexecute_command (array of strings) - The command used to execute the script. By default this is [\"/bin/sh\", \"-c\", \"{{.Vars}}\", \"{{.Script}}\"] on Unix and [\"cmd\", \"/c\", \"{{.Vars}}\", \"{{.Script}}\"] on Windows. This is treated as a template engine. There are two available variables: Script, which is the path to the script to run, and Vars, which is the list of environment_vars, if configured.\nIf you choose to set this option, make sure that the first element in the array is the shell program you want to use (for example, \"sh\"), and a later element in the array must be {{.Script}}.\nThis option provides you a great deal of flexibility. You may choose to provide your own shell program, for example \"/usr/local/bin/zsh\" or even \"powershell.exe\". However, with great power comes great responsibility - these commands are not officially supported and things like environment variables may not work if you use a different shell than the default.\nFor backwards compatibility, you may also use {{.Command}}, but it is decoded the same way as {{.Script}}. We recommend using {{.Script}} for the sake of clarity, as even when you set only a single command to run, Packer writes it to a temporary file and then runs it as a script.\nIf you are building a Windows VM on AWS, Azure, Google Compute, or OpenStack and would like to access the generated password that Packer uses to connect to the instance via WinRM, you can use the template variable {{.WinRMPassword}} to set this as an environment variable.\ninline_shebang (string) - The shebang value to use when running commands specified by inline. By default, this is /bin/sh -e. If you're not using inline, then this configuration has no effect. Important: If you customize this, be sure to include something like the -e flag, otherwise individual steps failing won't fail the provisioner.\nonly_on (array of strings) - This is an array of runtime operating systems where shell-local will execute. This allows you to execute shell-local only on specific operating systems. By default, shell-local will always run if only_on is not set.\"\nuse_linux_pathing (bool) - This is only relevant to Windows hosts. If you are running Packer in a Windows environment with the Windows Subsystem for Linux feature enabled, and would like to invoke a bash script rather than invoking a Cmd script, you'll need to set this flag to true; it tells Packer to use the Linux subsystem path for your script rather than the Windows path. (e.g. /mnt/c/path/to/your/file instead of C:/path/to/your/file). Please see the example below for more guidance on how to use this feature. If you are not on a Windows host, or you do not intend to use the shell-local provisioner to run a bash script, please ignore this option.\nvalid_exit_codes (list of ints) - Valid exit codes for the script. By default this is 0.\nParameters common to all provisioners:\npause_before (duration) - Sleep for duration before execution.\nmax_retries (int) - Max times the provisioner will retry in case of failure. Defaults to zero (0). Zero means an error will not be retried.\nonly (array of string) - Only run the provisioner for listed builder(s) by name.\noverride (object) - Override the builder with different settings for a specific builder, eg :\nsource \"null\" \"example1\" { communicator = \"none\" } source \"null\" \"example2\" { communicator = \"none\" } build { sources = [\"source.null.example1\", \"source.null.example2\"] provisioner \"shell-local\" { inline = [\"echo not overridden\"] override = { example1 = { inline = [\"echo yes overridden\"] } } } } \n{ \"builders\": [ { \"type\": \"null\", \"name\": \"example1\", \"communicator\": \"none\" }, { \"type\": \"null\", \"name\": \"example2\", \"communicator\": \"none\" } ], \"provisioners\": [ { \"type\": \"shell-local\", \"inline\": [\"echo not overridden\"], \"override\": { \"example1\": { \"inline\": [\"echo yes overridden\"] } } } ] } \ntimeout (duration) - If the provisioner takes more than for example 1h10m1s or 10m to finish, the provisioner will timeout and fail.\nTo many new users, the execute_command is puzzling. However, it provides an important function: customization of how the command is executed. The most common use case for this is dealing with sudo password prompts. You may also need to customize this if you use a non-POSIX shell, such as tcsh on FreeBSD.\nThe Windows Linux Subsystem\nThe shell-local provisioner was designed with the idea of allowing you to run commands in your local operating system's native shell. For Windows, we've assumed in our defaults that this is Cmd. However, it is possible to run a bash script as part of the Windows Linux Subsystem from the shell-local provisioner, by modifying the execute_command and the use_linux_pathing options in the provisioner config.\nThe example below is a fully functional test config.\nOne limitation of this offering is that \"inline\" and \"command\" options are not available to you; please limit yourself to using the \"script\" or \"scripts\" options instead.\nPlease note that the WSL is a beta feature, and this tool is not guaranteed to work as you expect it to.\nsource \"null\" \"example\" { communicator = \"none\" } build { sources = [ \"source.null.example\" ] provisioner \"shell-local\"{ environment_vars = [\"PROVISIONERTEST=ProvisionerTest1\"] execute_command = [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"] use_linux_pathing = true scripts = [\"C:/Users/me/scripts/example_bash.sh\"] } provisioner \"shell-local\"{ environment_vars = [\"PROVISIONERTEST=ProvisionerTest2\"] execute_command = [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"] use_linux_pathing = true script = \"C:/Users/me/scripts/example_bash.sh\" } } \nIn addition to being able to specify custom environmental variables using the environment_vars configuration, the provisioner automatically defines certain commonly useful environmental variables:\nPACKER_BUILD_NAME is set to the name of the build that Packer is running. This is most useful when Packer is making multiple builds and you want to distinguish them slightly from a common provisioning script.\nPACKER_BUILDER_TYPE is the type of the builder that was used to create the machine that the script is running on. This is useful if you want to run only certain parts of the script on systems built with certain builders.\nPACKER_HTTP_ADDR If using a builder that provides an HTTP server for file transfer (such as hyperv, parallels, qemu, virtualbox, and vmware), this will be set to the address. You can use this address in your provisioner to download large files over HTTP. This may be useful if you're experiencing slower speeds using the default file provisioner. A file provisioner using the winrm communicator may experience these types of difficulties.\nWhether you use the inline option, or pass it a direct script or scripts, it is important to understand a few things about how the shell-local provisioner works to run it safely and easily. This understanding will save you much time in the process.\nOnce Per Builder\nThe shell-local script(s) you pass are run once per builder. That means that if you have an amazon-ebs builder and a docker builder, your script will be run twice. If you have 3 builders, it will run 3 times, once for each builder.\nAlways Exit Intentionally\nIf any provisioner fails, the packer build stops and all interim artifacts are cleaned up.\nFor a shell script, that means the script must exit with a zero code. You must be extra careful to exit 0 when necessary.\nWindows Host\nExample of running a .cmd file on Windows:\nprovisioner \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest1\"] scripts = [\"./scripts/test_cmd.cmd\"] } \nContents of \"test_cmd.cmd\":\nExample of running an inline command on Windows: Required customization: tempfile_extension\nprovisioner \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest2\"], tempfile_extension = \".cmd\", inline = [echo \"%SHELLLOCALTEST%\"] } \nExample of running a bash command on Windows using WSL: Required customizations: use_linux_pathing and execute_command\nprovisioner \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest3\"], execute_command = [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"] use_linux_pathing = true script = \"./scripts/example_bash.sh\" } \nContents of example_bash.sh:\n#!/bin/bash echo $SHELLLOCALTEST \nExample of running a PowerShell script on Windows: Required customizations: env_var_format and execute_command\nprovisioner \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest4\"] execute_command = [\"powershell.exe\", \"{{.Vars}} {{.Script}}\"] env_var_format = \"$env:%s=\\\"%s\\\"; \" script = \"./scripts/example_ps.ps1\" } \nExample of running a PowerShell script on Windows as \"inline\": Required customizations: env_var_format, tempfile_extension, and execute_command\nprovisioner \"shell-local\" { tempfile_extension = \".ps1\" environment_vars = [\"SHELLLOCALTEST=ShellTest5\"] execute_command = [\"powershell.exe\", \"{{.Vars}} {{.Script}}\"] env_var_format = \"$env:%s=\\\"%s\\\"; \" inline = [\"write-output $env:SHELLLOCALTEST\"] } \nUnix Host\nExample of running a Shell script on Unix:\nprovisioner \"shell-local\" { environment_vars = [\"PROVISIONERTEST=ProvisionerTest1\"] scripts = [\"./scripts/example_bash.sh\"] } \nExample of running a Shell script \"inline\" on Unix:\nprovisioner \"shell-local\" { environment_vars = [\"PROVISIONERTEST=ProvisionerTest2\"] inline = [\"echo hello\", \"echo $PROVISIONERTEST\"] } \nExample of running a Python script on Unix:\nprovisioner \"shell-local\" { script = \"hello.py\" environment_vars = [\"HELLO_USER=packeruser\"] execute_command = [ \"/bin/sh\", \"-c\", \"{{.Vars}} /usr/local/bin/python {{.Script}}\" ] } \nWhere \"hello.py\" contains: import os print('Hello, %s!' % os.getenv(\"HELLO_USER\"))"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.10.x/provisioners/windows-shell",
  "text": "Windows Shell - Provisioners | Packer\nWindows Shell Provisioner\nType: windows-shell\nThe windows-shell Packer provisioner runs commands on a Windows machine using cmd. It assumes it is running over WinRM.\nThe example below is fully functional.\nprovisioner \"windows-shell\" { inline = [\"dir c:\\\\\"] } \nThe reference of available configuration options is listed below. The only required element is either \"inline\" or \"script\". Every other option is optional.\nExactly one of the following is required:\ninline (array of strings) - This is an array of commands to execute. The commands are concatenated by newlines and turned into a single file, so they are all executed within the same context. This allows you to change directories in one command and use something in the directory in the next and so on. Inline scripts are the easiest way to pull off simple tasks within the machine.\nscript (string) - The path to a script to upload and execute in the machine. This path can be absolute or relative. If it is relative, it is relative to the working directory when Packer is executed.\nscripts (array of strings) - An array of scripts to execute. The scripts will be uploaded and executed in the order specified. Each script is executed in isolation, so state such as variables from one script won't carry on to the next.\nOptional parameters:\nbinary (boolean) - If true, specifies that the script(s) are binary files, and Packer should therefore not convert Windows line endings to Unix line endings (if there are any). By default this is false.\nvalid_exit_codes (list of ints) - Valid exit codes for the script. By default this is just 0.\nenv (map of strings) - A map of key/value pairs to inject prior to the execute_command. Packer injects some environmental variables by default into the environment, as well, which are covered in the section below. Duplicate env settings override environment_vars settings.\nenvironment_vars (array of strings) - An array of key/value pairs to inject prior to the execute_command. The format should be key=value. Packer injects some environmental variables by default into the environment, as well, which are covered in the section below.\nexecute_command (string) - The command to use to execute the script. By default this is {{ .Vars }}\"{{ .Path }}\". The value of this is treated as template engine. This is a template engine. Therefore, you may use user variables and template functions in this field. In addition, there are two available extra variables:\nPath is the path to the script to run\nVars is the list of environment_vars, if configured.\nremote_path (string) - The path where the script will be uploaded to in the machine. This defaults to \"c:/Windows/Temp/script.bat\". This value must be a writable location and any parent directories must already exist.\nstart_retry_timeout (string) - The amount of time to attempt to start the remote process. By default this is \"5m\" or 5 minutes. This setting exists in order to deal with times when SSH may restart, such as a system reboot. Set this to a higher value if reboots take a longer amount of time.\nParameters common to all provisioners:\npause_before (duration) - Sleep for duration before execution.\nmax_retries (int) - Max times the provisioner will retry in case of failure. Defaults to zero (0). Zero means an error will not be retried.\nonly (array of string) - Only run the provisioner for listed builder(s) by name.\noverride (object) - Override the builder with different settings for a specific builder, eg :\nsource \"null\" \"example1\" { communicator = \"none\" } source \"null\" \"example2\" { communicator = \"none\" } build { sources = [\"source.null.example1\", \"source.null.example2\"] provisioner \"shell-local\" { inline = [\"echo not overridden\"] override = { example1 = { inline = [\"echo yes overridden\"] } } } } \n{ \"builders\": [ { \"type\": \"null\", \"name\": \"example1\", \"communicator\": \"none\" }, { \"type\": \"null\", \"name\": \"example2\", \"communicator\": \"none\" } ], \"provisioners\": [ { \"type\": \"shell-local\", \"inline\": [\"echo not overridden\"], \"override\": { \"example1\": { \"inline\": [\"echo yes overridden\"] } } } ] } \ntimeout (duration) - If the provisioner takes more than for example 1h10m1s or 10m to finish, the provisioner will timeout and fail.\nIn addition to being able to specify custom environmental variables using the environment_vars configuration, the provisioner automatically defines certain commonly useful environmental variables:\nPACKER_BUILD_NAME is set to the name of the build that Packer is running. This is most useful when Packer is making multiple builds and you want to distinguish them slightly from a common provisioning script.\nPACKER_BUILDER_TYPE is the type of the builder that was used to create the machine that the script is running on. This is useful if you want to run only certain parts of the script on systems built with certain builders.\nPACKER_HTTP_ADDR If using a builder that provides an HTTP server for file transfer (such as hyperv, parallels, qemu, virtualbox, and vmware), this will be set to the address. You can use this address in your provisioner to download large files over HTTP. This may be useful if you're experiencing slower speeds using the default file provisioner. A file provisioner using the winrm communicator may experience these types of difficulties."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.10.x/provisioners/windows-restart",
  "text": "Windows Restart - Provisioners | Packer\nWindows Restart Provisioner\nType: windows-restart\nThe Windows restart provisioner initiates a reboot on a Windows machine and waits for the machine to come back online.\nThe Windows provisioning process often requires multiple reboots, and this provisioner helps to ease that process.\nPacker expects the machine to be ready to continue provisioning after it reboots. Packer detects that the reboot has completed by making an RPC call through the Windows Remote Management (WinRM) service, not by ACPI functions, so Windows must be completely booted in order to continue.\nThe example below is fully functional.\nprovisioner \"windows-restart\" {} \nThe reference of available configuration options is listed below.\nOptional parameters:\ncheck_registry (bool) - if true, checks for several registry keys that indicate that the system is going to reboot. This is useful if an installation kicks off a reboot and you want the provisioner to wait for that reboot to complete before reconnecting. Please note that this option is a beta feature, and we generally recommend that you finish installs that auto-reboot (like Windows Updates) during your autounattend phase before the winrm provisioner connects.\nregistry_keys (array of strings) - if check-registry is true, windows-restart will not reconnect until after all of the listed keys are no longer present in the registry.\ndefault: var DefaultRegistryKeys = []string{ \"HKLM:SOFTWARE\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Component Based Servicing\\\\RebootPending\", \"HKLM:SOFTWARE\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Component Based Servicing\\\\PackagesPending\", \"HKLM:Software\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Component Based Servicing\\\\RebootInProgress\", } \nrestart_command (string) - The command to execute to initiate the restart. By default this is shutdown /r /f /t 0 /c \"packer restart\".\nrestart_check_command (string) - The command to run after executing restart_command to check if the guest machine has restarted. This command will retry until the connection to the guest machine has been restored or restart_timeout has exceeded.\nprovisioner \"windows-restart\" { restart_check_command = \"powershell -command \\\"& {Write-Output 'restarted.'}\\\"\" } \nrestart_timeout (string) - The timeout to wait for the restart. By default this is 5 minutes. Example value: 5m. If you are installing updates or have a lot of startup services, you will probably need to increase this duration."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.10.x/provisioners/custom",
  "text": "Custom - Provisioners | Packer\nPacker is extensible, allowing you to write new provisioners without having to modify the core source code of Packer itself. Documentation for creating new provisioners is covered in the custom provisioners page of the Packer plugin section."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.10.x/post-processors/artifice",
  "text": "Artifice - Post-Processors | Packer\nType: artifice Artifact BuilderId: packer.post-processor.artifice\nThe artifice post-processor overrides the artifact list from an upstream builder or post-processor. All downstream post-processors will see the new artifacts you specify.\nAfter overriding the artifact with artifice, you can use it with other post-processors, including most of the core post-processors and third-party post-processors.\nA major benefit of this is that you can modify builder artifacts using shell-local and pass those modified artifacts into post-processors that may not have worked with the original builder. For example, maybe you want to export a Docker container from an amazon-ebs builder and then use Docker-push to put that Docker container into your Docker Hub account.\nArtifice allows you to use the familiar packer workflow to create a fresh, stateless build environment for each build on the infrastructure of your choosing. You can use this to build just about anything: buildpacks, containers, jars, binaries, tarballs, msi installers, and more.\nPlease note that the artifice post-processor will not delete your old artifact files, even if it removes them from the artifact. If you want to delete the old artifact files, you can use the shell-local post-processor to do so.\nArtifice helps you tie together a few other packer features:\nA builder, which spins up a VM (or container) to build your artifact\nA provisioner, which performs the steps to create your artifact\nA file provisioner, which downloads the artifact from the VM\nThe artifice post-processor, which identifies which files have been downloaded from the VM\nAdditional post-processors, which push the artifact to Docker hub, etc.\nYou will want to perform as much work as possible inside the VM. Ideally the only other post-processor you need after artifice is one that uploads your artifact to the appropriate repository.\nThe configuration allows you to specify which files comprise your artifact.\nfiles (array of strings) - A list of files that comprise your artifact. These files must exist on your local disk after the provisioning phase of packer is complete. These will replace any of the builder's original artifacts (such as a VM snapshot).\nkeep_input_artifact (boolean) - if true, do not delete the original artifact files after creating your new artifact. Defaults to true.\nExample Configuration\nThis minimal example:\nSpins up a cloned VMware virtual machine\nInstalls a consul release\nDownloads the consul binary\nPackages it into a .tar.gz file\nUploads it to S3.\nVMX is a fast way to build and test locally, but you can easily substitute another builder.\n{ \"builders\": [ { \"type\": \"vmware-vmx\", \"source_path\": \"/opt/ubuntu-1404-vmware.vmx\", \"ssh_username\": \"vagrant\", \"ssh_password\": \"vagrant\", \"shutdown_command\": \"sudo shutdown -h now\", \"headless\": \"true\", \"skip_compaction\": \"true\" } ], \"provisioners\": [ { \"type\": \"shell\", \"inline\": [ \"sudo apt-get install -y python-pip\", \"sudo pip install ifs\", \"sudo ifs install consul --version=0.5.2\" ] }, { \"type\": \"file\", \"source\": \"/usr/local/bin/consul\", \"destination\": \"consul\", \"direction\": \"download\" } ], \"post-processors\": [ [ { \"type\": \"artifice\", \"files\": [\"consul\"] }, { \"type\": \"compress\", \"output\": \"consul-0.5.2.tar.gz\" }, { \"type\": \"shell-local\", \"inline\": [ \"/usr/local/bin/aws s3 cp consul-0.5.2.tar.gz s3://<s3 path>\" ] } ] ] } \nNotice that there are two sets of square brackets in the post-processor section. This creates a post-processor chain, where the output of the proceeding artifact is passed to subsequent post-processors. If you use only one set of square braces the post-processors will run individually against the build artifact (the vmx file in this case) and it will not have the desired result.\n{ \"post-processors\": [ [ // <--- Start post-processor chain { \"type\": \"artifice\", \"files\": [\"consul\"] }, { \"type\": \"compress\", ... } ], // <--- End post-processor chain { \"type\":\"compress\" // <-- Standalone post-processor } ] } \nYou can create multiple post-processor chains to handle multiple builders (for example, building Linux and Windows binaries during the same build)."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.10.x/provisioners/community-supported",
  "text": "Community - Provisioners | Packer\nThe following provisioners are developed and maintained by various members of the Packer community, not by HashiCorp. For more information on how to use community provisioners, see our docs on extending Packer.\nCommunity Provisioners\nComment Provisioner - Example provisioner that allows you to annotate your build with bubble-text comments.\nWindows Update provisioner - A provisioner for gracefully handling Windows updates and the reboots they cause."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.10.x/provisioners/powershell",
  "text": "PowerShell - Provisioners | Packer\nType: powershell\nThe PowerShell Packer provisioner runs PowerShell scripts on Windows machines. It assumes that the communicator in use is WinRM. However, the provisioner can work equally well (with a few caveats) when combined with the SSH communicator. See the section below for details.\nNote: If possible, try to always use a forward slash / as the path separator, especially when dealing with relative paths. A backward slash \\ will work on Windows and is the official Windows path separator, but when building from any system that is not Windows, Packer will only treat slashes / as path separators, and treat backslashes as plain text. Which could lead to pathing errors.\nThe example below is fully functional.\nprovisioner \"powershell\" { inline = [\"dir c:/\"] } \nThe reference of available configuration options is listed below. The only required element is either \"inline\" or \"script\". Every other option is optional.\nExactly one of the following is required:\ninline (array of strings) - This is an array of commands to execute. The commands are concatenated by newlines and turned into a single file, so they are all executed within the same context. This allows you to change directories in one command and use something in the directory in the next and so on. Inline scripts are the easiest way to pull off simple tasks within the machine.\nscript (string) - The path to a script to upload and execute in the machine. This path can be absolute or relative. If it is relative, it is relative to the working directory when Packer is executed.\nscripts (array of strings) - An array of scripts to execute. The scripts will be uploaded and executed in the order specified. Each script is executed in isolation, so state such as variables from one script won't carry on to the next.\nOptional parameters:\nbinary (boolean) - If true, specifies that the script(s) are binary files, and Packer should therefore not convert Windows line endings to Unix line endings (if there are any). By default this is false.\nvalid_exit_codes (list of ints) - Valid exit codes for the script. By default this is just 0.\ndebug_mode - If set, sets PowerShell's PSDebug mode in order to make script debugging easier. For instance, setting the value to 1 results in adding this to the execute command:\nelevated_execute_command (string) - The command to use to execute the elevated script. By default this is as follows:\npowershell -executionpolicy bypass \"& { if (Test-Path variable:global:ProgressPreference){$ProgressPreference='SilentlyContinue'};. {{.Vars}}; &'{{.Path}}'; exit $LastExitCode }\" \nThis is a template engine. Therefore, you may use user variables and template functions in this field. In addition, you may use two extra variables:\nPath: The path to the script to run\nVars: The location of a temp file containing the list of environment_vars, if configured.\nenv (map of strings) - A map of key/value pairs to inject prior to the execute_command. Packer injects some environmental variables by default into the environment, as well, which are covered in the section below. Duplicate env settings override environment_vars settings. This is not a JSON template engine enabled function. HCL interpolation works as usual.\nenvironment_vars (array of strings) - An array of key/value pairs to inject prior to the execute_command. The format should be key=value. Packer injects some environmental variables by default into the environment, as well, which are covered in the section below.\nuse_pwsh (boolean) - Run pwsh.exe instead of powershell.exe. Defaults to false.\nThis is a template engine. Therefore, you may use user variables and template functions in this field. If you are running on AWS, Azure, Google Compute, or OpenStack and would like to access the autogenerated password that Packer uses to connect to the instance via WinRM, you can use the build template engine to inject it using {{ build `Password` }}. In HCL templates, you can do the same thing by accessing the build variables For example:\nprovisioner \"powershell\" { environment_vars = [\"WINRMPASS=${build.Password}\"] inline = [\"Write-Host \\\"Automatically generated aws password is: $Env:WINRMPASS\\\"\"] } \nexecute_command (string) - The command to use to execute the script. By default this is as follows:\npowershell -executionpolicy bypass \"& { if (Test-Path variable:global:ProgressPreference){$ProgressPreference='SilentlyContinue'};. {{.Vars}}; &'{{.Path}}'; exit $LastExitCode }\" \nThis is a template engine. Therefore, you may use user variables and template functions in this field. In addition, you may use two extra variables:\nPath: The path to the script to run\nVars: The location of a temp file containing the list of environment_vars, if configured. The value of both Path and Vars can be manually configured by setting the values for remote_path and remote_env_var_path respectively.\nIf you use the SSH communicator and have changed your default shell, you may need to modify your execute_command to make sure that the command is valid and properly escaped; the default assumes that you have not changed the default shell away from cmd.\nelevated_user and elevated_password (string) - If specified, the PowerShell script will be run with elevated privileges using the given Windows user.\nThis is a template engine. Therefore, you may use user variables and template functions in this field. If you are running on AWS, Azure, Google Compute, or OpenStack and would like to access the autogenerated password that Packer uses to connect to the instance via WinRM, you can use the build template engine to inject it using {{ build `Password` }}. In HCL templates, you can do the same thing by accessing the build variables For example:\nprovisioner \"powershell\" { elevated_user = \"Administrator\" elevated_password = build.Password } \nIf you specify an empty elevated_password value then the PowerShell script is run as a service account. For example:\nprovisioner \"powershell\" { elevated_user = \"SYSTEM\" elevated_password = \"\" } \nexecution_policy - To run ps scripts on Windows, Packer defaults this to \"bypass\" and wraps the command to run. Setting this to \"none\" will prevent wrapping, allowing to see exit codes on Docker for Windows. Possible values are bypass, allsigned, default, remotesigned, restricted, undefined, unrestricted, and none.\nremote_path (string) - The path where the PowerShell script will be uploaded to within the target build machine. This defaults to C:/Windows/Temp/script-UUID.ps1 where UUID is replaced with a dynamically generated string that uniquely identifies the script.\nThis setting allows users to override the default upload location. The value must be a writable location and any parent directories must already exist.\nremote_env_var_path (string) - Environment variables required within the remote environment are uploaded within a PowerShell script and then enabled by 'dot sourcing' the script immediately prior to execution of the main command or script.\nThe path the environment variables script will be uploaded to defaults to C:/Windows/Temp/packer-ps-env-vars-UUID.ps1 where UUID is replaced with a dynamically generated string that uniquely identifies the script.\nThis setting allows users to override the location the environment variable script is uploaded to. The value must be a writable location and any parent directories must already exist.\nskip_clean (bool) - Whether to clean scripts up after executing the provisioner. Defaults to false. When true any script created by a non-elevated Powershell provisioner will be removed from the remote machine. Elevated scripts, along with the scheduled tasks, will always be removed regardless of the value set for skip_clean.\nstart_retry_timeout (string) - The amount of time to attempt to start the remote process. By default this is 5m or 5 minutes. This setting exists in order to deal with times when SSH may restart, such as a system reboot. Set this to a higher value if reboots take a longer amount of time.\npause_after (string) - Wait the amount of time after provisioning a PowerShell script, this pause be taken if all previous steps were successful.\nIn addition to being able to specify custom environmental variables using the environment_vars configuration, the provisioner automatically defines certain commonly useful environmental variables:\nPACKER_BUILD_NAME is set to the name of the build that Packer is running. This is most useful when Packer is making multiple builds and you want to distinguish them slightly from a common provisioning script.\nPACKER_BUILDER_TYPE is the type of the builder that was used to create the machine that the script is running on. This is useful if you want to run only certain parts of the script on systems built with certain builders.\nPACKER_HTTP_ADDR If using a builder that provides an HTTP server for file transfer (such as hyperv, parallels, qemu, virtualbox, and vmware), this will be set to the address. You can use this address in your provisioner to download large files over HTTP. This may be useful if you're experiencing slower speeds using the default file provisioner. A file provisioner using the winrm communicator may experience these types of difficulties.\nThe good news first. If you are using the Microsoft port of OpenSSH then the provisioner should just work as expected - no extra configuration effort is required.\nNow the caveats. If you are using an alternative configuration, and your SSH connection lands you in a *nix shell on the remote host, then you will most likely need to manually set the execute_command; The default execute_command used by Packer will not work for you. When configuring the command you will need to ensure that any dollar signs or other characters that may be incorrectly interpreted by the remote shell are escaped accordingly.\nThe following example shows how the standard execute_command can be reconfigured to work on a remote system with Cygwin/OpenSSH installed. The execute_command has each dollar sign backslash escaped so that it is not interpreted by the remote Bash shell - Bash being the default shell for Cygwin environments.\nprovisioner \"powershell\" { execute_command = \"powershell -executionpolicy bypass \\\"& { if (Test-Path variable:global:ProgressPreference){\\\\$ProgressPreference='SilentlyContinue'};. {{.Vars}}; &'{{.Path}}'; exit \\\\$LastExitCode }\\\"\" inline = [ \"Write-Host \\\"Hello from PowerShell\\\"\"] } \nThe escape character in PowerShell is the backtick, also sometimes referred to as the grave accent. When, and when not, to escape characters special to PowerShell is probably best demonstrated with a series of examples.\nWhen To Escape...\nUsers need to deal with escaping characters special to PowerShell when they appear directly in commands used in the inline PowerShell provisioner and when they appear directly in the users own scripts. Note that where double quotes appear within double quotes, the addition of a backslash escape is required for the JSON template to be parsed correctly.\nprovisioner \"powershell\" { inline = [ \"Write-Host \\\"A literal dollar `$ must be escaped\\\"\", \"Write-Host \\\"A literal backtick `` must be escaped\\\"\", \"Write-Host \\\"Here `\\\"double quotes`\\\" must be escaped\\\"\", \"Write-Host \\\"Here `'single quotes`' don`'t really need to be\\\"\", \"Write-Host \\\"escaped... but it doesn`'t hurt to do so.\\\"\", ] } \nThe above snippet should result in the following output on the Packer console:\n==> amazon-ebs: Provisioning with Powershell... ==> amazon-ebs: Provisioning with PowerShell script: /var/folders/15/d0f7gdg13rnd1cxp7tgmr55c0000gn/T/packer-powershell-provisioner508190439 amazon-ebs: A literal dollar $ must be escaped amazon-ebs: A literal backtick ` must be escaped amazon-ebs: Here \"double quotes\" must be escaped amazon-ebs: Here 'single quotes' don't really need to be amazon-ebs: escaped... but it doesn't hurt to do so. \nWhen Not To Escape...\nSpecial characters appearing in user environment variable values and in the elevated_user and elevated_password fields will be automatically dealt with for the user. There is no need to use escapes in these instances.\nvariable \"psvar\" { type = string default = \"My$tring\" } build { sources = [\"source.amazon-ebs.example\"] provisioner \"powershell\" { elevated_user = \"Administrator\" elevated_password = \"Super$3cr3t!\" inline = [\"Write-Output \\\"The dollar in the elevated_password is interpreted correctly\\\"\"] } provisioner \"powershell\" { environment_vars = [ \"VAR1=A$Dollar\", \"VAR2=A`Backtick\", \"VAR3=A'SingleQuote\", \"VAR4=A\\\"DoubleQuote\", \"VAR5=${var.psvar}\", ] inline = [ \"Write-Output \\\"In the following examples the special character is interpreted correctly:\\\"\", \"Write-Output \\\"The dollar in VAR1: $Env:VAR1\\\"\", \"Write-Output \\\"The backtick in VAR2: $Env:VAR2\\\"\", \"Write-Output \\\"The single quote in VAR3: $Env:VAR3\\\"\", \"Write-Output \\\"The double quote in VAR4: $Env:VAR4\\\"\", \"Write-Output \\\"The dollar in VAR5 (expanded from a user var): $Env:VAR5\\\"\", ] } } \nThe above snippet should result in the following output on the Packer console:\n==> amazon-ebs: Provisioning with Powershell... ==> amazon-ebs: Provisioning with PowerShell script: /var/folders/15/d0f7gdg13rnd1cxp7tgmr55c0000gn/T/packer-powershell-provisioner961728919 amazon-ebs: The dollar in the elevated_password is interpreted correctly ==> amazon-ebs: Provisioning with Powershell... ==> amazon-ebs: Provisioning with PowerShell script: /var/folders/15/d0f7gdg13rnd1cxp7tgmr55c0000gn/T/packer-powershell-provisioner142826554 amazon-ebs: In the following examples the special character is interpreted correctly: amazon-ebs: The dollar in VAR1: A$Dollar amazon-ebs: The backtick in VAR2: A`Backtick amazon-ebs: The single quote in VAR3: A'SingleQuote amazon-ebs: The double quote in VAR4: A\"DoubleQuote amazon-ebs: The dollar in VAR5 (expanded from a user var): My$tring"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.10.x/post-processors/checksum",
  "text": "Checksum - Post-Processors | Packer\nType: checksum Artifact BuilderId: packer.post-processor.checksum\nThe checksum post-processor computes specified checksum for the artifact list from an upstream builder or post-processor. All downstream post-processors will see the new artifacts. The primary use-case is compute checksum for artifact to verify it later.\nAfter computes checksum for artifacts, you can use new artifacts with other post-processors like artifice, compress, docker-push, or a third-party post-processor.\n{ \"type\": \"checksum\", \"checksum_types\": [\"sha1\", \"sha256\"], \"output\": \"packer_{{.BuildName}}_{{.ChecksumType}}.checksum\" } \nchecksum_types (array of strings) - An array of strings of checksum types to compute. If empty, Defaults to md5. Allowed values are:\nmd5\nsha1\nsha224\nsha256\nsha384\nsha512\noutput (string) - Specify filename to store checksums. This defaults to packer_{{.BuildName}}_{{.BuilderType}}_{{.ChecksumType}}.checksum. For example, if you had a builder named database, you might see the file written as packer_database_docker_md5.checksum. This is treated as a template engine. Therefore, you may use user variables and template functions in this field. The following special variables are also available to use in the output template:\nBuildName: The name of the builder that produced the artifact.\nBuilderType: The type of builder used to produce the artifact.\nChecksumType: The type of checksums the file contains. This should be used if you have more than one value in checksum_types."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.10.x/post-processors/community-supported",
  "text": "Community - Post-Processors | Packer\nThe following post-processors are developed and maintained by various members of the Packer community, not by HashiCorp. For more information on how to use community post-processors, see our docs on extending Packer.\nCommunity Post-Processors\nExoscale Import post-processor - A post-processor to import Exoscale custom templates from disk image files."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.10.x/post-processors/shell-local",
  "text": "Local Shell - Post-Processors | Packer\nLocal Shell Post Processor\nType: shell-local\nThe local shell post processor executes scripts locally during the post processing stage. Shell local provides a convenient way to automate executing some task with packer outputs and variables.\nThe example below is a fully functional self-contained build.\nsource \"file\" \"example\" { content = \"example content\" } build { source \"source.file.example\" { target = \"./test_artifact.txt\" } post-processor \"shell-local\" { inline = [\"echo foo\"] } } \nThe reference of available configuration options is listed below. The only required element is either \"inline\" or \"script\". Every other option is optional.\nExactly one of the following is required:\ncommand (string) - This is a single command to execute. It will be written to a temporary file and run using the execute_command call below.\ninline (array of strings) - This is an array of commands to execute. The commands are concatenated by newlines and turned into a single file, so they are all executed within the same context. This allows you to change directories in one command and use something in the directory in the next and so on. Inline scripts are the easiest way to pull off simple tasks within the machine.\nscript (string) - The path to a script to execute. This path can be absolute or relative. If it is relative, it is relative to the working directory when Packer is executed.\nscripts (array of strings) - An array of scripts to execute. The scripts will be executed in the order specified. Each script is executed in isolation, so state such as variables from one script won't carry on to the next.\nenv (map of strings) - A map of key/value pairs to inject prior to the execute_command. Packer injects some environmental variables by default into the environment, as well, which are covered in the section below. Duplicate env settings override environment_vars settings.\nenvironment_vars (array of strings) - An array of key/value pairs to inject prior to the execute_command. The format should be key=value. Packer injects some environmental variables by default into the environment, as well, which are covered in the section below.\nenv_var_format (string) - When we parse the environment_vars that you provide, this gives us a string template to use in order to make sure that we are setting the environment vars correctly. By default on Windows hosts this format is set %s=%s && and on Unix, it is %s='%s'. You probably won't need to change this format, but you can see usage examples for where it is necessary below.\nexecute_command (array of strings) - The command used to execute the script. By default, on *nix systems this is:\n[\"/bin/sh\", \"-c\", \"{{.Vars}} {{.Script}}\"] \nWhile on Windows, execute_command defaults to:\n[\"cmd\", \"/V\", \"/C\", \"{{.Vars}}\", \"call\", \"{{.Script}}\"] \nThis is treated as a template engine. There are several available variables: Script, which is the path to the script to run, and Vars, which is the list of environment_vars, if configured. In addition, you may access any of the variables stored in the generated data using the build template function. If you choose to set this option, make sure that the first element in the array is the shell program you want to use (for example, \"sh\" or \"/usr/local/bin/zsh\" or even \"powershell.exe\" although anything other than a flavor of the shell command language is not explicitly supported and may be broken by assumptions made within Packer). It's worth noting that if you choose to try to use shell-local for Powershell or other Windows commands, the environment variables will not be set properly for your environment.\nFor backwards compatibility, execute_command will accept a string instead of an array of strings. If a single string or an array of strings with only one element is provided, Packer will replicate past behavior by appending your execute_command to the array of strings [\"sh\", \"-c\"]. For example, if you set \"execute_command\": \"foo bar\", the final execute_command that Packer runs will be [\"sh\", \"-c\", \"foo bar\"]. If you set \"execute_command\": [\"foo\", \"bar\"], the final execute_command will remain [\"foo\", \"bar\"].\nAgain, the above is only provided as a backwards compatibility fix; we strongly recommend that you set execute_command as an array of strings.\ninline_shebang (string) - The shebang value to use when running commands specified by inline. By default, this is /bin/sh -e. If you're not using inline, then this configuration has no effect. Important: If you customize this, be sure to include something like the -e flag, otherwise individual steps failing won't fail the provisioner.\nkeep_input_artifact (boolean) - Unlike most other post-processors, the keep_input_artifact option will have no effect for the shell-local post-processor. Packer will always retain the input artifact for shell-local, since the shell-local post-processor merely passes forward the artifact it receives. If your shell-local post-processor produces a file or files which you would like to have replace the input artifact, you may overwrite the input artifact using the artifice post-processor after your shell-local processor has run.\nonly_on (array of strings) - This is an array of runtime operating systems where shell-local will execute. This allows you to execute shell-local only on specific operating systems. By default, shell-local will always run if only_on is not set.\"\nuse_linux_pathing (bool) - This is only relevant to Windows hosts. If you are running Packer in a Windows environment with the Windows Subsystem for Linux feature enabled, and would like to invoke a bash script rather than invoking a Cmd script, you'll need to set this flag to true; it tells Packer to use the Linux subsystem path for your script rather than the Windows path. (e.g. /mnt/c/path/to/your/file instead of C:/path/to/your/file). Please see the example below for more guidance on how to use this feature. If you are not on a Windows host, or you do not intend to use the shell-local post-processor to run a bash script, please ignore this option. If you set this flag to true, you still need to provide the standard Windows path to the script when providing a script. This is a beta feature.\nvalid_exit_codes (list of ints) - Valid exit codes for the script. By default this is 0.\nTo many new users, the execute_command is puzzling. However, it provides an important function: customization of how the command is executed. The most common use case for this is dealing with sudo password prompts. You may also need to customize this if you use a non-POSIX shell, such as tcsh on FreeBSD.\nThe Windows Linux Subsystem\nThe shell-local post-processor was designed with the idea of allowing you to run commands in your local operating system's native shell. For Windows, we've assumed in our defaults that this is Cmd. However, it is possible to run a bash script as part of the Windows Linux Subsystem from the shell-local post-processor, by modifying the execute_command and the use_linux_pathing options in the post-processor config.\nThe example below is a fully functional test config.\nOne limitation of this offering is that \"inline\" and \"command\" options are not available to you; please limit yourself to using the \"script\" or \"scripts\" options instead.\nPlease note that this feature is still in beta, as the underlying WSL is also still in beta. There will be some limitations as a result. For example, it will likely not work unless both Packer and the scripts you want to run are both on the C drive.\nsource \"null\" \"example\" { communicator = \"none\" } build { sources = [ \"source.null.example\" ] post-processor \"shell-local\"{ environment_vars = [\"PROVISIONERTEST=ProvisionerTest1\"] execute_command = [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"] use_linux_pathing = true scripts = [\"C:/Users/me/scripts/example_bash.sh\"] } post-processor \"shell-local\"{ environment_vars = [\"PROVISIONERTEST=ProvisionerTest2\"] execute_command = [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"] use_linux_pathing = true script = \"C:/Users/me/scripts/example_bash.sh\" } } \nIn addition to being able to specify custom environmental variables using the environment_vars configuration, the provisioner automatically defines certain commonly useful environmental variables:\nPACKER_BUILD_NAME is set to the name of the build that Packer is running. This is most useful when Packer is making multiple builds and you want to distinguish them slightly from a common provisioning script.\nPACKER_BUILDER_TYPE is the type of the builder that was used to create the machine that the script is running on. This is useful if you want to run only certain parts of the script on systems built with certain builders.\nWhether you use the inline option, or pass it a direct script or scripts, it is important to understand a few things about how the shell-local post-processor works to run it safely and easily. This understanding will save you much time in the process.\nOnce Per Builder\nThe shell-local script(s) you pass are run once per builder. This means that if you have an amazon-ebs builder and a docker builder, your script will be run twice. If you have 3 builders, it will run 3 times, once for each builder.\nInteracting with Build Artifacts\nIn order to interact with build artifacts, you may want to use the manifest post-processor. This will write the list of files produced by a builder to a json file after each builder is run.\nFor example, if you wanted to package a file from the file builder into a tarball, you might write this:\n{ \"builders\": [ { \"content\": \"Lorem ipsum dolor sit amet\", \"target\": \"dummy_artifact\", \"type\": \"file\" } ], \"post-processors\": [ [ { \"output\": \"manifest.json\", \"strip_path\": true, \"type\": \"manifest\" }, { \"inline\": [ \"jq \\\".builds[].files[].name\\\" manifest.json | xargs tar cfz artifacts.tgz\" ], \"type\": \"shell-local\" } ] ] } \nThis uses the jq tool to extract all of the file names from the manifest file and passes them to tar.\nAlways Exit Intentionally\nIf any post-processor fails, the packer build stops and all interim artifacts are cleaned up.\nFor a shell script, that means the script must exit with a zero code. You must be extra careful to exit 0 when necessary.\nWindows Host\nExample of running a .cmd file on Windows:\npost-processor \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest1\"] scripts = [\"./scripts/test_cmd.cmd\"] } \nContents of test_cmd.cmd:\nExample of running an inline command on Windows: Required customization: tempfile_extension\npost-processor \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest2\"], tempfile_extension = \".cmd\", inline = [\"echo %SHELLLOCALTEST%\"] } \nExample of running a bash command on Windows using WSL: Required customizations: use_linux_pathing and execute_command:\npost-processor \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest3\"], execute_command = [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"] use_linux_pathing = true script = \"./scripts/example_bash.sh\" } \nContents of example_bash.sh:\n#!/bin/bash echo $SHELLLOCALTEST \nExample of running a PowerShell script on Windows: Required customizations: env_var_format and execute_command.\npost-processor \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest4\"] execute_command = [\"powershell.exe\", \"{{.Vars}} {{.Script}}\"] env_var_format = \"$env:%s=\\\"%s\\\"; \" script = \"./scripts/example_ps.ps1\" } \nExample of running a PowerShell script on Windows as \"inline\": Required customizations: env_var_format, tempfile_extension, and execute_command\npost-processor \"shell-local\" { tempfile_extension = \".ps1\" environment_vars = [\"SHELLLOCALTEST=ShellTest5\"] execute_command = [\"powershell.exe\", \"{{.Vars}} {{.Script}}\"] env_var_format = \"$env:%s=\\\"%s\\\"; \" inline = [\"write-output $env:SHELLLOCALTEST\"] } \nUnix Host\nExample of running a Shell script on Unix:\npost-processor \"shell-local\" { environment_vars = [\"PROVISIONERTEST=ProvisionerTest1\"] scripts = [\"./scripts/example_bash.sh\"] } \nExample of running a bash \"inline\" on Unix:\npost-processor \"shell-local\" { environment_vars = [\"PROVISIONERTEST=ProvisionerTest2\"] inline = [\"echo hello\", \"echo $PROVISIONERTEST\"] } \nExample of running a Python script on Unix:\npost-processor \"shell-local\" { script = \"hello.py\" environment_vars = [\"HELLO_USER=packeruser\"] execute_command = [ \"/bin/sh\", \"-c\", \"{{.Vars}} /usr/local/bin/python {{.Script}}\" ] } \nWhere \"hello.py\" contains: import os print('Hello, %s!' % os.getenv(\"HELLO_USER\"))"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.9.x/commands/plugins",
  "text": "plugins Command | Packer | HashiCorp Developer\nThe plugins command groups subcommands for interacting with Packers' plugins.\n$ packer plugins -h Usage: packer plugins <subcommand> [options] [args] This command groups subcommands for interacting with Packer plugins. Related but not under the \"plugins\" command : - \"packer init <path>\" will install all plugins required by a config. Subcommands: install Install latest Packer plugin [matching version constraint] installed List all installed Packer plugin binaries remove Remove Packer plugins [matching a version] required List plugins required by a config \nRelated\npacker init will install all required plugins."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.9.x/builders/file",
  "text": "File - Builders | Packer\nType: file Artifact BuilderId: packer.file\nThe file Packer builder is not really a builder, it just creates an artifact from a file. It can be used to debug post-processors without incurring high wait times.\nBelow is a fully functioning example. It create a file at target with the specified content.\n{ \"type\": \"file\", \"content\": \"Lorem ipsum dolor sit amet\", \"target\": \"dummy_artifact\" } \nConfiguration options are organized below into two categories: required and optional. Within each category, the available options are alphabetized and described.\nAny communicator defined is ignored.\ntarget (string) - The path for the artifact file that will be created. If the path contains directories that don't exist, Packer will create them, too.\nYou can only define one of source or content. If none of them is defined the artifact will be empty.\nsource (string) - The path for a file which will be copied as the artifact.\ncontent (string) - The content that will be put into the artifact."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.9.x/builders/custom",
  "text": "Custom - Builders | Packer\nPacker is extensible, allowing you to write new builders without having to modify the core source code of Packer itself. Documentation for creating new builders is covered in the custom builders page of the Packer plugin section."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.9.x/builders/null",
  "text": "Null - Builders | Packer\nType: null\nThe null Packer builder is not really a builder, it just sets up an SSH connection and runs the provisioners. It can be used to debug provisioners without incurring high wait times. It does not create any kind of image or artifact.\nBelow is a fully functioning example. It doesn't do anything useful, since no provisioners are defined, but it will connect to the specified host via ssh.\n{ \"type\": \"null\", \"ssh_host\": \"127.0.0.1\", \"ssh_username\": \"foo\", \"ssh_password\": \"bar\" } \nThe null builder has no configuration parameters other than the communicator settings."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.9.x/datasources/http",
  "text": "HTTP - Data Sources | Packer\nType: http\nThe http data source makes an HTTP GET request to the given URL and exports information about the response.\ndata \"http\" \"example\" { url = \"https://checkpoint-api.hashicorp.com/v1/check/terraform\" # Optional request headers request_headers = { Accept = \"application/json\" } } \nConfiguration options are organized below into two categories: required and optional. Within each category, the available options are alphabetized and described.\nurl (string) - The URL to request data from. This URL must respond with a 200 OK response and a text/* or application/json Content-Type.\nNot Required:\nrequest_headers (map[string]string) - A map of strings representing additional HTTP headers to include in the request.\nThe outputs for this datasource are as follows:\nurl (string) - The URL the data was requested from.\nbody (string) - The raw body of the HTTP response.\nrequest_headers (map[string]string) - A map of strings representing the response HTTP headers. Duplicate headers are concatenated with, according to RFC2616."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.9.x/builders/community-supported",
  "text": "Community - Builders | Packer\nThe following builders are developed and maintained by various members of the Packer community, not by HashiCorp. For more information on how to use community builders, see our docs on extending Packer.\nCommunity Builders\nARM builders\npacker-plugin-arm-image - simple builder lets you extend on existing system images.\npacker-builder-arm - flexible builder lets you extend or build images from scratch with variety of options (ie. custom partition table).\nExoscale builder - A builder to create Exoscale custom templates based on a Compute instance snapshot.\nCitrix XenServer/Citrix Hypervisor - Plugin for creating Citrix XenServer/Citrix Hypervisor images from an iso image or from an existing template.\nXCP-NG/Citrix XenServer/Citrix Hypervisor/Updated Fork - Plugin for creating XCP-NG/Citrix XenServer/Citrix Hypervisor images from an iso image or from an existing template. This is a fork of the orginal, and reccomended by the developers of XCP-NG."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.9.x/provisioners/breakpoint",
  "text": "breakpoint - Provisioners | Packer\nBreakpoint Provisioner\nType: breakpoint\nThe breakpoint provisioner will pause until the user presses \"enter\" to resume the build. This is intended for debugging purposes, and allows you to halt at a particular part of the provisioning process.\nThis is independent of the -debug flag, which will instead halt at every step and between every provisioner.\n{ \"builders\": [ { \"type\": \"null\", \"communicator\": \"none\" } ], \"provisioners\": [ { \"type\": \"shell-local\", \"inline\": \"echo hi\" }, { \"type\": \"breakpoint\", \"disable\": false, \"note\": \"this is a breakpoint\" }, { \"type\": \"shell-local\", \"inline\": \"echo hi 2\" } ] } \ndisable (boolean) - If true, skip the breakpoint. Useful for when you have set multiple breakpoints and want to toggle them off or on. Default: false\nnote (string) - a string to include explaining the purpose or location of the breakpoint. For example, you may find it useful to number your breakpoints or label them with information about where in the build they occur\nInsert this provisioner wherever you want the build to pause. You'll see ui output prompting you to press \"enter\" to continue the build when you are ready.\n==> docker: Pausing at breakpoint provisioner with note \"foo bar baz\". ==> docker: Press enter to continue. \nOnce you press enter, the build will resume and run normally until it either completes or errors."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.9.x/provisioners/file",
  "text": "File - Provisioners | Packer\nType: file\nThe file Packer provisioner uploads files to machines built by Packer. The recommended usage of the file provisioner is to use it to upload files, and then use shell provisioner to move them to the proper place, set permissions, etc.\nWarning: You can only upload files to locations that the provisioning user (generally not root) has permission to access. Creating files in /tmp and using a shell provisioner to move them into the final location is the only way to upload files to root owned locations.\nThe file provisioner can upload both single files and complete directories.\n{ \"type\": \"file\", \"source\": \"app.tar.gz\", \"destination\": \"/tmp/app.tar.gz\" } \nThe available configuration options are listed below.\nRequired Parameters:\ncontent (string) - This is the content to copy to destination. If destination is a file, content will be written to that file, in case of a directory a file named pkr-file-content is created. It's recommended to use a file as the destination. The templatefile function might be used here, or any interpolation syntax. This attribute cannot be specified with source or sources.\nsource (string) - The path to a local file or directory to upload to the machine. The path can be absolute or relative. If it is relative, it is relative to the working directory when Packer is executed. If this is a directory, the existence of a trailing slash is important. Read below on uploading directories. Mandatory unless sources is set.\ndestination (string) - The path where the file will be uploaded to in the machine. This value must be a writable location and any parent directories must already exist. If the provisioning user (generally not root) cannot write to this directory, you will receive a \"Permission Denied\" error. If the source is a file, it's a good idea to make the destination a file as well, but if you set your destination as a directory, at least make sure that the destination ends in a trailing slash so that Packer knows to use the source's basename in the final upload path. Failure to do so may cause Packer to fail on file uploads. If the destination file already exists, it will be overwritten.\nOptional Parameters:\nsources ([]string) - A list of sources to upload. This can be used in place of the source option if you have several files that you want to upload to the same place. Note that the destination must be a directory with a trailing slash, and that all files listed in sources will be uploaded to the same directory with their file names preserved.\ndirection (string) - The direction of the file transfer. This defaults to \"upload\". If it is set to \"download\" then the file \"source\" in the machine will be downloaded locally to \"destination\"\ngenerated (bool) - For advanced users only. If true, check the file existence only before uploading, rather than upon pre-build validation. This allows users to upload files created on-the-fly. This defaults to false. We don't recommend using this feature, since it can cause Packer to become dependent on system state. We would prefer you generate your files before the Packer run, but realize that there are situations where this may be unavoidable.\nThe file provisioner is also able to upload a complete directory to the remote machine. When uploading a directory, there are a few important things you should know.\nFirst, the destination directory must already exist. If you need to create it, use a shell provisioner just prior to the file provisioner in order to create the directory. If the destination directory does not exist, the file provisioner may succeed, but it will have undefined results.\nNext, the existence of a trailing slash on the source path will determine whether the directory name will be embedded within the destination, or whether the destination will be created. An example explains this best:\nIf the source is /foo (no trailing slash), and the destination is /tmp, then the contents of /foo on the local machine will be uploaded to /tmp/foo on the remote machine. The foo directory on the remote machine will be created by Packer.\nIf the source, however, is /foo/ (a trailing slash is present), and the destination is /tmp, then the contents of /foo will be uploaded into /tmp directly.\nThis behavior was adopted from the standard behavior of rsync. Note that under the covers, rsync may or may not be used.\nIn general, local files used as the source must exist before Packer is run. This is great for catching typos and ensuring that once a build is started, that it will succeed. However, this also means that you can't generate a file during your build and then upload it using the file provisioner later. A convenient workaround is to upload a directory instead of a file. The directory still must exist, but its contents don't. You can write your generated file to the directory during the Packer run, and have it be uploaded later.\nThe behavior when uploading symbolic links depends on the communicator. The Docker communicator will preserve symlinks, but all other communicators will treat local symlinks as regular files. If you wish to preserve symlinks when uploading, it's recommended that you use tar. Below is an example of what that might look like:\n$ ls -l files total 16 drwxr-xr-x 3 mwhooker staff 102 Jan 27 17:10 a lrwxr-xr-x 1 mwhooker staff 1 Jan 27 17:10 b -> a -rw-r--r-- 1 mwhooker staff 0 Jan 27 17:10 file1 lrwxr-xr-x 1 mwhooker staff 5 Jan 27 17:10 file1link -> file1 $ ls -l toupload total 0 -rw-r--r-- 1 mwhooker staff 0 Jan 27 17:10 files.tar \n{ \"provisioners\": [ { \"type\": \"shell-local\", \"command\": \"tar cf toupload/files.tar files\" }, { \"destination\": \"/tmp/\", \"source\": \"./toupload\", \"type\": \"file\" }, { \"inline\": [ \"cd /tmp && tar xf toupload/files.tar\", \"rm toupload/files.tar\" ], \"type\": \"shell\" } ] } \nBecause of the way our WinRM transfers works, it can take a very long time to upload and download even moderately sized files. If you're experiencing slowness using the file provisioner on Windows, it's suggested that you set up an SSH server and use the ssh communicator. If you only want to transfer files to your guest, and if your builder supports it, you may also use the http_directory or http_content directives. This will cause that directory to be available to the guest over HTTP, and set the environment variable PACKER_HTTP_ADDR to the address."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.9.x/provisioners/powershell",
  "text": "PowerShell - Provisioners | Packer\nType: powershell\nThe PowerShell Packer provisioner runs PowerShell scripts on Windows machines. It assumes that the communicator in use is WinRM. However, the provisioner can work equally well (with a few caveats) when combined with the SSH communicator. See the section below for details.\nNote: If possible, try to always use a forward slash / as the path separator, especially when dealing with relative paths. A backward slash \\ will work on Windows and is the official Windows path separator, but when building from any system that is not Windows, Packer will only treat slashes / as path separators, and treat backslashes as plain text. Which could lead to pathing errors.\nprovisioner \"powershell\" { inline = [\"dir c:/\"] } \nThe reference of available configuration options is listed below. The only required element is either \"inline\" or \"script\". Every other option is optional.\ninline (array of strings) - This is an array of commands to execute. The commands are concatenated by newlines and turned into a single file, so they are all executed within the same context. This allows you to change directories in one command and use something in the directory in the next and so on. Inline scripts are the easiest way to pull off simple tasks within the machine.\nscript (string) - The path to a script to upload and execute in the machine. This path can be absolute or relative. If it is relative, it is relative to the working directory when Packer is executed.\nscripts (array of strings) - An array of scripts to execute. The scripts will be uploaded and executed in the order specified. Each script is executed in isolation, so state such as variables from one script won't carry on to the next.\nbinary (boolean) - If true, specifies that the script(s) are binary files, and Packer should therefore not convert Windows line endings to Unix line endings (if there are any). By default this is false.\nvalid_exit_codes (list of ints) - Valid exit codes for the script. By default this is just 0.\ndebug_mode - If set, sets PowerShell's PSDebug mode in order to make script debugging easier. For instance, setting the value to 1 results in adding this to the execute command:\nelevated_execute_command (string) - The command to use to execute the elevated script. By default this is as follows:\npowershell -executionpolicy bypass \"& { if (Test-Path variable:global:ProgressPreference){$ProgressPreference='SilentlyContinue'};. {{.Vars}}; &'{{.Path}}'; exit $LastExitCode }\" \nThis is a template engine. Therefore, you may use user variables and template functions in this field. In addition, you may use two extra variables:\nPath: The path to the script to run\nVars: The location of a temp file containing the list of environment_vars, if configured.\nenv (map of strings) - A map of key/value pairs to inject prior to the execute_command. Packer injects some environmental variables by default into the environment, as well, which are covered in the section below. Duplicate env settings override environment_vars settings. This is not a JSON template engine enabled function. HCL interpolation works as usual.\nenvironment_vars (array of strings) - An array of key/value pairs to inject prior to the execute_command. The format should be key=value. Packer injects some environmental variables by default into the environment, as well, which are covered in the section below.\nuse_pwsh (boolean) - Run pwsh.exe instead of powershell.exe. Defaults to false.\nThis is a template engine. Therefore, you may use user variables and template functions in this field. If you are running on AWS, Azure, Google Compute, or OpenStack and would like to access the autogenerated password that Packer uses to connect to the instance via WinRM, you can use the build template engine to inject it using {{ build `Password` }}. In HCL templates, you can do the same thing by accessing the build variables For example:\nprovisioner \"powershell\" { environment_vars = [\"WINRMPASS=${build.Password}\"] inline = [\"Write-Host \\\"Automatically generated aws password is: $Env:WINRMPASS\\\"\"] } \nexecute_command (string) - The command to use to execute the script. By default this is as follows:\npowershell -executionpolicy bypass \"& { if (Test-Path variable:global:ProgressPreference){$ProgressPreference='SilentlyContinue'};. {{.Vars}}; &'{{.Path}}'; exit $LastExitCode }\" \nThis is a template engine. Therefore, you may use user variables and template functions in this field. In addition, you may use two extra variables:\nPath: The path to the script to run\nVars: The location of a temp file containing the list of environment_vars, if configured. The value of both Path and Vars can be manually configured by setting the values for remote_path and remote_env_var_path respectively.\nIf you use the SSH communicator and have changed your default shell, you may need to modify your execute_command to make sure that the command is valid and properly escaped; the default assumes that you have not changed the default shell away from cmd.\nelevated_user and elevated_password (string) - If specified, the PowerShell script will be run with elevated privileges using the given Windows user.\nThis is a template engine. Therefore, you may use user variables and template functions in this field. If you are running on AWS, Azure, Google Compute, or OpenStack and would like to access the autogenerated password that Packer uses to connect to the instance via WinRM, you can use the build template engine to inject it using {{ build `Password` }}. In HCL templates, you can do the same thing by accessing the build variables For example:\nprovisioner \"powershell\" { elevated_user = \"Administrator\" elevated_password = build.Password } \nIf you specify an empty elevated_password value then the PowerShell script is run as a service account. For example:\nprovisioner \"powershell\" { elevated_user = \"SYSTEM\" elevated_password = \"\" } \nexecution_policy - To run ps scripts on Windows, Packer defaults this to \"bypass\" and wraps the command to run. Setting this to \"none\" will prevent wrapping, allowing to see exit codes on Docker for Windows. Possible values are bypass, allsigned, default, remotesigned, restricted, undefined, unrestricted, and none.\nremote_path (string) - The path where the PowerShell script will be uploaded to within the target build machine. This defaults to C:/Windows/Temp/script-UUID.ps1 where UUID is replaced with a dynamically generated string that uniquely identifies the script.\nThis setting allows users to override the default upload location. The value must be a writable location and any parent directories must already exist.\nremote_env_var_path (string) - Environment variables required within the remote environment are uploaded within a PowerShell script and then enabled by 'dot sourcing' the script immediately prior to execution of the main command or script.\nThe path the environment variables script will be uploaded to defaults to C:/Windows/Temp/packer-ps-env-vars-UUID.ps1 where UUID is replaced with a dynamically generated string that uniquely identifies the script.\nThis setting allows users to override the location the environment variable script is uploaded to. The value must be a writable location and any parent directories must already exist.\nskip_clean (bool) - Whether to clean scripts up after executing the provisioner. Defaults to false. When true any script created by a non-elevated Powershell provisioner will be removed from the remote machine. Elevated scripts, along with the scheduled tasks, will always be removed regardless of the value set for skip_clean.\nstart_retry_timeout (string) - The amount of time to attempt to start the remote process. By default this is 5m or 5 minutes. This setting exists in order to deal with times when SSH may restart, such as a system reboot. Set this to a higher value if reboots take a longer amount of time.\npause_after (string) - Wait the amount of time after provisioning a PowerShell script, this pause be taken if all previous steps were successful.\nPACKER_HTTP_ADDR If using a builder that provides an HTTP server for file transfer (such as hyperv, parallels, qemu, virtualbox, and vmware), this will be set to the address. You can use this address in your provisioner to download large files over HTTP. This may be useful if you're experiencing slower speeds using the default file provisioner. A file provisioner using the winrm communicator may experience these types of difficulties.\nThe good news first. If you are using the Microsoft port of OpenSSH then the provisioner should just work as expected - no extra configuration effort is required.\nNow the caveats. If you are using an alternative configuration, and your SSH connection lands you in a *nix shell on the remote host, then you will most likely need to manually set the execute_command; The default execute_command used by Packer will not work for you. When configuring the command you will need to ensure that any dollar signs or other characters that may be incorrectly interpreted by the remote shell are escaped accordingly.\nThe following example shows how the standard execute_command can be reconfigured to work on a remote system with Cygwin/OpenSSH installed. The execute_command has each dollar sign backslash escaped so that it is not interpreted by the remote Bash shell - Bash being the default shell for Cygwin environments.\nprovisioner \"powershell\" { execute_command = \"powershell -executionpolicy bypass \\\"& { if (Test-Path variable:global:ProgressPreference){\\\\$ProgressPreference='SilentlyContinue'};. {{.Vars}}; &'{{.Path}}'; exit \\\\$LastExitCode }\\\"\" inline = [ \"Write-Host \\\"Hello from PowerShell\\\"\"] } \nThe escape character in PowerShell is the backtick, also sometimes referred to as the grave accent. When, and when not, to escape characters special to PowerShell is probably best demonstrated with a series of examples.\nWhen To Escape...\nUsers need to deal with escaping characters special to PowerShell when they appear directly in commands used in the inline PowerShell provisioner and when they appear directly in the users own scripts. Note that where double quotes appear within double quotes, the addition of a backslash escape is required for the JSON template to be parsed correctly.\nprovisioner \"powershell\" { inline = [ \"Write-Host \\\"A literal dollar `$ must be escaped\\\"\", \"Write-Host \\\"A literal backtick `` must be escaped\\\"\", \"Write-Host \\\"Here `\\\"double quotes`\\\" must be escaped\\\"\", \"Write-Host \\\"Here `'single quotes`' don`'t really need to be\\\"\", \"Write-Host \\\"escaped... but it doesn`'t hurt to do so.\\\"\", ] } \nThe above snippet should result in the following output on the Packer console:\n==> amazon-ebs: Provisioning with Powershell... ==> amazon-ebs: Provisioning with PowerShell script: /var/folders/15/d0f7gdg13rnd1cxp7tgmr55c0000gn/T/packer-powershell-provisioner508190439 amazon-ebs: A literal dollar $ must be escaped amazon-ebs: A literal backtick ` must be escaped amazon-ebs: Here \"double quotes\" must be escaped amazon-ebs: Here 'single quotes' don't really need to be amazon-ebs: escaped... but it doesn't hurt to do so. \nWhen Not To Escape...\nSpecial characters appearing in user environment variable values and in the elevated_user and elevated_password fields will be automatically dealt with for the user. There is no need to use escapes in these instances.\nvariable \"psvar\" { type = string default = \"My$tring\" } build { sources = [\"source.amazon-ebs.example\"] provisioner \"powershell\" { elevated_user = \"Administrator\" elevated_password = \"Super$3cr3t!\" inline = [\"Write-Output \\\"The dollar in the elevated_password is interpreted correctly\\\"\"] } provisioner \"powershell\" { environment_vars = [ \"VAR1=A$Dollar\", \"VAR2=A`Backtick\", \"VAR3=A'SingleQuote\", \"VAR4=A\\\"DoubleQuote\", \"VAR5=${var.psvar}\", ] inline = [ \"Write-Output \\\"In the following examples the special character is interpreted correctly:\\\"\", \"Write-Output \\\"The dollar in VAR1: $Env:VAR1\\\"\", \"Write-Output \\\"The backtick in VAR2: $Env:VAR2\\\"\", \"Write-Output \\\"The single quote in VAR3: $Env:VAR3\\\"\", \"Write-Output \\\"The double quote in VAR4: $Env:VAR4\\\"\", \"Write-Output \\\"The dollar in VAR5 (expanded from a user var): $Env:VAR5\\\"\", ] } } \nThe above snippet should result in the following output on the Packer console:\n==> amazon-ebs: Provisioning with Powershell... ==> amazon-ebs: Provisioning with PowerShell script: /var/folders/15/d0f7gdg13rnd1cxp7tgmr55c0000gn/T/packer-powershell-provisioner961728919 amazon-ebs: The dollar in the elevated_password is interpreted correctly ==> amazon-ebs: Provisioning with Powershell... ==> amazon-ebs: Provisioning with PowerShell script: /var/folders/15/d0f7gdg13rnd1cxp7tgmr55c0000gn/T/packer-powershell-provisioner142826554 amazon-ebs: In the following examples the special character is interpreted correctly: amazon-ebs: The dollar in VAR1: A$Dollar amazon-ebs: The backtick in VAR2: A`Backtick amazon-ebs: The single quote in VAR3: A'SingleQuote amazon-ebs: The double quote in VAR4: A\"DoubleQuote amazon-ebs: The dollar in VAR5 (expanded from a user var): My$tring"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.9.x/provisioners/shell-local",
  "text": "Shell (Local) - Provisioners | Packer\nType: shell-local\nshell-local will run a shell script of your choosing on the machine where Packer is being run - in other words, shell-local will run the shell script on your build server, or your desktop, etc., rather than the remote/guest machine being provisioned by Packer.\nThe remote shell provisioner executes shell scripts on a remote machine.\nsource \"file\" \"example\" { content = \"example content\" } build { source \"source.file.example\" { target = \"./test_artifact.txt\" } provisioner \"shell-local\" { inline = [\"echo foo\"] } } \nThe reference of available configuration options is listed below. The only required element is command.\ncommand (string) - This is a single command to execute. It will be written to a temporary file and run using the execute_command call below. If you are building a Windows VM on AWS, Azure, Google Compute, or OpenStack and would like to access the generated password that Packer uses to connect to the instance via WinRM, you can use the template variable {{.WinRMPassword}} to set this as an environment variable.\ninline (array of strings) - This is an array of commands to execute. The commands are concatenated by newlines and turned into a single file, so they are all executed within the same context. This allows you to change directories in one command and use something in the directory in the next and so on. Inline scripts are the easiest way to pull off simple tasks within the machine in which Packer is running.\nscript (string) - The path to a script to execute. This path can be absolute or relative. If it is relative, it is relative to the working directory when Packer is executed.\nscripts (array of strings) - An array of scripts to execute. The scripts will be executed in the order specified. Each script is executed in isolation, so state such as variables from one script won't carry on to the next.\nenv (map of strings) - A map of key/value pairs to inject prior to the execute_command. Packer injects some environmental variables by default into the environment, as well, which are covered in the section below. Duplicate env settings override environment_vars settings.\nenvironment_vars (array of strings) - An array of key/value pairs to inject prior to the execute_command. The format should be key=value. Packer injects some environmental variables by default into the environment, as well, which are covered in the section below. If you are building a Windows VM on AWS, Azure, Google Compute, or OpenStack and would like to access the generated password that Packer uses to connect to the instance via WinRM, you can use the template variable {{.WinRMPassword}} to set this as an environment variable. For example: \"environment_vars\": \"WINRMPASS={{.WinRMPassword}}\"\nenv_var_format (string) - When we parse the environment_vars that you provide, this gives us a string template to use in order to make sure that we are setting the environment vars correctly. By default on Windows hosts this format is set %s=%s && and on Unix, it is %s='%s'. You probably won't need to change this format, but you can see usage examples for where it is necessary below.\nexecute_command (array of strings) - The command used to execute the script. By default this is [\"/bin/sh\", \"-c\", \"{{.Vars}}\", \"{{.Script}}\"] on Unix and [\"cmd\", \"/c\", \"{{.Vars}}\", \"{{.Script}}\"] on Windows. This is treated as a template engine. There are two available variables: Script, which is the path to the script to run, and Vars, which is the list of environment_vars, if configured.\nIf you choose to set this option, make sure that the first element in the array is the shell program you want to use (for example, \"sh\"), and a later element in the array must be {{.Script}}.\nThis option provides you a great deal of flexibility. You may choose to provide your own shell program, for example \"/usr/local/bin/zsh\" or even \"powershell.exe\". However, with great power comes great responsibility - these commands are not officially supported and things like environment variables may not work if you use a different shell than the default.\nFor backwards compatibility, you may also use {{.Command}}, but it is decoded the same way as {{.Script}}. We recommend using {{.Script}} for the sake of clarity, as even when you set only a single command to run, Packer writes it to a temporary file and then runs it as a script.\nIf you are building a Windows VM on AWS, Azure, Google Compute, or OpenStack and would like to access the generated password that Packer uses to connect to the instance via WinRM, you can use the template variable {{.WinRMPassword}} to set this as an environment variable.\ninline_shebang (string) - The shebang value to use when running commands specified by inline. By default, this is /bin/sh -e. If you're not using inline, then this configuration has no effect. Important: If you customize this, be sure to include something like the -e flag, otherwise individual steps failing won't fail the provisioner.\nonly_on (array of strings) - This is an array of runtime operating systems where shell-local will execute. This allows you to execute shell-local only on specific operating systems. By default, shell-local will always run if only_on is not set.\"\nuse_linux_pathing (bool) - This is only relevant to Windows hosts. If you are running Packer in a Windows environment with the Windows Subsystem for Linux feature enabled, and would like to invoke a bash script rather than invoking a Cmd script, you'll need to set this flag to true; it tells Packer to use the Linux subsystem path for your script rather than the Windows path. (e.g. /mnt/c/path/to/your/file instead of C:/path/to/your/file). Please see the example below for more guidance on how to use this feature. If you are not on a Windows host, or you do not intend to use the shell-local provisioner to run a bash script, please ignore this option.\nvalid_exit_codes (list of ints) - Valid exit codes for the script. By default this is 0.\nTo many new users, the execute_command is puzzling. However, it provides an important function: customization of how the command is executed. The most common use case for this is dealing with sudo password prompts. You may also need to customize this if you use a non-POSIX shell, such as tcsh on FreeBSD.\nThe Windows Linux Subsystem\nThe shell-local provisioner was designed with the idea of allowing you to run commands in your local operating system's native shell. For Windows, we've assumed in our defaults that this is Cmd. However, it is possible to run a bash script as part of the Windows Linux Subsystem from the shell-local provisioner, by modifying the execute_command and the use_linux_pathing options in the provisioner config.\nThe example below is a fully functional test config.\nOne limitation of this offering is that \"inline\" and \"command\" options are not available to you; please limit yourself to using the \"script\" or \"scripts\" options instead.\nPlease note that the WSL is a beta feature, and this tool is not guaranteed to work as you expect it to.\nsource \"null\" \"example\" { communicator = \"none\" } build { sources = [ \"source.null.example\" ] provisioner \"shell-local\"{ environment_vars = [\"PROVISIONERTEST=ProvisionerTest1\"] execute_command = [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"] use_linux_pathing = true scripts = [\"C:/Users/me/scripts/example_bash.sh\"] } provisioner \"shell-local\"{ environment_vars = [\"PROVISIONERTEST=ProvisionerTest2\"] execute_command = [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"] use_linux_pathing = true script = \"C:/Users/me/scripts/example_bash.sh\" } } \nWhether you use the inline option, or pass it a direct script or scripts, it is important to understand a few things about how the shell-local provisioner works to run it safely and easily. This understanding will save you much time in the process.\nOnce Per Builder\nThe shell-local script(s) you pass are run once per builder. That means that if you have an amazon-ebs builder and a docker builder, your script will be run twice. If you have 3 builders, it will run 3 times, once for each builder.\nAlways Exit Intentionally\nIf any provisioner fails, the packer build stops and all interim artifacts are cleaned up.\nFor a shell script, that means the script must exit with a zero code. You must be extra careful to exit 0 when necessary.\nWindows Host\nExample of running a .cmd file on Windows:\nprovisioner \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest1\"] scripts = [\"./scripts/test_cmd.cmd\"] } \nContents of \"test_cmd.cmd\":\nExample of running an inline command on Windows: Required customization: tempfile_extension\nprovisioner \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest2\"], tempfile_extension = \".cmd\", inline = [echo \"%SHELLLOCALTEST%\"] } \nExample of running a bash command on Windows using WSL: Required customizations: use_linux_pathing and execute_command\nprovisioner \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest3\"], execute_command = [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"] use_linux_pathing = true script = \"./scripts/example_bash.sh\" } \nContents of example_bash.sh:\n#!/bin/bash echo $SHELLLOCALTEST \nExample of running a PowerShell script on Windows: Required customizations: env_var_format and execute_command\nprovisioner \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest4\"] execute_command = [\"powershell.exe\", \"{{.Vars}} {{.Script}}\"] env_var_format = \"$env:%s=\\\"%s\\\"; \" script = \"./scripts/example_ps.ps1\" } \nExample of running a PowerShell script on Windows as \"inline\": Required customizations: env_var_format, tempfile_extension, and execute_command\nprovisioner \"shell-local\" { tempfile_extension = \".ps1\" environment_vars = [\"SHELLLOCALTEST=ShellTest5\"] execute_command = [\"powershell.exe\", \"{{.Vars}} {{.Script}}\"] env_var_format = \"$env:%s=\\\"%s\\\"; \" inline = [\"write-output $env:SHELLLOCALTEST\"] } \nUnix Host\nExample of running a Shell script on Unix:\nprovisioner \"shell-local\" { environment_vars = [\"PROVISIONERTEST=ProvisionerTest1\"] scripts = [\"./scripts/example_bash.sh\"] } \nExample of running a Shell script \"inline\" on Unix:\nprovisioner \"shell-local\" { environment_vars = [\"PROVISIONERTEST=ProvisionerTest2\"] inline = [\"echo hello\", \"echo $PROVISIONERTEST\"] } \nExample of running a Python script on Unix:\nprovisioner \"shell-local\" { script = \"hello.py\" environment_vars = [\"HELLO_USER=packeruser\"] execute_command = [ \"/bin/sh\", \"-c\", \"{{.Vars}} /usr/local/bin/python {{.Script}}\" ] } \nWhere \"hello.py\" contains: import os print('Hello, %s!' % os.getenv(\"HELLO_USER\"))"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.9.x/provisioners/windows-shell",
  "text": "Windows Shell - Provisioners | Packer\nWindows Shell Provisioner\nType: windows-shell\nThe windows-shell Packer provisioner runs commands on a Windows machine using cmd. It assumes it is running over WinRM.\nprovisioner \"windows-shell\" { inline = [\"dir c:\\\\\"] } \nscript (string) - The path to a script to upload and execute in the machine. This path can be absolute or relative. If it is relative, it is relative to the working directory when Packer is executed.\nscripts (array of strings) - An array of scripts to execute. The scripts will be uploaded and executed in the order specified. Each script is executed in isolation, so state such as variables from one script won't carry on to the next.\nbinary (boolean) - If true, specifies that the script(s) are binary files, and Packer should therefore not convert Windows line endings to Unix line endings (if there are any). By default this is false.\nvalid_exit_codes (list of ints) - Valid exit codes for the script. By default this is just 0.\nexecute_command (string) - The command to use to execute the script. By default this is {{ .Vars }}\"{{ .Path }}\". The value of this is treated as template engine. This is a template engine. Therefore, you may use user variables and template functions in this field. In addition, there are two available extra variables:\nPath is the path to the script to run\nVars is the list of environment_vars, if configured.\nremote_path (string) - The path where the script will be uploaded to in the machine. This defaults to \"c:/Windows/Temp/script.bat\". This value must be a writable location and any parent directories must already exist.\nstart_retry_timeout (string) - The amount of time to attempt to start the remote process. By default this is \"5m\" or 5 minutes. This setting exists in order to deal with times when SSH may restart, such as a system reboot. Set this to a higher value if reboots take a longer amount of time."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.9.x/provisioners/windows-restart",
  "text": "Windows Restart - Provisioners | Packer\nWindows Restart Provisioner\nType: windows-restart\nThe Windows restart provisioner initiates a reboot on a Windows machine and waits for the machine to come back online.\nThe Windows provisioning process often requires multiple reboots, and this provisioner helps to ease that process.\nPacker expects the machine to be ready to continue provisioning after it reboots. Packer detects that the reboot has completed by making an RPC call through the Windows Remote Management (WinRM) service, not by ACPI functions, so Windows must be completely booted in order to continue.\nprovisioner \"windows-restart\" {} \nThe reference of available configuration options is listed below.\ncheck_registry (bool) - if true, checks for several registry keys that indicate that the system is going to reboot. This is useful if an installation kicks off a reboot and you want the provisioner to wait for that reboot to complete before reconnecting. Please note that this option is a beta feature, and we generally recommend that you finish installs that auto-reboot (like Windows Updates) during your autounattend phase before the winrm provisioner connects.\nregistry_keys (array of strings) - if check-registry is true, windows-restart will not reconnect until after all of the listed keys are no longer present in the registry.\ndefault: var DefaultRegistryKeys = []string{ \"HKLM:SOFTWARE\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Component Based Servicing\\\\RebootPending\", \"HKLM:SOFTWARE\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Component Based Servicing\\\\PackagesPending\", \"HKLM:Software\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Component Based Servicing\\\\RebootInProgress\", } \nrestart_command (string) - The command to execute to initiate the restart. By default this is shutdown /r /f /t 0 /c \"packer restart\".\nrestart_check_command (string) - The command to run after executing restart_command to check if the guest machine has restarted. This command will retry until the connection to the guest machine has been restored or restart_timeout has exceeded.\nprovisioner \"windows-restart\" { restart_check_command = \"powershell -command \\\"& {Write-Output 'restarted.'}\\\"\" } \nrestart_timeout (string) - The timeout to wait for the restart. By default this is 5 minutes. Example value: 5m. If you are installing updates or have a lot of startup services, you will probably need to increase this duration."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.9.x/provisioners/custom",
  "text": "Custom - Provisioners | Packer\nPacker is extensible, allowing you to write new provisioners without having to modify the core source code of Packer itself. Documentation for creating new provisioners is covered in the custom provisioners page of the Packer plugin section."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.9.x/post-processors/artifice",
  "text": "Artifice - Post-Processors | Packer\nType: artifice Artifact BuilderId: packer.post-processor.artifice\nThe artifice post-processor overrides the artifact list from an upstream builder or post-processor. All downstream post-processors will see the new artifacts you specify.\nAfter overriding the artifact with artifice, you can use it with other post-processors, including most of the core post-processors and third-party post-processors.\nA major benefit of this is that you can modify builder artifacts using shell-local and pass those modified artifacts into post-processors that may not have worked with the original builder. For example, maybe you want to export a Docker container from an amazon-ebs builder and then use Docker-push to put that Docker container into your Docker Hub account.\nArtifice allows you to use the familiar packer workflow to create a fresh, stateless build environment for each build on the infrastructure of your choosing. You can use this to build just about anything: buildpacks, containers, jars, binaries, tarballs, msi installers, and more.\nPlease note that the artifice post-processor will not delete your old artifact files, even if it removes them from the artifact. If you want to delete the old artifact files, you can use the shell-local post-processor to do so.\nArtifice helps you tie together a few other packer features:\nA builder, which spins up a VM (or container) to build your artifact\nA provisioner, which performs the steps to create your artifact\nA file provisioner, which downloads the artifact from the VM\nThe artifice post-processor, which identifies which files have been downloaded from the VM\nAdditional post-processors, which push the artifact to Docker hub, etc.\nYou will want to perform as much work as possible inside the VM. Ideally the only other post-processor you need after artifice is one that uploads your artifact to the appropriate repository.\nThe configuration allows you to specify which files comprise your artifact.\nfiles (array of strings) - A list of files that comprise your artifact. These files must exist on your local disk after the provisioning phase of packer is complete. These will replace any of the builder's original artifacts (such as a VM snapshot).\nkeep_input_artifact (boolean) - if true, do not delete the original artifact files after creating your new artifact. Defaults to true.\nExample Configuration\nThis minimal example:\nSpins up a cloned VMware virtual machine\nInstalls a consul release\nDownloads the consul binary\nPackages it into a .tar.gz file\nUploads it to S3.\nVMX is a fast way to build and test locally, but you can easily substitute another builder.\n{ \"builders\": [ { \"type\": \"vmware-vmx\", \"source_path\": \"/opt/ubuntu-1404-vmware.vmx\", \"ssh_username\": \"vagrant\", \"ssh_password\": \"vagrant\", \"shutdown_command\": \"sudo shutdown -h now\", \"headless\": \"true\", \"skip_compaction\": \"true\" } ], \"provisioners\": [ { \"type\": \"shell\", \"inline\": [ \"sudo apt-get install -y python-pip\", \"sudo pip install ifs\", \"sudo ifs install consul --version=0.5.2\" ] }, { \"type\": \"file\", \"source\": \"/usr/local/bin/consul\", \"destination\": \"consul\", \"direction\": \"download\" } ], \"post-processors\": [ [ { \"type\": \"artifice\", \"files\": [\"consul\"] }, { \"type\": \"compress\", \"output\": \"consul-0.5.2.tar.gz\" }, { \"type\": \"shell-local\", \"inline\": [ \"/usr/local/bin/aws s3 cp consul-0.5.2.tar.gz s3://<s3 path>\" ] } ] ] } \nNotice that there are two sets of square brackets in the post-processor section. This creates a post-processor chain, where the output of the proceeding artifact is passed to subsequent post-processors. If you use only one set of square braces the post-processors will run individually against the build artifact (the vmx file in this case) and it will not have the desired result.\n{ \"post-processors\": [ [ // <--- Start post-processor chain { \"type\": \"artifice\", \"files\": [\"consul\"] }, { \"type\": \"compress\", ... } ], // <--- End post-processor chain { \"type\":\"compress\" // <-- Standalone post-processor } ] } \nYou can create multiple post-processor chains to handle multiple builders (for example, building Linux and Windows binaries during the same build)."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.9.x/post-processors/checksum",
  "text": "Checksum - Post-Processors | Packer\nType: checksum Artifact BuilderId: packer.post-processor.checksum\nThe checksum post-processor computes specified checksum for the artifact list from an upstream builder or post-processor. All downstream post-processors will see the new artifacts. The primary use-case is compute checksum for artifact to verify it later.\nAfter computes checksum for artifacts, you can use new artifacts with other post-processors like artifice, compress, docker-push, or a third-party post-processor.\n{ \"type\": \"checksum\", \"checksum_types\": [\"sha1\", \"sha256\"], \"output\": \"packer_{{.BuildName}}_{{.ChecksumType}}.checksum\" } \nchecksum_types (array of strings) - An array of strings of checksum types to compute. If empty, Defaults to md5. Allowed values are:\nmd5\nsha1\nsha224\nsha256\nsha384\nsha512\noutput (string) - Specify filename to store checksums. This defaults to packer_{{.BuildName}}_{{.BuilderType}}_{{.ChecksumType}}.checksum. For example, if you had a builder named database, you might see the file written as packer_database_docker_md5.checksum. This is treated as a template engine. Therefore, you may use user variables and template functions in this field. The following special variables are also available to use in the output template:\nBuildName: The name of the builder that produced the artifact.\nBuilderType: The type of builder used to produce the artifact.\nChecksumType: The type of checksums the file contains. This should be used if you have more than one value in checksum_types."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.9.x/provisioners/community-supported",
  "text": "Community - Provisioners | Packer\nThe following provisioners are developed and maintained by various members of the Packer community, not by HashiCorp. For more information on how to use community provisioners, see our docs on extending Packer.\nCommunity Provisioners\nComment Provisioner - Example provisioner that allows you to annotate your build with bubble-text comments.\nWindows Update provisioner - A provisioner for gracefully handling Windows updates and the reboots they cause."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.9.x/post-processors/shell-local",
  "text": "Local Shell - Post-Processors | Packer\nLocal Shell Post Processor\nType: shell-local\nThe local shell post processor executes scripts locally during the post processing stage. Shell local provides a convenient way to automate executing some task with packer outputs and variables.\nThe example below is a fully functional self-contained build.\nsource \"file\" \"example\" { content = \"example content\" } build { source \"source.file.example\" { target = \"./test_artifact.txt\" } post-processor \"shell-local\" { inline = [\"echo foo\"] } } \ncommand (string) - This is a single command to execute. It will be written to a temporary file and run using the execute_command call below.\nscript (string) - The path to a script to execute. This path can be absolute or relative. If it is relative, it is relative to the working directory when Packer is executed.\nscripts (array of strings) - An array of scripts to execute. The scripts will be executed in the order specified. Each script is executed in isolation, so state such as variables from one script won't carry on to the next.\nenv_var_format (string) - When we parse the environment_vars that you provide, this gives us a string template to use in order to make sure that we are setting the environment vars correctly. By default on Windows hosts this format is set %s=%s && and on Unix, it is %s='%s'. You probably won't need to change this format, but you can see usage examples for where it is necessary below.\nexecute_command (array of strings) - The command used to execute the script. By default, on *nix systems this is:\n[\"/bin/sh\", \"-c\", \"{{.Vars}} {{.Script}}\"] \nWhile on Windows, execute_command defaults to:\n[\"cmd\", \"/V\", \"/C\", \"{{.Vars}}\", \"call\", \"{{.Script}}\"] \nThis is treated as a template engine. There are several available variables: Script, which is the path to the script to run, and Vars, which is the list of environment_vars, if configured. In addition, you may access any of the variables stored in the generated data using the build template function. If you choose to set this option, make sure that the first element in the array is the shell program you want to use (for example, \"sh\" or \"/usr/local/bin/zsh\" or even \"powershell.exe\" although anything other than a flavor of the shell command language is not explicitly supported and may be broken by assumptions made within Packer). It's worth noting that if you choose to try to use shell-local for Powershell or other Windows commands, the environment variables will not be set properly for your environment.\nFor backwards compatibility, execute_command will accept a string instead of an array of strings. If a single string or an array of strings with only one element is provided, Packer will replicate past behavior by appending your execute_command to the array of strings [\"sh\", \"-c\"]. For example, if you set \"execute_command\": \"foo bar\", the final execute_command that Packer runs will be [\"sh\", \"-c\", \"foo bar\"]. If you set \"execute_command\": [\"foo\", \"bar\"], the final execute_command will remain [\"foo\", \"bar\"].\nAgain, the above is only provided as a backwards compatibility fix; we strongly recommend that you set execute_command as an array of strings.\ninline_shebang (string) - The shebang value to use when running commands specified by inline. By default, this is /bin/sh -e. If you're not using inline, then this configuration has no effect. Important: If you customize this, be sure to include something like the -e flag, otherwise individual steps failing won't fail the provisioner.\nkeep_input_artifact (boolean) - Unlike most other post-processors, the keep_input_artifact option will have no effect for the shell-local post-processor. Packer will always retain the input artifact for shell-local, since the shell-local post-processor merely passes forward the artifact it receives. If your shell-local post-processor produces a file or files which you would like to have replace the input artifact, you may overwrite the input artifact using the artifice post-processor after your shell-local processor has run.\nonly_on (array of strings) - This is an array of runtime operating systems where shell-local will execute. This allows you to execute shell-local only on specific operating systems. By default, shell-local will always run if only_on is not set.\"\nuse_linux_pathing (bool) - This is only relevant to Windows hosts. If you are running Packer in a Windows environment with the Windows Subsystem for Linux feature enabled, and would like to invoke a bash script rather than invoking a Cmd script, you'll need to set this flag to true; it tells Packer to use the Linux subsystem path for your script rather than the Windows path. (e.g. /mnt/c/path/to/your/file instead of C:/path/to/your/file). Please see the example below for more guidance on how to use this feature. If you are not on a Windows host, or you do not intend to use the shell-local post-processor to run a bash script, please ignore this option. If you set this flag to true, you still need to provide the standard Windows path to the script when providing a script. This is a beta feature.\nvalid_exit_codes (list of ints) - Valid exit codes for the script. By default this is 0.\nTo many new users, the execute_command is puzzling. However, it provides an important function: customization of how the command is executed. The most common use case for this is dealing with sudo password prompts. You may also need to customize this if you use a non-POSIX shell, such as tcsh on FreeBSD.\nThe Windows Linux Subsystem\nThe shell-local post-processor was designed with the idea of allowing you to run commands in your local operating system's native shell. For Windows, we've assumed in our defaults that this is Cmd. However, it is possible to run a bash script as part of the Windows Linux Subsystem from the shell-local post-processor, by modifying the execute_command and the use_linux_pathing options in the post-processor config.\nThe example below is a fully functional test config.\nOne limitation of this offering is that \"inline\" and \"command\" options are not available to you; please limit yourself to using the \"script\" or \"scripts\" options instead.\nPlease note that this feature is still in beta, as the underlying WSL is also still in beta. There will be some limitations as a result. For example, it will likely not work unless both Packer and the scripts you want to run are both on the C drive.\nsource \"null\" \"example\" { communicator = \"none\" } build { sources = [ \"source.null.example\" ] post-processor \"shell-local\"{ environment_vars = [\"PROVISIONERTEST=ProvisionerTest1\"] execute_command = [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"] use_linux_pathing = true scripts = [\"C:/Users/me/scripts/example_bash.sh\"] } post-processor \"shell-local\"{ environment_vars = [\"PROVISIONERTEST=ProvisionerTest2\"] execute_command = [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"] use_linux_pathing = true script = \"C:/Users/me/scripts/example_bash.sh\" } } \nWhether you use the inline option, or pass it a direct script or scripts, it is important to understand a few things about how the shell-local post-processor works to run it safely and easily. This understanding will save you much time in the process.\nOnce Per Builder\nThe shell-local script(s) you pass are run once per builder. This means that if you have an amazon-ebs builder and a docker builder, your script will be run twice. If you have 3 builders, it will run 3 times, once for each builder.\nInteracting with Build Artifacts\nIn order to interact with build artifacts, you may want to use the manifest post-processor. This will write the list of files produced by a builder to a json file after each builder is run.\nFor example, if you wanted to package a file from the file builder into a tarball, you might write this:\n{ \"builders\": [ { \"content\": \"Lorem ipsum dolor sit amet\", \"target\": \"dummy_artifact\", \"type\": \"file\" } ], \"post-processors\": [ [ { \"output\": \"manifest.json\", \"strip_path\": true, \"type\": \"manifest\" }, { \"inline\": [ \"jq \\\".builds[].files[].name\\\" manifest.json | xargs tar cfz artifacts.tgz\" ], \"type\": \"shell-local\" } ] ] } \nThis uses the jq tool to extract all of the file names from the manifest file and passes them to tar.\nAlways Exit Intentionally\nIf any post-processor fails, the packer build stops and all interim artifacts are cleaned up.\nFor a shell script, that means the script must exit with a zero code. You must be extra careful to exit 0 when necessary.\nWindows Host\nExample of running a .cmd file on Windows:\npost-processor \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest1\"] scripts = [\"./scripts/test_cmd.cmd\"] } \nContents of test_cmd.cmd:\nExample of running an inline command on Windows: Required customization: tempfile_extension\npost-processor \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest2\"], tempfile_extension = \".cmd\", inline = [\"echo %SHELLLOCALTEST%\"] } \nExample of running a bash command on Windows using WSL: Required customizations: use_linux_pathing and execute_command:\npost-processor \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest3\"], execute_command = [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"] use_linux_pathing = true script = \"./scripts/example_bash.sh\" } \nContents of example_bash.sh:\n#!/bin/bash echo $SHELLLOCALTEST \nExample of running a PowerShell script on Windows: Required customizations: env_var_format and execute_command.\npost-processor \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest4\"] execute_command = [\"powershell.exe\", \"{{.Vars}} {{.Script}}\"] env_var_format = \"$env:%s=\\\"%s\\\"; \" script = \"./scripts/example_ps.ps1\" } \nExample of running a PowerShell script on Windows as \"inline\": Required customizations: env_var_format, tempfile_extension, and execute_command\npost-processor \"shell-local\" { tempfile_extension = \".ps1\" environment_vars = [\"SHELLLOCALTEST=ShellTest5\"] execute_command = [\"powershell.exe\", \"{{.Vars}} {{.Script}}\"] env_var_format = \"$env:%s=\\\"%s\\\"; \" inline = [\"write-output $env:SHELLLOCALTEST\"] } \nUnix Host\nExample of running a Shell script on Unix:\npost-processor \"shell-local\" { environment_vars = [\"PROVISIONERTEST=ProvisionerTest1\"] scripts = [\"./scripts/example_bash.sh\"] } \nExample of running a bash \"inline\" on Unix:\npost-processor \"shell-local\" { environment_vars = [\"PROVISIONERTEST=ProvisionerTest2\"] inline = [\"echo hello\", \"echo $PROVISIONERTEST\"] } \nExample of running a Python script on Unix:\npost-processor \"shell-local\" { script = \"hello.py\" environment_vars = [\"HELLO_USER=packeruser\"] execute_command = [ \"/bin/sh\", \"-c\", \"{{.Vars}} /usr/local/bin/python {{.Script}}\" ] } \nWhere \"hello.py\" contains: import os print('Hello, %s!' % os.getenv(\"HELLO_USER\"))"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.8.x/builders/file",
  "text": "File - Builders | Packer\nType: file Artifact BuilderId: packer.file\nThe file Packer builder is not really a builder, it just creates an artifact from a file. It can be used to debug post-processors without incurring high wait times.\nBelow is a fully functioning example. It create a file at target with the specified content.\n{ \"type\": \"file\", \"content\": \"Lorem ipsum dolor sit amet\", \"target\": \"dummy_artifact\" } \nAny communicator defined is ignored.\ntarget (string) - The path for the artifact file that will be created. If the path contains directories that don't exist, Packer will create them, too.\nYou can only define one of source or content. If none of them is defined the artifact will be empty.\nsource (string) - The path for a file which will be copied as the artifact.\ncontent (string) - The content that will be put into the artifact."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.8.x/builders/null",
  "text": "Null - Builders | Packer\nType: null\nThe null Packer builder is not really a builder, it just sets up an SSH connection and runs the provisioners. It can be used to debug provisioners without incurring high wait times. It does not create any kind of image or artifact.\nBelow is a fully functioning example. It doesn't do anything useful, since no provisioners are defined, but it will connect to the specified host via ssh.\n{ \"type\": \"null\", \"ssh_host\": \"127.0.0.1\", \"ssh_username\": \"foo\", \"ssh_password\": \"bar\" } \nThe null builder has no configuration parameters other than the communicator settings."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.9.x/post-processors/community-supported",
  "text": "Community - Post-Processors | Packer\nThe following post-processors are developed and maintained by various members of the Packer community, not by HashiCorp. For more information on how to use community post-processors, see our docs on extending Packer.\nCommunity Post-Processors\nExoscale Import post-processor - A post-processor to import Exoscale custom templates from disk image files."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.8.x/builders/custom",
  "text": "Custom - Builders | Packer\nPacker is extensible, allowing you to write new builders without having to modify the core source code of Packer itself. Documentation for creating new builders is covered in the custom builders page of the Packer plugin section."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.8.x/builders/community-supported",
  "text": "Community - Builders | Packer\nThe following builders are developed and maintained by various members of the Packer community, not by HashiCorp. For more information on how to use community builders, see our docs on extending Packer.\nCommunity Builders\nARM builders\npacker-plugin-arm-image - simple builder lets you extend on existing system images.\npacker-builder-arm - flexible builder lets you extend or build images from scratch with variety of options (ie. custom partition table).\nExoscale builder - A builder to create Exoscale custom templates based on a Compute instance snapshot.\nCitrix XenServer/Citrix Hypervisor - Plugin for creating Citrix XenServer/Citrix Hypervisor images from an iso image or from an existing template.\nXCP-NG/Citrix XenServer/Citrix Hypervisor/Updated Fork - Plugin for creating XCP-NG/Citrix XenServer/Citrix Hypervisor images from an iso image or from an existing template. This is a fork of the orginal, and reccomended by the developers of XCP-NG."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.8.x/commands/plugins",
  "text": "plugins Command | Packer | HashiCorp Developer\nThe plugins command groups subcommands for interacting with Packers' plugins.\n$ packer plugins -h Usage: packer plugins <subcommand> [options] [args] This command groups subcommands for interacting with Packer plugins. Related but not under the \"plugins\" command : - \"packer init <path>\" will install all plugins required by a config. Subcommands: install Install latest Packer plugin [matching version constraint] installed List all installed Packer plugin binaries remove Remove Packer plugins [matching a version] required List plugins required by a config \nRelated\npacker init will install all required plugins."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.8.x/provisioners/powershell",
  "text": "PowerShell - Provisioners | Packer\nType: powershell\nThe PowerShell Packer provisioner runs PowerShell scripts on Windows machines. It assumes that the communicator in use is WinRM. However, the provisioner can work equally well (with a few caveats) when combined with the SSH communicator. See the section below for details.\nNote: If possible, try to always use a forward slash / as the path separator, especially when dealing with relative paths. A backward slash \\ will work on Windows and is the official Windows path separator, but when building from any system that is not Windows, Packer will only treat slashes / as path separators, and treat backslashes as plain text. Which could lead to pathing errors.\nprovisioner \"powershell\" { inline = [\"dir c:/\"] } \ndebug_mode - If set, sets PowerShell's PSDebug mode in order to make script debugging easier. For instance, setting the value to 1 results in adding this to the execute command:\nelevated_execute_command (string) - The command to use to execute the elevated script. By default this is as follows:\nVars: The location of a temp file containing the list of environment_vars, if configured.\nenv (map of strings) - A map of key/value pairs to inject prior to the execute_command. Packer injects some environmental variables by default into the environment, as well, which are covered in the section below. Duplicate env settings override environment_vars settings. This is not a JSON template engine enabled function. HCL interpolation works as usual.\nuse_pwsh (boolean) - Run pwsh.exe instead of powershell.exe. Defaults to false.\nprovisioner \"powershell\" { environment_vars = [\"WINRMPASS=${build.Password}\"] inline = [\"Write-Host \\\"Automatically generated aws password is: $Env:WINRMPASS\\\"\"] } \nexecute_command (string) - The command to use to execute the script. By default this is as follows:\nVars: The location of a temp file containing the list of environment_vars, if configured. The value of both Path and Vars can be manually configured by setting the values for remote_path and remote_env_var_path respectively.\nIf you use the SSH communicator and have changed your default shell, you may need to modify your execute_command to make sure that the command is valid and properly escaped; the default assumes that you have not changed the default shell away from cmd.\nelevated_user and elevated_password (string) - If specified, the PowerShell script will be run with elevated privileges using the given Windows user.\nprovisioner \"powershell\" { elevated_user = \"Administrator\" elevated_password = build.Password } \nIf you specify an empty elevated_password value then the PowerShell script is run as a service account. For example:\nprovisioner \"powershell\" { elevated_user = \"SYSTEM\" elevated_password = \"\" } \nexecution_policy - To run ps scripts on Windows, Packer defaults this to \"bypass\" and wraps the command to run. Setting this to \"none\" will prevent wrapping, allowing to see exit codes on Docker for Windows. Possible values are bypass, allsigned, default, remotesigned, restricted, undefined, unrestricted, and none.\nremote_path (string) - The path where the PowerShell script will be uploaded to within the target build machine. This defaults to C:/Windows/Temp/script-UUID.ps1 where UUID is replaced with a dynamically generated string that uniquely identifies the script.\nThis setting allows users to override the default upload location. The value must be a writable location and any parent directories must already exist.\nremote_env_var_path (string) - Environment variables required within the remote environment are uploaded within a PowerShell script and then enabled by 'dot sourcing' the script immediately prior to execution of the main command or script.\nThe path the environment variables script will be uploaded to defaults to C:/Windows/Temp/packer-ps-env-vars-UUID.ps1 where UUID is replaced with a dynamically generated string that uniquely identifies the script.\nThis setting allows users to override the location the environment variable script is uploaded to. The value must be a writable location and any parent directories must already exist.\nskip_clean (bool) - Whether to clean scripts up after executing the provisioner. Defaults to false. When true any script created by a non-elevated Powershell provisioner will be removed from the remote machine. Elevated scripts, along with the scheduled tasks, will always be removed regardless of the value set for skip_clean.\nstart_retry_timeout (string) - The amount of time to attempt to start the remote process. By default this is 5m or 5 minutes. This setting exists in order to deal with times when SSH may restart, such as a system reboot. Set this to a higher value if reboots take a longer amount of time.\npause_after (string) - Wait the amount of time after provisioning a PowerShell script, this pause be taken if all previous steps were successful.\nThe good news first. If you are using the Microsoft port of OpenSSH then the provisioner should just work as expected - no extra configuration effort is required.\nNow the caveats. If you are using an alternative configuration, and your SSH connection lands you in a *nix shell on the remote host, then you will most likely need to manually set the execute_command; The default execute_command used by Packer will not work for you. When configuring the command you will need to ensure that any dollar signs or other characters that may be incorrectly interpreted by the remote shell are escaped accordingly.\nThe following example shows how the standard execute_command can be reconfigured to work on a remote system with Cygwin/OpenSSH installed. The execute_command has each dollar sign backslash escaped so that it is not interpreted by the remote Bash shell - Bash being the default shell for Cygwin environments.\nprovisioner \"powershell\" { execute_command = \"powershell -executionpolicy bypass \\\"& { if (Test-Path variable:global:ProgressPreference){\\\\$ProgressPreference='SilentlyContinue'};. {{.Vars}}; &'{{.Path}}'; exit \\\\$LastExitCode }\\\"\" inline = [ \"Write-Host \\\"Hello from PowerShell\\\"\"] } \nThe escape character in PowerShell is the backtick, also sometimes referred to as the grave accent. When, and when not, to escape characters special to PowerShell is probably best demonstrated with a series of examples.\nWhen To Escape...\nUsers need to deal with escaping characters special to PowerShell when they appear directly in commands used in the inline PowerShell provisioner and when they appear directly in the users own scripts. Note that where double quotes appear within double quotes, the addition of a backslash escape is required for the JSON template to be parsed correctly.\nprovisioner \"powershell\" { inline = [ \"Write-Host \\\"A literal dollar `$ must be escaped\\\"\", \"Write-Host \\\"A literal backtick `` must be escaped\\\"\", \"Write-Host \\\"Here `\\\"double quotes`\\\" must be escaped\\\"\", \"Write-Host \\\"Here `'single quotes`' don`'t really need to be\\\"\", \"Write-Host \\\"escaped... but it doesn`'t hurt to do so.\\\"\", ] } \n==> amazon-ebs: Provisioning with Powershell... ==> amazon-ebs: Provisioning with PowerShell script: /var/folders/15/d0f7gdg13rnd1cxp7tgmr55c0000gn/T/packer-powershell-provisioner508190439 amazon-ebs: A literal dollar $ must be escaped amazon-ebs: A literal backtick ` must be escaped amazon-ebs: Here \"double quotes\" must be escaped amazon-ebs: Here 'single quotes' don't really need to be amazon-ebs: escaped... but it doesn't hurt to do so. \nWhen Not To Escape...\nSpecial characters appearing in user environment variable values and in the elevated_user and elevated_password fields will be automatically dealt with for the user. There is no need to use escapes in these instances.\nvariable \"psvar\" { type = string default = \"My$tring\" } build { sources = [\"source.amazon-ebs.example\"] provisioner \"powershell\" { elevated_user = \"Administrator\" elevated_password = \"Super$3cr3t!\" inline = [\"Write-Output \\\"The dollar in the elevated_password is interpreted correctly\\\"\"] } provisioner \"powershell\" { environment_vars = [ \"VAR1=A$Dollar\", \"VAR2=A`Backtick\", \"VAR3=A'SingleQuote\", \"VAR4=A\\\"DoubleQuote\", \"VAR5=${var.psvar}\", ] inline = [ \"Write-Output \\\"In the following examples the special character is interpreted correctly:\\\"\", \"Write-Output \\\"The dollar in VAR1: $Env:VAR1\\\"\", \"Write-Output \\\"The backtick in VAR2: $Env:VAR2\\\"\", \"Write-Output \\\"The single quote in VAR3: $Env:VAR3\\\"\", \"Write-Output \\\"The double quote in VAR4: $Env:VAR4\\\"\", \"Write-Output \\\"The dollar in VAR5 (expanded from a user var): $Env:VAR5\\\"\", ] } } \n==> amazon-ebs: Provisioning with Powershell... ==> amazon-ebs: Provisioning with PowerShell script: /var/folders/15/d0f7gdg13rnd1cxp7tgmr55c0000gn/T/packer-powershell-provisioner961728919 amazon-ebs: The dollar in the elevated_password is interpreted correctly ==> amazon-ebs: Provisioning with Powershell... ==> amazon-ebs: Provisioning with PowerShell script: /var/folders/15/d0f7gdg13rnd1cxp7tgmr55c0000gn/T/packer-powershell-provisioner142826554 amazon-ebs: In the following examples the special character is interpreted correctly: amazon-ebs: The dollar in VAR1: A$Dollar amazon-ebs: The backtick in VAR2: A`Backtick amazon-ebs: The single quote in VAR3: A'SingleQuote amazon-ebs: The double quote in VAR4: A\"DoubleQuote amazon-ebs: The dollar in VAR5 (expanded from a user var): My$tring"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.8.x/provisioners/breakpoint",
  "text": "breakpoint - Provisioners | Packer\nBreakpoint Provisioner\nType: breakpoint\nThe breakpoint provisioner will pause until the user presses \"enter\" to resume the build. This is intended for debugging purposes, and allows you to halt at a particular part of the provisioning process.\nThis is independent of the -debug flag, which will instead halt at every step and between every provisioner.\n{ \"builders\": [ { \"type\": \"null\", \"communicator\": \"none\" } ], \"provisioners\": [ { \"type\": \"shell-local\", \"inline\": \"echo hi\" }, { \"type\": \"breakpoint\", \"disable\": false, \"note\": \"this is a breakpoint\" }, { \"type\": \"shell-local\", \"inline\": \"echo hi 2\" } ] } \ndisable (boolean) - If true, skip the breakpoint. Useful for when you have set multiple breakpoints and want to toggle them off or on. Default: false\nnote (string) - a string to include explaining the purpose or location of the breakpoint. For example, you may find it useful to number your breakpoints or label them with information about where in the build they occur\nInsert this provisioner wherever you want the build to pause. You'll see ui output prompting you to press \"enter\" to continue the build when you are ready.\n==> docker: Pausing at breakpoint provisioner with note \"foo bar baz\". ==> docker: Press enter to continue. \nOnce you press enter, the build will resume and run normally until it either completes or errors."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.8.x/provisioners/file",
  "text": "File - Provisioners | Packer\nType: file\nThe file Packer provisioner uploads files to machines built by Packer. The recommended usage of the file provisioner is to use it to upload files, and then use shell provisioner to move them to the proper place, set permissions, etc.\nWarning: You can only upload files to locations that the provisioning user (generally not root) has permission to access. Creating files in /tmp and using a shell provisioner to move them into the final location is the only way to upload files to root owned locations.\nThe file provisioner can upload both single files and complete directories.\n{ \"type\": \"file\", \"source\": \"app.tar.gz\", \"destination\": \"/tmp/app.tar.gz\" } \nThe available configuration options are listed below.\nRequired Parameters:\ncontent (string) - This is the content to copy to destination. If destination is a file, content will be written to that file, in case of a directory a file named pkr-file-content is created. It's recommended to use a file as the destination. The templatefile function might be used here, or any interpolation syntax. This attribute cannot be specified with source or sources.\nsource (string) - The path to a local file or directory to upload to the machine. The path can be absolute or relative. If it is relative, it is relative to the working directory when Packer is executed. If this is a directory, the existence of a trailing slash is important. Read below on uploading directories. Mandatory unless sources is set.\ndestination (string) - The path where the file will be uploaded to in the machine. This value must be a writable location and any parent directories must already exist. If the provisioning user (generally not root) cannot write to this directory, you will receive a \"Permission Denied\" error. If the source is a file, it's a good idea to make the destination a file as well, but if you set your destination as a directory, at least make sure that the destination ends in a trailing slash so that Packer knows to use the source's basename in the final upload path. Failure to do so may cause Packer to fail on file uploads. If the destination file already exists, it will be overwritten.\nOptional Parameters:\nsources ([]string) - A list of sources to upload. This can be used in place of the source option if you have several files that you want to upload to the same place. Note that the destination must be a directory with a trailing slash, and that all files listed in sources will be uploaded to the same directory with their file names preserved.\ndirection (string) - The direction of the file transfer. This defaults to \"upload\". If it is set to \"download\" then the file \"source\" in the machine will be downloaded locally to \"destination\"\ngenerated (bool) - For advanced users only. If true, check the file existence only before uploading, rather than upon pre-build validation. This allows users to upload files created on-the-fly. This defaults to false. We don't recommend using this feature, since it can cause Packer to become dependent on system state. We would prefer you generate your files before the Packer run, but realize that there are situations where this may be unavoidable.\nThe file provisioner is also able to upload a complete directory to the remote machine. When uploading a directory, there are a few important things you should know.\nFirst, the destination directory must already exist. If you need to create it, use a shell provisioner just prior to the file provisioner in order to create the directory. If the destination directory does not exist, the file provisioner may succeed, but it will have undefined results.\nNext, the existence of a trailing slash on the source path will determine whether the directory name will be embedded within the destination, or whether the destination will be created. An example explains this best:\nIf the source is /foo (no trailing slash), and the destination is /tmp, then the contents of /foo on the local machine will be uploaded to /tmp/foo on the remote machine. The foo directory on the remote machine will be created by Packer.\nIf the source, however, is /foo/ (a trailing slash is present), and the destination is /tmp, then the contents of /foo will be uploaded into /tmp directly.\nThis behavior was adopted from the standard behavior of rsync. Note that under the covers, rsync may or may not be used.\nIn general, local files used as the source must exist before Packer is run. This is great for catching typos and ensuring that once a build is started, that it will succeed. However, this also means that you can't generate a file during your build and then upload it using the file provisioner later. A convenient workaround is to upload a directory instead of a file. The directory still must exist, but its contents don't. You can write your generated file to the directory during the Packer run, and have it be uploaded later.\nThe behavior when uploading symbolic links depends on the communicator. The Docker communicator will preserve symlinks, but all other communicators will treat local symlinks as regular files. If you wish to preserve symlinks when uploading, it's recommended that you use tar. Below is an example of what that might look like:\n$ ls -l files total 16 drwxr-xr-x 3 mwhooker staff 102 Jan 27 17:10 a lrwxr-xr-x 1 mwhooker staff 1 Jan 27 17:10 b -> a -rw-r--r-- 1 mwhooker staff 0 Jan 27 17:10 file1 lrwxr-xr-x 1 mwhooker staff 5 Jan 27 17:10 file1link -> file1 $ ls -l toupload total 0 -rw-r--r-- 1 mwhooker staff 0 Jan 27 17:10 files.tar \n{ \"provisioners\": [ { \"type\": \"shell-local\", \"command\": \"tar cf toupload/files.tar files\" }, { \"destination\": \"/tmp/\", \"source\": \"./toupload\", \"type\": \"file\" }, { \"inline\": [ \"cd /tmp && tar xf toupload/files.tar\", \"rm toupload/files.tar\" ], \"type\": \"shell\" } ] } \nBecause of the way our WinRM transfers works, it can take a very long time to upload and download even moderately sized files. If you're experiencing slowness using the file provisioner on Windows, it's suggested that you set up an SSH server and use the ssh communicator. If you only want to transfer files to your guest, and if your builder supports it, you may also use the http_directory or http_content directives. This will cause that directory to be available to the guest over HTTP, and set the environment variable PACKER_HTTP_ADDR to the address."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.8.x/provisioners/windows-restart",
  "text": "Windows Restart - Provisioners | Packer\nWindows Restart Provisioner\nType: windows-restart\nThe Windows restart provisioner initiates a reboot on a Windows machine and waits for the machine to come back online.\nThe Windows provisioning process often requires multiple reboots, and this provisioner helps to ease that process.\nPacker expects the machine to be ready to continue provisioning after it reboots. Packer detects that the reboot has completed by making an RPC call through the Windows Remote Management (WinRM) service, not by ACPI functions, so Windows must be completely booted in order to continue.\nprovisioner \"windows-restart\" {} \nThe reference of available configuration options is listed below.\ncheck_registry (bool) - if true, checks for several registry keys that indicate that the system is going to reboot. This is useful if an installation kicks off a reboot and you want the provisioner to wait for that reboot to complete before reconnecting. Please note that this option is a beta feature, and we generally recommend that you finish installs that auto-reboot (like Windows Updates) during your autounattend phase before the winrm provisioner connects.\nregistry_keys (array of strings) - if check-registry is true, windows-restart will not reconnect until after all of the listed keys are no longer present in the registry.\ndefault: var DefaultRegistryKeys = []string{ \"HKLM:SOFTWARE\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Component Based Servicing\\\\RebootPending\", \"HKLM:SOFTWARE\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Component Based Servicing\\\\PackagesPending\", \"HKLM:Software\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Component Based Servicing\\\\RebootInProgress\", } \nrestart_command (string) - The command to execute to initiate the restart. By default this is shutdown /r /f /t 0 /c \"packer restart\".\nrestart_check_command (string) - The command to run after executing restart_command to check if the guest machine has restarted. This command will retry until the connection to the guest machine has been restored or restart_timeout has exceeded.\nprovisioner \"windows-restart\" { restart_check_command = \"powershell -command \\\"& {Write-Output 'restarted.'}\\\"\" } \nrestart_timeout (string) - The timeout to wait for the restart. By default this is 5 minutes. Example value: 5m. If you are installing updates or have a lot of startup services, you will probably need to increase this duration."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.8.x/provisioners/windows-shell",
  "text": "Windows Shell - Provisioners | Packer\nWindows Shell Provisioner\nType: windows-shell\nThe windows-shell Packer provisioner runs commands on a Windows machine using cmd. It assumes it is running over WinRM.\nprovisioner \"windows-shell\" { inline = [\"dir c:\\\\\"] } \nexecute_command (string) - The command to use to execute the script. By default this is {{ .Vars }}\"{{ .Path }}\". The value of this is treated as template engine. This is a template engine. Therefore, you may use user variables and template functions in this field. In addition, there are two available extra variables:\nPath is the path to the script to run\nVars is the list of environment_vars, if configured.\nremote_path (string) - The path where the script will be uploaded to in the machine. This defaults to \"c:/Windows/Temp/script.bat\". This value must be a writable location and any parent directories must already exist.\nstart_retry_timeout (string) - The amount of time to attempt to start the remote process. By default this is \"5m\" or 5 minutes. This setting exists in order to deal with times when SSH may restart, such as a system reboot. Set this to a higher value if reboots take a longer amount of time."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.8.x/provisioners/shell-local",
  "text": "Shell (Local) - Provisioners | Packer\nshell-local will run a shell script of your choosing on the machine where Packer is being run - in other words, shell-local will run the shell script on your build server, or your desktop, etc., rather than the remote/guest machine being provisioned by Packer.\nThe remote shell provisioner executes shell scripts on a remote machine.\nsource \"file\" \"example\" { content = \"example content\" } build { source \"source.file.example\" { target = \"./test_artifact.txt\" } provisioner \"shell-local\" { inline = [\"echo foo\"] } } \nThe reference of available configuration options is listed below. The only required element is command.\ncommand (string) - This is a single command to execute. It will be written to a temporary file and run using the execute_command call below. If you are building a Windows VM on AWS, Azure, Google Compute, or OpenStack and would like to access the generated password that Packer uses to connect to the instance via WinRM, you can use the template variable {{.WinRMPassword}} to set this as an environment variable.\ninline (array of strings) - This is an array of commands to execute. The commands are concatenated by newlines and turned into a single file, so they are all executed within the same context. This allows you to change directories in one command and use something in the directory in the next and so on. Inline scripts are the easiest way to pull off simple tasks within the machine in which Packer is running.\nenvironment_vars (array of strings) - An array of key/value pairs to inject prior to the execute_command. The format should be key=value. Packer injects some environmental variables by default into the environment, as well, which are covered in the section below. If you are building a Windows VM on AWS, Azure, Google Compute, or OpenStack and would like to access the generated password that Packer uses to connect to the instance via WinRM, you can use the template variable {{.WinRMPassword}} to set this as an environment variable. For example: \"environment_vars\": \"WINRMPASS={{.WinRMPassword}}\"\nexecute_command (array of strings) - The command used to execute the script. By default this is [\"/bin/sh\", \"-c\", \"{{.Vars}}\", \"{{.Script}}\"] on Unix and [\"cmd\", \"/c\", \"{{.Vars}}\", \"{{.Script}}\"] on Windows. This is treated as a template engine. There are two available variables: Script, which is the path to the script to run, and Vars, which is the list of environment_vars, if configured.\nIf you choose to set this option, make sure that the first element in the array is the shell program you want to use (for example, \"sh\"), and a later element in the array must be {{.Script}}.\nThis option provides you a great deal of flexibility. You may choose to provide your own shell program, for example \"/usr/local/bin/zsh\" or even \"powershell.exe\". However, with great power comes great responsibility - these commands are not officially supported and things like environment variables may not work if you use a different shell than the default.\nFor backwards compatibility, you may also use {{.Command}}, but it is decoded the same way as {{.Script}}. We recommend using {{.Script}} for the sake of clarity, as even when you set only a single command to run, Packer writes it to a temporary file and then runs it as a script.\nIf you are building a Windows VM on AWS, Azure, Google Compute, or OpenStack and would like to access the generated password that Packer uses to connect to the instance via WinRM, you can use the template variable {{.WinRMPassword}} to set this as an environment variable.\nuse_linux_pathing (bool) - This is only relevant to Windows hosts. If you are running Packer in a Windows environment with the Windows Subsystem for Linux feature enabled, and would like to invoke a bash script rather than invoking a Cmd script, you'll need to set this flag to true; it tells Packer to use the Linux subsystem path for your script rather than the Windows path. (e.g. /mnt/c/path/to/your/file instead of C:/path/to/your/file). Please see the example below for more guidance on how to use this feature. If you are not on a Windows host, or you do not intend to use the shell-local provisioner to run a bash script, please ignore this option.\nThe shell-local provisioner was designed with the idea of allowing you to run commands in your local operating system's native shell. For Windows, we've assumed in our defaults that this is Cmd. However, it is possible to run a bash script as part of the Windows Linux Subsystem from the shell-local provisioner, by modifying the execute_command and the use_linux_pathing options in the provisioner config.\nPlease note that the WSL is a beta feature, and this tool is not guaranteed to work as you expect it to.\nsource \"null\" \"example\" { communicator = \"none\" } build { sources = [ \"source.null.example\" ] provisioner \"shell-local\"{ environment_vars = [\"PROVISIONERTEST=ProvisionerTest1\"] execute_command = [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"] use_linux_pathing = true scripts = [\"C:/Users/me/scripts/example_bash.sh\"] } provisioner \"shell-local\"{ environment_vars = [\"PROVISIONERTEST=ProvisionerTest2\"] execute_command = [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"] use_linux_pathing = true script = \"C:/Users/me/scripts/example_bash.sh\" } } \nWhether you use the inline option, or pass it a direct script or scripts, it is important to understand a few things about how the shell-local provisioner works to run it safely and easily. This understanding will save you much time in the process.\nThe shell-local script(s) you pass are run once per builder. That means that if you have an amazon-ebs builder and a docker builder, your script will be run twice. If you have 3 builders, it will run 3 times, once for each builder.\nIf any provisioner fails, the packer build stops and all interim artifacts are cleaned up.\nprovisioner \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest1\"] scripts = [\"./scripts/test_cmd.cmd\"] } \nContents of \"test_cmd.cmd\":\nprovisioner \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest2\"], tempfile_extension = \".cmd\", inline = [echo \"%SHELLLOCALTEST%\"] } \nExample of running a bash command on Windows using WSL: Required customizations: use_linux_pathing and execute_command\nprovisioner \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest3\"], execute_command = [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"] use_linux_pathing = true script = \"./scripts/example_bash.sh\" } \nExample of running a PowerShell script on Windows: Required customizations: env_var_format and execute_command\nprovisioner \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest4\"] execute_command = [\"powershell.exe\", \"{{.Vars}} {{.Script}}\"] env_var_format = \"$env:%s=\\\"%s\\\"; \" script = \"./scripts/example_ps.ps1\" } \nprovisioner \"shell-local\" { tempfile_extension = \".ps1\" environment_vars = [\"SHELLLOCALTEST=ShellTest5\"] execute_command = [\"powershell.exe\", \"{{.Vars}} {{.Script}}\"] env_var_format = \"$env:%s=\\\"%s\\\"; \" inline = [\"write-output $env:SHELLLOCALTEST\"] } \nprovisioner \"shell-local\" { environment_vars = [\"PROVISIONERTEST=ProvisionerTest1\"] scripts = [\"./scripts/example_bash.sh\"] } \nExample of running a Shell script \"inline\" on Unix:\nprovisioner \"shell-local\" { environment_vars = [\"PROVISIONERTEST=ProvisionerTest2\"] inline = [\"echo hello\", \"echo $PROVISIONERTEST\"] } \nprovisioner \"shell-local\" { script = \"hello.py\" environment_vars = [\"HELLO_USER=packeruser\"] execute_command = [ \"/bin/sh\", \"-c\", \"{{.Vars}} /usr/local/bin/python {{.Script}}\" ] } "
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.8.x/provisioners/custom",
  "text": "Custom - Provisioners | Packer\nPacker is extensible, allowing you to write new provisioners without having to modify the core source code of Packer itself. Documentation for creating new provisioners is covered in the custom provisioners page of the Packer plugin section."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.8.x/post-processors/artifice",
  "text": "Artifice - Post-Processors | Packer\nType: artifice Artifact BuilderId: packer.post-processor.artifice\nThe artifice post-processor overrides the artifact list from an upstream builder or post-processor. All downstream post-processors will see the new artifacts you specify.\nAfter overriding the artifact with artifice, you can use it with other post-processors, including most of the core post-processors and third-party post-processors.\nA major benefit of this is that you can modify builder artifacts using shell-local and pass those modified artifacts into post-processors that may not have worked with the original builder. For example, maybe you want to export a Docker container from an amazon-ebs builder and then use Docker-push to put that Docker container into your Docker Hub account.\nArtifice allows you to use the familiar packer workflow to create a fresh, stateless build environment for each build on the infrastructure of your choosing. You can use this to build just about anything: buildpacks, containers, jars, binaries, tarballs, msi installers, and more.\nPlease note that the artifice post-processor will not delete your old artifact files, even if it removes them from the artifact. If you want to delete the old artifact files, you can use the shell-local post-processor to do so.\nArtifice helps you tie together a few other packer features:\nA builder, which spins up a VM (or container) to build your artifact\nA provisioner, which performs the steps to create your artifact\nA file provisioner, which downloads the artifact from the VM\nThe artifice post-processor, which identifies which files have been downloaded from the VM\nAdditional post-processors, which push the artifact to Docker hub, etc.\nYou will want to perform as much work as possible inside the VM. Ideally the only other post-processor you need after artifice is one that uploads your artifact to the appropriate repository.\nThe configuration allows you to specify which files comprise your artifact.\nfiles (array of strings) - A list of files that comprise your artifact. These files must exist on your local disk after the provisioning phase of packer is complete. These will replace any of the builder's original artifacts (such as a VM snapshot).\nkeep_input_artifact (boolean) - if true, do not delete the original artifact files after creating your new artifact. Defaults to true.\nExample Configuration\nThis minimal example:\nSpins up a cloned VMware virtual machine\nInstalls a consul release\nDownloads the consul binary\nPackages it into a .tar.gz file\nUploads it to S3.\nVMX is a fast way to build and test locally, but you can easily substitute another builder.\n{ \"builders\": [ { \"type\": \"vmware-vmx\", \"source_path\": \"/opt/ubuntu-1404-vmware.vmx\", \"ssh_username\": \"vagrant\", \"ssh_password\": \"vagrant\", \"shutdown_command\": \"sudo shutdown -h now\", \"headless\": \"true\", \"skip_compaction\": \"true\" } ], \"provisioners\": [ { \"type\": \"shell\", \"inline\": [ \"sudo apt-get install -y python-pip\", \"sudo pip install ifs\", \"sudo ifs install consul --version=0.5.2\" ] }, { \"type\": \"file\", \"source\": \"/usr/local/bin/consul\", \"destination\": \"consul\", \"direction\": \"download\" } ], \"post-processors\": [ [ { \"type\": \"artifice\", \"files\": [\"consul\"] }, { \"type\": \"compress\", \"output\": \"consul-0.5.2.tar.gz\" }, { \"type\": \"shell-local\", \"inline\": [ \"/usr/local/bin/aws s3 cp consul-0.5.2.tar.gz s3://<s3 path>\" ] } ] ] } \nNotice that there are two sets of square brackets in the post-processor section. This creates a post-processor chain, where the output of the proceeding artifact is passed to subsequent post-processors. If you use only one set of square braces the post-processors will run individually against the build artifact (the vmx file in this case) and it will not have the desired result.\n{ \"post-processors\": [ [ // <--- Start post-processor chain { \"type\": \"artifice\", \"files\": [\"consul\"] }, { \"type\": \"compress\", ... } ], // <--- End post-processor chain { \"type\":\"compress\" // <-- Standalone post-processor } ] } \nYou can create multiple post-processor chains to handle multiple builders (for example, building Linux and Windows binaries during the same build)."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.8.x/post-processors/checksum",
  "text": "Checksum - Post-Processors | Packer\nType: checksum Artifact BuilderId: packer.post-processor.checksum\nThe checksum post-processor computes specified checksum for the artifact list from an upstream builder or post-processor. All downstream post-processors will see the new artifacts. The primary use-case is compute checksum for artifact to verify it later.\nAfter computes checksum for artifacts, you can use new artifacts with other post-processors like artifice, compress, docker-push, or a third-party post-processor.\n{ \"type\": \"checksum\", \"checksum_types\": [\"sha1\", \"sha256\"], \"output\": \"packer_{{.BuildName}}_{{.ChecksumType}}.checksum\" } \nchecksum_types (array of strings) - An array of strings of checksum types to compute. If empty, Defaults to md5. Allowed values are:\nmd5\nsha1\nsha224\nsha256\nsha384\nsha512\noutput (string) - Specify filename to store checksums. This defaults to packer_{{.BuildName}}_{{.BuilderType}}_{{.ChecksumType}}.checksum. For example, if you had a builder named database, you might see the file written as packer_database_docker_md5.checksum. This is treated as a template engine. Therefore, you may use user variables and template functions in this field. The following special variables are also available to use in the output template:\nBuildName: The name of the builder that produced the artifact.\nBuilderType: The type of builder used to produce the artifact.\nChecksumType: The type of checksums the file contains. This should be used if you have more than one value in checksum_types."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.8.x/post-processors/community-supported",
  "text": "Community - Post-Processors | Packer\nThe following post-processors are developed and maintained by various members of the Packer community, not by HashiCorp. For more information on how to use community post-processors, see our docs on extending Packer.\nCommunity Post-Processors\nExoscale Import post-processor - A post-processor to import Exoscale custom templates from disk image files."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/plugins/hcp-support",
  "text": "Packer | HashiCorp Developer"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.8.x/post-processors/shell-local",
  "text": "Local Shell - Post-Processors | Packer\nLocal Shell Post Processor\nThe local shell post processor executes scripts locally during the post processing stage. Shell local provides a convenient way to automate executing some task with packer outputs and variables.\nThe example below is a fully functional self-contained build.\nsource \"file\" \"example\" { content = \"example content\" } build { source \"source.file.example\" { target = \"./test_artifact.txt\" } post-processor \"shell-local\" { inline = [\"echo foo\"] } } \ncommand (string) - This is a single command to execute. It will be written to a temporary file and run using the execute_command call below.\nexecute_command (array of strings) - The command used to execute the script. By default, on *nix systems this is:\n[\"/bin/sh\", \"-c\", \"{{.Vars}} {{.Script}}\"] \nWhile on Windows, execute_command defaults to:\n[\"cmd\", \"/V\", \"/C\", \"{{.Vars}}\", \"call\", \"{{.Script}}\"] \nThis is treated as a template engine. There are several available variables: Script, which is the path to the script to run, and Vars, which is the list of environment_vars, if configured. In addition, you may access any of the variables stored in the generated data using the build template function. If you choose to set this option, make sure that the first element in the array is the shell program you want to use (for example, \"sh\" or \"/usr/local/bin/zsh\" or even \"powershell.exe\" although anything other than a flavor of the shell command language is not explicitly supported and may be broken by assumptions made within Packer). It's worth noting that if you choose to try to use shell-local for Powershell or other Windows commands, the environment variables will not be set properly for your environment.\nFor backwards compatibility, execute_command will accept a string instead of an array of strings. If a single string or an array of strings with only one element is provided, Packer will replicate past behavior by appending your execute_command to the array of strings [\"sh\", \"-c\"]. For example, if you set \"execute_command\": \"foo bar\", the final execute_command that Packer runs will be [\"sh\", \"-c\", \"foo bar\"]. If you set \"execute_command\": [\"foo\", \"bar\"], the final execute_command will remain [\"foo\", \"bar\"].\nAgain, the above is only provided as a backwards compatibility fix; we strongly recommend that you set execute_command as an array of strings.\nkeep_input_artifact (boolean) - Unlike most other post-processors, the keep_input_artifact option will have no effect for the shell-local post-processor. Packer will always retain the input artifact for shell-local, since the shell-local post-processor merely passes forward the artifact it receives. If your shell-local post-processor produces a file or files which you would like to have replace the input artifact, you may overwrite the input artifact using the artifice post-processor after your shell-local processor has run.\nuse_linux_pathing (bool) - This is only relevant to Windows hosts. If you are running Packer in a Windows environment with the Windows Subsystem for Linux feature enabled, and would like to invoke a bash script rather than invoking a Cmd script, you'll need to set this flag to true; it tells Packer to use the Linux subsystem path for your script rather than the Windows path. (e.g. /mnt/c/path/to/your/file instead of C:/path/to/your/file). Please see the example below for more guidance on how to use this feature. If you are not on a Windows host, or you do not intend to use the shell-local post-processor to run a bash script, please ignore this option. If you set this flag to true, you still need to provide the standard Windows path to the script when providing a script. This is a beta feature.\nThe shell-local post-processor was designed with the idea of allowing you to run commands in your local operating system's native shell. For Windows, we've assumed in our defaults that this is Cmd. However, it is possible to run a bash script as part of the Windows Linux Subsystem from the shell-local post-processor, by modifying the execute_command and the use_linux_pathing options in the post-processor config.\nPlease note that this feature is still in beta, as the underlying WSL is also still in beta. There will be some limitations as a result. For example, it will likely not work unless both Packer and the scripts you want to run are both on the C drive.\nsource \"null\" \"example\" { communicator = \"none\" } build { sources = [ \"source.null.example\" ] post-processor \"shell-local\"{ environment_vars = [\"PROVISIONERTEST=ProvisionerTest1\"] execute_command = [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"] use_linux_pathing = true scripts = [\"C:/Users/me/scripts/example_bash.sh\"] } post-processor \"shell-local\"{ environment_vars = [\"PROVISIONERTEST=ProvisionerTest2\"] execute_command = [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"] use_linux_pathing = true script = \"C:/Users/me/scripts/example_bash.sh\" } } \nWhether you use the inline option, or pass it a direct script or scripts, it is important to understand a few things about how the shell-local post-processor works to run it safely and easily. This understanding will save you much time in the process.\nThe shell-local script(s) you pass are run once per builder. This means that if you have an amazon-ebs builder and a docker builder, your script will be run twice. If you have 3 builders, it will run 3 times, once for each builder.\nInteracting with Build Artifacts\nIn order to interact with build artifacts, you may want to use the manifest post-processor. This will write the list of files produced by a builder to a json file after each builder is run.\nFor example, if you wanted to package a file from the file builder into a tarball, you might write this:\n{ \"builders\": [ { \"content\": \"Lorem ipsum dolor sit amet\", \"target\": \"dummy_artifact\", \"type\": \"file\" } ], \"post-processors\": [ [ { \"output\": \"manifest.json\", \"strip_path\": true, \"type\": \"manifest\" }, { \"inline\": [ \"jq \\\".builds[].files[].name\\\" manifest.json | xargs tar cfz artifacts.tgz\" ], \"type\": \"shell-local\" } ] ] } \nThis uses the jq tool to extract all of the file names from the manifest file and passes them to tar.\nIf any post-processor fails, the packer build stops and all interim artifacts are cleaned up.\npost-processor \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest1\"] scripts = [\"./scripts/test_cmd.cmd\"] } \nContents of test_cmd.cmd:\npost-processor \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest2\"], tempfile_extension = \".cmd\", inline = [\"echo %SHELLLOCALTEST%\"] } \nExample of running a bash command on Windows using WSL: Required customizations: use_linux_pathing and execute_command:\npost-processor \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest3\"], execute_command = [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"] use_linux_pathing = true script = \"./scripts/example_bash.sh\" } \nExample of running a PowerShell script on Windows: Required customizations: env_var_format and execute_command.\npost-processor \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest4\"] execute_command = [\"powershell.exe\", \"{{.Vars}} {{.Script}}\"] env_var_format = \"$env:%s=\\\"%s\\\"; \" script = \"./scripts/example_ps.ps1\" } \npost-processor \"shell-local\" { tempfile_extension = \".ps1\" environment_vars = [\"SHELLLOCALTEST=ShellTest5\"] execute_command = [\"powershell.exe\", \"{{.Vars}} {{.Script}}\"] env_var_format = \"$env:%s=\\\"%s\\\"; \" inline = [\"write-output $env:SHELLLOCALTEST\"] } \npost-processor \"shell-local\" { environment_vars = [\"PROVISIONERTEST=ProvisionerTest1\"] scripts = [\"./scripts/example_bash.sh\"] } \nExample of running a bash \"inline\" on Unix:\npost-processor \"shell-local\" { environment_vars = [\"PROVISIONERTEST=ProvisionerTest2\"] inline = [\"echo hello\", \"echo $PROVISIONERTEST\"] } \npost-processor \"shell-local\" { script = \"hello.py\" environment_vars = [\"HELLO_USER=packeruser\"] execute_command = [ \"/bin/sh\", \"-c\", \"{{.Vars}} /usr/local/bin/python {{.Script}}\" ] } "
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/builders/community-supported",
  "text": "Community - Builders | Packer\nThe following builders are developed and maintained by various members of the Packer community, not by HashiCorp. For more information on how to use community builders, see our docs on extending Packer.\nCommunity Builders\nARM builders\npacker-plugin-arm-image - simple builder lets you extend on existing system images.\npacker-builder-arm - flexible builder lets you extend or build images from scratch with variety of options (ie. custom partition table).\nExoscale builder - A builder to create Exoscale custom templates based on a Compute instance snapshot.\nHuawei Cloud ECS builder - Plugin for creating Huawei Cloud ECS images.\nCitrix XenServer/Citrix Hypervisor - Plugin for creating Citrix XenServer/Citrix Hypervisor images from an iso image or from an existing template.\nXCP-NG/Citrix XenServer/Citrix Hypervisor/Updated Fork - Plugin for creating XCP-NG/Citrix XenServer/Citrix Hypervisor images from an iso image or from an existing template. This is a fork of the orginal, and reccomended by the developers of XCP-NG."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/builders/file",
  "text": "File - Builders | Packer\nType: file Artifact BuilderId: packer.file\nThe file Packer builder is not really a builder, it just creates an artifact from a file. It can be used to debug post-processors without incurring high wait times.\nBelow is a fully functioning example. It create a file at target with the specified content.\n{ \"type\": \"file\", \"content\": \"Lorem ipsum dolor sit amet\", \"target\": \"dummy_artifact\" } \nAny communicator defined is ignored.\ntarget (string) - The path for the artifact file that will be created. If the path contains directories that don't exist, Packer will create them, too.\nYou can only define one of source or content. If none of them is defined the artifact will be empty.\nsource (string) - The path for a file which will be copied as the artifact.\ncontent (string) - The content that will be put into the artifact."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.8.x/provisioners/community-supported",
  "text": "Community - Provisioners | Packer\nThe following provisioners are developed and maintained by various members of the Packer community, not by HashiCorp. For more information on how to use community provisioners, see our docs on extending Packer.\nCommunity Provisioners\nComment Provisioner - Example provisioner that allows you to annotate your build with bubble-text comments.\nWindows Update provisioner - A provisioner for gracefully handling Windows updates and the reboots they cause."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/builders/custom",
  "text": "Custom - Builders | Packer\nPacker is extensible, allowing you to write new builders without having to modify the core source code of Packer itself. Documentation for creating new builders is covered in the custom builders page of the Packer plugin section."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/builders/null",
  "text": "Null - Builders | Packer\nType: null\nThe null Packer builder is not really a builder, it just sets up an SSH connection and runs the provisioners. It can be used to debug provisioners without incurring high wait times. It does not create any kind of image or artifact.\nBelow is a fully functioning example. It doesn't do anything useful, since no provisioners are defined, but it will connect to the specified host via ssh.\n{ \"type\": \"null\", \"ssh_host\": \"127.0.0.1\", \"ssh_username\": \"foo\", \"ssh_password\": \"bar\" } \nThe null builder has no configuration parameters other than the communicator settings."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.8.x/datasources/http",
  "text": "HTTP - Data Sources | Packer\nType: http\nThe http data source makes an HTTP GET request to the given URL and exports information about the response.\ndata \"http\" \"example\" { url = \"https://checkpoint-api.hashicorp.com/v1/check/terraform\" # Optional request headers request_headers = { Accept = \"application/json\" } } \nurl (string) - The URL to request data from. This URL must respond with a 200 OK response and a text/* or application/json Content-Type.\nNot Required:\nrequest_headers (map[string]string) - A map of strings representing additional HTTP headers to include in the request.\nThe outputs for this datasource are as follows:\nurl (string) - The URL the data was requested from.\nbody (string) - The raw body of the HTTP response.\nrequest_headers (map[string]string) - A map of strings representing the response HTTP headers. Duplicate headers are concatenated with, according to RFC2616."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/provisioners/breakpoint",
  "text": "breakpoint - Provisioners | Packer\nBreakpoint Provisioner\nType: breakpoint\nThe breakpoint provisioner will pause until the user presses \"enter\" to resume the build. This is intended for debugging purposes, and allows you to halt at a particular part of the provisioning process.\nThis is independent of the -debug flag, which will instead halt at every step and between every provisioner.\n{ \"builders\": [ { \"type\": \"null\", \"communicator\": \"none\" } ], \"provisioners\": [ { \"type\": \"shell-local\", \"inline\": \"echo hi\" }, { \"type\": \"breakpoint\", \"disable\": false, \"note\": \"this is a breakpoint\" }, { \"type\": \"shell-local\", \"inline\": \"echo hi 2\" } ] } \ndisable (boolean) - If true, skip the breakpoint. Useful for when you have set multiple breakpoints and want to toggle them off or on. Default: false\nnote (string) - a string to include explaining the purpose or location of the breakpoint. For example, you may find it useful to number your breakpoints or label them with information about where in the build they occur\nInsert this provisioner wherever you want the build to pause. You'll see ui output prompting you to press \"enter\" to continue the build when you are ready.\n==> docker: Pausing at breakpoint provisioner with note \"foo bar baz\". ==> docker: Press enter to continue. \nOnce you press enter, the build will resume and run normally until it either completes or errors."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/provisioners/breakpoint",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/provisioners/breakpoint",
  "text": "{ \"type\": \"breakpoint\", \"note\": \"foo bar baz\" } \n{ \"type\": \"shell\", \"script\": \"script.sh\", \"override\": { \"vmware-iso\": { \"execute_command\": \"echo 'password' | sudo -S bash {{.Path}}\" } } } "
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/provisioners/file",
  "text": "File - Provisioners | Packer\nType: file\nThe file Packer provisioner uploads files to machines built by Packer. The recommended usage of the file provisioner is to use it to upload files, and then use shell provisioner to move them to the proper place, set permissions, etc.\nWarning: You can only upload files to locations that the provisioning user (generally not root) has permission to access. Creating files in /tmp and using a shell provisioner to move them into the final location is the only way to upload files to root owned locations.\nThe file provisioner can upload both single files and complete directories.\n{ \"type\": \"file\", \"source\": \"app.tar.gz\", \"destination\": \"/tmp/app.tar.gz\" } \nThe available configuration options are listed below.\nRequired Parameters:\ncontent (string) - This is the content to copy to destination. If destination is a file, content will be written to that file, in case of a directory a file named pkr-file-content is created. It's recommended to use a file as the destination. A template_file might be referenced in here, or any interpolation syntax. This attribute cannot be specified with source or sources.\nsource (string) - The path to a local file or directory to upload to the machine. The path can be absolute or relative. If it is relative, it is relative to the working directory when Packer is executed. If this is a directory, the existence of a trailing slash is important. Read below on uploading directories. Mandatory unless sources is set.\ndestination (string) - The path where the file will be uploaded to in the machine. This value must be a writable location and any parent directories must already exist. If the provisioning user (generally not root) cannot write to this directory, you will receive a \"Permission Denied\" error. If the source is a file, it's a good idea to make the destination a file as well, but if you set your destination as a directory, at least make sure that the destination ends in a trailing slash so that Packer knows to use the source's basename in the final upload path. Failure to do so may cause Packer to fail on file uploads. If the destination file already exists, it will be overwritten.\nOptional Parameters:\nsources ([]string) - A list of sources to upload. This can be used in place of the source option if you have several files that you want to upload to the same place. Note that the destination must be a directory with a trailing slash, and that all files listed in sources will be uploaded to the same directory with their file names preserved.\ndirection (string) - The direction of the file transfer. This defaults to \"upload\". If it is set to \"download\" then the file \"source\" in the machine will be downloaded locally to \"destination\"\ngenerated (bool) - For advanced users only. If true, check the file existence only before uploading, rather than upon pre-build validation. This allows users to upload files created on-the-fly. This defaults to false. We don't recommend using this feature, since it can cause Packer to become dependent on system state. We would prefer you generate your files before the Packer run, but realize that there are situations where this may be unavoidable.\nThe file provisioner is also able to upload a complete directory to the remote machine. When uploading a directory, there are a few important things you should know.\nFirst, the destination directory must already exist. If you need to create it, use a shell provisioner just prior to the file provisioner in order to create the directory. If the destination directory does not exist, the file provisioner may succeed, but it will have undefined results.\nNext, the existence of a trailing slash on the source path will determine whether the directory name will be embedded within the destination, or whether the destination will be created. An example explains this best:\nIf the source is /foo (no trailing slash), and the destination is /tmp, then the contents of /foo on the local machine will be uploaded to /tmp/foo on the remote machine. The foo directory on the remote machine will be created by Packer.\nIf the source, however, is /foo/ (a trailing slash is present), and the destination is /tmp, then the contents of /foo will be uploaded into /tmp directly.\nThis behavior was adopted from the standard behavior of rsync. Note that under the covers, rsync may or may not be used.\nIn general, local files used as the source must exist before Packer is run. This is great for catching typos and ensuring that once a build is started, that it will succeed. However, this also means that you can't generate a file during your build and then upload it using the file provisioner later. A convenient workaround is to upload a directory instead of a file. The directory still must exist, but its contents don't. You can write your generated file to the directory during the Packer run, and have it be uploaded later.\nThe behavior when uploading symbolic links depends on the communicator. The Docker communicator will preserve symlinks, but all other communicators will treat local symlinks as regular files. If you wish to preserve symlinks when uploading, it's recommended that you use tar. Below is an example of what that might look like:\n$ ls -l files total 16 drwxr-xr-x 3 mwhooker staff 102 Jan 27 17:10 a lrwxr-xr-x 1 mwhooker staff 1 Jan 27 17:10 b -> a -rw-r--r-- 1 mwhooker staff 0 Jan 27 17:10 file1 lrwxr-xr-x 1 mwhooker staff 5 Jan 27 17:10 file1link -> file1 $ ls -l toupload total 0 -rw-r--r-- 1 mwhooker staff 0 Jan 27 17:10 files.tar \n{ \"provisioners\": [ { \"type\": \"shell-local\", \"command\": \"tar cf toupload/files.tar files\" }, { \"destination\": \"/tmp/\", \"source\": \"./toupload\", \"type\": \"file\" }, { \"inline\": [ \"cd /tmp && tar xf toupload/files.tar\", \"rm toupload/files.tar\" ], \"type\": \"shell\" } ] } \nBecause of the way our WinRM transfers works, it can take a very long time to upload and download even moderately sized files. If you're experiencing slowness using the file provisioner on Windows, it's suggested that you set up an SSH server and use the ssh communicator. If you only want to transfer files to your guest, and if your builder supports it, you may also use the http_directory or http_content directives. This will cause that directory to be available to the guest over http, and set the environment variable PACKER_HTTP_ADDR to the address."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/provisioners/file",
  "text": "Because of the way our WinRM transfers works, it can take a very long time to upload and download even moderately sized files. If you're experiencing slowness using the file provisioner on Windows, it's suggested that you set up an SSH server and use the ssh communicator. If you only want to transfer files to your guest, and if your builder supports it, you may also use the http_directory directive. This will cause that directory to be available to the guest over http, and set the environment variable PACKER_HTTP_ADDR to the address."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/provisioners/file",
  "text": "Required\nsource (string) - The path to a local file or directory to upload to the machine. The path can be absolute or relative. If it is relative, it is relative to the working directory when Packer is executed. If this is a directory, the existence of a trailing slash is important. Read below on uploading directories.\ngenerated (boolean) - For advanced users only. If true, check the file existence only before uploading, rather than upon pre-build validation. This allows to upload files created on-the-fly. This defaults to false. We don't recommend using this feature, since it can cause Packer to become dependent on system state. We would prefer you generate your files before the Packer run, but realize that there are situations where this may be unavoidable.\n{ \"type\": \"shell\", \"script\": \"script.sh\", \"override\": { \"vmware-iso\": { \"execute_command\": \"echo 'password' | sudo -S bash {{.Path}}\" } } } \nBecause of the way our WinRM transfers works, it can take a very long time to upload and download even moderately sized files. If you're experiencing slowness using the file provisioner on Windows, it's suggested that you set up an SSH server and use the ssh communicator. If you only want to transfer files to your guest, and if your builder supports it, you may also use the http_directory directive. This will cause that directory to be available to the guest over http, and set the environment variable PACKER_HTTP_ADDR to the address."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/provisioners/powershell",
  "text": "PowerShell - Provisioners | Packer\nType: powershell\nThe PowerShell Packer provisioner runs PowerShell scripts on Windows machines. It assumes that the communicator in use is WinRM. However, the provisioner can work equally well (with a few caveats) when combined with the SSH communicator. See the section below for details.\n{ \"type\": \"powershell\", \"inline\": [\"dir c:\\\\\"] } \ndebug_mode - If set, sets PowerShell's PSDebug mode in order to make script debugging easier. For instance, setting the value to 1 results in adding this to the execute command:\nelevated_execute_command (string) - The command to use to execute the elevated script. By default this is as follows:\nVars: The location of a temp file containing the list of environment_vars, if configured.\nThis is a template engine. Therefore, you may use user variables and template functions in this field. If you are running on AWS, Azure, Google Compute, or OpenStack and would like to access the autogenerated password that Packer uses to connect to the instance via WinRM, you can use the build template engine to inject it using {{ build Password}}. In HCL templates, you can do the same thing by accessing the build variables For example:\n{ \"type\": \"powershell\", \"environment_vars\": [\"WINRMPASS={{ build `Password`}}\"], \"inline\": [\"Write-Host \\\"Automatically generated aws password is: $Env:WINRMPASS\\\"\"] }, \nexecute_command (string) - The command to use to execute the script. By default this is as follows:\nVars: The location of a temp file containing the list of environment_vars, if configured. The value of both Path and Vars can be manually configured by setting the values for remote_path and remote_env_var_path respectively.\nIf you use the SSH communicator and have changed your default shell, you may need to modify your execute_command to make sure that the command is valid and properly escaped; the default assumes that you have not changed the default shell away from cmd.\nelevated_user and elevated_password (string) - If specified, the PowerShell script will be run with elevated privileges using the given Windows user.\nThis is a template engine. Therefore, you may use user variables and template functions in this field. If you are running on AWS, Azure, Google Compute, or OpenStack and would like to access the autogenerated password that Packer uses to connect to the instance via WinRM, you can use the build template engine to inject it using {{ build Password}}. In HCL templates, you can do the same thing by accessing the build variables For example:\n{ \"type\": \"powershell\", \"elevated_user\": \"Administrator\", \"elevated_password\": \"{{ build `Password`}}\", ... }, \nIf you specify an empty elevated_password value then the PowerShell script is run as a service account. For example:\n{ \"type\": \"powershell\", \"elevated_user\": \"SYSTEM\", \"elevated_password\": \"\", ... }, \nexecution_policy - To run ps scripts on windows packer defaults this to \"bypass\" and wraps the command to run. Setting this to \"none\" will prevent wrapping, allowing to see exit codes on docker for windows. Possible values are \"bypass\", \"allsigned\", \"default\", \"remotesigned\", \"restricted\", \"undefined\", \"unrestricted\", \"none\".\nremote_path (string) - The path where the PowerShell script will be uploaded to within the target build machine. This defaults to C:/Windows/Temp/script-UUID.ps1 where UUID is replaced with a dynamically generated string that uniquely identifies the script.\nThis setting allows users to override the default upload location. The value must be a writable location and any parent directories must already exist.\nremote_env_var_path (string) - Environment variables required within the remote environment are uploaded within a PowerShell script and then enabled by 'dot sourcing' the script immediately prior to execution of the main command or script.\nThe path the environment variables script will be uploaded to defaults to C:/Windows/Temp/packer-ps-env-vars-UUID.ps1 where UUID is replaced with a dynamically generated string that uniquely identifies the script.\nThis setting allows users to override the location the environment variable script is uploaded to. The value must be a writable location and any parent directories must already exist.\nskip_clean (bool) - Whether to clean scripts up after executing the provisioner. Defaults to false. When true any script created by a non-elevated Powershell provisioner will be removed from the remote machine. Elevated scripts, along with the scheduled tasks, will always be removed regardless of the value set for skip_clean.\nstart_retry_timeout (string) - The amount of time to attempt to start the remote process. By default this is \"5m\" or 5 minutes. This setting exists in order to deal with times when SSH may restart, such as a system reboot. Set this to a higher value if reboots take a longer amount of time.\nPACKER_HTTP_ADDR If using a builder that provides an http server for file transfer (such as hyperv, parallels, qemu, virtualbox, and vmware), this will be set to the address. You can use this address in your provisioner to download large files over http. This may be useful if you're experiencing slower speeds using the default file provisioner. A file provisioner using the winrm communicator may experience these types of difficulties.\nThe good news first. If you are using the Microsoft port of OpenSSH then the provisioner should just work as expected - no extra configuration effort is required.\nNow the caveats. If you are using an alternative configuration, and your SSH connection lands you in a *nix shell on the remote host, then you will most likely need to manually set the execute_command; The default execute_command used by Packer will not work for you. When configuring the command you will need to ensure that any dollar signs or other characters that may be incorrectly interpreted by the remote shell are escaped accordingly.\nThe following example shows how the standard execute_command can be reconfigured to work on a remote system with Cygwin/OpenSSH installed. The execute_command has each dollar sign backslash escaped so that it is not interpreted by the remote Bash shell - Bash being the default shell for Cygwin environments.\n\"provisioners\": [ { \"type\": \"powershell\", \"execute_command\": \"powershell -executionpolicy bypass \\\"& { if (Test-Path variable:global:ProgressPreference){\\\\$ProgressPreference='SilentlyContinue'};. {{.Vars}}; &'{{.Path}}'; exit \\\\$LastExitCode }\\\"\", \"inline\": [\"Write-Host \\\"Hello from PowerShell\\\"\"] } ] \nThe escape character in PowerShell is the backtick, also sometimes referred to as the grave accent. When, and when not, to escape characters special to PowerShell is probably best demonstrated with a series of examples.\nWhen To Escape...\nUsers need to deal with escaping characters special to PowerShell when they appear directly in commands used in the inline PowerShell provisioner and when they appear directly in the users own scripts. Note that where double quotes appear within double quotes, the addition of a backslash escape is required for the JSON template to be parsed correctly.\n\"provisioners\": [ { \"type\": \"powershell\", \"inline\": [ \"Write-Host \\\"A literal dollar `$ must be escaped\\\"\", \"Write-Host \\\"A literal backtick `` must be escaped\\\"\", \"Write-Host \\\"Here `\\\"double quotes`\\\" must be escaped\\\"\", \"Write-Host \\\"Here `'single quotes`' don`'t really need to be\\\"\", \"Write-Host \\\"escaped... but it doesn`'t hurt to do so.\\\"\" ] } ] \n==> amazon-ebs: Provisioning with Powershell... ==> amazon-ebs: Provisioning with powershell script: /var/folders/15/d0f7gdg13rnd1cxp7tgmr55c0000gn/T/packer-powershell-provisioner508190439 amazon-ebs: A literal dollar $ must be escaped amazon-ebs: A literal backtick ` must be escaped amazon-ebs: Here \"double quotes\" must be escaped amazon-ebs: Here 'single quotes' don't really need to be amazon-ebs: escaped... but it doesn't hurt to do so. \nWhen Not To Escape...\nSpecial characters appearing in user environment variable values and in the elevated_user and elevated_password fields will be automatically dealt with for the user. There is no need to use escapes in these instances.\n{ \"variables\": { \"psvar\": \"My$tring\" }, ... \"provisioners\": [ { \"type\": \"powershell\", \"elevated_user\": \"Administrator\", \"elevated_password\": \"Super$3cr3t!\", \"inline\": \"Write-Output \\\"The dollar in the elevated_password is interpreted correctly\\\"\" }, { \"type\": \"powershell\", \"environment_vars\": [ \"VAR1=A$Dollar\", \"VAR2=A`Backtick\", \"VAR3=A'SingleQuote\", \"VAR4=A\\\"DoubleQuote\", \"VAR5={{user `psvar`}}\" ], \"inline\": [ \"Write-Output \\\"In the following examples the special character is interpreted correctly:\\\"\", \"Write-Output \\\"The dollar in VAR1: $Env:VAR1\\\"\", \"Write-Output \\\"The backtick in VAR2: $Env:VAR2\\\"\", \"Write-Output \\\"The single quote in VAR3: $Env:VAR3\\\"\", \"Write-Output \\\"The double quote in VAR4: $Env:VAR4\\\"\", \"Write-Output \\\"The dollar in VAR5 (expanded from a user var): $Env:VAR5\\\"\" ] } ] ... } \n==> amazon-ebs: Provisioning with Powershell... ==> amazon-ebs: Provisioning with powershell script: /var/folders/15/d0f7gdg13rnd1cxp7tgmr55c0000gn/T/packer-powershell-provisioner961728919 amazon-ebs: The dollar in the elevated_password is interpreted correctly ==> amazon-ebs: Provisioning with Powershell... ==> amazon-ebs: Provisioning with powershell script: /var/folders/15/d0f7gdg13rnd1cxp7tgmr55c0000gn/T/packer-powershell-provisioner142826554 amazon-ebs: In the following examples the special character is interpreted correctly: amazon-ebs: The dollar in VAR1: A$Dollar amazon-ebs: The backtick in VAR2: A`Backtick amazon-ebs: The single quote in VAR3: A'SingleQuote amazon-ebs: The double quote in VAR4: A\"DoubleQuote amazon-ebs: The dollar in VAR5 (expanded from a user var): My$tring"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/provisioners/powershell",
  "text": "provisioner \"powershell\" { inline = [\"dir c:\\\\\"] } \nprovisioner \"powershell\" { environment_vars = [\"WINRMPASS=${build.Password}\"] inline = [\"Write-Host \\\"Automatically generated aws password is: $Env:WINRMPASS\\\"\"] } \nprovisioner \"powershell\" { elevated_user = \"Administrator\" elevated_password = build.Password } \nprovisioner \"powershell\" { elevated_user = \"SYSTEM\" elevated_password = \"\" } \nexecution_policy - To run ps scripts on windows packer defaults this to \"bypass\" and wraps the command to run. Setting this to \"none\" will prevent wrapping, allowing to see exit codes on docker for windows. Possible values are bypass, allsigned, default, remotesigned, restricted, undefined, unrestricted, and none.\nstart_retry_timeout (string) - The amount of time to attempt to start the remote process. By default this is 5m or 5 minutes. This setting exists in order to deal with times when SSH may restart, such as a system reboot. Set this to a higher value if reboots take a longer amount of time.\nprovisioner \"powershell\" { execute_command = \"powershell -executionpolicy bypass \\\"& { if (Test-Path variable:global:ProgressPreference){\\\\$ProgressPreference='SilentlyContinue'};. {{.Vars}}; &'{{.Path}}'; exit \\\\$LastExitCode }\\\"\" inline = [ \"Write-Host \\\"Hello from PowerShell\\\"\"] } \nprovisioner \"powershell\" { inline = [ \"Write-Host \\\"A literal dollar `$ must be escaped\\\"\", \"Write-Host \\\"A literal backtick `` must be escaped\\\"\", \"Write-Host \\\"Here `\\\"double quotes`\\\" must be escaped\\\"\", \"Write-Host \\\"Here `'single quotes`' don`'t really need to be\\\"\", \"Write-Host \\\"escaped... but it doesn`'t hurt to do so.\\\"\", ] } \n==> amazon-ebs: Provisioning with Powershell... ==> amazon-ebs: Provisioning with powershell script: /var/folders/15/d0f7gdg13rnd1cxp7tgmr55c0000gn/T/packer-powershell-provisioner508190439 amazon-ebs: A literal dollar $ must be escaped amazon-ebs: A literal backtick ` must be escaped amazon-ebs: Here \"double quotes\" must be escaped amazon-ebs: Here 'single quotes' don't really need to be amazon-ebs: escaped... but it doesn't hurt to do so. \nvariable \"psvar\" { type = string default = \"My$tring\" } build { sources = [\"source.amazon-ebs.example\"] provisioner \"powershell\" { elevated_user = \"Administrator\" elevated_password = \"Super$3cr3t!\" inline = [\"Write-Output \\\"The dollar in the elevated_password is interpreted correctly\\\"\"] } provisioner \"powershell\" { environment_vars = [ \"VAR1=A$Dollar\", \"VAR2=A`Backtick\", \"VAR3=A'SingleQuote\", \"VAR4=A\\\"DoubleQuote\", \"VAR5=${var.psvar}\", ] inline = [ \"Write-Output \\\"In the following examples the special character is interpreted correctly:\\\"\", \"Write-Output \\\"The dollar in VAR1: $Env:VAR1\\\"\", \"Write-Output \\\"The backtick in VAR2: $Env:VAR2\\\"\", \"Write-Output \\\"The single quote in VAR3: $Env:VAR3\\\"\", \"Write-Output \\\"The double quote in VAR4: $Env:VAR4\\\"\", \"Write-Output \\\"The dollar in VAR5 (expanded from a user var): $Env:VAR5\\\"\", ] } } \n==> amazon-ebs: Provisioning with Powershell... ==> amazon-ebs: Provisioning with powershell script: /var/folders/15/d0f7gdg13rnd1cxp7tgmr55c0000gn/T/packer-powershell-provisioner961728919 amazon-ebs: The dollar in the elevated_password is interpreted correctly ==> amazon-ebs: Provisioning with Powershell... ==> amazon-ebs: Provisioning with powershell script: /var/folders/15/d0f7gdg13rnd1cxp7tgmr55c0000gn/T/packer-powershell-provisioner142826554 amazon-ebs: In the following examples the special character is interpreted correctly: amazon-ebs: The dollar in VAR1: A$Dollar amazon-ebs: The backtick in VAR2: A`Backtick amazon-ebs: The single quote in VAR3: A'SingleQuote amazon-ebs: The double quote in VAR4: A\"DoubleQuote amazon-ebs: The dollar in VAR5 (expanded from a user var): My$tring"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/provisioners/powershell",
  "text": "{ \"type\": \"powershell\", \"inline\": [\"dir c:\\\\\"] } \nThis is a template engine. Therefore, you may use user variables and template functions in this field. If you are running on AWS, Azure, Google Compute, or OpenStack and would like to access the generated password that Packer uses to connect to the instance via WinRM, you can use the template variable {{.WinRMPassword}} to set this as an environment variable. For example:\n{ \"type\": \"powershell\", \"environment_vars\": \"WINRMPASS={{.WinRMPassword}}\", \"inline\": [\"Write-Host \\\"Automatically generated aws password is: $Env:WINRMPASS\\\"\"] }, \nThis is a template engine. Therefore, you may use user variables and template functions in this field. If you are running a build on AWS, Azure, Google Compute, or OpenStack and would like to run using the generated password that Packer uses to connect to the instance via WinRM, you may do so by using the template variable {{.WinRMPassword}}. For example:\n\"elevated_user\": \"Administrator\", \"elevated_password\": \"{{.WinRMPassword}}\", \n\"elevated_user\": \"SYSTEM\", \"elevated_password\": \"\", \nexecution_policy - To run ps scripts on windows packer defaults this to \"bypass\" and wraps the command to run. Setting this to \"none\" will prevent wrapping, allowing to see exit codes on docker for windows. Possible values are \"bypass\", \"allsigned\", \"default\", \"remotesigned\", \"restricted\", \"undefined\", \"unrestricted\", \"none\".\n{ \"type\": \"shell\", \"script\": \"script.sh\", \"override\": { \"vmware-iso\": { \"execute_command\": \"echo 'password' | sudo -S bash {{.Path}}\" } } } \nPACKER_HTTP_ADDR If using a builder that provides an http server for file transfer (such as hyperv, parallels, qemu, virtualbox, and vmware), this will be set to the address. You can use this address in your provisioner to download large files over http. This may be useful if you're experiencing slower speeds using the default file provisioner. A file provisioner using the winrm communicator may experience these types of difficulties.\n\"provisioners\": [ { \"type\": \"powershell\", \"execute_command\": \"powershell -executionpolicy bypass \\\"& { if (Test-Path variable:global:ProgressPreference){\\\\$ProgressPreference='SilentlyContinue'};. {{.Vars}}; &'{{.Path}}'; exit \\\\$LastExitCode }\\\"\", \"inline\": [ \"Write-Host \\\"Hello from PowerShell\\\"\", ] } ] \n\"provisioners\": [ { \"type\": \"powershell\", \"inline\": [ \"Write-Host \\\"A literal dollar `$ must be escaped\\\"\", \"Write-Host \\\"A literal backtick `` must be escaped\\\"\", \"Write-Host \\\"Here `\\\"double quotes`\\\" must be escaped\\\"\", \"Write-Host \\\"Here `'single quotes`' don`'t really need to be\\\"\", \"Write-Host \\\"escaped... but it doesn`'t hurt to do so.\\\"\", ] }, \n==> amazon-ebs: Provisioning with Powershell... ==> amazon-ebs: Provisioning with powershell script: /var/folders/15/d0f7gdg13rnd1cxp7tgmr55c0000gn/T/packer-powershell-provisioner508190439 amazon-ebs: A literal dollar $ must be escaped amazon-ebs: A literal backtick ` must be escaped amazon-ebs: Here \"double quotes\" must be escaped amazon-ebs: Here 'single quotes' don't really need to be amazon-ebs: escaped... but it doesn't hurt to do so. \n{ \"variables\": { \"psvar\": \"My$tring\" }, ... \"provisioners\": [ { \"type\": \"powershell\", \"elevated_user\": \"Administrator\", \"elevated_password\": \"Super$3cr3t!\", \"inline\": \"Write-Output \\\"The dollar in the elevated_password is interpreted correctly\\\"\" }, { \"type\": \"powershell\", \"environment_vars\": [ \"VAR1=A$Dollar\", \"VAR2=A`Backtick\", \"VAR3=A'SingleQuote\", \"VAR4=A\\\"DoubleQuote\", \"VAR5={{user `psvar`}}\" ], \"inline\": [ \"Write-Output \\\"In the following examples the special character is interpreted correctly:\\\"\", \"Write-Output \\\"The dollar in VAR1: $Env:VAR1\\\"\", \"Write-Output \\\"The backtick in VAR2: $Env:VAR2\\\"\", \"Write-Output \\\"The single quote in VAR3: $Env:VAR3\\\"\", \"Write-Output \\\"The double quote in VAR4: $Env:VAR4\\\"\", \"Write-Output \\\"The dollar in VAR5 (expanded from a user var): $Env:VAR5\\\"\" ] } ] ... } \n==> amazon-ebs: Provisioning with Powershell... ==> amazon-ebs: Provisioning with powershell script: /var/folders/15/d0f7gdg13rnd1cxp7tgmr55c0000gn/T/packer-powershell-provisioner961728919 amazon-ebs: The dollar in the elevated_password is interpreted correctly ==> amazon-ebs: Provisioning with Powershell... ==> amazon-ebs: Provisioning with powershell script: /var/folders/15/d0f7gdg13rnd1cxp7tgmr55c0000gn/T/packer-powershell-provisioner142826554 amazon-ebs: In the following examples the special character is interpreted correctly: amazon-ebs: The dollar in VAR1: A$Dollar amazon-ebs: The backtick in VAR2: A`Backtick amazon-ebs: The single quote in VAR3: A'SingleQuote amazon-ebs: The double quote in VAR4: A\"DoubleQuote amazon-ebs: The dollar in VAR5 (expanded from a user var): My$tring"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/provisioners/shell-local",
  "text": "Shell (Local) - Provisioners | Packer\nshell-local will run a shell script of your choosing on the machine where Packer is being run - in other words, shell-local will run the shell script on your build server, or your desktop, etc., rather than the remote/guest machine being provisioned by Packer.\nThe remote shell provisioner executes shell scripts on a remote machine.\nsource \"file\" \"example\" { content = \"example content\" } build { source \"source.file.example\" { target = \"./test_artifact.txt\" } provisioner \"shell-local\" { inline = [\"echo foo\"] } } \nThe reference of available configuration options is listed below. The only required element is command.\ncommand (string) - This is a single command to execute. It will be written to a temporary file and run using the execute_command call below. If you are building a windows vm on AWS, Azure, Google Compute, or OpenStack and would like to access the generated password that Packer uses to connect to the instance via WinRM, you can use the template variable {{.WinRMPassword}} to set this as an environment variable.\ninline (array of strings) - This is an array of commands to execute. The commands are concatenated by newlines and turned into a single file, so they are all executed within the same context. This allows you to change directories in one command and use something in the directory in the next and so on. Inline scripts are the easiest way to pull off simple tasks within the machine in which Packer is running.\nenvironment_vars (array of strings) - An array of key/value pairs to inject prior to the execute_command. The format should be key=value. Packer injects some environmental variables by default into the environment, as well, which are covered in the section below. If you are building a windows vm on AWS, Azure, Google Compute, or OpenStack and would like to access the generated password that Packer uses to connect to the instance via WinRM, you can use the template variable {{.WinRMPassword}} to set this as an environment variable. For example: \"environment_vars\": \"WINRMPASS={{.WinRMPassword}}\"\nexecute_command (array of strings) - The command used to execute the script. By default this is [\"/bin/sh\", \"-c\", \"{{.Vars}}\", \"{{.Script}}\"] on unix and [\"cmd\", \"/c\", \"{{.Vars}}\", \"{{.Script}}\"] on windows. This is treated as a template engine. There are two available variables: Script, which is the path to the script to run, and Vars, which is the list of environment_vars, if configured.\nIf you choose to set this option, make sure that the first element in the array is the shell program you want to use (for example, \"sh\"), and a later element in the array must be {{.Script}}.\nThis option provides you a great deal of flexibility. You may choose to provide your own shell program, for example \"/usr/local/bin/zsh\" or even \"powershell.exe\". However, with great power comes great responsibility - these commands are not officially supported and things like environment variables may not work if you use a different shell than the default.\nFor backwards compatibility, you may also use {{.Command}}, but it is decoded the same way as {{.Script}}. We recommend using {{.Script}} for the sake of clarity, as even when you set only a single command to run, Packer writes it to a temporary file and then runs it as a script.\nIf you are building a windows vm on AWS, Azure, Google Compute, or OpenStack and would like to access the generated password that Packer uses to connect to the instance via WinRM, you can use the template variable {{.WinRMPassword}} to set this as an environment variable.\nuse_linux_pathing (bool) - This is only relevant to windows hosts. If you are running Packer in a Windows environment with the Windows Subsystem for Linux feature enabled, and would like to invoke a bash script rather than invoking a Cmd script, you'll need to set this flag to true; it tells Packer to use the linux subsystem path for your script rather than the Windows path. (e.g. /mnt/c/path/to/your/file instead of C:/path/to/your/file). Please see the example below for more guidance on how to use this feature. If you are not on a Windows host, or you do not intend to use the shell-local provisioner to run a bash script, please ignore this option.\nThe shell-local provisioner was designed with the idea of allowing you to run commands in your local operating system's native shell. For Windows, we've assumed in our defaults that this is Cmd. However, it is possible to run a bash script as part of the Windows Linux Subsystem from the shell-local provisioner, by modifying the execute_command and the use_linux_pathing options in the provisioner config.\nPlease note that the WSL is a beta feature, and this tool is not guaranteed to work as you expect it to.\nsource \"null\" \"example\" { communicator = \"none\" } build { sources = [ \"source.null.example\" ] provisioner \"shell-local\"{ environment_vars = [\"PROVISIONERTEST=ProvisionerTest1\"] execute_command = [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"] use_linux_pathing = true scripts = [\"C:/Users/me/scripts/example_bash.sh\"] } provisioner \"shell-local\"{ environment_vars = [\"PROVISIONERTEST=ProvisionerTest2\"] execute_command = [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"] use_linux_pathing = true script = \"C:/Users/me/scripts/example_bash.sh\" } } \nWhether you use the inline option, or pass it a direct script or scripts, it is important to understand a few things about how the shell-local provisioner works to run it safely and easily. This understanding will save you much time in the process.\nThe shell-local script(s) you pass are run once per builder. That means that if you have an amazon-ebs builder and a docker builder, your script will be run twice. If you have 3 builders, it will run 3 times, once for each builder.\nIf any provisioner fails, the packer build stops and all interim artifacts are cleaned up.\nExample of running a .cmd file on windows:\nprovisioner \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest1\"] scripts = [\"./scripts/test_cmd.cmd\"] } \nContents of \"test_cmd.cmd\":\nExample of running an inline command on windows: Required customization: tempfile_extension\nprovisioner \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest2\"], tempfile_extension = \".cmd\", inline = [echo \"%SHELLLOCALTEST%\"] } \nExample of running a bash command on windows using WSL: Required customizations: use_linux_pathing and execute_command\nprovisioner \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest3\"], execute_command = [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"] use_linux_pathing = true script = \"./scripts/example_bash.sh\" } \nExample of running a powershell script on windows: Required customizations: env_var_format and execute_command\nprovisioner \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest4\"] execute_command = [\"powershell.exe\", \"{{.Vars}} {{.Script}}\"] env_var_format = \"$env:%s=\\\"%s\\\"; \" script = \"./scripts/example_ps.ps1\" } \nExample of running a powershell script on windows as \"inline\": Required customizations: env_var_format, tempfile_extension, and execute_command\nprovisioner \"shell-local\" { tempfile_extension = \".ps1\" environment_vars = [\"SHELLLOCALTEST=ShellTest5\"] execute_command = [\"powershell.exe\", \"{{.Vars}} {{.Script}}\"] env_var_format = \"$env:%s=\\\"%s\\\"; \" inline = [\"write-output $env:SHELLLOCALTEST\"] } \nExample of running a Shell script on unix:\nprovisioner \"shell-local\" { environment_vars = [\"PROVISIONERTEST=ProvisionerTest1\"] scripts = [\"./scripts/example_bash.sh\"] } \nExample of running a Shell script \"inline\" on unix:\nprovisioner \"shell-local\" { environment_vars = [\"PROVISIONERTEST=ProvisionerTest2\"] inline = [\"echo hello\", \"echo $PROVISIONERTEST\"] } \nExample of running a Python script on unix:\nprovisioner \"shell-local\" { script = \"hello.py\" environment_vars = [\"HELLO_USER=packeruser\"] execute_command = [ \"/bin/sh\", \"-c\", \"{{.Vars}} /usr/local/bin/python {{.Script}}\" ] } "
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/provisioners/shell-local",
  "text": "{ \"builders\": [ { \"type\": \"file\", \"name\": \"example\", \"target\": \"./test_artifact.txt\", \"content\": \"example content\" } ], \"provisioners\": [ { \"type\": \"shell-local\", \"inline\": [\"echo foo\"] } ] } \nThe reference of available configuration options is listed below. The only required element is \"command\".\ncommand (string) - This is a single command to execute. It will be written to a temporary file and run using the execute_command call below. If you are building a windows vm on AWS, Azure, Google Compute, or OpenStack and would like to access the generated password that Packer uses to connect to the instance via WinRM, you can use the template variable {{.WinRMPassword}} to set this as an environment variable.\ninline (array of strings) - This is an array of commands to execute. The commands are concatenated by newlines and turned into a single file, so they are all executed within the same context. This allows you to change directories in one command and use something in the directory in the next and so on. Inline scripts are the easiest way to pull off simple tasks within the machine in which packer is running.\nenvironment_vars (array of strings) - An array of key/value pairs to inject prior to the execute_command. The format should be key=value. Packer injects some environmental variables by default into the environment, as well, which are covered in the section below. If you are building a windows vm on AWS, Azure, Google Compute, or OpenStack and would like to access the generated password that Packer uses to connect to the instance via WinRM, you can use the template variable {{.WinRMPassword}} to set this as an environment variable. For example: \"environment_vars\": \"WINRMPASS={{.WinRMPassword}}\"\nexecute_command (array of strings) - The command used to execute the script. By default this is [\"/bin/sh\", \"-c\", \"{{.Vars}}\", \"{{.Script}}\"] on unix and [\"cmd\", \"/c\", \"{{.Vars}}\", \"{{.Script}}\"] on windows. This is treated as a template engine. There are two available variables: Script, which is the path to the script to run, and Vars, which is the list of environment_vars, if configured.\nIf you are building a windows vm on AWS, Azure, Google Compute, or OpenStack and would like to access the generated password that Packer uses to connect to the instance via WinRM, you can use the template variable {{.WinRMPassword}} to set this as an environment variable.\nuse_linux_pathing (bool) - This is only relevant to windows hosts. If you are running Packer in a Windows environment with the Windows Subsystem for Linux feature enabled, and would like to invoke a bash script rather than invoking a Cmd script, you'll need to set this flag to true; it tells Packer to use the linux subsystem path for your script rather than the Windows path. (e.g. /mnt/c/path/to/your/file instead of C:/path/to/your/file). Please see the example below for more guidance on how to use this feature. If you are not on a Windows host, or you do not intend to use the shell-local provisioner to run a bash script, please ignore this option.\n{ \"builders\": [ { \"type\": \"null\", \"communicator\": \"none\" } ], \"provisioners\": [ { \"type\": \"shell-local\", \"environment_vars\": [\"PROVISIONERTEST=ProvisionerTest1\"], \"execute_command\": [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"], \"use_linux_pathing\": true, \"scripts\": [\"C:/Users/me/scripts/example_bash.sh\"] }, { \"type\": \"shell-local\", \"environment_vars\": [\"PROVISIONERTEST=ProvisionerTest2\"], \"execute_command\": [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"], \"use_linux_pathing\": true, \"script\": \"C:/Users/me/scripts/example_bash.sh\" } ] } \nPACKER_HTTP_ADDR If using a builder that provides an http server for file transfer (such as hyperv, parallels, qemu, virtualbox, and vmware), this will be set to the address. You can use this address in your provisioner to download large files over http. This may be useful if you're experiencing slower speeds using the default file provisioner. A file provisioner using the winrm communicator may experience these types of difficulties.\nExample of running a .cmd file on windows:\n{ \"type\": \"shell-local\", \"environment_vars\": [\"SHELLLOCALTEST=ShellTest1\"], \"scripts\": [\"./scripts/test_cmd.cmd\"] } \nExample of running an inline command on windows: Required customization: tempfile_extension\n{ \"type\": \"shell-local\", \"environment_vars\": [\"SHELLLOCALTEST=ShellTest2\"], \"tempfile_extension\": \".cmd\", \"inline\": [\"echo %SHELLLOCALTEST%\"] } \nExample of running a bash command on windows using WSL: Required customizations: use_linux_pathing and execute_command\n{ \"type\": \"shell-local\", \"environment_vars\": [\"SHELLLOCALTEST=ShellTest3\"], \"execute_command\": [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"], \"use_linux_pathing\": true, \"script\": \"./scripts/example_bash.sh\" } \nContents of \"example_bash.sh\":\nExample of running a powershell script on windows: Required customizations: env_var_format and execute_command\n{ \"type\": \"shell-local\", \"environment_vars\": [\"SHELLLOCALTEST=ShellTest4\"], \"execute_command\": [\"powershell.exe\", \"{{.Vars}} {{.Script}}\"], \"env_var_format\": \"$env:%s=\\\"%s\\\"; \", \"script\": \"./scripts/example_ps.ps1\" } \nExample of running a powershell script on windows as \"inline\": Required customizations: env_var_format, tempfile_extension, and execute_command\n{ \"type\": \"shell-local\", \"tempfile_extension\": \".ps1\", \"environment_vars\": [\"SHELLLOCALTEST=ShellTest5\"], \"execute_command\": [\"powershell.exe\", \"{{.Vars}} {{.Script}}\"], \"env_var_format\": \"$env:%s=\\\"%s\\\"; \", \"inline\": [\"write-output $env:SHELLLOCALTEST\"] } \nExample of running a bash script on unix:\n{ \"type\": \"shell-local\", \"environment_vars\": [\"PROVISIONERTEST=ProvisionerTest1\"], \"scripts\": [\"./scripts/example_bash.sh\"] } \nExample of running a bash \"inline\" on unix:\n{ \"type\": \"shell-local\", \"environment_vars\": [\"PROVISIONERTEST=ProvisionerTest2\"], \"inline\": [\"echo hello\", \"echo $PROVISIONERTEST\"] } \nExample of running a python script on unix:\n{ \"type\": \"shell-local\", \"script\": \"hello.py\", \"environment_vars\": [\"HELLO_USER=packeruser\"], \"execute_command\": [ \"/bin/sh\", \"-c\", \"{{.Vars}} /usr/local/bin/python {{.Script}}\" ] } \nWhere \"hello.py\" contains:\nimport os print('Hello, %s!' % os.getenv(\"HELLO_USER\"))"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/provisioners/shell-local",
  "text": "{ \"type\": \"shell-local\", \"command\": \"echo foo\" } \nThe reference of available configuration options is listed below. The only required element is \"command\".\ncommand (string) - This is a single command to execute. It will be written to a temporary file and run using the execute_command call below. If you are building a windows vm on AWS, Azure, Google Compute, or OpenStack and would like to access the generated password that Packer uses to connect to the instance via WinRM, you can use the template variable {{.WinRMPassword}} to set this as an environment variable.\ninline (array of strings) - This is an array of commands to execute. The commands are concatenated by newlines and turned into a single file, so they are all executed within the same context. This allows you to change directories in one command and use something in the directory in the next and so on. Inline scripts are the easiest way to pull off simple tasks within the machine in which packer is running.\nenvironment_vars (array of strings) - An array of key/value pairs to inject prior to the execute_command. The format should be key=value. Packer injects some environmental variables by default into the environment, as well, which are covered in the section below. If you are building a windows vm on AWS, Azure, Google Compute, or OpenStack and would like to access the generated password that Packer uses to connect to the instance via WinRM, you can use the template variable {{.WinRMPassword}} to set this as an environment variable. For example: \"environment_vars\": \"WINRMPASS={{.WinRMPassword}}\"\nexecute_command (array of strings) - The command used to execute the script. By default this is [\"/bin/sh\", \"-c\", \"{{.Vars}}\", \"{{.Script}}\"] on unix and [\"cmd\", \"/c\", \"{{.Vars}}\", \"{{.Script}}\"] on windows. This is treated as a template engine. There are two available variables: Script, which is the path to the script to run, and Vars, which is the list of environment_vars, if configured.\nIf you are building a windows vm on AWS, Azure, Google Compute, or OpenStack and would like to access the generated password that Packer uses to connect to the instance via WinRM, you can use the template variable {{.WinRMPassword}} to set this as an environment variable.\nuse_linux_pathing (bool) - This is only relevant to windows hosts. If you are running Packer in a Windows environment with the Windows Subsystem for Linux feature enabled, and would like to invoke a bash script rather than invoking a Cmd script, you'll need to set this flag to true; it tells Packer to use the linux subsystem path for your script rather than the Windows path. (e.g. /mnt/c/path/to/your/file instead of C:/path/to/your/file). Please see the example below for more guidance on how to use this feature. If you are not on a Windows host, or you do not intend to use the shell-local provisioner to run a bash script, please ignore this option.\n{ \"type\": \"shell\", \"script\": \"script.sh\", \"override\": { \"vmware-iso\": { \"execute_command\": \"echo 'password' | sudo -S bash {{.Path}}\" } } } \n{ \"builders\": [ { \"type\": \"null\", \"communicator\": \"none\" } ], \"provisioners\": [ { \"type\": \"shell-local\", \"environment_vars\": [\"PROVISIONERTEST=ProvisionerTest1\"], \"execute_command\": [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"], \"use_linux_pathing\": true, \"scripts\": [\"C:/Users/me/scripts/example_bash.sh\"] }, { \"type\": \"shell-local\", \"environment_vars\": [\"PROVISIONERTEST=ProvisionerTest2\"], \"execute_command\": [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"], \"use_linux_pathing\": true, \"script\": \"C:/Users/me/scripts/example_bash.sh\" } ] } \nPACKER_HTTP_ADDR If using a builder that provides an http server for file transfer (such as hyperv, parallels, qemu, virtualbox, and vmware), this will be set to the address. You can use this address in your provisioner to download large files over http. This may be useful if you're experiencing slower speeds using the default file provisioner. A file provisioner using the winrm communicator may experience these types of difficulties.\nExample of running a .cmd file on windows:\n{ \"type\": \"shell-local\", \"environment_vars\": [\"SHELLLOCALTEST=ShellTest1\"], \"scripts\": [\"./scripts/test_cmd.cmd\"] } \nExample of running an inline command on windows: Required customization: tempfile_extension\n{ \"type\": \"shell-local\", \"environment_vars\": [\"SHELLLOCALTEST=ShellTest2\"], \"tempfile_extension\": \".cmd\", \"inline\": [\"echo %SHELLLOCALTEST%\"] } \nExample of running a bash command on windows using WSL: Required customizations: use_linux_pathing and execute_command\n{ \"type\": \"shell-local\", \"environment_vars\": [\"SHELLLOCALTEST=ShellTest3\"], \"execute_command\": [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"], \"use_linux_pathing\": true, \"script\": \"./scripts/example_bash.sh\" } \nContents of \"example_bash.sh\":\nExample of running a powershell script on windows: Required customizations: env_var_format and execute_command\n{ \"type\": \"shell-local\", \"environment_vars\": [\"SHELLLOCALTEST=ShellTest4\"], \"execute_command\": [\"powershell.exe\", \"{{.Vars}} {{.Script}}\"], \"env_var_format\": \"$env:%s=\\\"%s\\\"; \", \"script\": \"./scripts/example_ps.ps1\" } \nExample of running a powershell script on windows as \"inline\": Required customizations: env_var_format, tempfile_extension, and execute_command\n{ \"type\": \"shell-local\", \"tempfile_extension\": \".ps1\", \"environment_vars\": [\"SHELLLOCALTEST=ShellTest5\"], \"execute_command\": [\"powershell.exe\", \"{{.Vars}} {{.Script}}\"], \"env_var_format\": \"$env:%s=\\\"%s\\\"; \", \"inline\": [\"write-output $env:SHELLLOCALTEST\"] } \nExample of running a bash script on unix:\n{ \"type\": \"shell-local\", \"environment_vars\": [\"PROVISIONERTEST=ProvisionerTest1\"], \"scripts\": [\"./scripts/example_bash.sh\"] } \nExample of running a bash \"inline\" on unix:\n{ \"type\": \"shell-local\", \"environment_vars\": [\"PROVISIONERTEST=ProvisionerTest2\"], \"inline\": [\"echo hello\", \"echo $PROVISIONERTEST\"] } \nExample of running a python script on unix:\n{ \"type\": \"shell-local\", \"script\": \"hello.py\", \"environment_vars\": [\"HELLO_USER=packeruser\"], \"execute_command\": [\"/bin/sh\", \"-c\", \"{{.Vars}} /usr/local/bin/python {{.Script}}\"] } \nWhere \"hello.py\" contains:\nimport os print('Hello, %s!' % os.getenv(\"HELLO_USER\"))"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/provisioners/windows-shell",
  "text": "Windows Shell - Provisioners | Packer\nWindows Shell Provisioner\nType: windows-shell\nThe windows-shell Packer provisioner runs commands on a Windows machine using cmd. It assumes it is running over WinRM.\n{ \"type\": \"windows-shell\", \"inline\": [\"dir c:\\\\\"] } \nexecute_command (string) - The command to use to execute the script. By default this is {{ .Vars }}\"{{ .Path }}\". The value of this is treated as template engine. This is a template engine. Therefore, you may use user variables and template functions in this field. In addition, there are two available extra variables:\nPath is the path to the script to run\nVars is the list of environment_vars, if configured.\nremote_path (string) - The path where the script will be uploaded to in the machine. This defaults to \"c:/Windows/Temp/script.bat\". This value must be a writable location and any parent directories must already exist."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/provisioners/windows-shell",
  "text": "provisioner \"windows-shell\" { inline = [\"dir c:\\\\\"] } "
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/provisioners/windows-shell",
  "text": "{ \"type\": \"windows-shell\", \"inline\": [\"dir c:\\\\\"] } "
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/provisioners/custom",
  "text": "Custom - Provisioners | Packer\nPacker is extensible, allowing you to write new provisioners without having to modify the core source code of Packer itself. Documentation for creating new provisioners is covered in the custom provisioners page of the Packer plugin section."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/provisioners/windows-restart",
  "text": "Windows Restart - Provisioners | Packer\nWindows Restart Provisioner\nType: windows-restart\nThe Windows restart provisioner initiates a reboot on a Windows machine and waits for the machine to come back online.\nThe Windows provisioning process often requires multiple reboots, and this provisioner helps to ease that process.\nPacker expects the machine to be ready to continue provisioning after it reboots. Packer detects that the reboot has completed by making an RPC call through the Windows Remote Management (WinRM) service, not by ACPI functions, so Windows must be completely booted in order to continue.\nprovisioner \"windows-restart\" {} \nThe reference of available configuration options is listed below.\ncheck_registry (bool) - if true, checks for several registry keys that indicate that the system is going to reboot. This is useful if an installation kicks off a reboot and you want the provisioner to wait for that reboot to complete before reconnecting. Please note that this option is a beta feature, and we generally recommend that you finish installs that auto-reboot (like Windows Updates) during your autounattend phase before the winrm provisioner connects.\nregistry_keys (array of strings) - if check-registry is true, windows-restart will not reconnect until after all of the listed keys are no longer present in the registry.\ndefault: var DefaultRegistryKeys = []string{ \"HKLM:SOFTWARE\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Component Based Servicing\\\\RebootPending\", \"HKLM:SOFTWARE\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Component Based Servicing\\\\PackagesPending\", \"HKLM:Software\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Component Based Servicing\\\\RebootInProgress\", } \nrestart_command (string) - The command to execute to initiate the restart. By default this is shutdown /r /f /t 0 /c \"packer restart\".\nrestart_check_command (string) - The command to run after executing restart_command to check if the guest machine has restarted. This command will retry until the connection to the guest machine has been restored or restart_timeout has exceeded.\nprovisioner \"windows-restart\" { restart_check_command = \"powershell -command \\\"& {Write-Output 'restarted.'}\\\"\" } \nrestart_timeout (string) - The timeout to wait for the restart. By default this is 5 minutes. Example value: 5m. If you are installing updates or have a lot of startup services, you will probably need to increase this duration."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/provisioners/windows-restart",
  "text": "{ \"type\": \"windows-restart\" } \ncheck_registry (bool) - if true, checks for several registry keys that indicate that the system is going to reboot. This is useful if an installation kicks off a reboot and you want the provisioner to wait for that reboot to complete before reconnecting. Please note that this option is a beta feature, and we generally recommend that you finish installs that auto-reboot (like windows updates) during your autounattend phase before our winrm provisioner connects.\ndefault:\nvar DefaultRegistryKeys = []string{ \"HKLM:SOFTWARE\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Component Based Servicing\\\\RebootPending\", \"HKLM:SOFTWARE\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Component Based Servicing\\\\PackagesPending\", \"HKLM:Software\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Component Based Servicing\\\\RebootInProgress\", } \nrestart_check_command (string) - A command to execute to check if the restart succeeded. This will be done in a loop. Example usage:\n{ \"type\": \"windows-restart\", \"restart_check_command\": \"powershell -command \\\"& {Write-Output 'restarted.'}\\\"\" } "
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/provisioners/custom",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/provisioners/custom",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/provisioners/community-supported",
  "text": "Community - Provisioners | Packer\nThe following provisioners are developed and maintained by various members of the Packer community, not by HashiCorp. For more information on how to use community provisioners, see our docs on extending Packer.\nCommunity Provisioners\nComment Provisioner - Example provisioner that allows you to annotate your build with bubble-text comments.\nWindows Update provisioner - A provisioner for gracefully handling windows updates and the reboots they cause."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/provisioners/community-supported",
  "text": "Windows Update provisioner - A provisioner for gracefully handling windows updates and the reboots they cause."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/provisioners/windows-restart",
  "text": "{ \"type\": \"windows-restart\" } \ncheck_registry (bool) - if true, checks for several registry keys that indicate that the system is going to reboot. This is useful if an installation kicks off a reboot and you want the provisioner to wait for that reboot to complete before reconnecting. Please note that this option is a beta feature, and we generally recommend that you finish installs that auto-reboot (like windows updates) during your autounattend phase before our winrm provisioner connects.\ndefault:\nvar DefaultRegistryKeys = []string{ \"HKLM:SOFTWARE\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Component Based Servicing\\\\RebootPending\", \"HKLM:SOFTWARE\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Component Based Servicing\\\\PackagesPending\", \"HKLM:Software\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Component Based Servicing\\\\RebootInProgress\", } \nrestart_check_command (string) - A command to execute to check if the restart succeeded. This will be done in a loop. Example usage:\n{ \"type\": \"windows-restart\", \"restart_check_command\": \"powershell -command \\\"& {Write-Output 'restarted.'}\\\"\" } "
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/provisioners/community-supported",
  "text": "Windows Update provisioner - A provisioner for gracefully handling windows updates and the reboots they cause."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/post-processors/artifice",
  "text": "Artifice - Post-Processors | Packer\nType: artifice Artifact BuilderId: packer.post-processor.artifice\nThe artifice post-processor overrides the artifact list from an upstream builder or post-processor. All downstream post-processors will see the new artifacts you specify.\nAfter overriding the artifact with artifice, you can use it with other post-processors, including most of the core post-processors and third-party post-processors.\nA major benefit of this is that you can modify builder artifacts using shell-local and pass those modified artifacts into post-processors that may not have worked with the original builder. For example, maybe you want to export a docker container from an amazon-ebs builder and then use Docker-push to put that Docker container into your Docker Hub account.\nArtifice allows you to use the familiar packer workflow to create a fresh, stateless build environment for each build on the infrastructure of your choosing. You can use this to build just about anything: buildpacks, containers, jars, binaries, tarballs, msi installers, and more.\nPlease note that the artifice post-processor will not delete your old artifact files, even if it removes them from the artifact. If you want to delete the old artifact files, you can use the shell-local post-processor to do so.\nArtifice helps you tie together a few other packer features:\nA builder, which spins up a VM (or container) to build your artifact\nA provisioner, which performs the steps to create your artifact\nA file provisioner, which downloads the artifact from the VM\nThe artifice post-processor, which identifies which files have been downloaded from the VM\nAdditional post-processors, which push the artifact to Docker hub, etc.\nYou will want to perform as much work as possible inside the VM. Ideally the only other post-processor you need after artifice is one that uploads your artifact to the appropriate repository.\nThe configuration allows you to specify which files comprise your artifact.\nfiles (array of strings) - A list of files that comprise your artifact. These files must exist on your local disk after the provisioning phase of packer is complete. These will replace any of the builder's original artifacts (such as a VM snapshot).\nkeep_input_artifact (boolean) - if true, do not delete the original artifact files after creating your new artifact. Defaults to true.\nExample Configuration\nThis minimal example:\nSpins up a cloned VMware virtual machine\nInstalls a consul release\nDownloads the consul binary\nPackages it into a .tar.gz file\nUploads it to S3.\nVMX is a fast way to build and test locally, but you can easily substitute another builder.\n{ \"builders\": [ { \"type\": \"vmware-vmx\", \"source_path\": \"/opt/ubuntu-1404-vmware.vmx\", \"ssh_username\": \"vagrant\", \"ssh_password\": \"vagrant\", \"shutdown_command\": \"sudo shutdown -h now\", \"headless\": \"true\", \"skip_compaction\": \"true\" } ], \"provisioners\": [ { \"type\": \"shell\", \"inline\": [ \"sudo apt-get install -y python-pip\", \"sudo pip install ifs\", \"sudo ifs install consul --version=0.5.2\" ] }, { \"type\": \"file\", \"source\": \"/usr/local/bin/consul\", \"destination\": \"consul\", \"direction\": \"download\" } ], \"post-processors\": [ [ { \"type\": \"artifice\", \"files\": [\"consul\"] }, { \"type\": \"compress\", \"output\": \"consul-0.5.2.tar.gz\" }, { \"type\": \"shell-local\", \"inline\": [ \"/usr/local/bin/aws s3 cp consul-0.5.2.tar.gz s3://<s3 path>\" ] } ] ] } \nNotice that there are two sets of square brackets in the post-processor section. This creates a post-processor chain, where the output of the proceeding artifact is passed to subsequent post-processors. If you use only one set of square braces the post-processors will run individually against the build artifact (the vmx file in this case) and it will not have the desired result.\n{ \"post-processors\": [ [ // <--- Start post-processor chain { \"type\": \"artifice\", \"files\": [\"consul\"] }, { \"type\": \"compress\", ... } ], // <--- End post-processor chain { \"type\":\"compress\" // <-- Standalone post-processor } ] } \nYou can create multiple post-processor chains to handle multiple builders (for example, building linux and windows binaries during the same build)."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/post-processors/artifice",
  "text": "Type: artifice\nA major benefit of this is that you can modify builder artifacts using shell-local and pass those modified artifacts into post-processors that may not have worked with the original builder. For example, maybe you want to export a docker container from an amazon-ebs builder and then use Docker-push to put that Docker container into your Docker Hub account.\nYou can create multiple post-processor chains to handle multiple builders (for example, building linux and windows binaries during the same build)."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/post-processors/artifice",
  "text": "Type: artifice\nThe artifice post-processor overrides the artifact list from an upstream builder or post-processor. All downstream post-processors will see the new artifacts you specify. The primary use-case is to build artifacts inside a packer builder -- for example, spinning up an EC2 instance to build a docker container -- and then extracting the docker container and throwing away the EC2 instance.\nAfter overriding the artifact with artifice, you can use it with other post-processors like compress, docker-push, or a third-party post-processor.\nYou can create multiple post-processor chains to handle multiple builders (for example, building linux and windows binaries during the same build)."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/post-processors/checksum",
  "text": "Checksum - Post-Processors | Packer\nType: checksum Artifact BuilderId: packer.post-processor.checksum\nThe checksum post-processor computes specified checksum for the artifact list from an upstream builder or post-processor. All downstream post-processors will see the new artifacts. The primary use-case is compute checksum for artifact to verify it later.\nAfter computes checksum for artifacts, you can use new artifacts with other post-processors like artifice, compress, docker-push, or a third-party post-processor.\n{ \"type\": \"checksum\", \"checksum_types\": [\"sha1\", \"sha256\"], \"output\": \"packer_{{.BuildName}}_{{.ChecksumType}}.checksum\" } \nchecksum_types (array of strings) - An array of strings of checksum types to compute. If empty, Defaults to md5. Allowed values are:\nmd5\nsha1\nsha224\nsha256\nsha384\nsha512\noutput (string) - Specify filename to store checksums. This defaults to packer_{{.BuildName}}_{{.BuilderType}}_{{.ChecksumType}}.checksum. For example, if you had a builder named database, you might see the file written as packer_database_docker_md5.checksum. This is treated as a template engine. Therefore, you may use user variables and template functions in this field. The following special variables are also available to use in the output template:\nBuildName: The name of the builder that produced the artifact.\nBuilderType: The type of builder used to produce the artifact.\nChecksumType: The type of checksums the file contains. This should be used if you have more than one value in checksum_types."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/post-processors/checksum",
  "text": "Type: checksum"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/provisioners/ansible-local",
  "text": "Ansible Local - Provisioners | Packer\nAnsible Local Provisioner\nType: ansible-local\nThe ansible-local Packer provisioner will run ansible in ansible's \"local\" mode on the remote/guest VM using Playbook and Role files that exist on the guest VM. This means ansible must be installed on the remote/guest VM. Playbooks and Roles can be uploaded from your build machine (the one running Packer) to the vm. Ansible is then run on the guest machine in local mode via the ansible-playbook command.\nNote: Ansible will not be installed automatically by this provisioner. This provisioner expects that Ansible is already installed on the guest/remote machine. It is common practice to use the shell provisioner before the Ansible provisioner to do this.\n{ \"type\": \"ansible-local\", \"playbook_file\": \"local.yml\" } \nplaybook_file (string) - The playbook file to be executed by ansible. This file must exist on your local system and will be uploaded to the remote machine. This option is exclusive with playbook_files.\nplaybook_files (array of strings) - The playbook files to be executed by ansible. These files must exist on your local system. If the files don't exist in the playbook_dir or you don't set playbook_dir they will be uploaded to the remote machine. This option is exclusive with playbook_file.\ncommand (string) - The command to invoke ansible. Defaults to \"ANSIBLE_FORCE_COLOR=1 PYTHONUNBUFFERED=1 ansible-playbook\". Note, This disregards the value of -color when passed to packer build. To disable colors, set this to PYTHONUNBUFFERED=1 ansible-playbook.\nextra_arguments (array of strings) - An array of extra arguments to pass to the ansible command. By default, this is empty. These arguments will be passed through a shell and arguments should be quoted accordingly. Usage example:\n\"extra_arguments\": [ \"--extra-vars \\\"Region={{user `Region`}} Stage={{user `Stage`}}\\\"\" ] \ninventory_groups (string) - A comma-separated list of groups to which packer will assign the host 127.0.0.1. A value of my_group_1,my_group_2 will generate an Ansible inventory like:\n[my_group_1] 127.0.0.1 [my_group_2] 127.0.0.1 \ninventory_file (string) - The inventory file to be used by ansible. This file must exist on your local system and will be uploaded to the remote machine.\nWhen using an inventory file, it's also required to --limit the hosts to the specified host you're building. The --limit argument can be provided in the extra_arguments option.\nAn example inventory file may look like:\n[chi-dbservers] db-01 ansible_connection=local db-02 ansible_connection=local [chi-appservers] app-01 ansible_connection=local app-02 ansible_connection=local [chi:children] chi-dbservers chi-appservers [dbservers:children] chi-dbservers [appservers:children] chi-appservers \nplaybook_dir (string) - a path to the complete ansible directory structure on your local system to be copied to the remote machine as the staging_directory before all other files and directories.\nplaybook_paths (array of strings) - An array of directories of playbook files on your local system. These will be uploaded to the remote machine under staging_directory/playbooks. By default, this is empty.\ngalaxy_file (string) - A requirements file which provides a way to install roles with the ansible-galaxy cli on the remote machine. By default, this is empty.\ngalaxy_command (string) - The command to invoke ansible-galaxy. By default, this is ansible-galaxy.\ngroup_vars (string) - a path to the directory containing ansible group variables on your local system to be copied to the remote machine. By default, this is empty.\nhost_vars (string) - a path to the directory containing ansible host variables on your local system to be copied to the remote machine. By default, this is empty.\nrole_paths (array of strings) - An array of paths to role directories on your local system. These will be uploaded to the remote machine under staging_directory/roles. By default, this is empty.\nstaging_directory (string) - The directory where all the configuration of Ansible by Packer will be placed. By default this is /tmp/packer-provisioner-ansible-local/<uuid>, where <uuid> is replaced with a unique ID so that this provisioner can be run more than once. If you'd like to know the location of the staging directory in advance, you should set this to a known location. This directory doesn't need to exist but must have proper permissions so that the SSH user that Packer uses is able to create directories and write into this folder. If the permissions are not correct, use a shell provisioner prior to this to configure it properly.\nclean_staging_directory (boolean) - If set to true, the content of the staging_directory will be removed after executing ansible. By default, this is set to false.\nIn addition to being able to specify extra arguments using the extra_arguments configuration, the provisioner automatically defines certain commonly useful Ansible variables:\npacker_build_name is set to the name of the build that Packer is running. This is most useful when Packer is making multiple builds and you want to distinguish them slightly when using a common playbook.\npacker_builder_type is the type of the builder that was used to create the machine that the script is running on. This is useful if you want to run only certain parts of the playbook on systems built with certain builders.\npacker_http_addr If using a builder that provides an http server for file transfer (such as hyperv, parallels, qemu, virtualbox, and vmware), this will be set to the address. You can use this address in your provisioner to download large files over http. This may be useful if you're experiencing slower speeds using the default file provisioner. A file provisioner using the winrm communicator may experience these types of difficulties."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/provisioners/chef-client",
  "text": "Chef Client - Provisioners | Packer\nType: chef-client\nThe Chef Client Packer provisioner installs and configures software on machines built by Packer using chef-client. Packer configures a Chef client to talk to a remote Chef Server to provision the machine.\nThe provisioner will even install Chef onto your machine if it isn't already installed, using the official Chef installers provided by Chef.\nThe example below is fully functional. It will install Chef onto the remote machine and run Chef client.\n{ \"type\": \"chef-client\", \"server_url\": \"https://mychefserver.com/\" } \nNote: to properly clean up the Chef node and client the machine on which packer is running must have knife on the path and configured globally, i.e, ~/.chef/knife.rb must be present and configured for the target chef server\nThe reference of available configuration options is listed below. No configuration is actually required.\nchef_environment (string) - The name of the chef_environment sent to the Chef server. By default this is empty and will not use an environment.\nchef_license (string) - As of Chef v15, Chef requires users to accept a license. Defaults to accept-silent when skip_install is false and install_command is unset. Possible values are accept, accept-silent and accept-no-persist. For details see Accepting the Chef License.\nThis is a template engine. Therefore, you may use user variables and template functions in this field.\nconfig_template (string) - Path to a template that will be used for the Chef configuration file. By default Packer only sets configuration it needs to match the settings set in the provisioner configuration. If you need to set configurations that the Packer provisioner doesn't support, then you should use a custom configuration template. See the dedicated \"Chef Configuration\" section below for more details.\nelevated_user and elevated_password (string) - If specified, Chef will be run with elevated privileges using the given Windows user. See the powershell provisioner for the full details.\nencrypted_data_bag_secret_path (string) - The path to the file containing the secret for encrypted data bags. By default, this is empty, so no secret will be available.\nexecute_command (string) - The command used to execute Chef. This has various configuration template variables available. See below for more information.\nguest_os_type (string) - The target guest OS type, either \"unix\" or \"windows\". Setting this to \"windows\" will cause the provisioner to use Windows friendly paths and commands. By default, this is \"unix\".\ninstall_command (string) - The command used to install Chef. This has various configuration template variables available. See below for more information.\njson (object) - An arbitrary mapping of JSON that will be available as node attributes while running Chef.\nknife_command (string) - The command used to run Knife during node clean-up. This has various configuration template variables available. See below for more information.\nnode_name (string) - The name of the node to register with the Chef Server. This is optional and by default is packer-{{uuid}}.\npolicy_group (string) - The name of a policy group that exists on the Chef server. policy_name must also be specified.\npolicy_name (string) - The name of a policy, as identified by the name setting in a Policyfile.rb file. policy_group must also be specified.\nprevent_sudo (boolean) - By default, the configured commands that are executed to install and run Chef are executed with sudo. If this is true, then the sudo will be omitted. This has no effect when guest_os_type is windows.\nrun_list (array of strings) - The run list for Chef. By default this is empty, and will use the run list sent down by the Chef Server.\nserver_url (string) - The URL to the Chef server. This is required.\nskip_clean_client (boolean) - If true, Packer won't remove the client from the Chef server after it is done running. By default, this is false.\nskip_clean_node (boolean) - If true, Packer won't remove the node from the Chef server after it is done running. By default, this is false.\nskip_clean_staging_directory (boolean) - If true, Packer won't remove the Chef staging directory from the machine after it is done running. By default, this is false.\nskip_install (boolean) - If true, Chef will not automatically be installed on the machine using the Chef omnibus installers.\nssl_verify_mode (string) - Set to \"verify_none\" to skip validation of SSL certificates. If not set, this defaults to \"verify_peer\" which validates all SSL certifications.\ntrusted_certs_dir (string) - This is a directory that contains additional SSL certificates to trust. Any certificates in this directory will be added to whatever CA bundle ruby is using. Use this to add self-signed certs for your Chef Server or local HTTP file servers.\nstaging_directory (string) - This is the directory where all the configuration of Chef by Packer will be placed. By default this is /tmp/packer-chef-client when guest_os_type unix and $env:TEMP/packer-chef-client when windows. This directory doesn't need to exist but must have proper permissions so that the user that Packer uses is able to create directories and write into this folder. By default the provisioner will create and chmod 0777 this directory.\nclient_key (string) - Path to client key. If not set, this defaults to a file named client.pem in staging_directory.\nvalidation_client_name (string) - Name of the validation client. If not set, this won't be set in the configuration and the default that Chef uses will be used.\nvalidation_key_path (string) - Path to the validation key for communicating with the Chef Server. This will be uploaded to the remote machine. If this is NOT set, then it is your responsibility via other means (shell provisioner, etc.) to get a validation key to where Chef expects it.\nversion (string) - The version of Chef to be installed. By default this is empty which will install the latest version of Chef.\nBy default, Packer uses a simple Chef configuration file in order to set the options specified for the provisioner. But Chef is a complex tool that supports many configuration options. Packer allows you to specify a custom configuration template if you'd like to set custom configurations.\nThe default value for the configuration template is:\nlog_level :info log_location STDOUT chef_server_url \"{{.ServerUrl}}\" client_key \"{{.ClientKey}}\" chef_license \"{{.ChefLicense}}\" {{if ne .EncryptedDataBagSecretPath \"\"}} encrypted_data_bag_secret \"{{.EncryptedDataBagSecretPath}}\" {{end}} {{if ne .ValidationClientName \"\"}} validation_client_name \"{{.ValidationClientName}}\" {{else}} validation_client_name \"chef-validator\" {{end}} {{if ne .ValidationKeyPath \"\"}} validation_key \"{{.ValidationKeyPath}}\" {{end}} node_name \"{{.NodeName}}\" {{if ne .ChefEnvironment \"\"}} environment \"{{.ChefEnvironment}}\" {{end}} {{if ne .PolicyGroup \"\"}} policy_group \"{{.PolicyGroup}}\" {{end}} {{if ne .PolicyName \"\"}} policy_name \"{{.PolicyName}}\" {{end}} {{if ne .SslVerifyMode \"\"}} ssl_verify_mode :{{.SslVerifyMode}} {{end}} {{if ne .TrustedCertsDir \"\"}} trusted_certs_dir :{{.TrustedCertsDir}} {{end}} \nThis template is a configuration template and has a set of variables available to use:\nChefEnvironment - The Chef environment name.\nChefLicense - The Chef license acceptance value.\nEncryptedDataBagSecretPath - The path to the secret key file to decrypt encrypted data bags.\nNodeName - The node name set in the configuration.\nServerUrl - The URL of the Chef Server set in the configuration.\nSslVerifyMode - Whether Chef SSL verify mode is on or off.\nTrustedCertsDir - Path to dir with trusted certificates.\nValidationClientName - The name of the client used for validation.\nValidationKeyPath - Path to the validation key, if it is set.\nBy default, Packer uses the following command (broken across multiple lines for readability) to execute Chef:\n{{if .Sudo}}sudo {{end}}chef-client \\ --no-color \\ -c {{.ConfigPath}} \\ -j {{.JsonPath}} \nWhen guest_os_type is set to \"windows\", Packer uses the following command to execute Chef. The full path to Chef is required because the PATH environment variable changes don't immediately propagate to running processes.\nc:/opscode/chef/bin/chef-client.bat \\ --no-color \\ -c {{.ConfigPath}} \\ -j {{.JsonPath}} \nThis command can be customized using the execute_command configuration. As you can see from the default value above, the value of this configuration can contain various template variables, defined below:\nConfigPath - The path to the Chef configuration file.\nJsonPath - The path to the JSON attributes file for the node.\nSudo - A boolean of whether to sudo the command or not, depending on the value of the prevent_sudo configuration.\nBy default, Packer uses the following command (broken across multiple lines for readability) to install Chef. This command can be customized if you want to install Chef in another way.\ncurl -L https://omnitruck.chef.io/chef/install.sh | \\ {{if .Sudo}}sudo{{end}} bash \nWhen guest_os_type is set to \"windows\", Packer uses the following command to install the latest version of Chef:\npowershell.exe -Command \"(New-Object System.Net.WebClient).DownloadFile('http://chef.io/chef/install.msi', 'C:\\\\Windows\\\\Temp\\\\chef.msi');Start-Process 'msiexec' -ArgumentList '/qb /i C:\\\\Windows\\\\Temp\\\\chef.msi' -NoNewWindow -Wait\" \nThis command can be customized using the install_command configuration.\nBy default, Packer uses the following command (broken across multiple lines for readability) to execute Chef:\n{{if .Sudo}}sudo {{end}}knife \\ {{.Args}} \\ {{.Flags}} \nWhen guest_os_type is set to \"windows\", Packer uses the following command to execute Chef. The full path to Chef is required because the PATH environment variable changes don't immediately propagate to running processes.\nc:/opscode/chef/bin/knife.bat \\ {{.Args}} \\ {{.Flags}} \nThis command can be customized using the knife_command configuration. As you can see from the default value above, the value of this configuration can contain various template variables, defined below:\nArgs - The command arguments that are getting passed to the Knife command.\nFlags - The command flags that are getting passed to the Knife command..\nSudo - A boolean of whether to sudo the command or not, depending on the value of the prevent_sudo configuration.\nThe chef-client provisioner will chmod the directory with your Chef keys to 777. This is to ensure that Packer can upload and make use of that directory. However, once the machine is created, you usually don't want to keep these directories with those permissions. To change the permissions on the directories, append a shell provisioner after Chef to modify them.\nChef Client Local Mode - Simple\nThe following example shows how to run the chef-client provisioner in local mode.\nPacker variables\nSet the necessary Packer variables using environment variables or provide a var file.\n\"variables\": { \"chef_dir\": \"/tmp/packer-chef-client\" } \nSetup the chef-client provisioner\nMake sure we have the correct directories and permissions for the chef-client provisioner. You will need to bootstrap the Chef run by providing the necessary cookbooks using Berkshelf or some other means.\n\"provisioners\": [ ... { \"type\": \"shell\", \"inline\": [ \"mkdir -p {{user `chef_dir`}}\" ] }, { \"type\": \"file\", \"source\": \"./roles\", \"destination\": \"{{user `chef_dir`}}\" }, { \"type\": \"file\", \"source\": \"./cookbooks\", \"destination\": \"{{user `chef_dir`}}\" }, { \"type\": \"file\", \"source\": \"./data_bags\", \"destination\": \"{{user `chef_dir`}}\" }, { \"type\": \"file\", \"source\": \"./environments\", \"destination\": \"{{user `chef_dir`}}\" }, { \"type\": \"file\", \"source\": \"./scripts/install_chef.sh\", \"destination\": \"{{user `chef_dir`}}/install_chef.sh\" }, { \"type\": \"chef-client\", \"install_command\": \"sudo bash {{user `chef_dir`}}/install_chef.sh\", \"server_url\": \"http://localhost:8889\", \"config_template\": \"./config/client.rb.template\", \"run_list\": [ \"role[testing]\" ], \"skip_clean_node\": true, \"skip_clean_client\": true } ... ] \nAnd ./config/client.rb.template referenced by the above configuration:\nlog_level :info log_location STDOUT local_mode true chef_zero.enabled true ssl_verify_mode \"verify_peer\" role_path \"{{user `chef_dir`}}/roles\" data_bag_path \"{{user `chef_dir`}}/data_bags\" environment_path \"{{user `chef_dir`}}/environments\" cookbook_path [ \"{{user `chef_dir`}}/cookbooks\" ] \nChef Client Local Mode - Passing variables\nThe following example shows how to run the chef-client provisioner in local mode, while passing a run_list using a variable.\nLocal environment variables\n# Machine's Chef directory export PACKER_CHEF_DIR=/var/chef-packer # Comma separated run_list export PACKER_CHEF_RUN_LIST=\"recipe[apt],recipe[nginx]\" \nPacker variables\nSet the necessary Packer variables using environment variables or provide a var file.\n\"variables\": { \"chef_dir\": \"{{env `PACKER_CHEF_DIR`}}\", \"chef_run_list\": \"{{env `PACKER_CHEF_RUN_LIST`}}\", \"chef_client_config_tpl\": \"{{env `PACKER_CHEF_CLIENT_CONFIG_TPL`}}\", \"packer_chef_bootstrap_dir\": \"{{env `PACKER_CHEF_BOOTSTRAP_DIR`}}\" , \"packer_uid\": \"{{env `PACKER_UID`}}\", \"packer_gid\": \"{{env `PACKER_GID`}}\" } \nSetup the chef-client provisioner\nMake sure we have the correct directories and permissions for the chef-client provisioner. You will need to bootstrap the Chef run by providing the necessary cookbooks using Berkshelf or some other means.\n({ \"type\": \"file\", \"source\": \"{{user `packer_chef_bootstrap_dir`}}\", \"destination\": \"/tmp/bootstrap\" }, { \"type\": \"shell\", \"inline\": [ \"sudo mkdir -p {{user `chef_dir`}}\", \"sudo mkdir -p /tmp/packer-chef-client\", \"sudo chown {{user `packer_uid`}}.{{user `packer_gid`}} /tmp/packer-chef-client\", \"sudo sh /tmp/bootstrap/bootstrap.sh\" ] }, { \"type\": \"chef-client\", \"server_url\": \"http://localhost:8889\", \"config_template\": \"{{user `chef_client_config_tpl`}}/client.rb.tpl\", \"skip_clean_node\": true, \"skip_clean_client\": true, \"run_list\": \"{{user `chef_run_list`}}\" })"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/provisioners/ansible",
  "text": "Ansible - Provisioners | Packer\nType: ansible\nThe ansible Packer provisioner runs Ansible playbooks. It dynamically creates an Ansible inventory file configured to use SSH, runs an SSH server, executes ansible-playbook, and marshals Ansible plays through the SSH server to the machine being provisioned by Packer.\nNote:: Any remote_user defined in tasks will be ignored. Packer will always connect with the user given in the json config for this provisioner.\nThis is a fully functional template that will provision an image on DigitalOcean. Replace the mock api_token value with your own.\nExample Packer template:\n{ \"provisioners\": [ { \"type\": \"ansible\", \"playbook_file\": \"./playbook.yml\" } ], \"builders\": [ { \"type\": \"digitalocean\", \"api_token\": \"6a561151587389c7cf8faa2d83e94150a4202da0e2bad34dd2bf236018ffaeeb\", \"image\": \"ubuntu-14-04-x64\", \"region\": \"sfo1\" } ] } \nExample playbook:\n--- # playbook.yml - name: 'Provision Image' hosts: default become: true tasks: - name: install Apache package: name: 'httpd' state: present \nplaybook_file (string) - The playbook to be run by Ansible.\nansible_env_vars (array of strings) - Environment variables to set before running Ansible. Usage example:\n\"ansible_env_vars\": [ \"ANSIBLE_HOST_KEY_CHECKING=False\", \"ANSIBLE_SSH_ARGS='-o ForwardAgent=yes -o ControlMaster=auto -o ControlPersist=60s'\", \"ANSIBLE_NOCOLOR=True\" ] \nThis is a template engine. Therefore, you may use user variables and template functions in this field.\nFor example, if you are running a Windows build on AWS, Azure, Google Compute, or OpenStack and would like to access the auto-generated password that Packer uses to connect to a Windows instance via WinRM, you can use the template variable {{.WinRMPassword}} in this option. Example:\n\"ansible_env_vars\": [ \"WINRM_PASSWORD={{.WinRMPassword}}\" ], \ncommand (string) - The command to invoke ansible. Defaults to ansible-playbook. If you would like to provide a more complex command, for example, something that sets up a virtual environment before calling ansible, take a look at the ansible wrapper guide below for inspiration.\nempty_groups (array of strings) - The groups which should be present in inventory file but remain empty.\nextra_arguments (array of strings) - Extra arguments to pass to Ansible. These arguments will not be passed through a shell and arguments should not be quoted. Usage example:\n\"extra_arguments\": [ \"--extra-vars\", \"Region={{user `Region`}} Stage={{user `Stage`}}\" ] \nIf you are running a Windows build on AWS, Azure, Google Compute, or OpenStack and would like to access the auto-generated password that Packer uses to connect to a Windows instance via WinRM, you can use the template variable {{.WinRMPassword}} in this option. For example:\n\"extra_arguments\": [ \"--extra-vars\", \"winrm_password={{ .WinRMPassword }}\" ] \ngalaxy_file (string) - A requirements file which provides a way to install roles with the ansible-galaxy cli on the local machine before executing ansible-playbook. By default, this is empty.\ngalaxy_command (string) - The command to invoke ansible-galaxy. By default, this is ansible-galaxy.\ngalaxy_force_install (bool) - Force overwriting an existing role. Adds --force option to ansible-galaxy command. By default, this is false.\ngroups (array of strings) - The groups into which the Ansible host should be placed. When unspecified, the host is not associated with any groups.\nhost_alias (string) - The alias by which the Ansible host should be known. Defaults to default. This setting is ignored when using a custom inventory file.\ninventory_file (string) - The inventory file to use during provisioning. When unspecified, Packer will create a temporary inventory file and will use the host_alias.\ninventory_directory (string) - The directory in which to place the temporary generated Ansible inventory file. By default, this is the system-specific temporary file location. The fully-qualified name of this temporary file will be passed to the -i argument of the ansible command when this provisioner runs ansible. Specify this if you have an existing inventory directory with host_vars group_vars that you would like to use in the playbook that this provisioner will run.\nkeep_inventory_file (boolean) - If true, the Ansible provisioner will not delete the temporary inventory file it creates in order to connect to the instance. This is useful if you are trying to debug your ansible run and using \"--on-error=ask\" in order to leave your instance running while you test your playbook. this option is not used if you set an inventory_file.\nlocal_port (uint) - The port on which to attempt to listen for SSH connections. This value is a starting point. The provisioner will attempt listen for SSH connections on the first available of ten ports, starting at local_port. A system-chosen port is used when local_port is missing or empty.\nroles_path (string) - The path to the directory on your local system to install the roles in. Adds --roles-path /path/to/your/roles to ansible-galaxy command. By default, this is empty, and thus --roles-path option is not added to the command.\nsftp_command (string) - The command to run on the machine being provisioned by Packer to handle the SFTP protocol that Ansible will use to transfer files. The command should read and write on stdin and stdout, respectively. Defaults to /usr/lib/sftp-server -e.\nskip_version_check (boolean) - Check if ansible is installed prior to running. Set this to true, for example, if you're going to install ansible during the packer run.\nssh_host_key_file (string) - The SSH key that will be used to run the SSH server on the host machine to forward commands to the target machine. Ansible connects to this server and will validate the identity of the server using the system known_hosts. The default behavior is to generate and use a onetime key. Host key checking is disabled via the ANSIBLE_HOST_KEY_CHECKING environment variable if the key is generated.\nssh_authorized_key_file (string) - The SSH public key of the Ansible ssh_user. The default behavior is to generate and use a onetime key. If this key is generated, the corresponding private key is passed to ansible-playbook with the -e ansible_ssh_private_key_file option.\nuser (string) - The ansible_user to use. Defaults to the user running packer, NOT the user set for your communicator. If you want to use the same user as the communicator, you will need to manually set it again in this field.\nuse_proxy (boolean) - When true, set up a localhost proxy adapter so that Ansible has an IP address to connect to, even if your guest does not have an IP address. For example, the adapter is necessary for Docker builds to use the Ansible provisioner. If you set this option to false, but Packer cannot find an IP address to connect Ansible to, it will automatically set up the adapter anyway.\nIn order for Ansible to connect properly even when use_proxy is false, you need to make sure that you are either providing a valid username and ssh key to the ansible provisioner directly, or that the username and ssh key being used by the ssh communicator will work for your needs. If you do not provide a user to ansible, it will use the user associated with your builder, not the user running Packer.\nuse_proxy=false is currently only supported for SSH and WinRM.\nCurrently, this defaults to true for all connection types. In the future, this option will be changed to default to false for SSH and WinRM connections where the provisioner has access to a host IP.\nIn addition to being able to specify extra arguments using the extra_arguments configuration, the provisioner automatically defines certain commonly useful Ansible variables:\npacker_build_name is set to the name of the build that Packer is running. This is most useful when Packer is making multiple builds and you want to distinguish them slightly when using a common playbook.\npacker_builder_type is the type of the builder that was used to create the machine that the script is running on. This is useful if you want to run only certain parts of the playbook on systems built with certain builders.\npacker_http_addr If using a builder that provides an http server for file transfer (such as hyperv, parallels, qemu, virtualbox, and vmware), this will be set to the address. You can use this address in your provisioner to download large files over http. This may be useful if you're experiencing slower speeds using the default file provisioner. A file provisioner using the winrm communicator may experience these types of difficulties.\nTo debug underlying issues with Ansible, add \"-vvvv\" to \"extra_arguments\" to enable verbose logging.\n\"extra_arguments\": [ \"-vvvv\" ] \nRedhat / CentOS\nRedhat / CentOS builds have been known to fail with the following error due to sftp_command, which should be set to /usr/libexec/openssh/sftp-server -e:\n==> virtualbox-ovf: starting sftp subsystem virtualbox-ovf: fatal: [default]: UNREACHABLE! => {\"changed\": false, \"msg\": \"SSH Error: data could not be sent to the remote host. Make sure this host can be reached over ssh\", \"unreachable\": true} \nchroot communicator\nBuilding within a chroot (e.g. amazon-chroot) requires changing the Ansible connection to chroot and running Ansible as root/sudo.\n{ \"builders\": [ { \"type\": \"amazon-chroot\", \"mount_path\": \"/mnt/packer-amazon-chroot\", \"region\": \"us-east-1\", \"source_ami\": \"ami-123456\" } ], \"provisioners\": [ { \"type\": \"ansible\", \"extra_arguments\": [ \"--connection=chroot\", \"--inventory-file=/mnt/packer-amazon-chroot,\" ], \"playbook_file\": \"main.yml\" } ] } \nWinRM Communicator\nThere are two possible methods for using ansible with the WinRM communicator.\nMethod 1 (recommended)\nThe recommended way to use the WinRM communicator is to set \"use_proxy\": false and let the Ansible provisioner handle the rest for you. If you are using WinRM with HTTPS, and you are using a self-signed certificate you will also have to set ansible_winrm_server_cert_validation=ignore in your extra_arguments.\nBelow is a fully functioning Ansible example using WinRM:\n{ \"builders\": [ { \"type\": \"amazon-ebs\", \"region\": \"us-east-1\", \"instance_type\": \"t2.micro\", \"source_ami_filter\": { \"filters\": { \"virtualization-type\": \"hvm\", \"name\": \"*Windows_Server-2012*English-64Bit-Base*\", \"root-device-type\": \"ebs\" }, \"most_recent\": true, \"owners\": \"amazon\" }, \"ami_name\": \"default-packer\", \"user_data_file\": \"windows_bootstrap.txt\", \"communicator\": \"winrm\", \"force_deregister\": true, \"winrm_insecure\": true, \"winrm_username\": \"Administrator\", \"winrm_use_ssl\": true }], \"provisioners\": [ { \"type\": \"ansible\", \"playbook_file\": \"./playbook.yml\", \"user\": \"Administrator\", \"use_proxy\": false, \"extra_arguments\": [ \"-e\", \"ansible_winrm_server_cert_validation=ignore\" ] } ] } \nNote that you do have to set the \"Administrator\" user, because otherwise Ansible will default to using the user that is calling Packer, rather than the user configured inside of the Packer communicator. For the contents of windows_bootstrap.txt, see the winrm docs for the amazon-ebs communicator.\nWhen running from OSX, you may see an error like:\namazon-ebs: objc[9752]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called. amazon-ebs: objc[9752]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug. amazon-ebs: ERROR! A worker was found in a dead state \nIf you see this, you may be able to work around the issue by telling Ansible to explicitly not use any proxying; you can do this by setting the template option\n\"ansible_env_vars\": [\"no_proxy=\\\"*\\\"\"], \nin the above Ansible template.\nMethod 2 (Not recommended)\nIf you want to use the Packer ssh proxy, then you need a custom Ansible connection plugin and a particular configuration. You need a directory named connection_plugins next to the playbook which contains a file named packer.py` which implements the connection plugin. On versions of Ansible before 2.4.x, the following works as the connection plugin:\nfrom __future__ import (absolute_import, division, print_function) __metaclass__ = type from ansible.plugins.connection.ssh import Connection as SSHConnection class Connection(SSHConnection): ''' ssh based connections for powershell via packer''' transport = 'packer' has_pipelining = True become_methods = [] allow_executable = False module_implementation_preferences = ('.ps1', '') def __init__(self, *args, **kwargs): super(Connection, self).__init__(*args, **kwargs) \nNewer versions of Ansible require all plugins to have a documentation string. You can see if there is a plugin available for the version of Ansible you are using here.\nTo create the plugin yourself, you will need to copy all of the options from the DOCUMENTATION string from the ssh.py Ansible connection plugin of the Ansible version you are using and add it to a packer.py file similar to as follows\nfrom __future__ import (absolute_import, division, print_function) __metaclass__ = type from ansible.plugins.connection.ssh import Connection as SSHConnection DOCUMENTATION = ''' connection: packer short_description: ssh based connections for powershell via packer description: - This connection plugin allows ansible to communicate to the target packer machines via ssh based connections for powershell. author: Packer version_added: na options: **** Copy ALL the options from https://github.com/ansible/ansible/blob/devel/lib/ansible/plugins/connection/ssh.py for the version of Ansible you are using **** ''' class Connection(SSHConnection): ''' ssh based connections for powershell via packer''' transport = 'packer' has_pipelining = True become_methods = [] allow_executable = False module_implementation_preferences = ('.ps1', '') def __init__(self, *args, **kwargs): super(Connection, self).__init__(*args, **kwargs) \nThis template should build a Windows Server 2012 image on Google Cloud Platform:\n{ \"variables\": {}, \"provisioners\": [ { \"type\": \"ansible\", \"playbook_file\": \"./win-playbook.yml\", \"extra_arguments\": [ \"--connection\", \"packer\", \"--extra-vars\", \"ansible_shell_type=powershell ansible_shell_executable=None\" ] } ], \"builders\": [ { \"type\": \"googlecompute\", \"account_file\": \"{{ user `account_file`}}\", \"project_id\": \"{{user `project_id`}}\", \"source_image\": \"windows-server-2012-r2-dc-v20160916\", \"communicator\": \"winrm\", \"zone\": \"us-central1-a\", \"disk_size\": 50, \"winrm_username\": \"packer\", \"winrm_use_ssl\": true, \"winrm_insecure\": true, \"metadata\": { \"sysprep-specialize-script-cmd\": \"winrm set winrm/config/service/auth @{Basic=\\\"true\\\"}\" } } ] } \nWarning: Please note that if you're setting up WinRM for provisioning, you'll probably want to turn it off or restrict its permissions as part of a shutdown script at the end of Packer's provisioning process. For more details on the why/how, check out this useful blog post and the associated code: https://cloudywindows.io/post/winrm-for-provisioning-close-the-door-on-the-way-out-eh/\nPost i/o timeout errors\nIf you see unknown error: Post http://<ip>:<port>/wsman:dial tcp <ip>:<port>: i/o timeout errors while provisioning a Windows machine, try setting Ansible to copy files over ssh instead of sftp.\nToo many SSH keys\nSSH servers only allow you to attempt to authenticate a certain number of times. All of your loaded keys will be tried before the dynamically generated key. If you have too many SSH keys loaded in your ssh-agent, the Ansible provisioner may fail authentication with a message similar to this:\ngooglecompute: fatal: [default]: UNREACHABLE! => {\"changed\": false, \"msg\": \"Failed to connect to the host via ssh: Warning: Permanently added '[127.0.0.1]:62684' (RSA) to the list of known hosts.\\r\\nReceived disconnect from 127.0.0.1 port 62684:2: too many authentication failures\\r\\nAuthentication failed.\\r\\n\", \"unreachable\": true} \nTo unload all keys from your ssh-agent, run:\nBecome: yes\nWe recommend against running Packer as root; if you do then you won't be able to successfully run your ansible playbook as root; become: yes will fail.\nUsing a wrapping script for your ansible call\nSometimes, you may have extra setup that needs to be called as part of your ansible run. The easiest way to do this is by writing a small bash script and using that bash script in your \"command\" in place of the default \"ansible-playbook\". For example, you may need to launch a Python virtualenv before calling ansible. To do this, you'd want to create a bash script like\n#!/bin/bash source /tmp/venv/bin/activate && ANSIBLE_FORCE_COLOR=1 PYTHONUNBUFFERED=1 /tmp/venv/bin/ansible-playbook \"$@\" \nThe ansible provisioner template remains very simple. For example:\n{ \"type\": \"ansible\", \"command\": \"/Path/To/call_ansible.sh\", \"playbook_file\": \"./playbook.yml\" } \nNote that we're calling ansible-playbook at the end of this command and passing all command line arguments through into this call; this is necessary for making sure that --extra-vars and other important ansible arguments get set. Note the quoting around the bash array, too; if you don't use quotes, any arguments with spaces will not be read properly.\nDocker\nWhen trying to use Ansible with Docker, you need to tweak a few options.\nChange the ansible_connection from \"ssh\" to \"docker\"\nSet a Docker container name via the --name option.\nOn a CI server you probably want to overwrite ansible_host with a random name.\nExample Packer template:\n{ \"variables\": { \"ansible_host\": \"default\", \"ansible_connection\": \"docker\" }, \"builders\":[ { \"type\": \"docker\", \"image\": \"centos:7\", \"commit\": true, \"run_command\": [ \"-d\", \"-i\", \"-t\", \"--name\", \"{{user `ansible_host`}}\", \"{{.Image}}\", \"/bin/bash\" ] } ], \"provisioners\": [ { \"type\": \"ansible\", \"groups\": [ \"webserver\" ], \"playbook_file\": \"./webserver.yml\", \"extra_arguments\": [ \"--extra-vars\", \"ansible_host={{user `ansible_host`}} ansible_connection={{user `ansible_connection`}}\" ] } ] } \nExample playbook:\n- name: configure webserver hosts: webserver tasks: - name: install Apache yum: name: httpd \nTroubleshooting\nIf you are using an Ansible version >= 2.8 and Packer hangs in the \"Gathering Facts\" stage, this could be the result of a pipelineing issue with the proxy adapter that Packer uses. Setting use_proxy: false, in your Packer config should resolve the issue. In the future we will default to setting this, so you won't have to but for now it is a manual change you must make."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/provisioners/chef-solo",
  "text": "Chef Solo - Provisioners | Packer\nType: chef-solo\nThe Chef solo Packer provisioner installs and configures software on machines built by Packer using chef-solo. Cookbooks can be uploaded from your local machine to the remote machine or remote paths can be used.\nThe provisioner will even install Chef onto your machine if it isn't already installed, using the official Chef installers provided by Chef Inc.\nThe example below is fully functional and expects cookbooks in the \"cookbooks\" directory relative to your working directory.\n{ \"type\": \"chef-solo\", \"cookbook_paths\": [\"cookbooks\"] } \nThe reference of available configuration options is listed below. No configuration is actually required, but at least run_list is recommended.\nchef_environment (string) - The name of the chef_environment sent to the Chef server. By default this is empty and will not use an environment\nchef_license (string) - As of Chef v15, Chef requires users to accept a license. Defaults to accept-silent when skip_install is false and install_command is unset. Possible values are accept, accept-silent and accept-no-persist. For details see Accepting the Chef License.\nconfig_template (string) - Path to a template that will be used for the Chef configuration file. By default Packer only sets configuration it needs to match the settings set in the provisioner configuration. If you need to set configurations that the Packer provisioner doesn't support, then you should use a custom configuration template. See the dedicated \"Chef Configuration\" section below for more details.\ncookbook_paths (array of strings) - This is an array of paths to \"cookbooks\" directories on your local filesystem. These will be uploaded to the remote machine in the directory specified by the staging_directory. By default, this is empty.\ndata_bags_path (string) - The path to the \"data_bags\" directory on your local filesystem. These will be uploaded to the remote machine in the directory specified by the staging_directory. By default, this is empty.\nencrypted_data_bag_secret_path (string) - The path to the file containing the secret for encrypted data bags. By default, this is empty, so no secret will be available.\nenvironments_path (string) - The path to the \"environments\" directory on your local filesystem. These will be uploaded to the remote machine in the directory specified by the staging_directory. By default, this is empty.\nexecute_command (string) - The command used to execute Chef. This has various configuration template variables available. See below for more information.\nguest_os_type (string) - The target guest OS type, either \"unix\" or \"windows\". Setting this to \"windows\" will cause the provisioner to use Windows friendly paths and commands. By default, this is \"unix\".\ninstall_command (string) - The command used to install Chef. This has various configuration template variables available. See below for more information.\njson (object) - An arbitrary mapping of JSON that will be available as node attributes while running Chef.\nprevent_sudo (boolean) - By default, the configured commands that are executed to install and run Chef are executed with sudo. If this is true, then the sudo will be omitted. This has no effect when guest_os_type is windows.\nremote_cookbook_paths (array of strings) - A list of paths on the remote machine where cookbooks will already exist. These may exist from a previous provisioner or step. If specified, Chef will be configured to look for cookbooks here. By default, this is empty.\nroles_path (string) - The path to the \"roles\" directory on your local filesystem. These will be uploaded to the remote machine in the directory specified by the staging_directory. By default, this is empty.\nrun_list (array of strings) - The run list for Chef. By default this is empty.\nskip_install (boolean) - If true, Chef will not automatically be installed on the machine using the Chef omnibus installers.\nstaging_directory (string) - This is the directory where all the configuration of Chef by Packer will be placed. By default this is /tmp/packer-chef-solo when guest_os_type unix and $env:TEMP/packer-chef-solo when windows. This directory doesn't need to exist but must have proper permissions so that the user that Packer uses is able to create directories and write into this folder. If the permissions are not correct, use a shell provisioner prior to this to configure it properly.\nversion (string) - The version of Chef to be installed. By default this is empty which will install the latest version of Chef.\nBy default, Packer uses a simple Chef configuration file in order to set the options specified for the provisioner. But Chef is a complex tool that supports many configuration options. Packer allows you to specify a custom configuration template if you'd like to set custom configurations.\nThe default value for the configuration template is:\ncookbook_path [{{.CookbookPaths}}] \nThis template is a configuration template and has a set of variables available to use:\nChefEnvironment - The current enabled environment. Only non-empty if the environment path is set.\nChefLicense - The Chef license acceptance value.\nCookbookPaths is the set of cookbook paths ready to embedded directly into a Ruby array to configure Chef.\nDataBagsPath is the path to the data bags folder.\nEncryptedDataBagSecretPath - The path to the encrypted data bag secret\nEnvironmentsPath - The path to the environments folder.\nRolesPath - The path to the roles folder.\nBy default, Packer uses the following command (broken across multiple lines for readability) to execute Chef:\n{{if .Sudo}}sudo {{end}}chef-solo \\ --no-color \\ -c {{.ConfigPath}} \\ -j {{.JsonPath}} \nWhen guest_os_type is set to \"windows\", Packer uses the following command to execute Chef. The full path to Chef is required because the PATH environment variable changes don't immediately propagate to running processes.\nc:/opscode/chef/bin/chef-solo.bat \\ --no-color \\ -c {{.ConfigPath}} \\ -j {{.JsonPath}} \nThis command can be customized using the execute_command configuration. As you can see from the default value above, the value of this configuration can contain various template variables, defined below:\nConfigPath - The path to the Chef configuration file.\nJsonPath - The path to the JSON attributes file for the node.\nSudo - A boolean of whether to sudo the command or not, depending on the value of the prevent_sudo configuration.\nBy default, Packer uses the following command (broken across multiple lines for readability) to install Chef. This command can be customized if you want to install Chef in another way.\ncurl -L https://omnitruck.chef.io/install.sh | \\ {{if .Sudo}}sudo{{end}} bash -s --{{if .Version}} -v {{.Version}}{{end}} \nWhen guest_os_type is set to \"windows\", Packer uses the following command to install the latest version of Chef:\npowershell.exe -Command \\\". { iwr -useb https://omnitruck.chef.io/install.ps1 } | iex; install\\\" \nThis command can be customized using the install_command configuration."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/provisioners/converge",
  "text": "Converge - Provisioners | Packer\nType: converge\nThe Converge Packer provisioner uses Converge modules to provision the machine. It uploads module directories to use as source, or you can use remote modules.\nThe provisioner can optionally bootstrap the Converge client/server binary onto new images.\n{ \"type\": \"converge\", \"module\": \"https://raw.githubusercontent.com/asteris-llc/converge/master/samples/fileContent.hcl\", \"params\": { \"message\": \"Hello, Packer!\" } } \nThe reference of available configuration options is listed below. The only required element is \"module\". Every other option is optional.\nmodule (string) - Path (or URL) to the root module that Converge will apply.\nbootstrap (boolean, defaults to false) - Set to allow the provisioner to download the latest Converge bootstrap script and the specified version of Converge from the internet.\nversion (string) - Set to a released Converge version for bootstrap.\nmodule_dirs (array of directory specifications) - Module directories to transfer to the remote host for execution. See below for the specification.\nworking_directory (string) - The directory that Converge will change to before execution.\nparams (maps of string to string) - parameters to pass into the root module.\nexecute_command (string) - the command used to execute Converge. This is a configuration template variables. See below for detailed usage instructions.\nprevent_sudo (boolean) - stop Converge from running with administrator privileges via sudo\nbootstrap_command (string) - the command used to bootstrap Converge. This is a template engine. Therefore, you may use user variables and template functions in this field. The following extra variables are also avilable in this engine:\nVersion: The version of Converge to use.\nSudo: Boolean; intended to say whether to use sudo or not.\nBy default, this is \"curl -s https://get.converge.sh | {{if .Sudo}}sudo {{end}}sh {{if ne .Version \\\"\\\"}}-s -- -v {{.Version}}{{end}}\"\nprevent_bootstrap_sudo (boolean) - stop Converge from bootstrapping with administrator privileges via sudo\nModule Directories\nThe provisioner can transfer module directories to the remote host for provisioning. Of these fields, source and destination are required in every directory.\nsource (string) - the path to the folder on the local machine.\ndestination (string) - the path to the folder on the remote machine. Parent directories will not be created; use the shell module to do this.\nexclude (array of string) - files and directories to exclude from transfer.\nExecute Command\nBy default, Packer uses the following command (broken across multiple lines for readability) to execute Converge:\ncd {{.WorkingDirectory}} && \\ {{if .Sudo}}sudo {{end}}converge apply \\ --local \\ --log-level=WARNING \\ --paramsJSON '{{.ParamsJSON}}' \\ {{.Module}} \nThis command can be customized using the execute_command configuration. As you can see from the default value above, the value of this configuration can contain various template variables:\nWorkingDirectory - directory from the configuration.\nSudo - the opposite of prevent_sudo from the configuration.\nParamsJSON - The unquoted JSONified form of params from the configuration.\nModule - module from the configuration.\nBootstrap Command\nBy default, Packer uses the following command to bootstrap Converge:\ncurl -s https://get.converge.sh | {{if .Sudo}}sudo {{end}}sh {{if ne .Version \"\"}}-s -- -v {{.Version}}{{end}} \nThis command can be customized using the bootstrap_command configuration. As you can see from the default values above, the value of this configuration can contain various template variables:\nSudo - the opposite of prevent_bootstrap_sudo from the configuration.\nVersion - version from the configuration."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/provisioners/inspec",
  "text": "InSpec - Provisioners | Packer\nType: inspec\nThe inspec Packer provisioner runs InSpec profiles. It dynamically creates a target configured to use SSH, runs an SSH server, executes inspec exec, and marshals InSpec tests through the SSH server to the machine being provisioned by Packer.\nThis is a fully functional template that will test an image on DigitalOcean. Replace the mock api_token value with your own.\n{ \"provisioners\": [ { \"type\": \"inspec\", \"profile\": \"https://github.com/dev-sec/linux-baseline\" } ], \"builders\": [ { \"type\": \"digitalocean\", \"api_token\": \"<digital ocean api token>\", \"image\": \"ubuntu-14-04-x64\", \"region\": \"sfo1\" } ] } \nprofile (string) - The profile to be executed by InSpec.\ninspec_env_vars (array of strings) - Environment variables to set before running InSpec. Usage example:\n\"inspec_env_vars\": [ \"FOO=bar\" ] \ncommand (string) - The command to invoke InSpec. Defaults to inspec.\nextra_arguments (array of strings) - Extra arguments to pass to InSpec. These arguments will not be passed through a shell and arguments should not be quoted. Usage example:\n\"extra_arguments\": [ \"--sudo\", \"--reporter\", \"json\" ] \nattributes (array of strings) - Attribute Files used by InSpec which will be passed to the --input-file argument of the inspec command when this provisioner runs InSpec. Specify this if you want a different location. Note using also \"--input-file\" in extra_arguments will override this setting.\nattributes_directory (string) - The directory in which to place the temporary generated InSpec Attributes file. By default, this is the system-specific temporary file location. The fully-qualified name of this temporary file will be passed to the --input-file argument of the inspec command when this provisioner runs InSpec. Specify this if you want a different location.\nbackend (string) - Backend used by InSpec for connection. Defaults to SSH.\nhost (string) - Host used for by InSpec for connection. Defaults to localhost.\nlocal_port (uint) - The port on which to attempt to listen for SSH connections. This value is a starting point. The provisioner will attempt to listen for SSH connections on the first available of ten ports, starting at local_port. A system-chosen port is used when local_port is missing or empty.\nssh_host_key_file (string) - The SSH key that will be used to run the SSH server on the host machine to forward commands to the target machine. InSpec connects to this server and will validate the identity of the server using the system known_hosts. The default behavior is to generate and use a onetime key.\nssh_authorized_key_file (string) - The SSH public key of the InSpec ssh_user. The default behavior is to generate and use a onetime key. If this key is generated, the corresponding private key is passed to inspec command with the -i inspec_ssh_private_key_file option.\nuser (string) - The --user to use. Defaults to the user running Packer.\nIn addition to being able to specify extra arguments using the extra_arguments configuration, the provisioner automatically defines certain commonly useful InSpec Attributes:\npacker_build_name is set to the name of the build that Packer is running. This is most useful when Packer is making multiple builds and you want to distinguish them slightly when using a common profile.\npacker_builder_type is the type of the builder that was used to create the machine that the script is running on. This is useful if you want to run only certain parts of the profile on systems built with certain builders.\nTo debug underlying issues with InSpec, add \"-l\" to \"extra_arguments\" to enable verbose logging.\n{ \"extra_arguments\": [\"-l\", \"debug\"] }"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/post-processors/checksum",
  "text": "Type: checksum\nchecksum_types (array of strings) - An array of strings of checksum types to compute. Allowed values are md5, sha1, sha224, sha256, sha384, sha512.\nkeep_input_artifact (boolean) - Unlike most post-processors, setting keep_input_artifact will have no effect; the checksum post-processor always saves the artifact that it is calculating the checksum for."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/provisioners/puppet-masterless",
  "text": "Puppet Masterless - Provisioners | Packer\nPuppet (Masterless) Provisioner\nType: puppet-masterless\nThe masterless Puppet Packer provisioner configures Puppet to run on the machines by Packer from local modules and manifest files. Modules and manifests can be uploaded from your local machine to the remote machine or can simply use remote paths (perhaps obtained using something like the shell provisioner). Puppet is run in masterless mode, meaning it never communicates to a Puppet master.\nNote: Puppet will not be installed automatically by this provisioner. This provisioner expects that Puppet is already installed on the machine. It is common practice to use the shell provisioner before the Puppet provisioner to do this.\nThe example below is fully functional and expects the configured manifest file to exist relative to your working directory.\n{ \"type\": \"puppet-masterless\", \"manifest_file\": \"site.pp\" } \nRequired parameters:\nmanifest_file (string) - This is either a path to a puppet manifest (.pp file) or a directory containing multiple manifests that puppet will apply (the \"main manifest\"). These file(s) must exist on your local system and will be uploaded to the remote machine.\nexecute_command (string) - The command-line to execute Puppet. This is a template engine. Therefore, you may use user variables and template functions in this field. In addition, you may use the following extra variables:\nFacterVars: Additional facts to set when executing Puppet, joined for use in a command. This is internal and not settable by the user.\nHieraConfigPath: Path to a hiera configuration file to upload and use; set in the template option hiera_config_path\nManifestDir: The directory of the manifest file on the remote machine.\nManifestFile: The manifest file on the remote machine.\nModulePath: The module_paths, combined into one path variable.\nModulePathJoiner: A colon : on posix systems and a semicolon ; on Windows.\nPuppetBinDir: The directory that the Puppet binary is in.\nSudo: Returns false when PreventSudo is set, and true when it is not.\nWorkingDir: The value provided in the working_directory option. See below for execute_command's default value.\nextra_arguments (array of strings) - Additional options to pass to the Puppet command. This allows for customization of execute_command without having to completely replace or subsume its contents, making forward-compatible customizations much easier to maintain.\nThis string is lazy-evaluated so one can incorporate logic driven by template variables as well as private elements of ExecuteTemplate (see source: provisioner/puppet-masterless/provisioner.go).\n[ {{if ne \"{{user environment}}\" \"\"}}--environment={{user environment}}{{end}}, {{if ne \".ModulePath\" \"\"}}--modulepath=\"{{.ModulePath}}{{.ModulePathJoiner}}$(puppet config print {{if ne \"{{user `environment`}}\" \"\"}}--environment={{user `environment`}}{{end}} modulepath)\"{{end}} ] \nfacter (object of key:value strings) - Additional facts to make available to the Puppet run.\nguest_os_type (string) - The remote host's OS type ('windows' or 'unix') to tailor command-line and path separators. (default: unix).\nhiera_config_path (string) - Local path to self-contained Hiera data to be uploaded. NOTE: If you need data directories they must be previously transferred with a File provisioner.\nignore_exit_codes (boolean) - If true, Packer will ignore failures.\nmanifest_dir (string) - Local directory with manifests to be uploaded. This is useful if your main manifest uses imports, but the directory might not contain the manifest_file itself.\nmanifest_dir is passed to Puppet as --manifestdir option. This option was deprecated in puppet 3.6, and removed in puppet 4.0. If you have multiple manifests you should use manifest_file instead.\nmodule_paths (array of strings) - Array of local module directories to be uploaded.\nprevent_sudo (boolean) - On Unix platforms Puppet is typically invoked with sudo. If true, it will be omitted. (default: false)\npuppet_bin_dir (string) - Path to the Puppet binary. Ideally the program should be on the system (unix: $PATH, windows: %PATH%), but some builders (eg. Docker) do not run profile-setup scripts and therefore PATH might be empty or minimal. On Windows, spaces should be ^-escaped, i.e. c:/program^ files/puppet^ labs/puppet/bin.\nstaging_directory (string) - Directory to where uploaded files will be placed (unix: \"/tmp/packer-puppet-masterless\", windows: \"%SYSTEMROOT%/Temp/packer-puppet-masterless\"). It doesn't need to pre-exist, but the parent must have permissions sufficient for the account Packer connects as to create directories and write files. Use a Shell provisioner to prepare the way if needed.\nworking_directory (string) - Directory from which execute_command will be run. If using Hiera files with relative paths, this option can be helpful. (default: staging_directory)\nelevated_user and elevated_password (string) - If specified, Puppet will be run with elevated privileges using the given Windows user. See the powershell provisioner for the full details. This is a template engine. Therefore, you may use user variables and template functions in this field, including {{ build `Password`}} to use the password being used by the communicator to connect to your instance.\nBy default, Packer uses the following command (broken across multiple lines for readability) to execute Puppet:\ncd {{.WorkingDir}} && {{if ne .FacterVars \"\"}}{{.FacterVars}} {{end}} {{if .Sudo}}sudo -E {{end}} {{if ne .PuppetBinDir \"\"}}{{.PuppetBinDir}}/{{end}} puppet apply --detailed-exitcodes {{if .Debug}}--debug {{end}} {{if ne .ModulePath \"\"}}--modulepath='{{.ModulePath}}' {{end}} {{if ne .HieraConfigPath \"\"}}--hiera_config='{{.HieraConfigPath}}' {{end}} {{if ne .ManifestDir \"\"}}--manifestdir='{{.ManifestDir}}' {{end}} {{if ne .ExtraArguments \"\"}}{{.ExtraArguments}} {{end}} {{.ManifestFile}} \nThe following command is used if guest OS type is windows:\ncd {{.WorkingDir}} && {{if ne .FacterVars \"\"}}{{.FacterVars}} && {{end}} {{if ne .PuppetBinDir \"\"}}{{.PuppetBinDir}}/{{end}} puppet apply --detailed-exitcodes {{if .Debug}}--debug {{end}} {{if ne .ModulePath \"\"}}--modulepath='{{.ModulePath}}' {{end}} {{if ne .HieraConfigPath \"\"}}--hiera_config='{{.HieraConfigPath}}' {{end}} {{if ne .ManifestDir \"\"}}--manifestdir='{{.ManifestDir}}' {{end}} {{if ne .ExtraArguments \"\"}}{{.ExtraArguments}} {{end}} {{.ManifestFile}} \nIn addition to being able to specify custom Facter facts using the facter configuration, the provisioner automatically defines certain commonly useful facts:\npacker_build_name is set to the name of the build that Packer is running. This is most useful when Packer is making multiple builds and you want to distinguish them in your Hiera hierarchy.\npacker_builder_type is the type of the builder that was used to create the machine that Puppet is running on. This is useful if you want to run only certain parts of your Puppet code on systems built with certain builders."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/provisioners/puppet-server",
  "text": "Puppet Server - Provisioners | Packer\nPuppet Server Provisioner\nType: puppet-server\nThe puppet-server Packer provisioner provisions Packer machines with Puppet by connecting to a Puppet master.\nNote: Puppet will not be installed automatically by this provisioner. This provisioner expects that Puppet is already installed on the machine. It is common practice to use the shell provisioner before the Puppet provisioner to do this.\nThe example below is fully functional and expects a Puppet server to be accessible from your network.\n{ \"type\": \"puppet-server\", \"extra_arguments\": \"--test --pluginsync\", \"facter\": { \"server_role\": \"webserver\" } } \nThe provisioner takes various options. None are strictly required. They are listed below:\nclient_cert_path (string) - Path to the directory on your disk that contains the client certificate for the node. This defaults to nothing, in which case a client cert won't be uploaded.\nclient_private_key_path (string) - Path to the directory on your disk that contains the client private key for the node. This defaults to nothing, in which case a client private key won't be uploaded.\nexecute_command (string) - The command-line to execute Puppet. This is a template engine. Therefore, you may use user variables and template functions in this field. In addition, you may use the following extra variables:\nClientCertPath: The value set in the template option client_cert_path.\nClientPrivateKeyPath: The value set in the template option client_private_key_path.\nFacterVars: Additional facts to set when executing Puppet, joined for use in a command. This is internal and not settable by the user.\nPuppetNode: The hostname of the Puppet node, set in puppet_node.\nPuppetServer: The hostname of the Puppet server, set in puppet_server\nPuppetBinDir: The directory that the Puppet binary is in.\nSudo: Returns false when PreventSudo is set, and true when it is not.\nWorkingDir: The value provided in the working_directory option. See below for execute_command's default value.\nextra_arguments (array of strings) - Additional options to pass to the Puppet command. This allows for customization of execute_command without having to completely replace or subsume its contents, making forward-compatible customizations much easier to maintain.\nThis string is lazy-evaluated so one can incorporate logic driven by template variables as well as private elements of ExecuteTemplate (see source: provisioner/puppet-server/provisioner.go).\n[ {{if ne \"{{user environment}}\" \"\"}}--environment={{user environment}}{{end}} ] \nfacter (object of key/value strings) - Additional facts to make available to the Puppet run.\nguest_os_type (string) - The remote host's OS type ('windows' or 'unix') to tailor command-line and path separators. (default: unix).\nignore_exit_codes (boolean) - If true, Packer will ignore failures.\nprevent_sudo (boolean) - On Unix platforms Puppet is typically invoked with sudo. If true, it will be omitted. (default: false)\npuppet_bin_dir (string) - Path to the Puppet binary. Ideally the program should be on the system (unix: $PATH, windows: %PATH%), but some builders (eg. Docker) do not run profile-setup scripts and therefore PATH might be empty or minimal. On Windows, spaces should be ^-escaped, i.e. c:/program^ files/puppet^ labs/puppet/bin.\npuppet_node (string) - The name of the node. If this isn't set, the fully qualified domain name will be used.\npuppet_server (string) - Hostname of the Puppet server. By default \"puppet\" will be used.\nstaging_dir (string) - Directory to where uploaded files will be placed (unix: \"/tmp/packer-puppet-masterless\", windows: \"%SYSTEMROOT%/Temp/packer-puppet-masterless\"). It doesn't need to pre-exist, but the parent must have permissions sufficient for the account Packer connects as to create directories and write files. Use a Shell provisioner to prepare the way if needed.\nworking_directory (string) - Directory from which execute_command will be run. If using Hiera files with relative paths, this option can be helpful. (default: staging_directory)\nelevated_user and elevated_password (string) - If specified, Puppet will be run with elevated privileges using the given Windows user. See the powershell provisioner for the full details. This is a template engine. Therefore, you may use user variables and template functions in this field, including {{ build `Password`}} to use the password being used by the communicator to connect to your instance.\nBy default, Packer uses the following command (broken across multiple lines for readability) to execute Puppet:\ncd {{.WorkingDir}} && {{if ne .FacterVars \"\"}}{{.FacterVars}} {{end}} {{if .Sudo}}sudo -E {{end}} {{if ne .PuppetBinDir \"\"}}{{.PuppetBinDir}}/{{end}} puppet agent --onetime --no-daemonize --detailed-exitcodes {{if .Debug}}--debug {{end}} {{if ne .PuppetServer \"\"}}--server='{{.PuppetServer}}' {{end}} {{if ne .PuppetNode \"\"}}--certname='{{.PuppetNode}}' {{end}} {{if ne .ClientCertPath \"\"}}--certdir='{{.ClientCertPath}}' {{end}} {{if ne .ClientPrivateKeyPath \"\"}}--privatekeydir='{{.ClientPrivateKeyPath}}' {{end}} {{if ne .ExtraArguments \"\"}}{{.ExtraArguments}} {{end}} \nThe following command is used if guest OS type is windows:\ncd {{.WorkingDir}} && {{if ne .FacterVars \"\"}}{{.FacterVars}} && {{end}} {{if ne .PuppetBinDir \"\"}}{{.PuppetBinDir}}/{{end}} puppet agent --onetime --no-daemonize --detailed-exitcodes {{if .Debug}}--debug {{end}} {{if ne .PuppetServer \"\"}}--server='{{.PuppetServer}}' {{end}} {{if ne .PuppetNode \"\"}}--certname='{{.PuppetNode}}' {{end}} {{if ne .ClientCertPath \"\"}}--certdir='{{.ClientCertPath}}' {{end}} {{if ne .ClientPrivateKeyPath \"\"}}--privatekeydir='{{.ClientPrivateKeyPath}}' {{end}} {{if ne .ExtraArguments \"\"}}{{.ExtraArguments}} {{end}} \nIn addition to being able to specify custom Facter facts using the facter configuration, the provisioner automatically defines certain commonly useful facts:\npacker_build_name is set to the name of the build that Packer is running. This is most useful when Packer is making multiple builds and you want to distinguish them in your Hiera hierarchy.\npacker_builder_type is the type of the builder that was used to create the machine that Puppet is running on. This is useful if you want to run only certain parts of your Puppet code on systems built with certain builders."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/provisioners/salt-masterless",
  "text": "Salt Masterless - Provisioners | Packer\nSalt Masterless Provisioner\nType: salt-masterless\nThe salt-masterless Packer provisioner provisions machines built by Packer using Salt states, without connecting to a Salt master.\n{ \"type\": \"salt-masterless\", \"local_state_tree\": \"/Users/me/salt\" } \nThe reference of available configuration options is listed below. The only required element is \"local_state_tree\".\nlocal_state_tree (string) - The path to your local state tree. This will be uploaded to the remote_state_tree on the remote.\nbootstrap_args (string) - Arguments to send to the bootstrap script. Usage is somewhat documented on github, but the script itself has more detailed usage instructions. By default, no arguments are sent to the script.\ndisable_sudo (boolean) - By default, the bootstrap install command is prefixed with sudo. When using a Docker builder, you will likely want to pass true since sudo is often not pre-installed.\nremote_pillar_roots (string) - The path to your remote pillar roots. default: /srv/pillar. This option cannot be used with minion_config.\nremote_state_tree (string) - The path to your remote state tree. default: /srv/salt. This option cannot be used with minion_config.\nlocal_pillar_roots (string) - The path to your local pillar roots. This will be uploaded to the remote_pillar_roots on the remote.\ncustom_state (string) - A state to be run instead of state.highstate. Defaults to state.highstate if unspecified.\nminion_config (string) - The path to your local minion config file. This will be uploaded to the /etc/salt on the remote. This option overrides the remote_state_tree or remote_pillar_roots options.\ngrains_file (string) - The path to your local grains file. This will be uploaded to /etc/salt/grains on the remote.\nskip_bootstrap (boolean) - By default the salt provisioner runs salt bootstrap to install salt. Set this to true to skip this step.\ntemp_config_dir (string) - Where your local state tree will be copied before moving to the /srv/salt directory. Default is /tmp/salt.\nno_exit_on_failure (boolean) - Packer will exit if the salt-call command fails. Set this option to true to ignore Salt failures.\nlog_level (string) - Set the logging level for the salt-call run.\nsalt_call_args (string) - Additional arguments to pass directly to salt-call. See salt-call documentation for more information. By default no additional arguments (besides the ones Packer generates) are passed to salt-call.\nsalt_bin_dir (string) - Path to the salt-call executable. Useful if it is not on the PATH.\nguest_os_type (string) - The target guest OS type, either \"unix\" or \"windows\"."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/provisioners/ansible-local",
  "text": "Ansible Local - Provisioners | Packer\nAnsible Local Provisioner\nType: ansible-local\nThe ansible-local Packer provisioner will run ansible in ansible's \"local\" mode on the remote/guest VM using Playbook and Role files that exist on the guest VM. This means ansible must be installed on the remote/guest VM. Playbooks and Roles can be uploaded from your build machine (the one running Packer) to the vm. Ansible is then run on the guest machine in local mode via the ansible-playbook command.\nNote: Ansible will not be installed automatically by this provisioner. This provisioner expects that Ansible is already installed on the guest/remote machine. It is common practice to use the shell provisioner before the Ansible provisioner to do this.\n{ \"builders\": [ { \"type\": \"docker\", \"image\": \"williamyeh/ansible:ubuntu14.04\", \"export_path\": \"packer_example\", \"run_command\": [\"-d\", \"-i\", \"-t\", \"--entrypoint=/bin/bash\", \"{{.Image}}\"] } ], \"variables\": { \"topping\": \"mushroom\" }, \"provisioners\": [ { \"type\": \"ansible-local\", \"playbook_file\": \"./playbook.yml\", \"extra_arguments\": [ \"--extra-vars\", \"\\\"pizza_toppings={{ user `topping`}}\\\"\" ] } ] } \nwhere ./playbook.yml contains\n--- - name: hello world hosts: 127.0.0.1 connection: local tasks: - command: echo {{ pizza_toppings }} - debug: msg=\"{{ pizza_toppings }}\" \nplaybook_file (string) - The playbook file to be executed by ansible. This file must exist on your local system and will be uploaded to the remote machine. This option is exclusive with playbook_files.\nplaybook_files (array of strings) - The playbook files to be executed by ansible. These files must exist on your local system. If the files don't exist in the playbook_dir or you don't set playbook_dir they will be uploaded to the remote machine. This option is exclusive with playbook_file.\ncommand (string) - The command to invoke ansible. Defaults to \"ANSIBLE_FORCE_COLOR=1 PYTHONUNBUFFERED=1 ansible-playbook\". Note, This disregards the value of -color when passed to packer build. To disable colors, set this to PYTHONUNBUFFERED=1 ansible-playbook.\nextra_arguments (array of strings) - An array of extra arguments to pass to the ansible command. By default, this is empty. These arguments will be passed through a shell and arguments should be quoted accordingly. Usage example:\n\"extra_arguments\": [ \"--extra-vars \\\"Region={{user `Region`}} Stage={{user `Stage`}}\\\"\" ] \ninventory_groups (string) - A comma-separated list of groups to which packer will assign the host 127.0.0.1. A value of my_group_1,my_group_2 will generate an Ansible inventory like:\n[my_group_1] 127.0.0.1 [my_group_2] 127.0.0.1 \ninventory_file (string) - The inventory file to be used by ansible. This file must exist on your local system and will be uploaded to the remote machine.\nWhen using an inventory file, it's also required to --limit the hosts to the specified host you're building. The --limit argument can be provided in the extra_arguments option.\nAn example inventory file may look like:\n[chi-dbservers] db-01 ansible_connection=local db-02 ansible_connection=local [chi-appservers] app-01 ansible_connection=local app-02 ansible_connection=local [chi:children] chi-dbservers chi-appservers [dbservers:children] chi-dbservers [appservers:children] chi-appservers \nplaybook_dir (string) - a path to the complete ansible directory structure on your local system to be copied to the remote machine as the staging_directory before all other files and directories.\nplaybook_paths (array of strings) - An array of directories of playbook files on your local system. These will be uploaded to the remote machine under staging_directory/playbooks. By default, this is empty.\ngalaxy_file (string) - A requirements file which provides a way to install roles with the ansible-galaxy cli on the remote machine. By default, this is empty.\ngalaxy_command (string) - The command to invoke ansible-galaxy. By default, this is ansible-galaxy.\ngroup_vars (string) - a path to the directory containing ansible group variables on your local system to be copied to the remote machine. By default, this is empty.\nhost_vars (string) - a path to the directory containing ansible host variables on your local system to be copied to the remote machine. By default, this is empty.\nrole_paths (array of strings) - An array of paths to role directories on your local system. These will be uploaded to the remote machine under staging_directory/roles. By default, this is empty.\nstaging_directory (string) - The directory where all the configuration of Ansible by Packer will be placed. By default this is /tmp/packer-provisioner-ansible-local/<uuid>, where <uuid> is replaced with a unique ID so that this provisioner can be run more than once. If you'd like to know the location of the staging directory in advance, you should set this to a known location. This directory doesn't need to exist but must have proper permissions so that the SSH user that Packer uses is able to create directories and write into this folder. If the permissions are not correct, use a shell provisioner prior to this to configure it properly.\nclean_staging_directory (boolean) - If set to true, the content of the staging_directory will be removed after executing ansible. By default, this is set to false.\nIn addition to being able to specify extra arguments using the extra_arguments configuration, the provisioner automatically defines certain commonly useful Ansible variables:\npacker_build_name is set to the name of the build that Packer is running. This is most useful when Packer is making multiple builds and you want to distinguish them slightly when using a common playbook.\npacker_builder_type is the type of the builder that was used to create the machine that the script is running on. This is useful if you want to run only certain parts of the playbook on systems built with certain builders.\npacker_http_addr If using a builder that provides an http server for file transfer (such as hyperv, parallels, qemu, virtualbox, and vmware), this will be set to the address. You can use this address in your provisioner to download large files over http. This may be useful if you're experiencing slower speeds using the default file provisioner. A file provisioner using the winrm communicator may experience these types of difficulties."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/provisioners/converge",
  "text": "Converge - Provisioners | Packer\nThis is community maintained provisioner is currently unmaintained; if you are interested in contributing or taking ownership of it, please reach out to us at packer@hashicorp.com. More details can be found in the README.\nType: converge\nThe Converge Packer provisioner uses Converge modules to provision the machine. It uploads module directories to use as source, or you can use remote modules.\nThe provisioner can optionally bootstrap the Converge client/server binary onto new images.\n{ \"type\": \"converge\", \"module\": \"https://raw.githubusercontent.com/asteris-llc/converge/master/samples/fileContent.hcl\", \"params\": { \"message\": \"Hello, Packer!\" } } \nThe reference of available configuration options is listed below. The only required element is \"module\". Every other option is optional.\nmodule (string) - Path (or URL) to the root module that Converge will apply.\nbootstrap (boolean, defaults to false) - Set to allow the provisioner to download the latest Converge bootstrap script and the specified version of Converge from the internet.\nversion (string) - Set to a released Converge version for bootstrap.\nmodule_dirs (array of directory specifications) - Module directories to transfer to the remote host for execution. See below for the specification.\nworking_directory (string) - The directory that Converge will change to before execution.\nparams (maps of string to string) - parameters to pass into the root module.\nexecute_command (string) - the command used to execute Converge. This is a configuration template variables. See below for detailed usage instructions.\nprevent_sudo (boolean) - stop Converge from running with administrator privileges via sudo\nbootstrap_command (string) - the command used to bootstrap Converge. This is a template engine. Therefore, you may use user variables and template functions in this field. The following extra variables are also avilable in this engine:\nVersion: The version of Converge to use.\nSudo: Boolean; intended to say whether to use sudo or not.\nBy default, this is \"curl -s https://get.converge.sh | {{if .Sudo}}sudo {{end}}sh {{if ne .Version \\\"\\\"}}-s -- -v {{.Version}}{{end}}\"\nprevent_bootstrap_sudo (boolean) - stop Converge from bootstrapping with administrator privileges via sudo\nModule Directories\nThe provisioner can transfer module directories to the remote host for provisioning. Of these fields, source and destination are required in every directory.\nsource (string) - the path to the folder on the local machine.\ndestination (string) - the path to the folder on the remote machine. Parent directories will not be created; use the shell module to do this.\nexclude (array of string) - files and directories to exclude from transfer.\nExecute Command\nBy default, Packer uses the following command (broken across multiple lines for readability) to execute Converge:\ncd {{.WorkingDirectory}} && \\ {{if .Sudo}}sudo {{end}}converge apply \\ --local \\ --log-level=WARNING \\ --paramsJSON '{{.ParamsJSON}}' \\ {{.Module}} \nThis command can be customized using the execute_command configuration. As you can see from the default value above, the value of this configuration can contain various template variables:\nWorkingDirectory - directory from the configuration.\nSudo - the opposite of prevent_sudo from the configuration.\nParamsJSON - The unquoted JSONified form of params from the configuration.\nModule - module from the configuration.\nBootstrap Command\nBy default, Packer uses the following command to bootstrap Converge:\ncurl -s https://get.converge.sh | {{if .Sudo}}sudo {{end}}sh {{if ne .Version \"\"}}-s -- -v {{.Version}}{{end}} \nThis command can be customized using the bootstrap_command configuration. As you can see from the default values above, the value of this configuration can contain various template variables:\nSudo - the opposite of prevent_bootstrap_sudo from the configuration.\nVersion - version from the configuration."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/provisioners/chef-client",
  "text": "Chef Client - Provisioners | Packer\nThis is community maintained provisioner is currently unmaintained; if you are interested in contributing or taking ownership of it, please reach out to us at packer@hashicorp.com. More details can be found in the README.\nType: chef-client\nThe Chef Client Packer provisioner installs and configures software on machines built by Packer using chef-client. Packer configures a Chef client to talk to a remote Chef Server to provision the machine.\nThe provisioner will even install Chef onto your machine if it isn't already installed, using the official Chef installers provided by Chef.\nThe example below is fully functional. It will install Chef onto the remote machine and run Chef client.\n{ \"type\": \"chef-client\", \"server_url\": \"https://mychefserver.com/\" } \nNote: to properly clean up the Chef node and client the machine on which packer is running must have knife on the path and configured globally, i.e, ~/.chef/knife.rb must be present and configured for the target chef server\nThe reference of available configuration options is listed below. No configuration is actually required.\nchef_environment (string) - The name of the chef_environment sent to the Chef server. By default this is empty and will not use an environment.\nchef_license (string) - As of Chef v15, Chef requires users to accept a license. Defaults to accept-silent when skip_install is false and install_command is unset. Possible values are accept, accept-silent and accept-no-persist. For details see Accepting the Chef License.\nThis is a template engine. Therefore, you may use user variables and template functions in this field.\nconfig_template (string) - Path to a template that will be used for the Chef configuration file. By default Packer only sets configuration it needs to match the settings set in the provisioner configuration. If you need to set configurations that the Packer provisioner doesn't support, then you should use a custom configuration template. See the dedicated \"Chef Configuration\" section below for more details.\nelevated_user and elevated_password (string) - If specified, Chef will be run with elevated privileges using the given Windows user. See the powershell provisioner for the full details.\nencrypted_data_bag_secret_path (string) - The path to the file containing the secret for encrypted data bags. By default, this is empty, so no secret will be available.\nexecute_command (string) - The command used to execute Chef. This has various configuration template variables available. See below for more information.\nguest_os_type (string) - The target guest OS type, either \"unix\" or \"windows\". Setting this to \"windows\" will cause the provisioner to use Windows friendly paths and commands. By default, this is \"unix\".\ninstall_command (string) - The command used to install Chef. This has various configuration template variables available. See below for more information.\njson (object) - An arbitrary mapping of JSON that will be available as node attributes while running Chef.\nknife_command (string) - The command used to run Knife during node clean-up. This has various configuration template variables available. See below for more information.\nnode_name (string) - The name of the node to register with the Chef Server. This is optional and by default is packer-{{uuid}}.\npolicy_group (string) - The name of a policy group that exists on the Chef server. policy_name must also be specified.\npolicy_name (string) - The name of a policy, as identified by the name setting in a Policyfile.rb file. policy_group must also be specified.\nprevent_sudo (boolean) - By default, the configured commands that are executed to install and run Chef are executed with sudo. If this is true, then the sudo will be omitted. This has no effect when guest_os_type is windows.\nrun_list (array of strings) - The run list for Chef. By default this is empty, and will use the run list sent down by the Chef Server.\nserver_url (string) - The URL to the Chef server. This is required.\nskip_clean_client (boolean) - If true, Packer won't remove the client from the Chef server after it is done running. By default, this is false.\nskip_clean_node (boolean) - If true, Packer won't remove the node from the Chef server after it is done running. By default, this is false.\nskip_clean_staging_directory (boolean) - If true, Packer won't remove the Chef staging directory from the machine after it is done running. By default, this is false.\nskip_install (boolean) - If true, Chef will not automatically be installed on the machine using the Chef omnibus installers.\nssl_verify_mode (string) - Set to \"verify_none\" to skip validation of SSL certificates. If not set, this defaults to \"verify_peer\" which validates all SSL certifications.\ntrusted_certs_dir (string) - This is a directory that contains additional SSL certificates to trust. Any certificates in this directory will be added to whatever CA bundle ruby is using. Use this to add self-signed certs for your Chef Server or local HTTP file servers.\nstaging_directory (string) - This is the directory where all the configuration of Chef by Packer will be placed. By default this is /tmp/packer-chef-client when guest_os_type unix and $env:TEMP/packer-chef-client when windows. This directory doesn't need to exist but must have proper permissions so that the user that Packer uses is able to create directories and write into this folder. By default the provisioner will create and chmod 0777 this directory.\nclient_key (string) - Path to client key. If not set, this defaults to a file named client.pem in staging_directory.\nvalidation_client_name (string) - Name of the validation client. If not set, this won't be set in the configuration and the default that Chef uses will be used.\nvalidation_key_path (string) - Path to the validation key for communicating with the Chef Server. This will be uploaded to the remote machine. If this is NOT set, then it is your responsibility via other means (shell provisioner, etc.) to get a validation key to where Chef expects it.\nversion (string) - The version of Chef to be installed. By default this is empty which will install the latest version of Chef.\nBy default, Packer uses a simple Chef configuration file in order to set the options specified for the provisioner. But Chef is a complex tool that supports many configuration options. Packer allows you to specify a custom configuration template if you'd like to set custom configurations.\nThe default value for the configuration template is:\nlog_level :info log_location STDOUT chef_server_url \"{{.ServerUrl}}\" client_key \"{{.ClientKey}}\" chef_license \"{{.ChefLicense}}\" {{if ne .EncryptedDataBagSecretPath \"\"}} encrypted_data_bag_secret \"{{.EncryptedDataBagSecretPath}}\" {{end}} {{if ne .ValidationClientName \"\"}} validation_client_name \"{{.ValidationClientName}}\" {{else}} validation_client_name \"chef-validator\" {{end}} {{if ne .ValidationKeyPath \"\"}} validation_key \"{{.ValidationKeyPath}}\" {{end}} node_name \"{{.NodeName}}\" {{if ne .ChefEnvironment \"\"}} environment \"{{.ChefEnvironment}}\" {{end}} {{if ne .PolicyGroup \"\"}} policy_group \"{{.PolicyGroup}}\" {{end}} {{if ne .PolicyName \"\"}} policy_name \"{{.PolicyName}}\" {{end}} {{if ne .SslVerifyMode \"\"}} ssl_verify_mode :{{.SslVerifyMode}} {{end}} {{if ne .TrustedCertsDir \"\"}} trusted_certs_dir :{{.TrustedCertsDir}} {{end}} \nThis template is a configuration template and has a set of variables available to use:\nChefEnvironment - The Chef environment name.\nChefLicense - The Chef license acceptance value.\nEncryptedDataBagSecretPath - The path to the secret key file to decrypt encrypted data bags.\nNodeName - The node name set in the configuration.\nServerUrl - The URL of the Chef Server set in the configuration.\nSslVerifyMode - Whether Chef SSL verify mode is on or off.\nTrustedCertsDir - Path to dir with trusted certificates.\nValidationClientName - The name of the client used for validation.\nValidationKeyPath - Path to the validation key, if it is set.\nBy default, Packer uses the following command (broken across multiple lines for readability) to execute Chef:\n{{if .Sudo}}sudo {{end}}chef-client \\ --no-color \\ -c {{.ConfigPath}} \\ -j {{.JsonPath}} \nWhen guest_os_type is set to \"windows\", Packer uses the following command to execute Chef. The full path to Chef is required because the PATH environment variable changes don't immediately propagate to running processes.\nc:/opscode/chef/bin/chef-client.bat \\ --no-color \\ -c {{.ConfigPath}} \\ -j {{.JsonPath}} \nThis command can be customized using the execute_command configuration. As you can see from the default value above, the value of this configuration can contain various template variables, defined below:\nConfigPath - The path to the Chef configuration file.\nJsonPath - The path to the JSON attributes file for the node.\nSudo - A boolean of whether to sudo the command or not, depending on the value of the prevent_sudo configuration.\nBy default, Packer uses the following command (broken across multiple lines for readability) to install Chef. This command can be customized if you want to install Chef in another way.\ncurl -L https://omnitruck.chef.io/chef/install.sh | \\ {{if .Sudo}}sudo{{end}} bash \nWhen guest_os_type is set to \"windows\", Packer uses the following command to install the latest version of Chef:\npowershell.exe -Command \"(New-Object System.Net.WebClient).DownloadFile('http://chef.io/chef/install.msi', 'C:\\\\Windows\\\\Temp\\\\chef.msi');Start-Process 'msiexec' -ArgumentList '/qb /i C:\\\\Windows\\\\Temp\\\\chef.msi' -NoNewWindow -Wait\" \nThis command can be customized using the install_command configuration.\n{{if .Sudo}}sudo {{end}}knife \\ {{.Args}} \\ {{.Flags}} \nc:/opscode/chef/bin/knife.bat \\ {{.Args}} \\ {{.Flags}} \nThis command can be customized using the knife_command configuration. As you can see from the default value above, the value of this configuration can contain various template variables, defined below:\nArgs - The command arguments that are getting passed to the Knife command.\nFlags - The command flags that are getting passed to the Knife command..\nThe chef-client provisioner will chmod the directory with your Chef keys to 777. This is to ensure that Packer can upload and make use of that directory. However, once the machine is created, you usually don't want to keep these directories with those permissions. To change the permissions on the directories, append a shell provisioner after Chef to modify them.\nChef Client Local Mode - Simple\nThe following example shows how to run the chef-client provisioner in local mode.\nPacker variables\nSet the necessary Packer variables using environment variables or provide a var file.\n\"variables\": { \"chef_dir\": \"/tmp/packer-chef-client\" } \nSetup the chef-client provisioner\nMake sure we have the correct directories and permissions for the chef-client provisioner. You will need to bootstrap the Chef run by providing the necessary cookbooks using Berkshelf or some other means.\n\"provisioners\": [ ... { \"type\": \"shell\", \"inline\": [ \"mkdir -p {{user `chef_dir`}}\" ] }, { \"type\": \"file\", \"source\": \"./roles\", \"destination\": \"{{user `chef_dir`}}\" }, { \"type\": \"file\", \"source\": \"./cookbooks\", \"destination\": \"{{user `chef_dir`}}\" }, { \"type\": \"file\", \"source\": \"./data_bags\", \"destination\": \"{{user `chef_dir`}}\" }, { \"type\": \"file\", \"source\": \"./environments\", \"destination\": \"{{user `chef_dir`}}\" }, { \"type\": \"file\", \"source\": \"./scripts/install_chef.sh\", \"destination\": \"{{user `chef_dir`}}/install_chef.sh\" }, { \"type\": \"chef-client\", \"install_command\": \"sudo bash {{user `chef_dir`}}/install_chef.sh\", \"server_url\": \"http://localhost:8889\", \"config_template\": \"./config/client.rb.template\", \"run_list\": [ \"role[testing]\" ], \"skip_clean_node\": true, \"skip_clean_client\": true } ... ] \nAnd ./config/client.rb.template referenced by the above configuration:\nlog_level :info log_location STDOUT local_mode true chef_zero.enabled true ssl_verify_mode \"verify_peer\" role_path \"{{user `chef_dir`}}/roles\" data_bag_path \"{{user `chef_dir`}}/data_bags\" environment_path \"{{user `chef_dir`}}/environments\" cookbook_path [ \"{{user `chef_dir`}}/cookbooks\" ] \nChef Client Local Mode - Passing variables\nThe following example shows how to run the chef-client provisioner in local mode, while passing a run_list using a variable.\nLocal environment variables\n# Machine's Chef directory export PACKER_CHEF_DIR=/var/chef-packer # Comma separated run_list export PACKER_CHEF_RUN_LIST=\"recipe[apt],recipe[nginx]\" \nPacker variables\nSet the necessary Packer variables using environment variables or provide a var file.\n\"variables\": { \"chef_dir\": \"{{env `PACKER_CHEF_DIR`}}\", \"chef_run_list\": \"{{env `PACKER_CHEF_RUN_LIST`}}\", \"chef_client_config_tpl\": \"{{env `PACKER_CHEF_CLIENT_CONFIG_TPL`}}\", \"packer_chef_bootstrap_dir\": \"{{env `PACKER_CHEF_BOOTSTRAP_DIR`}}\" , \"packer_uid\": \"{{env `PACKER_UID`}}\", \"packer_gid\": \"{{env `PACKER_GID`}}\" } \nSetup the chef-client provisioner\nMake sure we have the correct directories and permissions for the chef-client provisioner. You will need to bootstrap the Chef run by providing the necessary cookbooks using Berkshelf or some other means.\n({ \"type\": \"file\", \"source\": \"{{user `packer_chef_bootstrap_dir`}}\", \"destination\": \"/tmp/bootstrap\" }, { \"type\": \"shell\", \"inline\": [ \"sudo mkdir -p {{user `chef_dir`}}\", \"sudo mkdir -p /tmp/packer-chef-client\", \"sudo chown {{user `packer_uid`}}.{{user `packer_gid`}} /tmp/packer-chef-client\", \"sudo sh /tmp/bootstrap/bootstrap.sh\" ] }, { \"type\": \"chef-client\", \"server_url\": \"http://localhost:8889\", \"config_template\": \"{{user `chef_client_config_tpl`}}/client.rb.tpl\", \"skip_clean_node\": true, \"skip_clean_client\": true, \"run_list\": \"{{user `chef_run_list`}}\" })"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/provisioners/chef-solo",
  "text": "Chef Solo - Provisioners | Packer\nThis is community maintained provisioner is currently unmaintained; if you are interested in contributing or taking ownership of it, please reach out to us at packer@hashicorp.com. More details can be found in the README.\nType: chef-solo\nThe Chef solo Packer provisioner installs and configures software on machines built by Packer using chef-solo. Cookbooks can be uploaded from your local machine to the remote machine or remote paths can be used.\nThe provisioner will even install Chef onto your machine if it isn't already installed, using the official Chef installers provided by Chef Inc.\nThe example below is fully functional and expects cookbooks in the \"cookbooks\" directory relative to your working directory.\n{ \"type\": \"chef-solo\", \"cookbook_paths\": [\"cookbooks\"] } \nThe reference of available configuration options is listed below. No configuration is actually required, but at least run_list is recommended.\nchef_environment (string) - The name of the chef_environment sent to the Chef server. By default this is empty and will not use an environment\nchef_license (string) - As of Chef v15, Chef requires users to accept a license. Defaults to accept-silent when skip_install is false and install_command is unset. Possible values are accept, accept-silent and accept-no-persist. For details see Accepting the Chef License.\nconfig_template (string) - Path to a template that will be used for the Chef configuration file. By default Packer only sets configuration it needs to match the settings set in the provisioner configuration. If you need to set configurations that the Packer provisioner doesn't support, then you should use a custom configuration template. See the dedicated \"Chef Configuration\" section below for more details.\ncookbook_paths (array of strings) - This is an array of paths to \"cookbooks\" directories on your local filesystem. These will be uploaded to the remote machine in the directory specified by the staging_directory. By default, this is empty.\ndata_bags_path (string) - The path to the \"data_bags\" directory on your local filesystem. These will be uploaded to the remote machine in the directory specified by the staging_directory. By default, this is empty.\nencrypted_data_bag_secret_path (string) - The path to the file containing the secret for encrypted data bags. By default, this is empty, so no secret will be available.\nenvironments_path (string) - The path to the \"environments\" directory on your local filesystem. These will be uploaded to the remote machine in the directory specified by the staging_directory. By default, this is empty.\nexecute_command (string) - The command used to execute Chef. This has various configuration template variables available. See below for more information.\nguest_os_type (string) - The target guest OS type, either \"unix\" or \"windows\". Setting this to \"windows\" will cause the provisioner to use Windows friendly paths and commands. By default, this is \"unix\".\ninstall_command (string) - The command used to install Chef. This has various configuration template variables available. See below for more information.\njson (object) - An arbitrary mapping of JSON that will be available as node attributes while running Chef.\nprevent_sudo (boolean) - By default, the configured commands that are executed to install and run Chef are executed with sudo. If this is true, then the sudo will be omitted. This has no effect when guest_os_type is windows.\nremote_cookbook_paths (array of strings) - A list of paths on the remote machine where cookbooks will already exist. These may exist from a previous provisioner or step. If specified, Chef will be configured to look for cookbooks here. By default, this is empty.\nroles_path (string) - The path to the \"roles\" directory on your local filesystem. These will be uploaded to the remote machine in the directory specified by the staging_directory. By default, this is empty.\nrun_list (array of strings) - The run list for Chef. By default this is empty.\nskip_install (boolean) - If true, Chef will not automatically be installed on the machine using the Chef omnibus installers.\nstaging_directory (string) - This is the directory where all the configuration of Chef by Packer will be placed. By default this is /tmp/packer-chef-solo when guest_os_type unix and $env:TEMP/packer-chef-solo when windows. This directory doesn't need to exist but must have proper permissions so that the user that Packer uses is able to create directories and write into this folder. If the permissions are not correct, use a shell provisioner prior to this to configure it properly.\nversion (string) - The version of Chef to be installed. By default this is empty which will install the latest version of Chef.\nBy default, Packer uses a simple Chef configuration file in order to set the options specified for the provisioner. But Chef is a complex tool that supports many configuration options. Packer allows you to specify a custom configuration template if you'd like to set custom configurations.\nThe default value for the configuration template is:\ncookbook_path [{{.CookbookPaths}}] \nThis template is a configuration template and has a set of variables available to use:\nChefEnvironment - The current enabled environment. Only non-empty if the environment path is set.\nChefLicense - The Chef license acceptance value.\nCookbookPaths is the set of cookbook paths ready to embedded directly into a Ruby array to configure Chef.\nDataBagsPath is the path to the data bags folder.\nEncryptedDataBagSecretPath - The path to the encrypted data bag secret\nEnvironmentsPath - The path to the environments folder.\nRolesPath - The path to the roles folder.\n{{if .Sudo}}sudo {{end}}chef-solo \\ --no-color \\ -c {{.ConfigPath}} \\ -j {{.JsonPath}} \nc:/opscode/chef/bin/chef-solo.bat \\ --no-color \\ -c {{.ConfigPath}} \\ -j {{.JsonPath}} \nThis command can be customized using the execute_command configuration. As you can see from the default value above, the value of this configuration can contain various template variables, defined below:\nConfigPath - The path to the Chef configuration file.\nJsonPath - The path to the JSON attributes file for the node.\nBy default, Packer uses the following command (broken across multiple lines for readability) to install Chef. This command can be customized if you want to install Chef in another way.\ncurl -L https://omnitruck.chef.io/install.sh | \\ {{if .Sudo}}sudo{{end}} bash -s --{{if .Version}} -v {{.Version}}{{end}} \nWhen guest_os_type is set to \"windows\", Packer uses the following command to install the latest version of Chef:\npowershell.exe -Command \\\". { iwr -useb https://omnitruck.chef.io/install.ps1 } | iex; install\\\" \nThis command can be customized using the install_command configuration."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/provisioners/inspec",
  "text": "InSpec - Provisioners | Packer\nType: inspec\nThe inspec Packer provisioner runs InSpec profiles. It dynamically creates a target configured to use SSH, runs an SSH server, executes inspec exec, and marshals InSpec tests through the SSH server to the machine being provisioned by Packer.\nNote: Inspec is required to be installed on the host machine and available in it's corresponding path. It is not required to be installed on the provisioned image.\nThis is a fully functional template that will test an image on DigitalOcean. Replace the mock api_token value with your own.\n{ \"provisioners\": [ { \"type\": \"inspec\", \"profile\": \"https://github.com/dev-sec/linux-baseline\" } ], \"builders\": [ { \"type\": \"digitalocean\", \"api_token\": \"<digital ocean api token>\", \"image\": \"ubuntu-14-04-x64\", \"region\": \"sfo1\" } ] } \nprofile (string) - The profile to be executed by InSpec.\ninspec_env_vars (array of strings) - Environment variables to set before running InSpec. Usage example:\n\"inspec_env_vars\": [ \"FOO=bar\" ] \ncommand (string) - The command to invoke InSpec. Defaults to inspec.\nextra_arguments (array of strings) - Extra arguments to pass to InSpec. These arguments will not be passed through a shell and arguments should not be quoted. Usage example:\n\"extra_arguments\": [ \"--sudo\", \"--reporter\", \"json\" ] \nattributes (array of strings) - Attribute Files used by InSpec which will be passed to the --input-file argument of the inspec command when this provisioner runs InSpec. Specify this if you want a different location. Note using also \"--input-file\" in extra_arguments will override this setting.\nattributes_directory (string) - The directory in which to place the temporary generated InSpec Attributes file. By default, this is the system-specific temporary file location. The fully-qualified name of this temporary file will be passed to the --input-file argument of the inspec command when this provisioner runs InSpec. Specify this if you want a different location.\nbackend (string) - Backend used by InSpec for connection. Defaults to SSH.\nhost (string) - Host used for by InSpec for connection. Defaults to localhost.\nlocal_port (uint) - The port on which to attempt to listen for SSH connections. This value is a starting point. The provisioner will attempt to listen for SSH connections on the first available of ten ports, starting at local_port. A system-chosen port is used when local_port is missing or empty.\nssh_host_key_file (string) - The SSH key that will be used to run the SSH server on the host machine to forward commands to the target machine. InSpec connects to this server and will validate the identity of the server using the system known_hosts. The default behavior is to generate and use a onetime key.\nssh_authorized_key_file (string) - The SSH public key of the InSpec ssh_user. The default behavior is to generate and use a onetime key. If this key is generated, the corresponding private key is passed to inspec command with the -i inspec_ssh_private_key_file option.\nuser (string) - The --user to use. Defaults to the user running Packer.\nChef InSpec requires accepting the license before starting to use the tool. This can be done via inspec_env_vars in the template:\n\"provisioners\": [ { \"type\": \"inspec\", \"inspec_env_vars\": [ \"CHEF_LICENSE=accept\"], \"profile\": \"https://github.com/dev-sec/linux-baseline\" } ] \nSee their official docs to learn other ways to accept the license.\nIn addition to being able to specify extra arguments using the extra_arguments configuration, the provisioner automatically defines certain commonly useful InSpec Attributes:\npacker_build_name is set to the name of the build that Packer is running. This is most useful when Packer is making multiple builds and you want to distinguish them slightly when using a common profile.\npacker_builder_type is the type of the builder that was used to create the machine that the script is running on. This is useful if you want to run only certain parts of the profile on systems built with certain builders.\nTo debug underlying issues with InSpec, add \"-l\" to \"extra_arguments\" to enable verbose logging.\n\"extra_arguments\": [\"-l\", \"debug\"]"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/provisioners/puppet-masterless",
  "text": "Puppet Masterless - Provisioners | Packer\nPuppet (Masterless) Provisioner\nThis is community maintained provisioner is currently unmaintained; if you are interested in contributing or taking ownership of it, please reach out to us at packer@hashicorp.com. More details can be found in the README.\nType: puppet-masterless\nThe masterless Puppet Packer provisioner configures Puppet to run on the machines by Packer from local modules and manifest files. Modules and manifests can be uploaded from your local machine to the remote machine or can simply use remote paths (perhaps obtained using something like the shell provisioner). Puppet is run in masterless mode, meaning it never communicates to a Puppet master.\nNote: Puppet will not be installed automatically by this provisioner. This provisioner expects that Puppet is already installed on the machine. It is common practice to use the shell provisioner before the Puppet provisioner to do this.\nThe example below is fully functional and expects the configured manifest file to exist relative to your working directory.\n{ \"type\": \"puppet-masterless\", \"manifest_file\": \"site.pp\" } \nRequired parameters:\nmanifest_file (string) - This is either a path to a puppet manifest (.pp file) or a directory containing multiple manifests that puppet will apply (the \"main manifest\"). These file(s) must exist on your local system and will be uploaded to the remote machine.\nexecute_command (string) - The command-line to execute Puppet. This is a template engine. Therefore, you may use user variables and template functions in this field. In addition, you may use the following extra variables:\nFacterVars: Additional facts to set when executing Puppet, joined for use in a command. This is internal and not settable by the user.\nHieraConfigPath: Path to a hiera configuration file to upload and use; set in the template option hiera_config_path\nManifestDir: The directory of the manifest file on the remote machine.\nManifestFile: The manifest file on the remote machine.\nModulePath: The module_paths, combined into one path variable.\nModulePathJoiner: A colon : on posix systems and a semicolon ; on Windows.\nPuppetBinDir: The directory that the Puppet binary is in.\nSudo: Returns false when PreventSudo is set, and true when it is not.\nWorkingDir: The value provided in the working_directory option. See below for execute_command's default value.\nextra_arguments (array of strings) - Additional options to pass to the Puppet command. This allows for customization of execute_command without having to completely replace or subsume its contents, making forward-compatible customizations much easier to maintain.\nThis string is lazy-evaluated so one can incorporate logic driven by template variables as well as private elements of ExecuteTemplate (see source: provisioner/puppet-masterless/provisioner.go).\n[ {{if ne \"{{user environment}}\" \"\"}}--environment={{user environment}}{{end}}, {{if ne \".ModulePath\" \"\"}}--modulepath=\"{{.ModulePath}}{{.ModulePathJoiner}}$(puppet config print {{if ne \"{{user `environment`}}\" \"\"}}--environment={{user `environment`}}{{end}} modulepath)\"{{end}} ] \nfacter (object of key:value strings) - Additional facts to make available to the Puppet run.\nguest_os_type (string) - The remote host's OS type ('windows' or 'unix') to tailor command-line and path separators. (default: unix).\nhiera_config_path (string) - Local path to self-contained Hiera data to be uploaded. NOTE: If you need data directories they must be previously transferred with a File provisioner.\nignore_exit_codes (boolean) - If true, Packer will ignore failures.\nmanifest_dir (string) - Local directory with manifests to be uploaded. This is useful if your main manifest uses imports, but the directory might not contain the manifest_file itself.\nmanifest_dir is passed to Puppet as --manifestdir option. This option was deprecated in puppet 3.6, and removed in puppet 4.0. If you have multiple manifests you should use manifest_file instead.\nmodule_paths (array of strings) - Array of local module directories to be uploaded.\nprevent_sudo (boolean) - On Unix platforms Puppet is typically invoked with sudo. If true, it will be omitted. (default: false)\npuppet_bin_dir (string) - Path to the Puppet binary. Ideally the program should be on the system (unix: $PATH, windows: %PATH%), but some builders (eg. Docker) do not run profile-setup scripts and therefore PATH might be empty or minimal. On Windows, spaces should be ^-escaped, i.e. c:/program^ files/puppet^ labs/puppet/bin.\nstaging_directory (string) - Directory to where uploaded files will be placed (unix: \"/tmp/packer-puppet-masterless\", windows: \"%SYSTEMROOT%/Temp/packer-puppet-masterless\"). It doesn't need to pre-exist, but the parent must have permissions sufficient for the account Packer connects as to create directories and write files. Use a Shell provisioner to prepare the way if needed.\nworking_directory (string) - Directory from which execute_command will be run. If using Hiera files with relative paths, this option can be helpful. (default: staging_directory)\nelevated_user and elevated_password (string) - If specified, Puppet will be run with elevated privileges using the given Windows user. See the powershell provisioner for the full details. This is a template engine. Therefore, you may use user variables and template functions in this field, including {{ build `Password`}} to use the password being used by the communicator to connect to your instance.\nBy default, Packer uses the following command (broken across multiple lines for readability) to execute Puppet:\ncd {{.WorkingDir}} && {{if ne .FacterVars \"\"}}{{.FacterVars}} {{end}} {{if .Sudo}}sudo -E {{end}} {{if ne .PuppetBinDir \"\"}}{{.PuppetBinDir}}/{{end}} puppet apply --detailed-exitcodes {{if .Debug}}--debug {{end}} {{if ne .ModulePath \"\"}}--modulepath='{{.ModulePath}}' {{end}} {{if ne .HieraConfigPath \"\"}}--hiera_config='{{.HieraConfigPath}}' {{end}} {{if ne .ManifestDir \"\"}}--manifestdir='{{.ManifestDir}}' {{end}} {{if ne .ExtraArguments \"\"}}{{.ExtraArguments}} {{end}} {{.ManifestFile}} \nThe following command is used if guest OS type is windows:\ncd {{.WorkingDir}} && {{if ne .FacterVars \"\"}}{{.FacterVars}} && {{end}} {{if ne .PuppetBinDir \"\"}}{{.PuppetBinDir}}/{{end}} puppet apply --detailed-exitcodes {{if .Debug}}--debug {{end}} {{if ne .ModulePath \"\"}}--modulepath='{{.ModulePath}}' {{end}} {{if ne .HieraConfigPath \"\"}}--hiera_config='{{.HieraConfigPath}}' {{end}} {{if ne .ManifestDir \"\"}}--manifestdir='{{.ManifestDir}}' {{end}} {{if ne .ExtraArguments \"\"}}{{.ExtraArguments}} {{end}} {{.ManifestFile}} \nIn addition to being able to specify custom Facter facts using the facter configuration, the provisioner automatically defines certain commonly useful facts:\npacker_build_name is set to the name of the build that Packer is running. This is most useful when Packer is making multiple builds and you want to distinguish them in your Hiera hierarchy.\npacker_builder_type is the type of the builder that was used to create the machine that Puppet is running on. This is useful if you want to run only certain parts of your Puppet code on systems built with certain builders."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/provisioners/ansible",
  "text": "Ansible - Provisioners | Packer\nType: ansible\nThe ansible Packer provisioner runs Ansible playbooks. It dynamically creates an Ansible inventory file configured to use SSH, runs an SSH server, executes ansible-playbook, and marshals Ansible plays through the SSH server to the machine being provisioned by Packer.\nNote: Any remote_user defined in tasks will be ignored. Packer will always connect with the user given in the json config for this provisioner.\nNote: Options below that use the Packer template engine won't be able to accept jinja2 {{ function }} macro syntax in a way that can be preserved to the Ansible run. If you need to set variables using Ansible macros, you need to do so inside your playbooks or inventory files.\nPlease see the Debugging, Limitations, or Troubleshooting if you are having trouble getting started.\nThis is a fully functional template that will provision an image on DigitalOcean. Replace the mock api_token value with your own.\nExample Packer template:\n{ \"builders\": [ { \"type\": \"digitalocean\", \"api_token\": \"6a561151587389c7cf8faa2d83e94150a4202da0e2bad34dd2bf236018ffaeeb\", \"image\": \"ubuntu-14-04-x64\", \"region\": \"sfo1\" } ], \"provisioners\": [ { \"type\": \"ansible\", \"playbook_file\": \"./playbook.yml\" } ] } \nExample playbook:\n--- # playbook.yml - name: 'Provision Image' hosts: default become: true tasks: - name: install Apache package: name: 'httpd' state: present \nplaybook_file (string) - The playbook to be run by Ansible.\ncommand (string) - The command to invoke ansible. Defaults to ansible-playbook. If you would like to provide a more complex command, for example, something that sets up a virtual environment before calling ansible, take a look at the ansible wrapper guide below for inspiration. Please note that Packer expects Command to be a path to an executable. Arbitrary bash scripting will not work and needs to go inside an executable script.\nextra_arguments ([]string) - Extra arguments to pass to Ansible. These arguments will not be passed through a shell and arguments should not be quoted. Usage example:\n\"extra_arguments\": [ \"--extra-vars\", \"Region={{user `Region`}} Stage={{user `Stage`}}\" ] \nIf you are running a Windows build on AWS, Azure, Google Compute, or OpenStack and would like to access the auto-generated password that Packer uses to connect to a Windows instance via WinRM, you can use the template variable {{.WinRMPassword}} in this option. For example:\n\"extra_arguments\": [ \"--extra-vars\", \"winrm_password={{ .WinRMPassword }}\" ] \nansible_env_vars ([]string) - Environment variables to set before running Ansible. Usage example:\n\"ansible_env_vars\": [ \"ANSIBLE_HOST_KEY_CHECKING=False\", \"ANSIBLE_SSH_ARGS='-o ForwardAgent=yes -o ControlMaster=auto -o ControlPersist=60s'\", \"ANSIBLE_NOCOLOR=True\" ] \nThis is a template engine. Therefore, you may use user variables and template functions in this field.\nFor example, if you are running a Windows build on AWS, Azure, Google Compute, or OpenStack and would like to access the auto-generated password that Packer uses to connect to a Windows instance via WinRM, you can use the template variable {{.WinRMPassword}} in this option. Example:\n\"ansible_env_vars\": [ \"WINRM_PASSWORD={{.WinRMPassword}}\" ], \nansible_ssh_extra_args ([]string) - Specifies --ssh-extra-args on command line defaults to -o IdentitiesOnly=yes\ngroups ([]string) - The groups into which the Ansible host should be placed. When unspecified, the host is not associated with any groups.\nempty_groups ([]string) - The groups which should be present in inventory file but remain empty.\nhost_alias (string) - The alias by which the Ansible host should be known. Defaults to default. This setting is ignored when using a custom inventory file.\nuser (string) - The ansible_user to use. Defaults to the user running packer, NOT the user set for your communicator. If you want to use the same user as the communicator, you will need to manually set it again in this field.\nlocal_port (int) - The port on which to attempt to listen for SSH connections. This value is a starting point. The provisioner will attempt listen for SSH connections on the first available of ten ports, starting at local_port. A system-chosen port is used when local_port is missing or empty.\nssh_host_key_file (string) - The SSH key that will be used to run the SSH server on the host machine to forward commands to the target machine. Ansible connects to this server and will validate the identity of the server using the system known_hosts. The default behavior is to generate and use a onetime key. Host key checking is disabled via the ANSIBLE_HOST_KEY_CHECKING environment variable if the key is generated.\nssh_authorized_key_file (string) - The SSH public key of the Ansible ssh_user. The default behavior is to generate and use a onetime key. If this key is generated, the corresponding private key is passed to ansible-playbook with the -e ansible_ssh_private_key_file option.\nsftp_command (string) - The command to run on the machine being provisioned by Packer to handle the SFTP protocol that Ansible will use to transfer files. The command should read and write on stdin and stdout, respectively. Defaults to /usr/lib/sftp-server -e.\nskip_version_check (bool) - Check if ansible is installed prior to running. Set this to true, for example, if you're going to install ansible during the packer run.\nuse_sftp (bool) - Use SFTP\ninventory_directory (string) - The directory in which to place the temporary generated Ansible inventory file. By default, this is the system-specific temporary file location. The fully-qualified name of this temporary file will be passed to the -i argument of the ansible command when this provisioner runs ansible. Specify this if you have an existing inventory directory with host_vars group_vars that you would like to use in the playbook that this provisioner will run.\ninventory_file_template (string) - This template represents the format for the lines added to the temporary inventory file that Packer will create to run Ansible against your image. The default for recent versions of Ansible is: \"{{ .HostAlias }} ansible_host={{ .Host }} ansible_user={{ .User }} ansible_port={{ .Port }}\\n\" Available template engines are: This option is a template engine; variables available to you include the examples in the default (Host, HostAlias, User, Port) as well as any variables available to you via the \"build\" template engine.\ninventory_file (string) - The inventory file to use during provisioning. When unspecified, Packer will create a temporary inventory file and will use the host_alias.\nkeep_inventory_file (bool) - If true, the Ansible provisioner will not delete the temporary inventory file it creates in order to connect to the instance. This is useful if you are trying to debug your ansible run and using \"--on-error=ask\" in order to leave your instance running while you test your playbook. this option is not used if you set an inventory_file.\ngalaxy_file (string) - A requirements file which provides a way to install roles or collections with the ansible-galaxy cli on the local machine before executing ansible-playbook. By default, this is empty.\ngalaxy_command (string) - The command to invoke ansible-galaxy. By default, this is ansible-galaxy.\ngalaxy_force_install (bool) - Force overwriting an existing role. Adds --force option to ansible-galaxy command. By default, this is false.\nroles_path (string) - The path to the directory on your local system in which to install the roles. Adds --roles-path /path/to/your/roles to ansible-galaxy command. By default, this is empty, and thus --roles-path option is not added to the command.\ncollections_path (string) - The path to the directory on your local system in which to install the collections. Adds --collections-path /path/to/your/collections to ansible-galaxy command. By default, this is empty, and thus --collections-path option is not added to the command.\nuse_proxy (boolean) - When true, set up a localhost proxy adapter so that Ansible has an IP address to connect to, even if your guest does not have an IP address. For example, the adapter is necessary for Docker builds to use the Ansible provisioner. If you set this option to false, but Packer cannot find an IP address to connect Ansible to, it will automatically set up the adapter anyway.\nIn order for Ansible to connect properly even when use_proxy is false, you need to make sure that you are either providing a valid username and ssh key to the ansible provisioner directly, or that the username and ssh key being used by the ssh communicator will work for your needs. If you do not provide a user to ansible, it will use the user associated with your builder, not the user running Packer. use_proxy=false is currently only supported for SSH and WinRM.\nCurrently, this defaults to true for all connection types. In the future, this option will be changed to default to false for SSH and WinRM connections where the provisioner has access to a host IP.\nIn addition to being able to specify extra arguments using the extra_arguments configuration, the provisioner automatically defines certain commonly useful Ansible variables:\npacker_build_name is set to the name of the build that Packer is running. This is most useful when Packer is making multiple builds and you want to distinguish them slightly when using a common playbook.\npacker_builder_type is the type of the builder that was used to create the machine that the script is running on. This is useful if you want to run only certain parts of the playbook on systems built with certain builders.\npacker_http_addr If using a builder that provides an http server for file transfer (such as hyperv, parallels, qemu, virtualbox, and vmware), this will be set to the address. You can use this address in your provisioner to download large files over http. This may be useful if you're experiencing slower speeds using the default file provisioner. A file provisioner using the winrm communicator may experience these types of difficulties.\nTo debug underlying issues with Ansible, add \"-vvvv\" to \"extra_arguments\" to enable verbose logging.\n\"extra_arguments\": [ \"-vvvv\" ] \nRedhat / CentOS\nRedhat / CentOS builds have been known to fail with the following error due to sftp_command, which should be set to /usr/libexec/openssh/sftp-server -e:\n==> virtualbox-ovf: starting sftp subsystem virtualbox-ovf: fatal: [default]: UNREACHABLE! => {\"changed\": false, \"msg\": \"SSH Error: data could not be sent to the remote host. Make sure this host can be reached over ssh\", \"unreachable\": true} \nchroot communicator\nBuilding within a chroot (e.g. amazon-chroot) requires changing the Ansible connection to chroot and running Ansible as root/sudo.\n{ \"builders\": [ { \"type\": \"amazon-chroot\", \"mount_path\": \"/mnt/packer-amazon-chroot\", \"region\": \"us-east-1\", \"source_ami\": \"ami-123456\" } ], \"provisioners\": [ { \"type\": \"ansible\", \"extra_arguments\": [ \"--connection=chroot\", \"--inventory-file=/mnt/packer-amazon-chroot\" ], \"playbook_file\": \"main.yml\" } ] } \nWinRM Communicator\nThere are two possible methods for using ansible with the WinRM communicator.\nPlease note that if you're having trouble getting Ansible to connect, you may want to take a look at the script that the Ansible project provides to help configure remoting for Ansible: https://github.com/ansible/ansible/blob/devel/examples/scripts/ConfigureRemotingForAnsible.ps1\nMethod 1 (recommended)\nThe recommended way to use the WinRM communicator is to set \"use_proxy\": false and let the Ansible provisioner handle the rest for you. If you are using WinRM with HTTPS, and you are using a self-signed certificate you will also have to set ansible_winrm_server_cert_validation=ignore in your extra_arguments.\nBelow is a fully functioning Ansible example using WinRM:\n{ \"builders\": [ { \"type\": \"amazon-ebs\", \"region\": \"us-east-1\", \"instance_type\": \"t2.micro\", \"source_ami_filter\": { \"filters\": { \"virtualization-type\": \"hvm\", \"name\": \"*Windows_Server-2012*English-64Bit-Base*\", \"root-device-type\": \"ebs\" }, \"most_recent\": true, \"owners\": \"amazon\" }, \"ami_name\": \"test-ansible-packer\", \"user_data_file\": \"windows_bootstrap.txt\", \"communicator\": \"winrm\", \"force_deregister\": true, \"winrm_insecure\": true, \"winrm_username\": \"Administrator\", \"winrm_use_ssl\": true } ], \"provisioners\": [ { \"type\": \"ansible\", \"playbook_file\": \"./playbook.yml\", \"user\": \"Administrator\", \"use_proxy\": false, \"extra_arguments\": [\"-e\", \"ansible_winrm_server_cert_validation=ignore\"] } ] } \nNote that you do have to set the \"Administrator\" user, because otherwise Ansible will default to using the user that is calling Packer, rather than the user configured inside of the Packer communicator. For the contents of windows_bootstrap.txt, see the winrm docs for the amazon-ebs communicator.\nWhen running from OSX, you may see an error like:\namazon-ebs: objc[9752]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called. amazon-ebs: objc[9752]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug. amazon-ebs: ERROR! A worker was found in a dead state \nIf you see this, you may be able to work around the issue by telling Ansible to explicitly not use any proxying; you can do this by setting the template option\n\"ansible_env_vars\": [\"no_proxy=\\\"*\\\"\"], \nin the above Ansible template.\nMethod 2 (Not recommended)\nIf you want to use the Packer ssh proxy, then you need a custom Ansible connection plugin and a particular configuration. You need a directory named connection_plugins next to the playbook which contains a file named packer.py` which implements the connection plugin. On versions of Ansible before 2.4.x, the following works as the connection plugin:\nfrom __future__ import (absolute_import, division, print_function) __metaclass__ = type from ansible.plugins.connection.ssh import Connection as SSHConnection class Connection(SSHConnection): ''' ssh based connections for powershell via packer''' transport = 'packer' has_pipelining = True become_methods = [] allow_executable = False module_implementation_preferences = ('.ps1', '') def __init__(self, *args, **kwargs): super(Connection, self).__init__(*args, **kwargs) \nNewer versions of Ansible require all plugins to have a documentation string. You can see if there is a plugin available for the version of Ansible you are using here.\nTo create the plugin yourself, you will need to copy all of the options from the DOCUMENTATION string from the ssh.py Ansible connection plugin of the Ansible version you are using and add it to a packer.py file similar to as follows\nfrom __future__ import (absolute_import, division, print_function) __metaclass__ = type from ansible.plugins.connection.ssh import Connection as SSHConnection DOCUMENTATION = ''' connection: packer short_description: ssh based connections for powershell via packer description: - This connection plugin allows ansible to communicate to the target packer machines via ssh based connections for powershell. author: Packer version_added: na options: **** Copy ALL the options from https://github.com/ansible/ansible/blob/devel/lib/ansible/plugins/connection/ssh.py for the version of Ansible you are using **** ''' class Connection(SSHConnection): ''' ssh based connections for powershell via packer''' transport = 'packer' has_pipelining = True become_methods = [] allow_executable = False module_implementation_preferences = ('.ps1', '') def __init__(self, *args, **kwargs): super(Connection, self).__init__(*args, **kwargs) \nThis template should build a Windows Server 2012 image on Google Cloud Platform:\n{ \"variables\": {}, \"provisioners\": [ { \"type\": \"ansible\", \"playbook_file\": \"./win-playbook.yml\", \"extra_arguments\": [ \"--connection\", \"packer\", \"--extra-vars\", \"ansible_shell_type=powershell ansible_shell_executable=None\" ] } ], \"builders\": [ { \"type\": \"googlecompute\", \"account_file\": \"{{ user `account_file`}}\", \"project_id\": \"{{user `project_id`}}\", \"source_image\": \"windows-server-2012-r2-dc-v20160916\", \"communicator\": \"winrm\", \"zone\": \"us-central1-a\", \"disk_size\": 50, \"winrm_username\": \"packer\", \"winrm_use_ssl\": true, \"winrm_insecure\": true, \"metadata\": { \"sysprep-specialize-script-cmd\": \"winrm set winrm/config/service/auth @{Basic=\\\"true\\\"}\" } } ] } \nWarning: Please note that if you're setting up WinRM for provisioning, you'll probably want to turn it off or restrict its permissions as part of a shutdown script at the end of Packer's provisioning process. For more details on the why/how, check out this useful blog post and the associated code: https://cloudywindows.io/post/winrm-for-provisioning-close-the-door-on-the-way-out-eh/\nPost i/o timeout errors\nIf you see unknown error: Post http://<ip>:<port>/wsman:dial tcp <ip>:<port>: i/o timeout errors while provisioning a Windows machine, try setting Ansible to copy files over ssh instead of sftp.\nToo many SSH keys\nSSH servers only allow you to attempt to authenticate a certain number of times. All of your loaded keys will be tried before the dynamically generated key. If you have too many SSH keys loaded in your ssh-agent, the Ansible provisioner may fail authentication with a message similar to this:\ngooglecompute: fatal: [default]: UNREACHABLE! => {\"changed\": false, \"msg\": \"Failed to connect to the host via ssh: Warning: Permanently added '[127.0.0.1]:62684' (RSA) to the list of known hosts.\\r\\nReceived disconnect from 127.0.0.1 port 62684:2: too many authentication failures\\r\\nAuthentication failed.\\r\\n\", \"unreachable\": true} \nTo unload all keys from your ssh-agent, run:\nBecome: yes\nWe recommend against running Packer as root; if you do then you won't be able to successfully run your ansible playbook as root; become: yes will fail.\nUsing a wrapping script for your ansible call\nSometimes, you may have extra setup that needs to be called as part of your ansible run. The easiest way to do this is by writing a small bash script and using that bash script in your \"command\" in place of the default \"ansible-playbook\". For example, you may need to launch a Python virtualenv before calling ansible. To do this, you'd want to create a bash script like\n#!/bin/bash source /tmp/venv/bin/activate && ANSIBLE_FORCE_COLOR=1 PYTHONUNBUFFERED=1 /tmp/venv/bin/ansible-playbook \"$@\" \nThe ansible provisioner template remains very simple. For example:\n{ \"type\": \"ansible\", \"command\": \"/Path/To/call_ansible.sh\", \"playbook_file\": \"./playbook.yml\" } \nNote that we're calling ansible-playbook at the end of this command and passing all command line arguments through into this call; this is necessary for making sure that --extra-vars and other important ansible arguments get set. Note the quoting around the bash array, too; if you don't use quotes, any arguments with spaces will not be read properly.\nDocker\nWhen trying to use Ansible with Docker, it should \"just work\" but if it doesn't you may need to tweak a few options.\nChange the ansible_connection from \"ssh\" to \"docker\"\nSet a Docker container name via the --name option.\nOn a CI server you probably want to overwrite ansible_host with a random name.\nExample Packer template:\n{ \"variables\": { \"ansible_host\": \"default\", \"ansible_connection\": \"docker\" }, \"builders\": [ { \"type\": \"docker\", \"image\": \"centos:7\", \"commit\": true, \"run_command\": [ \"-d\", \"-i\", \"-t\", \"--name\", \"{{user `ansible_host`}}\", \"{{.Image}}\", \"/bin/bash\" ] } ], \"provisioners\": [ { \"type\": \"ansible\", \"groups\": [\"webserver\"], \"playbook_file\": \"./webserver.yml\", \"extra_arguments\": [ \"--extra-vars\", \"ansible_host={{user `ansible_host`}} ansible_connection={{user `ansible_connection`}}\" ] } ] } \nExample playbook:\n- name: configure webserver hosts: webserver tasks: - name: install Apache yum: name: httpd \nAmazon Session Manager\nWhen trying to use Ansible with Amazon's Session Manager, you may run into an error where Ansible is unable to connect to the remote Amazon instance if the local proxy adapter for Ansible use_proxy is false.\nThe error may look something like the following:\namazon-ebs: fatal: [default]: UNREACHABLE! => {\"changed\": false, \"msg\": \"Failed to connect to the host via ssh: ssh: connect to host 127.0.0.1 port 8362: Connection timed out\", \"unreachable\": true} \nThe error is caused by a limitation on using Amazon's SSM default Port Forwarding session which only allows for one remote connection on the forwarded port. Since Ansible's SSH communication is not using the local proxy adapter it will try to make a new SSH connection to the same forwarded localhost port and fail.\nIn order to workaround this issue Ansible can be configured via a custom inventory file to use the AWS session-manager-plugin directly to create a new session, separate from the one created by Packer, at runtime to connect and remotely provision the instance.\nWarning: Please note that the default region configured for the aws cli must match the build region where the instance is being provisioned otherwise you may run into a TargetNotConnected error. Users can use AWS_DEFAULT_REGION to temporarily override their configured region.\n\"provisioners\": [ { \"type\": \"ansible\", \"use_proxy\": false, \"ansible_env_vars\": [\"PACKER_BUILD_NAME={{ build_name }}\"], \"playbook_file\": \"./playbooks/playbook_remote.yml\", \"inventory_file_template\": \"{{ .HostAlias }} ansible_host={{ .ID }} ansible_user={{ .User }} ansible_ssh_common_args='-o StrictHostKeyChecking=no -o ProxyCommand=\\\"sh -c \\\\\\\"aws ssm start-session --target %h --document-name AWS-StartSSHSession --parameters portNumber=%p\\\\\\\"\\\"'\\n\" } ] \nFull Packer template example:\n{ \"variables\": { \"instance_role\": \"SSMInstanceProfile\" }, \"builders\": [ { \"type\": \"amazon-ebs\", \"region\": \"us-east-1\", \"ami_name\": \"packer-ami-ansible\", \"instance_type\": \"t2.micro\", \"source_ami_filter\": { \"filters\": { \"virtualization-type\": \"hvm\", \"name\": \"ubuntu/images/*ubuntu-xenial-16.04-amd64-server-*\", \"root-device-type\": \"ebs\" }, \"owners\": [\"099720109477\"], \"most_recent\": true }, \"communicator\": \"ssh\", \"ssh_username\": \"ubuntu\", \"ssh_interface\": \"session_manager\", \"iam_instance_profile\": \"{{user `instance_role`}}\" } ], \"provisioners\": [ { \"type\": \"ansible\", \"use_proxy\": false, \"ansible_env_vars\": [\"PACKER_BUILD_NAME={{ build_name }}\"], \"playbook_file\": \"./playbooks/playbook_remote.yml\", \"inventory_file_template\": \"{{ .HostAlias }} ansible_host={{ .ID }} ansible_user={{ .User }} ansible_ssh_common_args='-o StrictHostKeyChecking=no -o ProxyCommand=\\\"sh -c \\\\\\\"aws ssm start-session --target %h --document-name AWS-StartSSHSession --parameters portNumber=%p\\\\\\\"\\\"'\\n\" } ] } \nTroubleshooting\nIf you are using an Ansible version >= 2.8 and Packer hangs in the \"Gathering Facts\" stage, this could be the result of a pipelineing issue with the proxy adapter that Packer uses. Setting use_proxy: false, in your Packer config should resolve the issue. In the future we will default to setting this, so you won't have to but for now it is a manual change you must make."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/provisioners/puppet-server",
  "text": "Puppet Server - Provisioners | Packer\nPuppet Server Provisioner\nType: puppet-server\nThe puppet-server Packer provisioner provisions Packer machines with Puppet by connecting to a Puppet master.\nNote: Puppet will not be installed automatically by this provisioner. This provisioner expects that Puppet is already installed on the machine. It is common practice to use the shell provisioner before the Puppet provisioner to do this.\nThe example below is fully functional and expects a Puppet server to be accessible from your network.\n{ \"type\": \"puppet-server\", \"extra_arguments\": \"--test --pluginsync\", \"facter\": { \"server_role\": \"webserver\" } } \nThe provisioner takes various options. None are strictly required. They are listed below:\nclient_cert_path (string) - Path to the directory on your disk that contains the client certificate for the node. This defaults to nothing, in which case a client cert won't be uploaded.\nclient_private_key_path (string) - Path to the directory on your disk that contains the client private key for the node. This defaults to nothing, in which case a client private key won't be uploaded.\nexecute_command (string) - The command-line to execute Puppet. This is a template engine. Therefore, you may use user variables and template functions in this field. In addition, you may use the following extra variables:\nClientCertPath: The value set in the template option client_cert_path.\nClientPrivateKeyPath: The value set in the template option client_private_key_path.\nFacterVars: Additional facts to set when executing Puppet, joined for use in a command. This is internal and not settable by the user.\nPuppetNode: The hostname of the Puppet node, set in puppet_node.\nPuppetServer: The hostname of the Puppet server, set in puppet_server\nPuppetBinDir: The directory that the Puppet binary is in.\nSudo: Returns false when PreventSudo is set, and true when it is not.\nWorkingDir: The value provided in the working_directory option. See below for execute_command's default value.\nextra_arguments (array of strings) - Additional options to pass to the Puppet command. This allows for customization of execute_command without having to completely replace or subsume its contents, making forward-compatible customizations much easier to maintain.\nThis string is lazy-evaluated so one can incorporate logic driven by template variables as well as private elements of ExecuteTemplate (see source: provisioner/puppet-server/provisioner.go).\n[ {{if ne \"{{user environment}}\" \"\"}}--environment={{user environment}}{{end}} ] \nfacter (object of key/value strings) - Additional facts to make available to the Puppet run.\nguest_os_type (string) - The remote host's OS type ('windows' or 'unix') to tailor command-line and path separators. (default: unix).\nignore_exit_codes (boolean) - If true, Packer will ignore failures.\nprevent_sudo (boolean) - On Unix platforms Puppet is typically invoked with sudo. If true, it will be omitted. (default: false)\npuppet_bin_dir (string) - Path to the Puppet binary. Ideally the program should be on the system (unix: $PATH, windows: %PATH%), but some builders (eg. Docker) do not run profile-setup scripts and therefore PATH might be empty or minimal. On Windows, spaces should be ^-escaped, i.e. c:/program^ files/puppet^ labs/puppet/bin.\npuppet_node (string) - The name of the node. If this isn't set, the fully qualified domain name will be used.\npuppet_server (string) - Hostname of the Puppet server. By default \"puppet\" will be used.\nstaging_dir (string) - Directory to where uploaded files will be placed (unix: \"/tmp/packer-puppet-masterless\", windows: \"%SYSTEMROOT%/Temp/packer-puppet-masterless\"). It doesn't need to pre-exist, but the parent must have permissions sufficient for the account Packer connects as to create directories and write files. Use a Shell provisioner to prepare the way if needed.\nworking_directory (string) - Directory from which execute_command will be run. If using Hiera files with relative paths, this option can be helpful. (default: staging_directory)\nelevated_user and elevated_password (string) - If specified, Puppet will be run with elevated privileges using the given Windows user. See the powershell provisioner for the full details. This is a template engine. Therefore, you may use user variables and template functions in this field, including {{ build `Password`}} to use the password being used by the communicator to connect to your instance.\nBy default, Packer uses the following command (broken across multiple lines for readability) to execute Puppet:\ncd {{.WorkingDir}} && {{if ne .FacterVars \"\"}}{{.FacterVars}} {{end}} {{if .Sudo}}sudo -E {{end}} {{if ne .PuppetBinDir \"\"}}{{.PuppetBinDir}}/{{end}} puppet agent --onetime --no-daemonize --detailed-exitcodes {{if .Debug}}--debug {{end}} {{if ne .PuppetServer \"\"}}--server='{{.PuppetServer}}' {{end}} {{if ne .PuppetNode \"\"}}--certname='{{.PuppetNode}}' {{end}} {{if ne .ClientCertPath \"\"}}--certdir='{{.ClientCertPath}}' {{end}} {{if ne .ClientPrivateKeyPath \"\"}}--privatekeydir='{{.ClientPrivateKeyPath}}' {{end}} {{if ne .ExtraArguments \"\"}}{{.ExtraArguments}} {{end}} \nThe following command is used if guest OS type is windows:\ncd {{.WorkingDir}} && {{if ne .FacterVars \"\"}}{{.FacterVars}} && {{end}} {{if ne .PuppetBinDir \"\"}}{{.PuppetBinDir}}/{{end}} puppet agent --onetime --no-daemonize --detailed-exitcodes {{if .Debug}}--debug {{end}} {{if ne .PuppetServer \"\"}}--server='{{.PuppetServer}}' {{end}} {{if ne .PuppetNode \"\"}}--certname='{{.PuppetNode}}' {{end}} {{if ne .ClientCertPath \"\"}}--certdir='{{.ClientCertPath}}' {{end}} {{if ne .ClientPrivateKeyPath \"\"}}--privatekeydir='{{.ClientPrivateKeyPath}}' {{end}} {{if ne .ExtraArguments \"\"}}{{.ExtraArguments}} {{end}} \nIn addition to being able to specify custom Facter facts using the facter configuration, the provisioner automatically defines certain commonly useful facts:\npacker_build_name is set to the name of the build that Packer is running. This is most useful when Packer is making multiple builds and you want to distinguish them in your Hiera hierarchy.\npacker_builder_type is the type of the builder that was used to create the machine that Puppet is running on. This is useful if you want to run only certain parts of your Puppet code on systems built with certain builders."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/post-processors/community-supported",
  "text": "Community - Post-Processors | Packer\nThe following post-processors are developed and maintained by various members of the Packer community, not by HashiCorp. For more information on how to use community post-processors, see our docs on extending Packer.\nCommunity Post-Processors\nExoscale Import post-processor - A post-processor to import Exoscale custom templates from disk image files."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/provisioners/salt-masterless",
  "text": "Salt Masterless - Provisioners | Packer\nSalt Masterless Provisioner\nType: salt-masterless\nThe salt-masterless Packer provisioner provisions machines built by Packer using Salt states, without connecting to a Salt master.\n{ \"type\": \"salt-masterless\", \"local_state_tree\": \"/Users/me/salt\" } \nThe reference of available configuration options is listed below. The only required element is \"local_state_tree\".\nlocal_state_tree (string) - The path to your local state tree. This will be uploaded to the remote_state_tree on the remote.\nbootstrap_args (string) - Arguments to send to the bootstrap script. Usage is somewhat documented on github, but the script itself has more detailed usage instructions. By default, no arguments are sent to the script.\ndisable_sudo (boolean) - By default, the bootstrap install command is prefixed with sudo. When using a Docker builder, you will likely want to pass true since sudo is often not pre-installed.\nremote_pillar_roots (string) - The path to your remote pillar roots. default: /srv/pillar. This option cannot be used with minion_config.\nremote_state_tree (string) - The path to your remote state tree. default: /srv/salt. This option cannot be used with minion_config.\nlocal_pillar_roots (string) - The path to your local pillar roots. This will be uploaded to the remote_pillar_roots on the remote.\ncustom_state (string) - A state to be run instead of state.highstate. Defaults to state.highstate if unspecified.\nminion_config (string) - The path to your local minion config file. This will be uploaded to the /etc/salt on the remote. This option overrides the remote_state_tree or remote_pillar_roots options.\ngrains_file (string) - The path to your local grains file. This will be uploaded to /etc/salt/grains on the remote.\nskip_bootstrap (boolean) - By default the salt provisioner runs salt bootstrap to install salt. Set this to true to skip this step.\ntemp_config_dir (string) - Where your local state tree will be copied before moving to the /srv/salt directory. Default is /tmp/salt.\nno_exit_on_failure (boolean) - Packer will exit if the salt-call command fails. Set this option to true to ignore Salt failures.\nlog_level (string) - Set the logging level for the salt-call run.\nsalt_call_args (string) - Additional arguments to pass directly to salt-call. See salt-call documentation for more information. By default no additional arguments (besides the ones Packer generates) are passed to salt-call.\nsalt_bin_dir (string) - Path to the salt-call executable. Useful if it is not on the PATH.\nguest_os_type (string) - The target guest OS type, either \"unix\" or \"windows\".\nformulas (array of strings) - An array of git source formulas to be downloaded to the local state tree prior to moving to the remote state tree. Note: //directory must be included in the URL to download the appropriate formula directory. Example: git::https://github.com/saltstack-formulas/vault-formula.git//vault?ref=v1.2.3"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/post-processors/alicloud-import",
  "text": "Alicloud Import Post-Processor | Packer\nType: alicloud-import\nThe Packer Alicloud Import post-processor takes a RAW or VHD artifact from various builders and imports it to an Alicloud ECS Image.\nThe import process operates by making a temporary copy of the RAW or VHD to an OSS bucket, and calling an import task in ECS on the RAW or VHD file. Once completed, an Alicloud ECS Image is returned. The temporary RAW or VHD copy in OSS can be discarded after the import is complete.\nThere are some configuration options available for the post-processor. There are two categories: required and optional parameters.\naccess_key (string) - This is the Alicloud access key. It must be provided, but it can also be sourced from the ALICLOUD_ACCESS_KEY environment variable.\nsecret_key (string) - This is the Alicloud secret key. It must be provided, but it can also be sourced from the ALICLOUD_SECRET_KEY environment variable.\nregion (string) - This is the Alicloud region. It must be provided, but it can also be sourced from the ALICLOUD_REGION environment variables.\nimage_name (string) - The name of the user-defined image, [2, 128] English or Chinese characters. It must begin with an uppercase/lowercase letter or a Chinese character, and may contain numbers, _ or -. It cannot begin with http:// or https://\noss_bucket_name (string) - The name of the OSS bucket where the RAW or VHD file will be copied to for import. If the Bucket isn't exist, post-process will create it for you.\nimage_os_type (string) - Type of the OS linux/windows\nimage_platform (string) - platform such CentOS\nimage_architecture (string) - Platform type of the image system: i386 or x86_64\nformat (string) - The format of the image for import, now alicloud only support RAW and VHD.\nkeep_input_artifact (boolean) - if true, do not delete the RAW or VHD disk image after importing it to the cloud. Defaults to false.\noss_key_name (string) - The name of the object key in oss_bucket_name where the RAW or VHD file will be copied to for import. This is treated as a template engine, and you may access any of the variables stored in the generated data using the build template function.\nskip_clean (boolean) - Whether we should skip removing the RAW or VHD file uploaded to OSS after the import process has completed. true means that we should leave it in the OSS bucket, false means to clean it out. Defaults to false.\nimage_description (string) - The description of the image, with a length limit of 0 to 256 characters. Leaving it blank means null, which is the default value. It cannot begin with http:// or https://.\nimage_force_delete (boolean) - If this value is true, when the target image name is duplicated with an existing image, it will delete the existing image and then create the target image, otherwise, the creation will fail. The default value is false.\nimage_system_size (number) - Size of the system disk, in GB, values range:\ncloud - 5 ~ 2000\ncloud_efficiency - 20 ~ 2048\ncloud_ssd - 20 ~ 2048\nHere is a basic example. This assumes that the builder has produced a RAW artifact. The user must have the role AliyunECSImageImportDefaultRole with AliyunECSImageImportRolePolicy, post-process will automatically configure the role and policy for you if you have the privilege, otherwise, you have to ask the administrator configure for you in advance.\n\"post-processors\":[ { \"access_key\":\"{{user `access_key`}}\", \"secret_key\":\"{{user `secret_key`}}\", \"type\":\"alicloud-import\", \"oss_bucket_name\": \"packer\", \"image_name\": \"packer_import\", \"image_os_type\": \"linux\", \"image_platform\": \"CentOS\", \"image_architecture\": \"x86_64\", \"image_system_size\": \"40\", \"region\":\"cn-beijing\" } ] \nThis will take the RAW generated by a builder and upload it to OSS. In this case, an existing bucket called packer in the cn-beijing region will be where the copy is placed.\nOnce uploaded, the import process will start, creating an Alicloud ECS image in the cn-beijing region with the name you specified in template file."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/post-processors/shell-local",
  "text": "Local Shell - Post-Processors | Packer\nLocal Shell Post Processor\nThe local shell post processor executes scripts locally during the post processing stage. Shell local provides a convenient way to automate executing some task with packer outputs and variables.\n{ \"type\": \"shell-local\", \"inline\": [\"echo foo\"] } \ncommand (string) - This is a single command to execute. It will be written to a temporary file and run using the execute_command call below.\nexecute_command (array of strings) - The command used to execute the script. By default, on *nix systems this is:\n[\"/bin/sh\", \"-c\", \"{{.Vars}} {{.Script}}\"] \nWhile on Windows, execute_command defaults to:\n[\"cmd\", \"/V\", \"/C\", \"{{.Vars}}\", \"call\", \"{{.Script}}\"] \nThis is treated as a template engine. There are several available variables: Script, which is the path to the script to run, and Vars, which is the list of environment_vars, if configured. In addition, you may access any of the variables stored in the generated data using the build template function. If you choose to set this option, make sure that the first element in the array is the shell program you want to use (for example, \"sh\" or \"/usr/local/bin/zsh\" or even \"powershell.exe\" although anything other than a flavor of the shell command language is not explicitly supported and may be broken by assumptions made within Packer). It's worth noting that if you choose to try to use shell-local for Powershell or other Windows commands, the environment variables will not be set properly for your environment.\nFor backwards compatibility, execute_command will accept a string instead of an array of strings. If a single string or an array of strings with only one element is provided, Packer will replicate past behavior by appending your execute_command to the array of strings [\"sh\", \"-c\"]. For example, if you set \"execute_command\": \"foo bar\", the final execute_command that Packer runs will be [\"sh\", \"-c\", \"foo bar\"]. If you set \"execute_command\": [\"foo\", \"bar\"], the final execute_command will remain [\"foo\", \"bar\"].\nAgain, the above is only provided as a backwards compatibility fix; we strongly recommend that you set execute_command as an array of strings.\nkeep_input_artifact (boolean) - Unlike most other post-processors, the keep_input_artifact option will have no effect for the shell-local post-processor. Packer will always retain the input artifact for shell-local, since the shell-local post-processor merely passes forward the artifact it receives. If your shell-local post-processor produces a file or files which you would like to have replace the input artifact, you may overwrite the input artifact using the artifice post-processor after your shell-local processor has run.\nuse_linux_pathing (bool) - This is only relevant to windows hosts. If you are running Packer in a Windows environment with the Windows Subsystem for Linux feature enabled, and would like to invoke a bash script rather than invoking a Cmd script, you'll need to set this flag to true; it tells Packer to use the linux subsystem path for your script rather than the Windows path. (e.g. /mnt/c/path/to/your/file instead of C:/path/to/your/file). Please see the example below for more guidance on how to use this feature. If you are not on a Windows host, or you do not intend to use the shell-local post-processor to run a bash script, please ignore this option. If you set this flag to true, you still need to provide the standard windows path to the script when providing a script. This is a beta feature.\nThe shell-local post-processor was designed with the idea of allowing you to run commands in your local operating system's native shell. For Windows, we've assumed in our defaults that this is Cmd. However, it is possible to run a bash script as part of the Windows Linux Subsystem from the shell-local post-processor, by modifying the execute_command and the use_linux_pathing options in the post-processor config.\nPlease note that this feature is still in beta, as the underlying WSL is also still in beta. There will be some limitations as a result. For example, it will likely not work unless both Packer and the scripts you want to run are both on the C drive.\n{ \"builders\": [ { \"type\": \"null\", \"communicator\": \"none\" } ], \"provisioners\": [ { \"type\": \"shell-local\", \"environment_vars\": [\"PROVISIONERTEST=ProvisionerTest1\"], \"execute_command\": [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"], \"use_linux_pathing\": true, \"scripts\": [\"C:/Users/me/scripts/example_bash.sh\"] }, { \"type\": \"shell-local\", \"environment_vars\": [\"PROVISIONERTEST=ProvisionerTest2\"], \"execute_command\": [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"], \"use_linux_pathing\": true, \"script\": \"C:/Users/me/scripts/example_bash.sh\" } ] } \nWhether you use the inline option, or pass it a direct script or scripts, it is important to understand a few things about how the shell-local post-processor works to run it safely and easily. This understanding will save you much time in the process.\nInteracting with Build Artifacts\nIn order to interact with build artifacts, you may want to use the manifest post-processor. This will write the list of files produced by a builder to a json file after each builder is run.\nFor example, if you wanted to package a file from the file builder into a tarball, you might write this:\n{ \"builders\": [ { \"content\": \"Lorem ipsum dolor sit amet\", \"target\": \"dummy_artifact\", \"type\": \"file\" } ], \"post-processors\": [ [ { \"output\": \"manifest.json\", \"strip_path\": true, \"type\": \"manifest\" }, { \"inline\": [ \"jq \\\".builds[].files[].name\\\" manifest.json | xargs tar cfz artifacts.tgz\" ], \"type\": \"shell-local\" } ] ] } \nThis uses the jq tool to extract all of the file names from the manifest file and passes them to tar.\nIf any post-processor fails, the packer build stops and all interim artifacts are cleaned up.\nExample of running a .cmd file on windows:\n{ \"type\": \"shell-local\", \"environment_vars\": [\"SHELLLOCALTEST=ShellTest1\"], \"scripts\": [\"./scripts/test_cmd.cmd\"] } \nExample of running an inline command on windows: Required customization: tempfile_extension\n{ \"type\": \"shell-local\", \"environment_vars\": [\"SHELLLOCALTEST=ShellTest2\"], \"tempfile_extension\": \".cmd\", \"inline\": [\"echo %SHELLLOCALTEST%\"] } \nExample of running a bash command on windows using WSL: Required customizations: use_linux_pathing and execute_command\n{ \"type\": \"shell-local\", \"environment_vars\": [\"SHELLLOCALTEST=ShellTest3\"], \"execute_command\": [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"], \"use_linux_pathing\": true, \"script\": \"./scripts/example_bash.sh\" } \nContents of \"example_bash.sh\":\nExample of running a powershell script on windows: Required customizations: env_var_format and execute_command\n{ \"type\": \"shell-local\", \"environment_vars\": [\"SHELLLOCALTEST=ShellTest4\"], \"execute_command\": [\"powershell.exe\", \"{{.Vars}} {{.Script}}\"], \"env_var_format\": \"$env:%s=\\\"%s\\\"; \", \"script\": \"./scripts/example_ps.ps1\" } \nExample of running a powershell script on windows as \"inline\": Required customizations: env_var_format, tempfile_extension, and execute_command\n{ \"type\": \"shell-local\", \"tempfile_extension\": \".ps1\", \"environment_vars\": [\"SHELLLOCALTEST=ShellTest5\"], \"execute_command\": [\"powershell.exe\", \"{{.Vars}} {{.Script}}\"], \"env_var_format\": \"$env:%s=\\\"%s\\\"; \", \"inline\": [\"write-output $env:SHELLLOCALTEST\"] } \nExample of running a bash script on unix:\n{ \"type\": \"shell-local\", \"environment_vars\": [\"PROVISIONERTEST=ProvisionerTest1\"], \"scripts\": [\"./scripts/example_bash.sh\"] } \nExample of running a bash \"inline\" on unix:\n{ \"type\": \"shell-local\", \"environment_vars\": [\"PROVISIONERTEST=ProvisionerTest2\"], \"inline\": [\"echo hello\", \"echo $PROVISIONERTEST\"] } \nExample of running a python script on unix:\n{ \"type\": \"shell-local\", \"script\": \"hello.py\", \"environment_vars\": [\"HELLO_USER=packeruser\"], \"execute_command\": [ \"/bin/sh\", \"-c\", \"{{.Vars}} /usr/local/bin/python {{.Script}}\" ] } \nWhere \"hello.py\" contains:\nimport os print('Hello, %s!' % os.getenv(\"HELLO_USER\"))"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/post-processors/shell-local",
  "text": "The example below is a fully functional self-contained build.\nsource \"file\" \"example\" { content = \"example content\" } build { source \"source.file.example\" { target = \"./test_artifact.txt\" } post-processor \"shell-local\" { inline = [\"echo foo\"] } } \nuse_linux_pathing (bool) - This is only relevant to windows hosts. If you are running Packer in a Windows environment with the Windows Subsystem for Linux feature enabled, and would like to invoke a bash script rather than invoking a Cmd script, you'll need to set this flag to true; it tells Packer to use the linux subsystem path for your script rather than the Windows path. (e.g. /mnt/c/path/to/your/file instead of C:/path/to/your/file). Please see the example below for more guidance on how to use this feature. If you are not on a Windows host, or you do not intend to use the shell-local post-processor to run a bash script, please ignore this option. If you set this flag to true, you still need to provide the standard windows path to the script when providing a script. This is a beta feature.\nsource \"null\" \"example\" { communicator = \"none\" } build { sources = [ \"source.null.example\" ] post-processor \"shell-local\"{ environment_vars = [\"PROVISIONERTEST=ProvisionerTest1\"] execute_command = [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"] use_linux_pathing = true scripts = [\"C:/Users/me/scripts/example_bash.sh\"] } post-processor \"shell-local\"{ environment_vars = [\"PROVISIONERTEST=ProvisionerTest2\"] execute_command = [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"] use_linux_pathing = true script = \"C:/Users/me/scripts/example_bash.sh\" } } \nThe shell-local script(s) you pass are run once per builder. This means that if you have an amazon-ebs builder and a docker builder, your script will be run twice. If you have 3 builders, it will run 3 times, once for each builder.\npost-processor \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest1\"] scripts = [\"./scripts/test_cmd.cmd\"] } \nContents of test_cmd.cmd:\npost-processor \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest2\"], tempfile_extension = \".cmd\", inline = [\"echo %SHELLLOCALTEST%\"] } \nExample of running a bash command on windows using WSL: Required customizations: use_linux_pathing and execute_command:\npost-processor \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest3\"], execute_command = [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"] use_linux_pathing = true script = \"./scripts/example_bash.sh\" } \nExample of running a PowerShell script on Windows: Required customizations: env_var_format and execute_command.\npost-processor \"shell-local\" { environment_vars = [\"SHELLLOCALTEST=ShellTest4\"] execute_command = [\"powershell.exe\", \"{{.Vars}} {{.Script}}\"] env_var_format = \"$env:%s=\\\"%s\\\"; \" script = \"./scripts/example_ps.ps1\" } \npost-processor \"shell-local\" { tempfile_extension = \".ps1\" environment_vars = [\"SHELLLOCALTEST=ShellTest5\"] execute_command = [\"powershell.exe\", \"{{.Vars}} {{.Script}}\"] env_var_format = \"$env:%s=\\\"%s\\\"; \" inline = [\"write-output $env:SHELLLOCALTEST\"] } \nExample of running a Shell script on unix:\npost-processor \"shell-local\" { environment_vars = [\"PROVISIONERTEST=ProvisionerTest1\"] scripts = [\"./scripts/example_bash.sh\"] } \nExample of running a bash \"inline\" on unix:\npost-processor \"shell-local\" { environment_vars = [\"PROVISIONERTEST=ProvisionerTest2\"] inline = [\"echo hello\", \"echo $PROVISIONERTEST\"] } \nExample of running a python script on unix:\npost-processor \"shell-local\" { script = \"hello.py\" environment_vars = [\"HELLO_USER=packeruser\"] execute_command = [ \"/bin/sh\", \"-c\", \"{{.Vars}} /usr/local/bin/python {{.Script}}\" ] } "
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/post-processors/amazon-import",
  "text": "Amazon Import - Post-Processors | Packer\nAmazon Import Post-Processor\nType: amazon-import\nThe Packer Amazon Import post-processor takes an OVA artifact from various builders and imports it to an AMI available to Amazon Web Services EC2.\nThis post-processor is for advanced users. It depends on specific IAM roles inside AWS and is best used with images that operate with the EC2 configuration model (eg, cloud-init for Linux systems). Please ensure you read the prerequisites for import before using this post-processor.\nThe import process operates making a temporary copy of the OVA to an S3 bucket, and calling an import task in EC2 on the OVA file. Once completed, an AMI is returned containing the converted virtual machine. The temporary OVA copy in S3 can be discarded after the import is complete.\nThe import process itself run by AWS includes modifications to the image uploaded, to allow it to boot and operate in the AWS EC2 environment. However, not all modifications required to make the machine run well in EC2 are performed. Take care around console output from the machine, as debugging can be very difficult without it. You may also want to include tools suitable for instances in EC2 such as cloud-init for Linux.\nFurther information about the import process can be found in AWS's EC2 Import/Export Instance documentation.\nThere are some configuration options available for the post-processor. They are segmented below into two categories: required and optional parameters. Within each category, the available configuration keys are alphabetized.\naccess_key (string) - The access key used to communicate with AWS. Learn how to set this.\nregion (string) - The name of the region, such as us-east-1 in which to upload the OVA file to S3 and create the AMI. A list of valid regions can be obtained with AWS CLI tools or by consulting the AWS website.\ns3_bucket_name (string) - The name of the S3 bucket where the OVA file will be copied to for import. This bucket must exist when the post-processor is run.\nsecret_key (string) - The secret key used to communicate with AWS. Learn how to set this.\nami_description (string) - The description to set for the resulting imported AMI. By default this description is generated by the AMI import process.\nami_encrypt (boolean) - Encrypt the resulting AMI using KMS. This defaults to false.\nami_groups (array of strings) - A list of groups that have access to launch the imported AMI. By default no groups have permission to launch the AMI. all will make the AMI publicly accessible. AWS currently doesn't accept any value other than \"all\".\nami_kms_key (string) - The ID of the KMS key used to encrypt the AMI if ami_encrypt is true. If set, the role specified in role_name must be granted access to use this key. If not set, the account default KMS key will be used.\nami_name (string) - The name of the ami within the console. If not specified, this will default to something like ami-import-sfwerwf. Please note, specifying this option will result in a slightly longer execution time.\nami_users (array of strings) - A list of account IDs that have access to launch the imported AMI. By default no additional users other than the user importing the AMI has permission to launch it.\ncustom_endpoint_ec2 (string) - This option is useful if you use a cloud provider whose API is compatible with aws EC2. Specify another endpoint like this https://ec2.custom.endpoint.com.\nformat (string) - One of: ova, raw, vhd, vhdx, or vmdk. This specifies the format of the source virtual machine image. The resulting artifact from the builder is assumed to have a file extension matching the format. This defaults to ova.\ninsecure_skip_tls_verify (boolean) - This allows skipping TLS verification of the AWS EC2 endpoint. The default is false.\nkeep_input_artifact (boolean) - if true, do not delete the source virtual machine image after importing it to the cloud. Defaults to false.\nlicense_type (string) - The license type to be used for the Amazon Machine Image (AMI) after importing. Valid values: AWS or BYOL (default). For more details regarding licensing, see Prerequisites in the VM Import/Export User Guide.\nmfa_code (string) - The MFA TOTP code. This should probably be a user variable since it changes all the time.\nprofile (string) - The profile to use in the shared credentials file for AWS. See Amazon's documentation on specifying profiles for more details.\nrole_name (string) - The name of the role to use when not using the default role, 'vmimport'\ns3_encryption (string) - One of: aws:kms, or AES256. The algorithm used to encrypt the artifact in S3. This does not encrypt the resulting AMI, and is only used to encrypt the uploaded artifact before it becomes an AMI. By default no encryption is used.\ns3_encryption_key (string) - The KMS key ID to use when aws:kms is specified in s3_encryption. This setting is ignored if AES is used as Amazon does not currently support custom AES keys when using the VM import service. If set, the role specified in role_name must be granted access to use this key. If not set, and s3_encryption is set to aws:kms, the account default KMS key will be used.\ns3_key_name (string) - The name of the key in s3_bucket_name where the OVA file will be copied to for import. If not specified, this will default to \"packer-import-{{timestamp}}.ova\". This key (i.e., the uploaded OVA) will be removed after import, unless skip_clean is true. This is treated as a template engine. Therefore, you may use user variables and template functions in this field.\nskip_clean (boolean) - Whether we should skip removing the OVA file uploaded to S3 after the import process has completed. \"true\" means that we should leave it in the S3 bucket, \"false\" means to clean it out. Defaults to false.\nskip_region_validation (boolean) - Set to true if you want to skip validation of the region configuration option. Default false.\ntags (object of key/value strings) - Tags applied to the created AMI and relevant snapshots.\ntoken (string) - The access token to use. This is different from the access key and secret key. If you're not sure what this is, then you probably don't need it. This will also be read from the AWS_SESSION_TOKEN environmental variable.\nHere is a basic example. This assumes that the builder has produced an OVA artifact for us to work with, and IAM roles for import exist in the AWS account being imported into.\n{ \"type\": \"amazon-import\", \"access_key\": \"YOUR KEY HERE\", \"secret_key\": \"YOUR SECRET KEY HERE\", \"region\": \"us-east-1\", \"s3_bucket_name\": \"importbucket\", \"license_type\": \"BYOL\", \"tags\": { \"Description\": \"packer amazon-import {{timestamp}}\" } } \nThis is an example that uses vmware-iso builder and exports the .ova file using ovftool.\n\"post-processors\" : [ [ { \"type\": \"shell-local\", \"inline\": [ \"/usr/bin/ovftool <packer-output-directory>/<vmware-name>.vmx <packer-output-directory>/<vmware-name>.ova\" ] }, { \"files\": [ \"<packer-output-directory>/<vmware-name>.ova\" ], \"type\": \"artifice\" }, { \"type\": \"amazon-import\", \"access_key\": \"YOUR KEY HERE\", \"secret_key\": \"YOUR SECRET KEY HERE\", \"region\": \"us-east-1\", \"s3_bucket_name\": \"importbucket\", \"license_type\": \"BYOL\", \"tags\": { \"Description\": \"packer amazon-import {{timestamp}}\" } } ] ] \nYou'll need at least the following permissions in the policy for your IAM user in order to successfully upload an image via the amazon-import post-processor.\n\"ec2:CancelImportTask\", \"ec2:CopyImage\", \"ec2:CreateTags\", \"ec2:DescribeImages\", \"ec2:DescribeImportImageTasks\", \"ec2:ImportImage\", \"ec2:ModifyImageAttribute\" \"ec2:DeregisterImage\" \nThe amazon-import feature can take a long time to upload and convert your OVAs into AMIs; if you find that your build is failing because you have exceeded your max retries or find yourself being rate limited, you can override the max retries and the delay in between retries by setting the environment variables AWS_MAX_ATTEMPTS and AWS_POLL_DELAY_SECONDS on the machine running the Packer build. By default, the waiter that waits for your image to be imported from s3 will retry for up to an hour: it retries up to 720 times with a 5 second delay in between retries.\nThis is dramatically higher than many of our other waiters, to account for how long this process can take.\nNote: Packer can also read the access key and secret access key from environmental variables. See the configuration reference in the section above for more information on what environmental variables Packer will look for.\nThis will take the OVA generated by a builder and upload it to S3. In this case, an existing bucket called importbucket in the us-east-1 region will be where the copy is placed. The key name of the copy will be a default name generated by packer.\nOnce uploaded, the import process will start, creating an AMI in the \"us-east-1\" region with a \"Description\" tag applied to both the AMI and the snapshots associated with it. Note: the import process does not allow you to name the AMI, the name is automatically generated by AWS.\nAfter tagging is completed, the OVA uploaded to S3 will be removed."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/post-processors/shell-local",
  "text": "{ \"builders\": [ { \"type\": \"file\", \"name\": \"example\", \"target\": \"./test_artifact.txt\", \"content\": \"example content\" } ], \"post-processors\": [ { \"type\": \"shell-local\", \"inline\": [\"echo foo\"] } ] } \nuse_linux_pathing (bool) - This is only relevant to windows hosts. If you are running Packer in a Windows environment with the Windows Subsystem for Linux feature enabled, and would like to invoke a bash script rather than invoking a Cmd script, you'll need to set this flag to true; it tells Packer to use the linux subsystem path for your script rather than the Windows path. (e.g. /mnt/c/path/to/your/file instead of C:/path/to/your/file). Please see the example below for more guidance on how to use this feature. If you are not on a Windows host, or you do not intend to use the shell-local post-processor to run a bash script, please ignore this option. If you set this flag to true, you still need to provide the standard windows path to the script when providing a script. This is a beta feature.\n{ \"builders\": [ { \"type\": \"null\", \"communicator\": \"none\" } ], \"post-processors\": [ { \"type\": \"shell-local\", \"environment_vars\": [\"PROVISIONERTEST=ProvisionerTest1\"], \"execute_command\": [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"], \"use_linux_pathing\": true, \"scripts\": [\"C:/Users/me/scripts/example_bash.sh\"] }, { \"type\": \"shell-local\", \"environment_vars\": [\"PROVISIONERTEST=ProvisionerTest2\"], \"execute_command\": [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"], \"use_linux_pathing\": true, \"script\": \"C:/Users/me/scripts/example_bash.sh\" } ] } \n{ \"type\": \"shell-local\", \"environment_vars\": [\"SHELLLOCALTEST=ShellTest1\"], \"scripts\": [\"./scripts/test_cmd.cmd\"] } \n{ \"type\": \"shell-local\", \"environment_vars\": [\"SHELLLOCALTEST=ShellTest2\"], \"tempfile_extension\": \".cmd\", \"inline\": [\"echo %SHELLLOCALTEST%\"] } \n{ \"type\": \"shell-local\", \"environment_vars\": [\"SHELLLOCALTEST=ShellTest3\"], \"execute_command\": [\"bash\", \"-c\", \"{{.Vars}} {{.Script}}\"], \"use_linux_pathing\": true, \"script\": \"./scripts/example_bash.sh\" } \nContents of \"example_bash.sh\":\n{ \"type\": \"shell-local\", \"environment_vars\": [\"SHELLLOCALTEST=ShellTest4\"], \"execute_command\": [\"powershell.exe\", \"{{.Vars}} {{.Script}}\"], \"env_var_format\": \"$env:%s=\\\"%s\\\"; \", \"script\": \"./scripts/example_ps.ps1\" } \n{ \"type\": \"shell-local\", \"tempfile_extension\": \".ps1\", \"environment_vars\": [\"SHELLLOCALTEST=ShellTest5\"], \"execute_command\": [\"powershell.exe\", \"{{.Vars}} {{.Script}}\"], \"env_var_format\": \"$env:%s=\\\"%s\\\"; \", \"inline\": [\"write-output $env:SHELLLOCALTEST\"] } \nExample of running a bash script on unix:\n{ \"type\": \"shell-local\", \"environment_vars\": [\"PROVISIONERTEST=ProvisionerTest1\"], \"scripts\": [\"./scripts/example_bash.sh\"] } \n{ \"type\": \"shell-local\", \"environment_vars\": [\"PROVISIONERTEST=ProvisionerTest2\"], \"inline\": [\"echo hello\", \"echo $PROVISIONERTEST\"] } \n{ \"type\": \"shell-local\", \"script\": \"hello.py\", \"environment_vars\": [\"HELLO_USER=packeruser\"], \"execute_command\": [ \"/bin/sh\", \"-c\", \"{{.Vars}} /usr/local/bin/python {{.Script}}\" ] } \nWhere \"hello.py\" contains:\nimport os print('Hello, %s!' % os.getenv(\"HELLO_USER\"))"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/post-processors/community-supported",
  "text": "Exoscale Import - Import a builder artifact as a new Exoscale Compute instance template."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/post-processors/docker-save",
  "text": "Docker Save - Post-Processors | Packer\nDocker Save Post-Processor\nType: docker-save\nThe Packer Docker Save post-processor takes an artifact from the docker builder that was committed and saves it to a file. This is similar to exporting the Docker image directly from the builder, except that it preserves the hierarchy of images and metadata.\nWe understand the terminology can be a bit confusing, but we've adopted the terminology from Docker, so if you're familiar with that, then you'll be familiar with this and vice versa.\nRequired\nThe configuration for this post-processor only requires one option.\npath (string) - The path to save the image.\nkeep_input_artifact (boolean) - if true, do not delete the docker container, and only save the .tar created by docker save. Defaults to true.\nAn example is shown below, showing only the post-processor configuration:\n{ \"type\": \"docker-save\", \"path\": \"foo.tar\" }"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/post-processors/docker-import",
  "text": "Docker Import - Post-Processors | Packer\nDocker Import Post-Processor\nType: docker-import\nThe Packer Docker import post-processor takes an artifact from the docker builder and imports it with Docker locally. This allows you to apply a repository and tag to the image and lets you use the other Docker post-processors such as docker-push to push the image to a registry.\nThe configuration for this post-processor only requires a repository, a tag is optional.\nrepository (string) - The repository of the imported image.\ntag (string) - The tag for the imported image. By default this is not set.\nchanges (array of strings) - Dockerfile instructions to add to the commit. Example of instructions are CMD, ENTRYPOINT, ENV, and EXPOSE. Example: [ \"USER ubuntu\", \"WORKDIR /app\", \"EXPOSE 8080\" ]\nkeep_input_artifact (boolean) - if true, do not delete the source tar after importing it to docker. Defaults to false.\nAn example is shown below, showing only the post-processor configuration:\n{ \"type\": \"docker-import\", \"repository\": \"hashicorp/packer\", \"tag\": \"0.7\" } \nThis example would take the image created by the Docker builder and import it into the local Docker process with a name of hashicorp/packer:0.7.\nFollowing this, you can use the docker-push post-processor to push it to a registry, if you want.\nBelow is an example using the changes argument of the post-processor. This feature allows the tarball metadata to be changed when imported into the Docker environment. It is derived from the docker import --change command line option to Docker.\nExample uses of all of the options, assuming one is building an NGINX image from ubuntu as an simple example:\n{ \"type\": \"docker-import\", \"repository\": \"local/centos6\", \"tag\": \"latest\", \"changes\": [ \"USER www-data\", \"WORKDIR /var/www\", \"ENV HOSTNAME www.example.com\", \"VOLUME /test1 /test2\", \"EXPOSE 80 443\", \"LABEL version=1.0\", \"ONBUILD RUN date\", \"CMD [\\\"nginx\\\", \\\"-g\\\", \\\"daemon off;\\\"]\", \"ENTRYPOINT /var/www/start.sh\" ] } \nAllowed metadata fields that can be changed are:\nCMD\nString, supports both array (escaped) and string form\nEX: \"CMD [\\\"nginx\\\", \\\"-g\\\", \\\"daemon off;\\\"]\"\nEX: \"CMD nginx -g daemon off;\"\nENTRYPOINT\nString\nEX: \"ENTRYPOINT /var/www/start.sh\"\nENV\nString, note there is no equal sign:\nEX: \"ENV HOSTNAME www.example.com\" not \"ENV HOSTNAME=www.example.com\"\nEXPOSE\nString, space separated ports\nEX: \"EXPOSE 80 443\"\nLABEL\nString, space separated key=value pairs\nEX: \"LABEL version=1.0\"\nONBUILD\nString\nEX: \"ONBUILD RUN date\"\nMAINTAINER\nString, deprecated in Docker version 1.13.0\nEX: \"MAINTAINER NAME\"\nUSER\nString\nEX: \"USER USERNAME\"\nVOLUME\nString\nEX: \"VOLUME FROM TO\"\nWORKDIR\nEX: \"WORKDIR PATH\""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/post-processors/docker-push",
  "text": "Docker Push - Post-Processors | Packer\nDocker Push Post-Processor\nType: docker-push\nThe Packer Docker push post-processor takes an artifact from the docker-import post-processor and pushes it to a Docker registry.\nThis post-processor has only optional configuration:\naws_access_key (string) - The AWS access key used to communicate with AWS. Learn how to set this.\naws_secret_key (string) - The AWS secret key used to communicate with AWS. Learn how to set this.\naws_token (string) - The AWS access token to use. This is different from the access key and secret key. If you're not sure what this is, then you probably don't need it. This will also be read from the AWS_SESSION_TOKEN environmental variable.\naws_profile (string) - The AWS shared credentials profile used to communicate with AWS. Learn how to set this.\necr_login (boolean) - Defaults to false. If true, the post-processor will login in order to push the image to Amazon EC2 Container Registry (ECR). The post-processor only logs in for the duration of the push. If true login_server is required and login, login_username, and login_password will be ignored.\nkeep_input_artifact (boolean) - if true, do not delete the docker image after pushing it to the cloud. Defaults to true, but can be set to false if you do not need to save your local copy of the docker container.\nlogin (boolean) - Defaults to false. If true, the post-processor will login prior to pushing. For log into ECR see ecr_login.\nlogin_username (string) - The username to use to authenticate to login.\nlogin_password (string) - The password to use to authenticate to login.\nlogin_server (string) - The server address to login to.\nNote: When using Docker Hub or Quay registry servers, login must to be set to true and login_username, and login_password must to be set to your registry credentials. When using Docker Hub, login_server can be omitted.\nNote: If you login using the credentials above, the post-processor will automatically log you out afterwards (just the server specified).\nFor an example of using docker-push, see the section on using generated artifacts from the docker builder."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/post-processors/digitalocean-import",
  "text": "DigitalOcean Import - Post-Processors | Packer\nDigitalOcean Import Post-Processor\nType: digitalocean-import\nThe Packer DigitalOcean Import post-processor takes an image artifact from various builders and imports it to DigitalOcean.\nThe import process operates uploading a temporary copy of the image to DigitalOcean Spaces and then importing it as a custom image via the DigialOcean API. The temporary copy in Spaces can be discarded after the import is complete.\nFor information about the requirements to use an image for a DigitalOcean Droplet, see DigitalOcean's Custom Images documentation.\nThere are some configuration options available for the post-processor.\napi_token (string) - A personal access token used to communicate with the DigitalOcean v2 API. This may also be set using the DIGITALOCEAN_API_TOKEN environmental variable.\nspaces_key (string) - The access key used to communicate with Spaces. This may also be set using the DIGITALOCEAN_SPACES_ACCESS_KEY environmental variable.\nspaces_secret (string) - The secret key used to communicate with Spaces. This may also be set using the DIGITALOCEAN_SPACES_SECRET_KEY environmental variable.\nspaces_region (string) - The name of the region, such as nyc3, in which to upload the image to Spaces.\nspace_name (string) - The name of the specific Space where the image file will be copied to for import. This Space must exist when the post-processor is run.\nimage_name (string) - The name to be used for the resulting DigitalOcean custom image.\nimage_regions (array of string) - A list of DigitalOcean regions, such as nyc3, where the resulting image will be available for use in creating Droplets.\nimage_description (string) - The description to set for the resulting imported image.\nimage_distribution (string) - The name of the distribution to set for the resulting imported image.\nimage_tags (array of strings) - A list of tags to apply to the resulting imported image.\nkeep_input_artifact (boolean) - if true, do not delete the source virtual machine image after importing it to the cloud. Defaults to false.\nskip_clean (boolean) - Whether we should skip removing the image file uploaded to Spaces after the import process has completed. \"true\" means that we should leave it in the Space, \"false\" means to clean it out. Defaults to false.\nspace_object_name (string) - The name of the key used in the Space where the image file will be copied to for import. This is treated as a template engine. Therefore, you may use user variables and template functions in this field. If not specified, this will default to packer-import-{{timestamp}}.\ntimeout (number) - The length of time in minutes to wait for individual steps in the process to successfully complete. This includes both importing the image from Spaces as well as distributing the resulting image to additional regions. If not specified, this will default to 20.\nHere is a basic example:\n{ \"type\": \"digitalocean-import\", \"api_token\": \"{{user `token`}}\", \"spaces_key\": \"{{user `key`}}\", \"spaces_secret\": \"{{user `secret`}}\", \"spaces_region\": \"nyc3\", \"space_name\": \"import-bucket\", \"image_name\": \"ubuntu-18.10-minimal-amd64\", \"image_description\": \"Packer import {{timestamp}}\", \"image_regions\": [\"nyc3\", \"nyc2\"], \"image_tags\": [\"custom\", \"packer\"] }"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/post-processors/docker-tag",
  "text": "Docker Tag - Post-Processors | Packer\nDocker Tag Post-Processor\nType: docker-tag\nThe Packer Docker Tag post-processor takes an artifact from the docker builder that was committed and tags it into a repository. This allows you to use the other Docker post-processors such as docker-push to push the image to a registry.\nThis is very similar to the docker-import post-processor except that this works with committed resources, rather than exported.\nThe configuration for this post-processor requires repository, all other settings are optional.\nrepository (string) - The repository of the image.\ntag (array of strings) - The tag for the image. By default this is not set.\nforce (boolean) - If true, this post-processor forcibly tag the image even if tag name is collided. Default to false. But it will be ignored if Docker >= 1.12.0 was detected, since the force option was removed after 1.12.0. reference\nkeep_input_artifact (boolean) - Unlike most other post-processors, the keep_input_artifact option will have no effect for the docker-tag post-processor. We will always retain the input artifact for docker-tag, since deleting the image we just tagged is not a behavior anyone should ever expect. keep_input_artifact will therefore always be evaluated as true, regardless of the value you enter into this field.\n{ \"type\": \"docker-tag\", \"repository\": \"hashicorp/packer\", \"tag\": \"0.7,anothertag\" } \nThis example would take the image created by the Docker builder and tag it into the local Docker process with a name of hashicorp/packer:0.7.\nFollowing this, you can use the docker-push post-processor to push it to a registry, if you want."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/post-processors/exoscale-import",
  "text": "Exoscale Import - Post-Processors | Packer\nExoscale Import Post-Processor\nType: exoscale-import\nThe Packer Exoscale Import post-processor takes an image artifact from the QEMU, Artifice, or File builders and imports it to Exoscale.\nThe import process operates uploading a temporary copy of the image to Exoscale's Object Storage (SOS) and then importing it as a Custom Template via the Exoscale API. The temporary copy in SOS can be discarded after the import is complete.\nFor more information about Exoscale Custom Templates, see the documentation.\nThere are some configuration options available for the post-processor.\napi_key (string) - The API key used to communicate with Exoscale services. This may also be set using the EXOSCALE_API_KEY environmental variable.\napi_secret (string) - The API secret used to communicate with Exoscale services. This may also be set using the EXOSCALE_API_SECRET environmental variable.\nimage_bucket (string) - The name of the bucket in which to upload the template image to SOS. The bucket must exist when the post-processor is run.\ntemplate_name (string) - The name to be used for registering the template.\ntemplate_description (string) - The description for the registered template.\napi_endpoint (string) - The API endpoint used to communicate with the Exoscale API. Defaults to https://api.exoscale.com/compute.\nsos_endpoint (string) - The endpoint used to communicate with SOS. Defaults to https://sos-ch-gva-2.exo.io.\ntemplate_zone (string) - The Exoscale zone in which to register the template. Defaults to ch-gva-2.\ntemplate_username (string) - An optional username to be used to log into Compute instances using this template.\ntemplate_disable_password (boolean) - Whether the registered template should disable Compute instance password reset. Defaults to false.\ntemplate_disable_sshkey (boolean) - Whether the registered template should disable SSH key installation during Compute instance creation. Defaults to false.\nskip_clean (boolean) - Whether we should skip removing the image file uploaded to SOS after the import process has completed. \"true\" means that we should leave it in the bucket, \"false\" means deleting it. Defaults to false.\nHere is a basic example:\n{ \"type\": \"exoscale-import\", \"api_key\": \"{{user `exoscale_api_key`}}\", \"api_secret\": \"{{user `exoscale_api_secret`}}\", \"image_bucket\": \"my-templates\", \"template_name\": \"myapp\", \"template_description\": \"myapp v1.2.3\", \"template_username\": \"admin\" }"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/post-processors/googlecompute-import",
  "text": "Google Compute Image Import - Post-Processors | Packer\nType: googlecompute-import\nThe Google Compute Image Import post-processor takes a compressed raw disk image and imports it to a GCE image available to Google Compute Engine.\nThis post-processor is for advanced users. Please ensure you read the GCE import documentation before using this post-processor.\nThe import process operates by uploading a temporary copy of the compressed raw disk image to a GCS bucket, and calling an import task in GCP on the raw disk file. Once completed, a GCE image is created containing the converted virtual machine. The temporary raw disk image copy in GCS can be discarded after the import is complete.\nGoogle Cloud has very specific requirements for images being imported. Please see the GCE import documentation for details.\nRequired\naccount_file (string) - The JSON file containing your account credentials.\nbucket (string) - The name of the GCS bucket where the raw disk image will be uploaded.\nimage_name (string) - The unique name of the resulting image.\nproject_id (string) - The project ID where the GCS bucket exists and where the GCE image is stored.\ngcs_object_name (string) - The name of the GCS object in bucket where the RAW disk image will be copied for import. This is treated as a template engine. Therefore, you may use user variables and template functions in this field. Defaults to packer-import-{{timestamp}}.tar.gz.\nimage_description (string) - The description of the resulting image.\nimage_family (string) - The name of the image family to which the resulting image belongs.\nimage_labels (object of key/value strings) - Key/value pair labels to apply to the created image.\nimage_guest_os_features (array of strings) - A list of features to enable on the guest operating system. Applicable only for bootable images. Valid values are MULTI_IP_SUBNET, SECURE_BOOT, UEFI_COMPATIBLE, VIRTIO_SCSI_MULTIQUEUE and WINDOWS currently.\nkeep_input_artifact (boolean) - if true, do not delete the compressed RAW disk image. Defaults to false.\nskip_clean (boolean) - Skip removing the TAR file uploaded to the GCS bucket after the import process has completed. \"true\" means that we should leave it in the GCS bucket, \"false\" means to clean it out. Defaults to false.\nHere is a basic example. This assumes that the builder has produced an compressed raw disk image artifact for us to work with, and that the GCS bucket has been created.\n{ \"type\": \"googlecompute-import\", \"account_file\": \"account.json\", \"project_id\": \"my-project\", \"bucket\": \"my-bucket\", \"image_name\": \"my-gce-image\" } \nHere is a complete example for building a Fedora 28 server GCE image. For this example packer was run from a CentOS 7 server with KVM installed. The CentOS 7 server was running in GCE with the nested hypervisor feature enabled.\n$ packer build -var serial=$(tty) build.json \n{ \"variables\": { \"serial\": \"\" }, \"builders\": [ { \"type\": \"qemu\", \"accelerator\": \"kvm\", \"communicator\": \"none\", \"boot_command\": [ \"<tab> console=ttyS0,115200n8 inst.text inst.ks=http://{{ .HTTPIP }}:{{ .HTTPPort }}/fedora-28-ks.cfg rd.live.check=0<enter><wait>\" ], \"disk_size\": \"15000\", \"format\": \"raw\", \"iso_checksum_type\": \"sha256\", \"iso_checksum\": \"ea1efdc692356b3346326f82e2f468903e8da59324fdee8b10eac4fea83f23fe\", \"iso_url\": \"https://download-ib01.fedoraproject.org/pub/fedora/linux/releases/28/Server/x86_64/iso/Fedora-Server-netinst-x86_64-28-1.1.iso\", \"headless\": \"true\", \"http_directory\": \"http\", \"http_port_max\": \"10089\", \"http_port_min\": \"10082\", \"output_directory\": \"output\", \"shutdown_timeout\": \"30m\", \"vm_name\": \"disk.raw\", \"qemu_binary\": \"/usr/libexec/qemu-kvm\", \"qemuargs\": [ [\"-m\", \"1024\"], [\"-cpu\", \"host\"], [\"-chardev\", \"tty,id=pts,path={{user `serial`}}\"], [\"-device\", \"isa-serial,chardev=pts\"], [\"-device\", \"virtio-net,netdev=user.0\"] ] } ], \"post-processors\": [ [ { \"type\": \"compress\", \"output\": \"output/disk.raw.tar.gz\" }, { \"type\": \"googlecompute-import\", \"project_id\": \"my-project\", \"account_file\": \"account.json\", \"bucket\": \"my-bucket\", \"image_name\": \"fedora28-server-{{timestamp}}\", \"image_description\": \"Fedora 28 Server\", \"image_family\": \"fedora28-server\" } ] ] }"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/post-processors/ucloud-import",
  "text": "UCloud Import Post-Processors | Packer\nUCloud Import Post-Processor\nType: ucloud-import\nThe Packer UCloud Import post-processor takes the RAW, VHD, VMDK, or qcow2 artifact from various builders and imports it to UCloud customized image list for UHost Instance.\nNote Some regions don't support image import. You may refer to ucloud console for more detail. If you want to import to unsupported regions, please import the image in cn-bj2 first, and then copy the image to the target region.\nThe import process operates by making a temporary copy of the RAW, VHD, VMDK, or qcow2 to an UFile bucket, and calling an import task in UHost on the RAW, VHD, VMDK, or qcow2 file. Once completed, an UCloud UHost Image is returned. The temporary RAW, VHD, VMDK, or qcow2 copy in UFile can be discarded after the import is complete.\nThere are some configuration options available for the post-processor. There are two categories: required and optional parameters.\npublic_key - (string) This is the UCloud public key. It must be provided, but it can also be sourced from the UCLOUD_PUBLIC_KEY environment variable.\nprivate_key - (string) This is the UCloud private key. It must be provided, but it can also be sourced from the UCLOUD_PRIVATE_KEY environment variable.\nproject_id - (string) This is the UCloud project id. It must be provided, but it can also be sourced from the UCLOUD_PROJECT_ID environment variable.\nregion - (string) This is the UCloud region. It must be provided, but it can also be sourced from the UCLOUD_REGION environment variable.\nimage_name - (string) The name of the user-defined image, which contains 1-63 characters and only supports Chinese, English, numbers, '-_,.:[]'.\nufile_bucket_name (string) - The name of the UFile bucket where the RAW, VHD, VMDK, or qcow2 file will be copied to for import. This bucket must exist when the post-processor is run.\nimage_os_type (string) - Type of the OS. Possible values are: CentOS, Ubuntu, Windows, RedHat, Debian, Other. You may refer to ucloud_api_docs for detail.\nimage_os_name (string) - The name of OS. Such as: CentOS 7.2 64, set Other When image_os_type is Other. You may refer to ucloud_api_docs for detail.\nformat (string) - The format of the import image , Possible values are: raw, vhd, vmdk, or qcow2.\nbase_url - (string) This is the base url. (Default: https://api.ucloud.cn).\nufile_key_name (string) - The name of the object key in ufile_bucket_name where the RAW, VHD, VMDK, or qcow2 file will be copied to import. This is a template engine. Therefore, you may use user variables and template functions in this field.\nskip_clean (boolean) - Whether we should skip removing the RAW, VHD, VMDK, or qcow2 file uploaded to UFile after the import process has completed. Possible values are: true to leave it in the UFile bucket, false to remove it. (Default: false).\nimage_description (string) - The description of the image.\nwait_image_ready_timeout(number) - Timeout of importing image. The default timeout is 3600 seconds if this option is not set or is set to 0.\nHere is a basic example. This assumes that the builder has produced a RAW artifact for us to work with. This will take the RAW image generated by a builder and upload it to UFile. Once uploaded, the import process will start, creating an UCloud UHost image to the region cn-bj2.\n\"post-processors\":[ { \"type\":\"ucloud-import\", \"public_key\": \"{{user `ucloud_public_key`}}\", \"private_key\": \"{{user `ucloud_private_key`}}\", \"project_id\": \"{{user `ucloud_project_id`}}\", \"region\":\"cn-bj2\", \"ufile_bucket_name\": \"packer-import\", \"image_name\": \"packer_import\", \"image_os_type\": \"CentOS\", \"image_os_name\": \"CentOS 6.10 64\", \"format\": \"raw\" } ]"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/post-processors/googlecompute-export",
  "text": "Google Compute Image Exporter - Post-Processors | Packer\nType: googlecompute-export\nThe Google Compute Image Exporter post-processor exports the resultant image from a googlecompute build as a gzipped tarball to Google Cloud Storage (GCS).\nThe exporter uses the same Google Cloud Platform (GCP) project and authentication credentials as the googlecompute build that produced the image. A temporary VM is started in the GCP project using these credentials. The VM mounts the built image as a disk then dumps, compresses, and tars the image. The VM then uploads the tarball to the provided GCS paths using the same credentials.\nAs such, the authentication credentials that built the image must have write permissions to the GCS paths.\nRequired\npaths (list of string) - The list of GCS paths, e.g. 'gs://mybucket/path/to/file.tar.gz', where the image will be exported.\naccount_file (string) - The JSON file containing your account credentials. If specified, this take precedence over googlecompute builder authentication method.\ndisk_size (number) - The size of the export instances disk, this disk is unused for the export but a larger size increase pd-ssd read speed. This defaults to 200, which is 200GB.\ndisk_type (string) - Type of disk used to back export instance, like pd-ssd or pd-standard. Defaults to pd-ssd.\nkeep_input_artifact (boolean) - If true, do not delete the Google Compute Engine (GCE) image being exported. defaults to false.\nmachine_type (string) - The export instance machine type. Defaults to \"n1-highcpu-4\".\nnetwork (string) - The Google Compute network id or URL to use for the export instance. Defaults to \"default\". If the value is not a URL, it will be interpolated to projects/((builder_project_id))/global/networks/((network)). This value is not required if a subnet is specified.\nsubnetwork (string) - The Google Compute subnetwork id or URL to use for the export instance. Only required if the network has been created with custom subnetting. Note, the region of the subnetwork must match the zone in which the VM is launched. If the value is not a URL, it will be interpolated to projects/((builder_project_id))/regions/((region))/subnetworks/((subnetwork))\nzone (string) - The zone in which to launch the export instance. Defaults to googlecompute builder zone. Example: \"us-central1-a\"\nThe following example builds a GCE image in the project, my-project, with an account whose keyfile is account.json. After the image build, a temporary VM will be created to export the image as a gzipped tarball to gs://mybucket1/path/to/file1.tar.gz and gs://mybucket2/path/to/file2.tar.gz. keep_input_artifact is true, so the GCE image won't be deleted after the export.\nIn order for this example to work, the account associated with account.json must have write access to both gs://mybucket1/path/to/file1.tar.gz and gs://mybucket2/path/to/file2.tar.gz.\n{ \"builders\": [ { \"type\": \"googlecompute\", \"account_file\": \"account.json\", \"project_id\": \"my-project\", \"source_image\": \"debian-7-wheezy-v20150127\", \"zone\": \"us-central1-a\" } ], \"post-processors\": [ { \"type\": \"googlecompute-export\", \"paths\": [ \"gs://mybucket1/path/to/file1.tar.gz\", \"gs://mybucket2/path/to/file2.tar.gz\" ], \"keep_input_artifact\": true } ] }"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/post-processors/vsphere",
  "text": "vSphere - Post-Processors | Packer\nType: vsphere\nThe Packer vSphere post-processor takes an artifact and uploads it to a vSphere endpoint. The artifact must have a vmx/ova/ovf image.\nThere are many configuration options available for the post-processor. They are segmented below into two categories: required and optional parameters. Within each category, the available configuration keys are alphabetized.\ncluster (string) - The cluster to upload the VM to.\ndatacenter (string) - The name of the datacenter within vSphere to add the VM to.\ndatastore (string) - The name of the datastore to store this VM. This is not required if resource_pool is specified.\nhost (string) - The vSphere host that will be contacted to perform the VM upload.\npassword (string) - Password to use to authenticate to the vSphere endpoint.\nusername (string) - The username to use to authenticate to the vSphere endpoint.\nvm_name (string) - The name of the VM once it is uploaded.\nesxi_host (string) - Target vSphere host. Used to assign specific esx host to upload the resulting VM to, when a vCenter Server is used as host. Can be either a hostname (e.g. \"packer-esxi1\", requires proper DNS setup and/or correct DNS search domain setting) or an ipv4 address.\ndisk_mode (string) - Target disk format. See ovftool manual for available options. By default, \"thick\" will be used.\ninsecure (boolean) - Whether or not the connection to vSphere can be done over an insecure connection. By default this is false.\nkeep_input_artifact (boolean) - When true, preserve the local VM files, even after importing them to vsphere. Defaults to false.\nresource_pool (string) - The resource pool to upload the VM to.\nvm_folder (string) - The folder within the datastore to store the VM.\nvm_network (string) - The name of the VM network this VM will be added to.\noverwrite (boolean) - If it's true force the system to overwrite the existing files instead create new ones. Default is false\noptions (array of strings) - Custom options to add in ovftool. See ovftool --help to list all the options"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/post-processors/vagrant",
  "text": "Vagrant - Post-Processors | Packer\nType: vagrant\nThe Packer Vagrant post-processor takes a build and converts the artifact into a valid Vagrant box, if it can. This lets you use Packer to automatically create arbitrarily complex Vagrant boxes, and is in fact how the official boxes distributed by Vagrant are created.\nIf you've never used a post-processor before, please read the documentation on using post-processors in templates. This knowledge will be expected for the remainder of this document.\nBecause Vagrant boxes are provider-specific, the Vagrant post-processor is hardcoded to understand how to convert the artifacts of certain builders into proper boxes for their respective providers.\nCurrently, the Vagrant post-processor can create boxes for the following providers.\nAWS\nAzure\nDigitalOcean\nDocker\nHyper-V\nLXC\nParallels\nQEMU\nVirtualBox\nVMware\nSupport for additional providers is planned. If the Vagrant post-processor doesn't support creating boxes for a provider you care about, please help by contributing to Packer and adding support for it.\nPlease note that if you are using the Vagrant builder, then the Vagrant post-processor is unnecesary because the output of the Vagrant builder is already a Vagrant box; using this post-processor with the Vagrant builder will cause your build to fail.\nThe simplest way to use the post-processor is to just enable it. No configuration is required by default. This will mostly do what you expect and will build functioning boxes for many of the built-in builders of Packer.\nHowever, if you want to configure things a bit more, the post-processor does expose some configuration options. The available options are listed below, with more details about certain options in following sections.\ncompression_level (number) - An integer representing the compression level to use when creating the Vagrant box. Valid values range from 0 to 9, with 0 being no compression and 9 being the best compression. By default, compression is enabled at level 6.\ninclude (array of strings) - Paths to files to include in the Vagrant box. These files will each be copied into the top level directory of the Vagrant box (regardless of their paths). They can then be used from the Vagrantfile.\nkeep_input_artifact (boolean) - When true, preserve the artifact we use to create the vagrant box. Defaults to false, except when you set a cloud provider (e.g. aws, azure, google, digitalocean). In these cases deleting the input artifact would render the vagrant box useless, so we always keep these artifacts -- even if you specifically set \"keep_input_artifact\":false\noutput (string) - The full path to the box file that will be created by this post-processor. This is a template engine. Therefore, you may use user variables and template functions in this field. The following extra variables are also avilable in this engine:\nProvider: The Vagrant provider the box is for\nArtifactId: The ID of the input artifact.\nBuildName: The name of the build.\nBy default, the value of this config is packer_{{.BuildName}}_{{.Provider}}.box.\nvagrantfile_template (string) - Path to a template to use for the Vagrantfile that is packaged with the box.\nvagrantfile_template_generated (boolean) - By default, Packer will exit with an error if the file specified using the vagrantfile_template variable is not found. However, under certain circumstances, it may be desirable to dynamically generate the Vagrantfile during the course of the build. Setting this variable to true skips the start up check and allows the user to script the creation of the Vagrantfile at some previous point in the build. Defaults to false.\nIf you have a Packer template with multiple builder types within it, you may want to configure the box creation for each type a little differently. For example, the contents of the Vagrantfile for a Vagrant box for AWS might be different from the contents of the Vagrantfile you want for VMware. The post-processor lets you do this.\nSpecify overrides within the override configuration by provider name:\n{ \"type\": \"vagrant\", \"compression_level\": 1, \"override\": { \"vmware\": { \"compression_level\": 0 } } } \nIn the example above, the compression level will be set to 1 except for VMware, where it will be set to 0.\nThe available provider names are:\naws\nazure\ndigitalocean\ngoogle\nhyperv\nparallels\nlibvirt\nlxc\nscaleway\nvirtualbox\nvmware\ndocker\nBy default, Packer will delete the original input artifact, assuming you only want the final Vagrant box as the result. If you wish to keep the input artifact (the raw virtual machine, for example), then you must configure Packer to keep it.\nPlease see the documentation on input artifacts for more information.\nDocker\nUsing a Docker input artifact will include a reference to the image in the Vagrantfile. If the image tag is not specified in the post-processor, the sha256 hash will be used.\nThe following Docker input artifacts are supported:\ndocker builder with commit: true, always uses the sha256 hash\ndocker-import\ndocker-tag\ndocker-push\nQEMU/libvirt\nThe libvirt provider supports QEMU artifacts built using any these accelerators: none, kvm, tcg, or hvf.\nVMWare\nIf you are using the Vagrant post-processor with the vmware-esxi builder, you must export the builder artifact locally; the Vagrant post-processor will not work on remote artifacts."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/post-processors/vagrant-cloud",
  "text": "Vagrant Cloud - Post-Processors | Packer\nVagrant Cloud Post-Processor\nType: vagrant-cloud\nVagrant Cloud hosts and serves boxes to Vagrant, allowing you to version and distribute boxes to an organization in a simple way.\nThe Vagrant Cloud post-processor enables the upload of Vagrant boxes to Vagrant Cloud. Currently, the Vagrant Cloud post-processor will accept and upload boxes supplied to it from the Vagrant or Artifice post-processors and the Vagrant builder.\nYou'll need to be familiar with Vagrant Cloud, have an upgraded account to enable box hosting, and be distributing your box via the shorthand name configuration.\nIt's important to understand the workflow that using this post-processor enforces in order to take full advantage of Vagrant and Vagrant Cloud.\nThe use of this processor assume that you currently distribute, or plan to distribute, boxes via Vagrant Cloud. It also assumes you create Vagrant Boxes and deliver them to your team in some fashion.\nHere is an example workflow:\nYou use Packer to build a Vagrant Box for the virtualbox provider\nThe vagrant-cloud post-processor is configured to point to the box hashicorp/foobar on Vagrant Cloud via the box_tag configuration\nThe post-processor receives the box from the vagrant post-processor\nIt then creates the configured version, or verifies the existence of it, on Vagrant Cloud\nA provider matching the name of the Vagrant provider is then created\nThe box is uploaded to Vagrant Cloud\nThe upload is verified\nThe version is released and available to users of the box\nThe configuration allows you to specify the target box that you have access to on Vagrant Cloud, as well as authentication and version information.\nbox_tag (string) - The shorthand tag for your box that maps to Vagrant Cloud, for example hashicorp/precise64, which is short for vagrantcloud.com/hashicorp/precise64.\nversion (string) - The version number, typically incrementing a previous version. The version string is validated based on Semantic Versioning. The string must match a pattern that could be semver, and doesn't validate that the version comes after your previous versions.\naccess_token (string) - Your access token for the Vagrant Cloud API. This can be generated on your tokens page. If not specified, the environment will be searched. First, VAGRANT_CLOUD_TOKEN is checked, and if nothing is found, finally ATLAS_TOKEN will be used. This is required unless you are using a private hosting solution (i.e. vagrant_cloud_url has been populated).\nor\nvagrant_cloud_url (string) - Override the base URL for Vagrant Cloud. This is useful if you're using Vagrant Private Cloud in your own network. Defaults to https://vagrantcloud.com/api/v1. If this value is set to something other than the default then access_token can be left blank and no Authorization header will be added to requests sent by this post-processor.\nno_release (string) - If set to true, does not release the version on Vagrant Cloud, making it active. You can manually release the version via the API or Web UI. Defaults to false.\ninsecure_skip_tls_verify (boolean) - If set to true and vagrant_cloud_url is set to something different than its default, it will set TLS InsecureSkipVerify to true. In other words, this will disable security checks of SSL. You may need to set this option to true if your host at vagrant_cloud_url is using a self-signed certificate.\nkeep_input_artifact (boolean) - When true, preserve the local box after uploading to Vagrant cloud. Defaults to true.\nversion_description (string) - Optionally markdown text used as a full-length and in-depth description of the version, typically for denoting changes introduced\nbox_download_url (string) - Optional URL for a self-hosted box. If this is set the box will not be uploaded to the Vagrant Cloud. This is a template engine. Therefore, you may use user variables and template functions in this field. The following extra variables are also avilable in this engine:\nProvider: The Vagrant provider the box is for\nArtifactId: The ID of the input artifact.\nUse with the Vagrant Post-Processor\nAn example configuration is shown below. Note the use of the nested array that wraps both the Vagrant and Vagrant Cloud post-processors within the post-processor section. Chaining the post-processors together in this way tells Packer that the artifact produced by the Vagrant post-processor should be passed directly to the Vagrant Cloud Post-Processor. It also sets the order in which the post-processors should run.\nFailure to chain the post-processors together in this way will result in the wrong artifact being supplied to the Vagrant Cloud post-processor. This will likely cause the Vagrant Cloud post-processor to error and fail.\n{ \"variables\": { \"cloud_token\": \"{{ env `VAGRANT_CLOUD_TOKEN` }}\", \"version\": \"1.0.{{timestamp}}\" }, \"post-processors\": [ { \"type\": \"shell-local\", \"inline\": [\"echo Doing stuff...\"] }, [ { \"type\": \"vagrant\", \"include\": [\"image.iso\"], \"vagrantfile_template\": \"vagrantfile.tpl\", \"output\": \"proxycore_{{.Provider}}.box\" }, { \"type\": \"vagrant-cloud\", \"box_tag\": \"hashicorp/precise64\", \"access_token\": \"{{user `cloud_token`}}\", \"version\": \"{{user `version`}}\" } ] ] } \nUse with the Artifice Post-Processor\nAn example configuration is shown below. Note the use of the nested array that wraps both the Artifice and Vagrant Cloud post-processors within the post-processor section. Chaining the post-processors together in this way tells Packer that the artifact produced by the Artifice post-processor should be passed directly to the Vagrant Cloud Post-Processor. It also sets the order in which the post-processors should run.\nFailure to chain the post-processors together in this way will result in the wrong artifact being supplied to the Vagrant Cloud post-processor. This will likely cause the Vagrant Cloud post-processor to error and fail.\nNote that the Vagrant box specified in the Artifice post-processor files array must end in the .box extension. It must also be the first file in the array. Additional files bundled by the Artifice post-processor will be ignored.\n{ \"variables\": { \"cloud_token\": \"{{ env `VAGRANT_CLOUD_TOKEN` }}\" }, \"builders\": [ { \"type\": \"null\", \"communicator\": \"none\" } ], \"post-processors\": [ { \"type\": \"shell-local\", \"inline\": [\"echo Doing stuff...\"] }, [ { \"type\": \"artifice\", \"files\": [\"./path/to/my.box\"] }, { \"type\": \"vagrant-cloud\", \"box_tag\": \"myorganisation/mybox\", \"access_token\": \"{{user `cloud_token`}}\", \"version\": \"0.1.0\" } ] ] }"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/post-processors/alicloud-import",
  "text": "Alicloud Import Post-Processor | Packer\nType: alicloud-import\nThe Packer Alicloud Import post-processor takes a RAW or VHD artifact from various builders and imports it to an Alicloud ECS Image.\nThe import process operates by making a temporary copy of the RAW or VHD to an OSS bucket, and calling an import task in ECS on the RAW or VHD file. Once completed, an Alicloud ECS Image is returned. The temporary RAW or VHD copy in OSS can be discarded after the import is complete.\nThere are some configuration options available for the post-processor. There are two categories: required and optional parameters.\naccess_key (string) - Alicloud access key must be provided unless profile is set, but it can also be sourced from the ALICLOUD_ACCESS_KEY environment variable.\nsecret_key (string) - Alicloud secret key must be provided unless profile is set, but it can also be sourced from the ALICLOUD_SECRET_KEY environment variable.\nregion (string) - Alicloud region must be provided unless profile is set, but it can also be sourced from the ALICLOUD_REGION environment variable.\nimage_name (string) - The name of the user-defined image, [2, 128] English or Chinese characters. It must begin with an uppercase/lowercase letter or a Chinese character, and may contain numbers, _ or -. It cannot begin with http:// or https://.\noss_bucket_name (string) - The name of the OSS bucket where the RAW or VHD file will be copied to for import. If the Bucket doesn't exist, the post-process will create it for you.\nimage_os_type (string) - Type of the OS, like linux/windows\nimage_platform (string) - Platform such as CentOS\nimage_architecture (string) - Platform type of the image system: i386 or x86_64\nformat (string) - The format of the image for import, now alicloud only support RAW and VHD.\nkeep_input_artifact (boolean) - if true, do not delete the RAW or VHD disk image after importing it to the cloud. Defaults to false.\noss_key_name (string) - The name of the object key in oss_bucket_name where the RAW or VHD file will be copied to for import. This is treated as a template engine, and you may access any of the variables stored in the generated data using the build template function.\nskip_clean (bool) - Whether we should skip removing the RAW or VHD file uploaded to OSS after the import process has completed. true means that we should leave it in the OSS bucket, false means to clean it out. Defaults to false.\ntags (map[string]string) - Tags\nimage_description (string) - The description of the image, with a length limit of 0 to 256 characters. Leaving it blank means null, which is the default value. It cannot begin with http:// or https://.\nimage_share_account ([]string) - Alicloud Image Share Accounts\nimage_copy_regions ([]string) - Alicloud Image Destination Regions\nimage_system_size (string) - Size of the system disk, in GB, values range:\ncloud - 5 ~ 2000\ncloud_efficiency - 20 ~ 2048\ncloud_ssd - 20 ~ 2048\nimage_force_delete (bool) - If this value is true, when the target image name is duplicated with an existing image, it will delete the existing image and then create the target image, otherwise, the creation will fail. The default value is false.\nHere is a basic example. This assumes that the builder has produced a RAW artifact. The user must have the role AliyunECSImageImportDefaultRole with AliyunECSImageImportRolePolicy, post-process will automatically configure the role and policy for you if you have the privilege, otherwise, you have to ask the administrator configure for you in advance.\n\"post-processors\":[ { \"access_key\":\"{{user `access_key`}}\", \"secret_key\":\"{{user `secret_key`}}\", \"type\":\"alicloud-import\", \"oss_bucket_name\": \"packer\", \"image_name\": \"packer_import\", \"image_os_type\": \"linux\", \"image_platform\": \"CentOS\", \"image_architecture\": \"x86_64\", \"image_system_size\": \"40\", \"region\":\"cn-beijing\" } ] \nThis will take the RAW generated by a builder and upload it to OSS. In this case, an existing bucket called packer in the cn-beijing region will be where the copy is placed.\nOnce uploaded, the import process will start, creating an Alicloud ECS image in the cn-beijing region with the name you specified in template file."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/post-processors/amazon-import",
  "text": "Amazon Import - Post-Processors | Packer\nAmazon Import Post-Processor\nType: amazon-import\nThe Packer Amazon Import post-processor takes an OVA artifact from various builders and imports it to an AMI available to Amazon Web Services EC2.\nThis post-processor is for advanced users. It depends on specific IAM roles inside AWS and is best used with images that operate with the EC2 configuration model (eg, cloud-init for Linux systems). Please ensure you read the prerequisites for import before using this post-processor.\nThe import process operates making a temporary copy of the OVA to an S3 bucket, and calling an import task in EC2 on the OVA file. Once completed, an AMI is returned containing the converted virtual machine. The temporary OVA copy in S3 can be discarded after the import is complete.\nThe import process itself run by AWS includes modifications to the image uploaded, to allow it to boot and operate in the AWS EC2 environment. However, not all modifications required to make the machine run well in EC2 are performed. Take care around console output from the machine, as debugging can be very difficult without it. You may also want to include tools suitable for instances in EC2 such as cloud-init for Linux.\nFurther information about the import process can be found in AWS's EC2 Import/Export Instance documentation.\nThere are some configuration options available for the post-processor. They are segmented below into two categories: required and optional parameters. Within each category, the available configuration keys are alphabetized.\naccess_key (string) - The access key used to communicate with AWS. Learn how to set this.\nregion (string) - The name of the region, such as us-east-1 in which to upload the OVA file to S3 and create the AMI. A list of valid regions can be obtained with AWS CLI tools or by consulting the AWS website.\ns3_bucket_name (string) - The name of the S3 bucket where the OVA file will be copied to for import. This bucket must exist when the post-processor is run.\nsecret_key (string) - The secret key used to communicate with AWS. Learn how to set this.\nami_description (string) - The description to set for the resulting imported AMI. By default this description is generated by the AMI import process.\nami_encrypt (boolean) - Encrypt the resulting AMI using KMS. This defaults to false.\nami_groups (array of strings) - A list of groups that have access to launch the imported AMI. By default no groups have permission to launch the AMI. all will make the AMI publicly accessible. AWS currently doesn't accept any value other than \"all\".\nami_kms_key (string) - The ID of the KMS key used to encrypt the AMI if ami_encrypt is true. If set, the role specified in role_name must be granted access to use this key. If not set, the account default KMS key will be used.\nami_name (string) - The name of the ami within the console. If not specified, this will default to something like ami-import-sfwerwf. Please note, specifying this option will result in a slightly longer execution time.\nami_users (array of strings) - A list of account IDs that have access to launch the imported AMI. By default no additional users other than the user importing the AMI has permission to launch it.\ncustom_endpoint_ec2 (string) - This option is useful if you use a cloud provider whose API is compatible with aws EC2. Specify another endpoint like this https://ec2.custom.endpoint.com.\nformat (string) - One of: ova, raw, vhd, vhdx, or vmdk. This specifies the format of the source virtual machine image. The resulting artifact from the builder is assumed to have a file extension matching the format. This defaults to ova.\ninsecure_skip_tls_verify (boolean) - This allows skipping TLS verification of the AWS EC2 endpoint. The default is false.\nkeep_input_artifact (boolean) - if true, do not delete the source virtual machine image after importing it to the cloud. Defaults to false.\nlicense_type (string) - The license type to be used for the Amazon Machine Image (AMI) after importing. Valid values: AWS or BYOL (default). For more details regarding licensing, see Prerequisites in the VM Import/Export User Guide.\nmfa_code (string) - The MFA TOTP code. This should probably be a user variable since it changes all the time.\nprofile (string) - The profile to use in the shared credentials file for AWS. See Amazon's documentation on specifying profiles for more details.\nrole_name (string) - The name of the role to use when not using the default role, 'vmimport'\ns3_encryption (string) - One of: aws:kms, or AES256. The algorithm used to encrypt the artifact in S3. This does not encrypt the resulting AMI, and is only used to encrypt the uploaded artifact before it becomes an AMI. By default no encryption is used.\ns3_encryption_key (string) - The KMS key ID to use when aws:kms is specified in s3_encryption. This setting is ignored if AES is used as Amazon does not currently support custom AES keys when using the VM import service. If set, the role specified in role_name must be granted access to use this key. If not set, and s3_encryption is set to aws:kms, the account default KMS key will be used.\ns3_key_name (string) - The name of the key in s3_bucket_name where the OVA file will be copied to for import. If not specified, this will default to \"packer-import-{{timestamp}}.ova\". This key (i.e., the uploaded OVA) will be removed after import, unless skip_clean is true. This is treated as a template engine. Therefore, you may use user variables and template functions in this field.\nskip_clean (boolean) - Whether we should skip removing the OVA file uploaded to S3 after the import process has completed. \"true\" means that we should leave it in the S3 bucket, \"false\" means to clean it out. Defaults to false.\nskip_region_validation (boolean) - Set to true if you want to skip validation of the region configuration option. Default false.\ntags (object of key/value strings) - Tags applied to the created AMI and relevant snapshots.\ntoken (string) - The access token to use. This is different from the access key and secret key. If you're not sure what this is, then you probably don't need it. This will also be read from the AWS_SESSION_TOKEN environmental variable.\nHere is a basic example. This assumes that the builder has produced an OVA artifact for us to work with, and IAM roles for import exist in the AWS account being imported into.\n{ \"type\": \"amazon-import\", \"access_key\": \"YOUR KEY HERE\", \"secret_key\": \"YOUR SECRET KEY HERE\", \"region\": \"us-east-1\", \"s3_bucket_name\": \"importbucket\", \"license_type\": \"BYOL\", \"tags\": { \"Description\": \"packer amazon-import {{timestamp}}\" } } \nThis is an example that uses vmware-iso builder and exports the .ova file using ovftool.\n\"post-processors\" : [ [ { \"type\": \"shell-local\", \"inline\": [ \"/usr/bin/ovftool <packer-output-directory>/<vmware-name>.vmx <packer-output-directory>/<vmware-name>.ova\" ] }, { \"files\": [ \"<packer-output-directory>/<vmware-name>.ova\" ], \"type\": \"artifice\" }, { \"type\": \"amazon-import\", \"access_key\": \"YOUR KEY HERE\", \"secret_key\": \"YOUR SECRET KEY HERE\", \"region\": \"us-east-1\", \"s3_bucket_name\": \"importbucket\", \"license_type\": \"BYOL\", \"tags\": { \"Description\": \"packer amazon-import {{timestamp}}\" } } ] ] \nYou'll need at least the following permissions in the policy for your IAM user in order to successfully upload an image via the amazon-import post-processor.\n(\"ec2:CancelImportTask\", \"ec2:CopyImage\", \"ec2:CreateTags\", \"ec2:DescribeImages\", \"ec2:DescribeImportImageTasks\", \"ec2:ImportImage\", \"ec2:ModifyImageAttribute\", \"ec2:DeregisterImage\") \nThe amazon-import feature can take a long time to upload and convert your OVAs into AMIs; if you find that your build is failing because you have exceeded your max retries or find yourself being rate limited, you can override the max retries and the delay in between retries by setting the environment variables AWS_MAX_ATTEMPTS and AWS_POLL_DELAY_SECONDS on the machine running the Packer build. By default, the waiter that waits for your image to be imported from s3 will retry for up to an hour: it retries up to 720 times with a 5 second delay in between retries.\nThis is dramatically higher than many of our other waiters, to account for how long this process can take.\nNote: Packer can also read the access key and secret access key from environmental variables. See the configuration reference in the section above for more information on what environmental variables Packer will look for.\nThis will take the OVA generated by a builder and upload it to S3. In this case, an existing bucket called importbucket in the us-east-1 region will be where the copy is placed. The key name of the copy will be a default name generated by packer.\nOnce uploaded, the import process will start, creating an AMI in the \"us-east-1\" region with a \"Description\" tag applied to both the AMI and the snapshots associated with it. Note: the import process does not allow you to name the AMI, the name is automatically generated by AWS.\nAfter tagging is completed, the OVA uploaded to S3 will be removed."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/post-processors/docker-import",
  "text": "Docker Import - Post-Processors | Packer\nDocker Import Post-Processor\nType: docker-import\nThe Packer Docker import post-processor takes an artifact from the docker builder and imports it with Docker locally. This allows you to apply a repository and tag to the image and lets you use the other Docker post-processors such as docker-push to push the image to a registry.\n{ \"builders\": [ { \"type\": \"docker\", \"image\": \"ubuntu:18.04\", \"export_path\": \"party_parrot.tar\" } ], \"post-processors\": [ { \"type\": \"docker-import\", \"repository\": \"local/ubuntu\", \"tag\": \"latest\" } ] } \nThe configuration for this post-processor only requires a repository, a tag is optional.\nrepository (string) - The repository of the imported image.\ntag (string) - The tag for the imported image. By default this is not set.\nchanges (array of strings) - Dockerfile instructions to add to the commit. Example of instructions are CMD, ENTRYPOINT, ENV, and EXPOSE. Example: [ \"USER ubuntu\", \"WORKDIR /app\", \"EXPOSE 8080\" ]\nkeep_input_artifact (boolean) - if true, do not delete the source tar after importing it to docker. Defaults to false.\n{ \"type\": \"docker-import\", \"repository\": \"hashicorp/packer\", \"tag\": \"0.7\" } \nThis example would take the image created by the Docker builder and import it into the local Docker process with a name of hashicorp/packer:0.7.\nFollowing this, you can use the docker-push post-processor to push it to a registry, if you want.\nBelow is an example using the changes argument of the post-processor. This feature allows the tarball metadata to be changed when imported into the Docker environment. It is derived from the docker import --change command line option to Docker.\nExample uses of all of the options, assuming one is building an NGINX image from ubuntu as an simple example:\n{ \"type\": \"docker-import\", \"repository\": \"local/centos6\", \"tag\": \"latest\", \"changes\": [ \"USER www-data\", \"WORKDIR /var/www\", \"ENV HOSTNAME www.example.com\", \"VOLUME /test1 /test2\", \"EXPOSE 80 443\", \"LABEL version=1.0\", \"ONBUILD RUN date\", \"CMD [\\\"nginx\\\", \\\"-g\\\", \\\"daemon off;\\\"]\", \"ENTRYPOINT /var/www/start.sh\" ] } \nAllowed metadata fields that can be changed are:\nCMD\nString, supports both array (escaped) and string form\nEX: \"CMD [\\\"nginx\\\", \\\"-g\\\", \\\"daemon off;\\\"]\"\nEX: \"CMD nginx -g daemon off;\"\nENTRYPOINT\nEX: \"ENTRYPOINT /var/www/start.sh\"\nENV\nString, note there is no equal sign:\nEX: \"ENV HOSTNAME www.example.com\" not \"ENV HOSTNAME=www.example.com\"\nEXPOSE\nString, space separated ports\nEX: \"EXPOSE 80 443\"\nLABEL\nString, space separated key=value pairs\nEX: \"LABEL version=1.0\"\nONBUILD\nEX: \"ONBUILD RUN date\"\nMAINTAINER\nString, deprecated in Docker version 1.13.0\nEX: \"MAINTAINER NAME\"\nUSER\nEX: \"USER USERNAME\"\nVOLUME\nEX: \"VOLUME FROM TO\"\nWORKDIR\nEX: \"WORKDIR PATH\""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/post-processors/vsphere-template",
  "text": "vSphere Template - Post-Processors | Packer\nvSphere Template Post-Processor\nType: vsphere-template\nThe Packer vSphere Template post-processor takes an artifact from the VMware-iso builder, built on ESXi (i.e. remote) or an artifact from the vSphere post-processor, marks the VM as a template, and leaves it in the path of your choice.\n{ \"type\": \"vsphere-template\", \"host\": \"vcenter.local\", \"insecure\": true, \"username\": \"root\", \"password\": \"secret\", \"datacenter\": \"mydatacenter\", \"folder\": \"/packer-templates/os/distro-7\" } \nThere are many configuration options available for the post-processor. They are segmented below into two categories: required and optional parameters. Within each category, the available configuration keys are alphabetized.\nhost (string) - The vSphere host that contains the VM built by the vmware-iso.\npassword (string) - Password to use to authenticate to the vSphere endpoint.\nusername (string) - The username to use to authenticate to the vSphere endpoint.\ndatacenter (string) - If you have more than one, you will need to specify which one the ESXi used.\nfolder (string) - Target path where the template will be created.\ninsecure (boolean) - If it's true skip verification of server certificate. Default is false\nkeep_input_artifact (boolean) - Unlike most post-processors, this option has no effect for vsphere-template. This is because in order for a template to work, you can't delete the vm that you generate the template from. The vsphere template post-processor will therefore always preserve the original vm.\nsnapshot_enable (boolean) - Create a snapshot before marking as a template. Default is false\nsnapshot_name (string) - Name for the snapshot. Required when snapshot_enable is true\nsnapshot_description (string) - Description for the snapshot. Required when snapshot_enable is true\nreregister_vm (boolean) - Use the method of unregister VM and reregister as a template, rather than using the markAsTemplate method in vmWare. NOTE: If you are getting permission denied errors when trying to mark as a template, but it works fine in the vSphere UI, try setting this to false. Default is true.\nOnce the vSphere takes an artifact from the VMware builder and uploads it to a vSphere endpoint, you will likely want to mark that VM as template. Packer can do this for you automatically using a sequence definition (a collection of post-processors that are treated as as single pipeline, see Post-Processors for more information):\n{ \"post-processors\": [ [ { \"type\": \"vsphere\", ... }, { \"type\": \"vsphere-template\", ... } ], { \"type\": \"...\", ... } ] } \nIn the example above, the result of each builder is passed through the defined sequence of post-processors starting with the vsphere post-processor which will upload the artifact to a vSphere endpoint. The resulting artifact is then passed on to the vsphere-template post-processor which handles marking a VM as a template. Note that the vsphere and vsphere-template post-processors are paired together in their own JSON array."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/post-processors/digitalocean-import",
  "text": "DigitalOcean Import - Post-Processors | Packer\nDigitalOcean Import Post-Processor\nType: digitalocean-import\nThe Packer DigitalOcean Import post-processor is used to import images created by other Packer builders to DigitalOcean.\nNote: Users looking to create custom images, and reusable snapshots, directly on DigitalOcean can use the DigitalOcean builder without this post-processor.\nThe import process operates uploading a temporary copy of the image to DigitalOcean Spaces and then importing it as a custom image via the DigialOcean API. The temporary copy in Spaces can be discarded after the import is complete.\nFor information about the requirements to use an image for a DigitalOcean Droplet, see DigitalOcean's Custom Images documentation.\nThere are some configuration options available for the post-processor.\napi_token (string) - A personal access token used to communicate with the DigitalOcean v2 API. This may also be set using the DIGITALOCEAN_API_TOKEN environmental variable.\nspaces_key (string) - The access key used to communicate with Spaces. This may also be set using the DIGITALOCEAN_SPACES_ACCESS_KEY environmental variable.\nspaces_secret (string) - The secret key used to communicate with Spaces. This may also be set using the DIGITALOCEAN_SPACES_SECRET_KEY environmental variable.\nspaces_region (string) - The name of the region, such as nyc3, in which to upload the image to Spaces.\nspace_name (string) - The name of the specific Space where the image file will be copied to for import. This Space must exist when the post-processor is run.\nimage_name (string) - The name to be used for the resulting DigitalOcean custom image.\nimage_regions (array of string) - A list of DigitalOcean regions, such as nyc3, where the resulting image will be available for use in creating Droplets.\nimage_description (string) - The description to set for the resulting imported image.\nimage_distribution (string) - The name of the distribution to set for the resulting imported image.\nimage_tags (array of strings) - A list of tags to apply to the resulting imported image.\nkeep_input_artifact (boolean) - if true, do not delete the source virtual machine image after importing it to the cloud. Defaults to false.\nskip_clean (boolean) - Whether we should skip removing the image file uploaded to Spaces after the import process has completed. \"true\" means that we should leave it in the Space, \"false\" means to clean it out. Defaults to false.\nspace_object_name (string) - The name of the key used in the Space where the image file will be copied to for import. This is treated as a template engine. Therefore, you may use user variables and template functions in this field. If not specified, this will default to packer-import-{{timestamp}}.\ntimeout (number) - The length of time in minutes to wait for individual steps in the process to successfully complete. This includes both importing the image from Spaces as well as distributing the resulting image to additional regions. If not specified, this will default to 20.\nHere is a basic example:\n{ \"type\": \"digitalocean-import\", \"api_token\": \"{{user `token`}}\", \"spaces_key\": \"{{user `key`}}\", \"spaces_secret\": \"{{user `secret`}}\", \"spaces_region\": \"nyc3\", \"space_name\": \"import-bucket\", \"image_name\": \"ubuntu-18.10-minimal-amd64\", \"image_description\": \"Packer import {{timestamp}}\", \"image_regions\": [\"nyc3\", \"nyc2\"], \"image_tags\": [\"custom\", \"packer\"] }"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/post-processors/docker-push",
  "text": "Docker Push - Post-Processors | Packer\nDocker Push Post-Processor\nType: docker-push\nThe Packer Docker push post-processor takes an artifact from the docker-import post-processor and pushes it to a Docker registry.\nThis post-processor has only optional configuration:\naws_access_key (string) - The AWS access key used to communicate with AWS. Learn how to set this.\naws_secret_key (string) - The AWS secret key used to communicate with AWS. Learn how to set this.\naws_token (string) - The AWS access token to use. This is different from the access key and secret key. If you're not sure what this is, then you probably don't need it. This will also be read from the AWS_SESSION_TOKEN environmental variable.\naws_profile (string) - The AWS shared credentials profile used to communicate with AWS. Learn how to set this.\necr_login (boolean) - Defaults to false. If true, the post-processor will login in order to push the image to Amazon EC2 Container Registry (ECR). The post-processor only logs in for the duration of the push. If true login_server is required and login, login_username, and login_password will be ignored.\nkeep_input_artifact (boolean) - if true, do not delete the docker image after pushing it to the cloud. Defaults to true, but can be set to false if you do not need to save your local copy of the docker container.\nlogin (boolean) - Defaults to false. If true, the post-processor will login prior to pushing. For log into ECR see ecr_login.\nlogin_username (string) - The username to use to authenticate to login.\nlogin_password (string) - The password to use to authenticate to login.\nlogin_server (string) - The server address to login to.\nNote: When using Docker Hub or Quay registry servers, login must to be set to true and login_username, and login_password must to be set to your registry credentials. When using Docker Hub, login_server can be omitted.\nNote: If you login using the credentials above, the post-processor will automatically log you out afterwards (just the server specified).\nFor an example of using docker-push, see the section on using generated artifacts from the docker builder."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/post-processors/docker-save",
  "text": "Docker Save - Post-Processors | Packer\nDocker Save Post-Processor\nType: docker-save\nThe Packer Docker Save post-processor takes an artifact from the docker builder that was committed and saves it to a file. This is similar to exporting the Docker image directly from the builder, except that it preserves the hierarchy of images and metadata.\nWe understand the terminology can be a bit confusing, but we've adopted the terminology from Docker, so if you're familiar with that, then you'll be familiar with this and vice versa.\nThe configuration for this post-processor only requires one option.\npath (string) - The path to save the image.\nkeep_input_artifact (boolean) - if true, do not delete the docker container, and only save the .tar created by docker save. Defaults to true.\n{ \"type\": \"docker-save\", \"path\": \"foo.tar\" }"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/post-processors/docker-tag",
  "text": "Docker Tag - Post-Processors | Packer\nDocker Tag Post-Processor\nType: docker-tag\nThe Packer Docker Tag post-processor takes an artifact from the docker builder that was committed and tags it into a repository. This allows you to use the other Docker post-processors such as docker-push to push the image to a registry.\nThis is very similar to the docker-import post-processor except that this works with committed resources, rather than exported.\nThe configuration for this post-processor requires repository, all other settings are optional.\nrepository (string) - The repository of the image.\ntags (array of strings) - A list of tags for the image. By default this is not set. Valid examples include: \"tags\": \"mytag\" or \"tags\": [\"mytag-1\", \"mytag-2\"]\nforce (boolean) - If true, this post-processor forcibly tag the image even if tag name is collided. Default to false. But it will be ignored if Docker >= 1.12.0 was detected, since the force option was removed after 1.12.0. reference\nkeep_input_artifact (boolean) - Unlike most other post-processors, the keep_input_artifact option will have no effect for the docker-tag post-processor. We will always retain the input artifact for docker-tag, since deleting the image we just tagged is not a behavior anyone should ever expect. keep_input_artifact will therefore always be evaluated as true, regardless of the value you enter into this field.\n{ \"type\": \"docker-tag\", \"repository\": \"hashicorp/packer\", \"tag\": \"0.7,anothertag\" } \nThis example would take the image created by the Docker builder and tag it into the local Docker process with a name of hashicorp/packer:0.7.\nFollowing this, you can use the docker-push post-processor to push it to a registry, if you want."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/post-processors/exoscale-import",
  "text": "Exoscale Import - Post-Processors | Packer\nExoscale Import Post-Processor\nType: exoscale-import\nThe Packer Exoscale Import post-processor takes an image artifact from the QEMU, Artifice, or File builders and imports it to Exoscale.\nThe import process operates uploading a temporary copy of the image to Exoscale's Object Storage (SOS) and then importing it as a Custom Template via the Exoscale API. The temporary copy in SOS can be discarded after the import is complete.\nFor more information about Exoscale Custom Templates, see the documentation.\nThere are some configuration options available for the post-processor.\napi_key (string) - The API key used to communicate with Exoscale services. This may also be set using the EXOSCALE_API_KEY environmental variable.\napi_secret (string) - The API secret used to communicate with Exoscale services. This may also be set using the EXOSCALE_API_SECRET environmental variable.\nimage_bucket (string) - The name of the bucket in which to upload the template image to SOS. The bucket must exist when the post-processor is run.\ntemplate_name (string) - The name to be used for registering the template.\ntemplate_description (string) - The description for the registered template.\napi_endpoint (string) - The API endpoint used to communicate with the Exoscale API. Defaults to https://api.exoscale.com/compute.\nsos_endpoint (string) - The endpoint used to communicate with SOS. Defaults to https://sos-ch-gva-2.exo.io.\ntemplate_zone (string) - The Exoscale zone in which to register the template. Defaults to ch-gva-2.\ntemplate_username (string) - An optional username to be used to log into Compute instances using this template.\ntemplate_disable_password (boolean) - Whether the registered template should disable Compute instance password reset. Defaults to false.\ntemplate_disable_sshkey (boolean) - Whether the registered template should disable SSH key installation during Compute instance creation. Defaults to false.\nskip_clean (boolean) - Whether we should skip removing the image file uploaded to SOS after the import process has completed. \"true\" means that we should leave it in the bucket, \"false\" means deleting it. Defaults to false.\nHere is a basic example:\n{ \"type\": \"exoscale-import\", \"api_key\": \"{{user `exoscale_api_key`}}\", \"api_secret\": \"{{user `exoscale_api_secret`}}\", \"image_bucket\": \"my-templates\", \"template_name\": \"myapp\", \"template_description\": \"myapp v1.2.3\", \"template_username\": \"admin\" }"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/post-processors/googlecompute-export",
  "text": "Google Compute Image Exporter - Post-Processors | Packer\nType: googlecompute-export\nThe Google Compute Image Exporter post-processor exports the resultant image from a googlecompute build as a gzipped tarball to Google Cloud Storage (GCS).\nThe exporter uses the same Google Cloud Platform (GCP) project and authentication credentials as the googlecompute build that produced the image. A temporary VM is started in the GCP project using these credentials. The VM mounts the built image as a disk then dumps, compresses, and tars the image. The VM then uploads the tarball to the provided GCS paths using the same credentials.\nAs such, the authentication credentials that built the image must have write permissions to the GCS paths.\nNote: By default the GCE image being exported will be deleted once the image has been exported. To prevent Packer from deleting the image set the keep_input_artifact configuration option to true. See Post-Processor Input Artifacts for more details.\npaths ([]string) - A list of GCS paths where the image will be exported. For example 'gs://mybucket/path/to/file.tar.gz'\naccount_file (string) - The JSON file containing your account credentials. If specified, the account file will take precedence over any googlecompute builder authentication method.\nimpersonate_service_account (string) - This allows service account impersonation as per the docs.\ndisk_size (int64) - The size of the export instances disk. The disk is unused for the export but a larger size will increase pd-ssd read speed. This defaults to 200, which is 200GB.\ndisk_type (string) - Type of disk used to back the export instance, like pd-ssd or pd-standard. Defaults to pd-ssd.\nmachine_type (string) - The export instance machine type. Defaults to \"n1-highcpu-4\".\nnetwork (string) - The Google Compute network id or URL to use for the export instance. Defaults to \"default\". If the value is not a URL, it will be interpolated to projects/((builder_project_id))/global/networks/((network)). This value is not required if a subnet is specified.\nsubnetwork (string) - The Google Compute subnetwork id or URL to use for the export instance. Only required if the network has been created with custom subnetting. Note, the region of the subnetwork must match the zone in which the VM is launched. If the value is not a URL, it will be interpolated to projects/((builder_project_id))/regions/((region))/subnetworks/((subnetwork))\nzone (string) - The zone in which to launch the export instance. Defaults to googlecompute builder zone. Example: \"us-central1-a\"\nvault_gcp_oauth_engine (string) - Vault GCP Oauth Engine\nservice_account_email (string) - Service Account Email\nThe following example builds a GCE image in the project, my-project, with an account whose keyfile is account.json. After the image build, a temporary VM will be created to export the image as a gzipped tarball to gs://mybucket1/path/to/file1.tar.gz and gs://mybucket2/path/to/file2.tar.gz. keep_input_artifact is true, so the GCE image won't be deleted after the export.\nIn order for this example to work, the account associated with account.json must have write access to both gs://mybucket1/path/to/file1.tar.gz and gs://mybucket2/path/to/file2.tar.gz.\n{ \"builders\": [ { \"type\": \"googlecompute\", \"account_file\": \"account.json\", \"project_id\": \"my-project\", \"source_image\": \"debian-7-wheezy-v20150127\", \"zone\": \"us-central1-a\" } ], \"post-processors\": [ { \"type\": \"googlecompute-export\", \"paths\": [ \"gs://mybucket1/path/to/file1.tar.gz\", \"gs://mybucket2/path/to/file2.tar.gz\" ], \"keep_input_artifact\": true } ] }"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/post-processors/googlecompute-import",
  "text": "Google Compute Image Import - Post-Processors | Packer\n{ \"variables\": { \"account_file\": \"account.json\", \"bucket\": \"my-bucket\", \"project\": \"my-project\", \"serial\": \"\" }, \"builders\": [ { \"type\": \"qemu\", \"accelerator\": \"kvm\", \"boot_command\": [ \"<tab> console=ttyS0,115200n8 inst.text inst.ks=http://{{ .HTTPIP }}:{{ .HTTPPort }}/fedora-31-ks.cfg rd.live.check=0<enter><wait>\" ], \"disk_size\": \"15000\", \"format\": \"raw\", \"iso_checksum\": \"sha256:225ebc160e40bb43c5de28bad9680e3a78a9db40c9e3f4f42f3ee3f10f95dbeb\", \"iso_url\": \"https://download-ib01.fedoraproject.org/pub/fedora/linux/releases/31/Server/x86_64/iso/Fedora-Server-dvd-x86_64-31-1.9.iso\", \"headless\": \"true\", \"http_directory\": \"http\", \"http_port_max\": \"10089\", \"http_port_min\": \"10082\", \"output_directory\": \"output\", \"shutdown_timeout\": \"30m\", \"shutdown_command\": \"echo 'vagrant'|sudo -S shutdown -P now\", \"ssh_username\": \"vagrant\", \"ssh_password\": \"vagrant\", \"vm_name\": \"disk.raw\", \"qemu_binary\": \"/usr/bin/kvm\", \"qemuargs\": [ [\"-m\", \"1024\"], [\"-cpu\", \"host\"], [\"-chardev\", \"tty,id=pts,path={{user `serial`}}\"], [\"-device\", \"isa-serial,chardev=pts\"], [\"-device\", \"virtio-net,netdev=user.0\"] ] } ], \"post-processors\": [ [ { \"type\": \"compress\", \"output\": \"output/disk.raw.tar.gz\" }, { \"type\": \"googlecompute-import\", \"project_id\": \"{{user `project`}}\", \"account_file\": \"{{user `account_file`}}\", \"bucket\": \"{{user `bucket`}}\", \"image_name\": \"fedora31-server-packertest\", \"image_description\": \"Fedora 31 Server\", \"image_family\": \"fedora31-server\" } ] ] }"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/post-processors/ucloud-import",
  "text": "UCloud Import Post-Processors | Packer\nUCloud Import Post-Processor\nType: ucloud-import\nThe Packer UCloud Import post-processor takes the RAW, VHD, VMDK, or qcow2 artifact from various builders and imports it to UCloud customized image list for UHost Instance.\nNote Some regions don't support image import. You may refer to ucloud console for more detail. If you want to import to unsupported regions, please import the image in cn-bj2 first, and then copy the image to the target region.\nThe import process operates by making a temporary copy of the RAW, VHD, VMDK, or qcow2 to an UFile bucket, and calling an import task in UHost on the RAW, VHD, VMDK, or qcow2 file. Once completed, an UCloud UHost Image is returned. The temporary RAW, VHD, VMDK, or qcow2 copy in UFile can be discarded after the import is complete.\nThere are some configuration options available for the post-processor. There are two categories: required and optional parameters.\npublic_key (string) - This is the UCloud public key. It must be provided unless profile is set, but it can also be sourced from the UCLOUD_PUBLIC_KEY environment variable.\nprivate_key (string) - This is the UCloud private key. It must be provided unless profile is set, but it can also be sourced from the UCLOUD_PRIVATE_KEY environment variable.\nregion (string) - This is the UCloud region. It must be provided, but it can also be sourced from the UCLOUD_REGION environment variables.\nproject_id (string) - This is the UCloud project id. It must be provided, but it can also be sourced from the UCLOUD_PROJECT_ID environment variables.\nufile_bucket_name (string) - The name of the UFile bucket where the RAW, VHD, VMDK, or qcow2 file will be copied to for import. This bucket must exist when the post-processor is run.\nimage_name (string) - The name of the user-defined image, which contains 1-63 characters and only supports Chinese, English, numbers, '-_,.:[]'.\nimage_os_type (string) - Type of the OS. Possible values are: CentOS, Ubuntu, Windows, RedHat, Debian, Other. You may refer to ucloud_api_docs for detail.\nimage_os_name (string) - The name of OS. Such as: CentOS 7.2 64, set Other When image_os_type is Other. You may refer to ucloud_api_docs for detail.\nformat (string) - The format of the import image , Possible values are: raw, vhd, vmdk, or qcow2.\nbase_url (string) - This is the base url. (Default: https://api.ucloud.cn).\nprofile (string) - This is the UCloud profile name as set in the shared credentials file, it can also be sourced from the UCLOUD_PROFILE environment variables.\nshared_credentials_file (string) - This is the path to the shared credentials file, it can also be sourced from the UCLOUD_SHARED_CREDENTIAL_FILE environment variables. If this is not set and a profile is specified, ~/.ucloud/credential.json will be used.\nufile_key_name (string) - The name of the object key in ufile_bucket_name where the RAW, VHD, VMDK, or qcow2 file will be copied to import. This is a template engine. Therefore, you may use user variables and template functions in this field.\nskip_clean (bool) - Whether we should skip removing the RAW, VHD, VMDK, or qcow2 file uploaded to UFile after the import process has completed. Possible values are: true to leave it in the UFile bucket, false to remove it. (Default: false).\nimage_description (string) - The description of the image.\nwait_image_ready_timeout (int) - Timeout of importing image. The default timeout is 3600 seconds if this option is not set or is set.\nHere is a basic example. This assumes that the builder has produced a RAW artifact for us to work with. This will take the RAW image generated by a builder and upload it to UFile. Once uploaded, the import process will start, creating an UCloud UHost image to the region cn-bj2.\n\"post-processors\":[ { \"type\":\"ucloud-import\", \"public_key\": \"{{user `ucloud_public_key`}}\", \"private_key\": \"{{user `ucloud_private_key`}}\", \"project_id\": \"{{user `ucloud_project_id`}}\", \"region\":\"cn-bj2\", \"ufile_bucket_name\": \"packer-import\", \"image_name\": \"packer_import\", \"image_os_type\": \"CentOS\", \"image_os_name\": \"CentOS 6.10 64\", \"format\": \"raw\" } ]"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/post-processors/vagrant",
  "text": "Vagrant - Post-Processors | Packer\nType: vagrant\nThe Packer Vagrant post-processor takes a build and converts the artifact into a valid Vagrant box, if it can. This lets you use Packer to automatically create arbitrarily complex Vagrant boxes, and is in fact how the official boxes distributed by Vagrant are created.\nIf you've never used a post-processor before, please read the documentation on using post-processors in templates. This knowledge will be expected for the remainder of this document.\nBecause Vagrant boxes are provider-specific, the Vagrant post-processor is hardcoded to understand how to convert the artifacts of certain builders into proper boxes for their respective providers.\nCurrently, the Vagrant post-processor can create boxes for the following providers.\nAWS\nAzure\nDigitalOcean\nHyper-V\nLXC\nParallels\nQEMU\nVirtualBox\nVMware\nSupport for additional providers is planned. If the Vagrant post-processor doesn't support creating boxes for a provider you care about, please help by contributing to Packer and adding support for it.\nPlease note that if you are using the Vagrant builder, then the Vagrant post-processor is unnecesary because the output of the Vagrant builder is already a Vagrant box; using this post-processor with the Vagrant builder will cause your build to fail.\nThe simplest way to use the post-processor is to just enable it. No configuration is required by default. This will mostly do what you expect and will build functioning boxes for many of the built-in builders of Packer.\nHowever, if you want to configure things a bit more, the post-processor does expose some configuration options. The available options are listed below, with more details about certain options in following sections.\ncompression_level (number) - An integer representing the compression level to use when creating the Vagrant box. Valid values range from 0 to 9, with 0 being no compression and 9 being the best compression. By default, compression is enabled at level 6.\ninclude (array of strings) - Paths to files to include in the Vagrant box. These files will each be copied into the top level directory of the Vagrant box (regardless of their paths). They can then be used from the Vagrantfile.\nkeep_input_artifact (boolean) - When true, preserve the artifact we use to create the vagrant box. Defaults to false, except when you set a cloud provider (e.g. aws, azure, google, digitalocean). In these cases deleting the input artifact would render the vagrant box useless, so we always keep these artifacts -- even if you specifically set \"keep_input_artifact\":false\noutput (string) - The full path to the box file that will be created by this post-processor. This is a template engine. Therefore, you may use user variables and template functions in this field. The following extra variables are also avilable in this engine:\nProvider: The Vagrant provider the box is for\nArtifactId: The ID of the input artifact.\nBuildName: The name of the build.\nBy default, the value of this config is packer_{{.BuildName}}_{{.Provider}}.box.\nprovider_override (string) - this option will override the internal logic that decides which Vagrant provider to set for a particular Packer builder's or post-processor's artifact. It is required when the artifact comes from the Artifice post-processor, but is otherwise optional. Valid options are: digitalocean, virtualbox, azure, vmware, libvirt, docker, lxc, scaleway, hyperv, parallels, aws, or google.\nvagrantfile_template (string) - Path to a template to use for the Vagrantfile that is packaged with the box. This option supports the usage of the template engine for JSON and the contextual variables for HCL2.\nvagrantfile_template_generated (boolean) - By default, Packer will exit with an error if the file specified using the vagrantfile_template variable is not found. However, under certain circumstances, it may be desirable to dynamically generate the Vagrantfile during the course of the build. Setting this variable to true skips the start up check and allows the user to script the creation of the Vagrantfile at some previous point in the build. Defaults to false.\nUsing together with the Artifice post-processor\nSometimes you may want to run several builds in a pipeline rather than running this post-processor inside a long-running Packer build. Here is an example of how to do this:\n{ \"builders\": [ { \"type\": \"null\", \"communicator\": \"none\" } ], \"post-processors\": [ [ { \"type\": \"artifice\", \"files\": [ \"output-virtualbox-iso/vbox-example-disk001.vmdk\", \"output-virtualbox-iso/vbox-example.ovf\" ] }, { \"type\": \"vagrant\", \"keep_input_artifact\": true, \"provider_override\": \"virtualbox\" } ] ] } \nIf you have a Packer template with multiple builder types within it, you may want to configure the box creation for each type a little differently. For example, the contents of the Vagrantfile for a Vagrant box for AWS might be different from the contents of the Vagrantfile you want for VMware. The post-processor lets you do this.\nSpecify overrides within the override configuration by provider name:\n{ \"type\": \"vagrant\", \"compression_level\": 1, \"override\": { \"vmware\": { \"compression_level\": 0 } } } \nIn the example above, the compression level will be set to 1 except for VMware, where it will be set to 0.\nThe available provider names are:\naws\nazure\ndigitalocean\ngoogle\nhyperv\nparallels\nlibvirt\nlxc\nscaleway\nvirtualbox\nvmware\ndocker\nBy default, Packer will delete the original input artifact, assuming you only want the final Vagrant box as the result. If you wish to keep the input artifact (the raw virtual machine, for example), then you must configure Packer to keep it.\nPlease see the documentation on input artifacts for more information.\nUsing a Docker input artifact will include a reference to the image in the Vagrantfile. If the image tag is not specified in the post-processor, the sha256 hash will be used.\nThe following Docker input artifacts are supported:\ndocker builder with commit: true, always uses the sha256 hash\ndocker-import\ndocker-tag\ndocker-push\nQEMU/libvirt\nThe libvirt provider supports QEMU artifacts built using any these accelerators: none, kvm, tcg, or hvf.\nVMWare\nIf you are using the Vagrant post-processor with the vmware-esxi builder, you must export the builder artifact locally; the Vagrant post-processor will not work on remote artifacts.\nArtifice\nIf you are using this post-processor after defining an artifact using the Artifice post-processor, then you must set the \"provider_override\" template option so that the Vagrant post-processor knows what provider to use to create the Vagrant box."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/post-processors/vagrant-cloud",
  "text": "Vagrant Cloud - Post-Processors | Packer\nVagrant Cloud Post-Processor\nType: vagrant-cloud\nVagrant Cloud hosts and serves boxes to Vagrant, allowing you to version and distribute boxes to an organization in a simple way.\nThe Vagrant Cloud post-processor enables the upload of Vagrant boxes to Vagrant Cloud. Currently, the Vagrant Cloud post-processor will accept and upload boxes supplied to it from the Vagrant or Artifice post-processors and the Vagrant builder.\nYou'll need to be familiar with Vagrant Cloud, have an upgraded account to enable box hosting, and be distributing your box via the shorthand name configuration.\nIt's important to understand the workflow that using this post-processor enforces in order to take full advantage of Vagrant and Vagrant Cloud.\nThe use of this processor assume that you currently distribute, or plan to distribute, boxes via Vagrant Cloud. It also assumes you create Vagrant Boxes and deliver them to your team in some fashion.\nHere is an example workflow:\nYou use Packer to build a Vagrant Box for the virtualbox provider\nThe vagrant-cloud post-processor is configured to point to the box hashicorp/foobar on Vagrant Cloud via the box_tag configuration\nThe post-processor receives the box from the vagrant post-processor\nIt then creates the configured version, or verifies the existence of it, on Vagrant Cloud\nA provider matching the name of the Vagrant provider is then created\nThe box is uploaded to Vagrant Cloud\nThe upload is verified\nThe version is released and available to users of the box\nThe Vagrant Cloud box (hashicorp/foobar in this example) must already exist. Packer will not create the box automatically. If running Packer in automation, consider using the Vagrant Cloud API to create the Vagrant Cloud box if it doesn't already exist.\nThe configuration allows you to specify the target box that you have access to on Vagrant Cloud, as well as authentication and version information.\nbox_tag (string) - The shorthand tag for your box that maps to Vagrant Cloud, for example hashicorp/precise64, which is short for vagrantcloud.com/hashicorp/precise64. This box must already exist in Vagrant Cloud. Packer will not create the box automatically.\nversion (string) - The version number, typically incrementing a previous version. The version string is validated based on Semantic Versioning. The string must match a pattern that could be semver, and doesn't validate that the version comes after your previous versions.\naccess_token (string) - Your access token for the Vagrant Cloud API. This can be generated on your tokens page. If not specified, the environment will be searched. First, VAGRANT_CLOUD_TOKEN is checked, and if nothing is found, finally ATLAS_TOKEN will be used. This is required unless you are using a private hosting solution (i.e. vagrant_cloud_url has been populated).\nor\nvagrant_cloud_url (string) - Override the base URL for Vagrant Cloud. This is useful if you're using Vagrant Private Cloud in your own network. Defaults to https://vagrantcloud.com/api/v1. If this value is set to something other than the default then access_token can be left blank and no Authorization header will be added to requests sent by this post-processor.\nno_release (string) - If set to true, does not release the version on Vagrant Cloud, making it active. You can manually release the version via the API or Web UI. Defaults to false.\ninsecure_skip_tls_verify (boolean) - If set to true and vagrant_cloud_url is set to something different than its default, it will set TLS InsecureSkipVerify to true. In other words, this will disable security checks of SSL. You may need to set this option to true if your host at vagrant_cloud_url is using a self-signed certificate.\nkeep_input_artifact (boolean) - When true, preserve the local box after uploading to Vagrant cloud. Defaults to true.\nversion_description (string) - Optionally markdown text used as a full-length and in-depth description of the version, typically for denoting changes introduced\nbox_download_url (string) - Optional URL for a self-hosted box. If this is set the box will not be uploaded to the Vagrant Cloud. This is a template engine. Therefore, you may use user variables and template functions in this field. The following extra variables are also avilable in this engine:\nProvider: The Vagrant provider the box is for\nArtifactId: The ID of the input artifact.\nno_direct_upload (boolean) - When true, upload the box artifact through Vagrant Cloud instead of directly to the backend storage.\nUse with the Vagrant Post-Processor\nAn example configuration is shown below. Note the use of the nested array that wraps both the Vagrant and Vagrant Cloud post-processors within the post-processor section. Chaining the post-processors together in this way tells Packer that the artifact produced by the Vagrant post-processor should be passed directly to the Vagrant Cloud Post-Processor. It also sets the order in which the post-processors should run.\nFailure to chain the post-processors together in this way will result in the wrong artifact being supplied to the Vagrant Cloud post-processor. This will likely cause the Vagrant Cloud post-processor to error and fail.\n{ \"variables\": { \"cloud_token\": \"{{ env `VAGRANT_CLOUD_TOKEN` }}\", \"version\": \"1.0.{{timestamp}}\" }, \"post-processors\": [ { \"type\": \"shell-local\", \"inline\": [\"echo Doing stuff...\"] }, [ { \"type\": \"vagrant\", \"include\": [\"image.iso\"], \"vagrantfile_template\": \"vagrantfile.tpl\", \"output\": \"proxycore_{{.Provider}}.box\" }, { \"type\": \"vagrant-cloud\", \"box_tag\": \"hashicorp/precise64\", \"access_token\": \"{{user `cloud_token`}}\", \"version\": \"{{user `version`}}\" } ] ] } \nUse with the Artifice Post-Processor\nAn example configuration is shown below. Note the use of the nested array that wraps both the Artifice and Vagrant Cloud post-processors within the post-processor section. Chaining the post-processors together in this way tells Packer that the artifact produced by the Artifice post-processor should be passed directly to the Vagrant Cloud Post-Processor. It also sets the order in which the post-processors should run.\nFailure to chain the post-processors together in this way will result in the wrong artifact being supplied to the Vagrant Cloud post-processor. This will likely cause the Vagrant Cloud post-processor to error and fail.\nNote that the Vagrant box specified in the Artifice post-processor files array must end in the .box extension. It must also be the first file in the array. Additional files bundled by the Artifice post-processor will be ignored.\n{ \"variables\": { \"cloud_token\": \"{{ env `VAGRANT_CLOUD_TOKEN` }}\" }, \"builders\": [ { \"type\": \"null\", \"communicator\": \"none\" } ], \"post-processors\": [ { \"type\": \"shell-local\", \"inline\": [\"echo Doing stuff...\"] }, [ { \"type\": \"artifice\", \"files\": [\"./path/to/my.box\"] }, { \"type\": \"vagrant-cloud\", \"box_tag\": \"myorganisation/mybox\", \"access_token\": \"{{user `cloud_token`}}\", \"version\": \"0.1.0\" } ] ] }"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/post-processors/vsphere",
  "text": "vSphere - Post-Processors | Packer\nType: vsphere\nThe Packer vSphere post-processor takes an artifact and uploads it to a vSphere endpoint. The artifact must have a vmx/ova/ovf image.\nThere are many configuration options available for the post-processor. They are segmented below into two categories: required and optional parameters. Within each category, the available configuration keys are alphabetized.\ncluster (string) - The cluster or host to upload the VM to. This can be either the name of the cluster, or the IP address of the esx host that you want to upload to.\ndatacenter (string) - The name of the datacenter within vSphere to add the VM to.\ndatastore (string) - The name of the datastore to store this VM. This is not required if resource_pool is specified.\nhost (string) - The vSphere host that will be contacted to perform the VM upload.\npassword (string) - Password to use to authenticate to the vSphere endpoint.\nvm_name (string) - The name of the VM once it is uploaded.\nesxi_host (string) - Target vSphere host. Used to assign specific esx host to upload the resulting VM to, when a vCenter Server is used as host. Can be either a hostname (e.g. \"packer-esxi1\", requires proper DNS setup and/or correct DNS search domain setting) or an ipv4 address.\ndisk_mode (string) - Target disk format. See ovftool manual for available options. By default, \"thick\" will be used.\ninsecure (boolean) - Whether or not the connection to vSphere can be done over an insecure connection. By default this is false.\nkeep_input_artifact (boolean) - When true, preserve the local VM files, even after importing them to vsphere. Defaults to false.\nresource_pool (string) - The resource pool to upload the VM to.\nvm_folder (string) - The folder within the datastore to store the VM.\nvm_network (string) - The name of the VM network this VM will be added to.\noverwrite (boolean) - If it's true force the system to overwrite the existing files instead create new ones. Default is false\noptions (array of strings) - Custom options to add in ovftool. See ovftool --help to list all the options\nThe following is an example of the vSphere post-processor being used in conjunction with the null builder and artifice post-processor to upload a vmx to a vSphere cluster.\nYou can also use this post-processor with the vmx artifact from a vmware build.\n{ \"builders\": [ { \"type\": \"null\", \"communicator\": \"none\" } ], \"post-processors\": [ [ { \"type\": \"artifice\", \"files\": [\"output-vmware-iso/packer-vmware-iso.vmx\"] }, { \"type\": \"vsphere\", \"keep_input_artifact\": true, \"vm_name\": \"packerparty\", \"vm_network\": \"VM Network\", \"cluster\": \"123.45.678.1\", \"datacenter\": \"PackerDatacenter\", \"datastore\": \"datastore1\", \"host\": \"123.45.678.9\", \"password\": \"SuperSecretPassword\", \"username\": \"Administrator@vsphere.local\" } ] ] } \nThe vsphere post processor uses ovftool and therefore needs the same privileges as ovftool. Rather than giving full administrator access, you can create a role to give the post-processor the permissions necessary to run. Below is an example role. Please note that this is a user-supplied list so there may be a few extraneous permissions that are not strictly required.\nFor Vsphere 5.5 the role needs the following privileges:\nDatastore.AllocateSpace Host.Config.AdvancedConfig Host.Config.NetService Host.Config.Network Network.Assign System.Anonymous System.Read System.View VApp.Import VirtualMachine.Config.AddNewDisk VirtualMachine.Config.AdvancedConfig VirtualMachine.Inventory.Delete \nAnd this role must be authorized on the:\nCluster of the host The destination folder (not on Datastore, on the Vsphere logical view) The network to be assigned The destination datastore."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/post-processors/vsphere-template",
  "text": "vSphere Template - Post-Processors | Packer\nvSphere Template Post-Processor\nType: vsphere-template\nThe Packer vSphere Template post-processor takes an artifact from the VMware-iso builder, built on ESXi (i.e. remote) or an artifact from the vSphere post-processor, marks the VM as a template, and leaves it in the path of your choice.\n{ \"type\": \"vsphere-template\", \"host\": \"vcenter.local\", \"insecure\": true, \"username\": \"root\", \"password\": \"secret\", \"datacenter\": \"mydatacenter\", \"folder\": \"/packer-templates/os/distro-7\" } \nThere are many configuration options available for the post-processor. They are segmented below into two categories: required and optional parameters. Within each category, the available configuration keys are alphabetized.\nhost (string) - The vSphere host that contains the VM built by the vmware-iso.\npassword (string) - Password to use to authenticate to the vSphere endpoint.\ndatacenter (string) - If you have more than one, you will need to specify which one the ESXi used.\nfolder (string) - Target path where the template will be created.\ninsecure (boolean) - If it's true skip verification of server certificate. Default is false\nkeep_input_artifact (boolean) - Unlike most post-processors, this option has no effect for vsphere-template. This is because in order for a template to work, you can't delete the vm that you generate the template from. The vsphere template post-processor will therefore always preserve the original vm.\nsnapshot_enable (boolean) - Create a snapshot before marking as a template. Default is false\nsnapshot_name (string) - Name for the snapshot. Required when snapshot_enable is true\nsnapshot_description (string) - Description for the snapshot. Required when snapshot_enable is true\nreregister_vm (boolean) - Use the method of unregister VM and reregister as a template, rather than using the markAsTemplate method in vmWare. NOTE: If you are getting permission denied errors when trying to mark as a template, but it works fine in the vSphere UI, try setting this to false. Default is true.\nOnce the vSphere takes an artifact from the VMware builder and uploads it to a vSphere endpoint, you will likely want to mark that VM as template. Packer can do this for you automatically using a sequence definition (a collection of post-processors that are treated as as single pipeline, see Post-Processors for more information):\n{ \"post-processors\": [ [ { \"type\": \"vsphere\", ... }, { \"type\": \"vsphere-template\", ... } ], { \"type\": \"...\", ... } ] } \nIn the example above, the result of each builder is passed through the defined sequence of post-processors starting with the vsphere post-processor which will upload the artifact to a vSphere endpoint. The resulting artifact is then passed on to the vsphere-template post-processor which handles marking a VM as a template. Note that the vsphere and vsphere-template post-processors are paired together in their own JSON array.\nThe vsphere post processor needs several permissions to be able to mark the vm as a template. Rather than giving full administrator access, you can create a role to give the post-processor the permissions necessary to run. Here is an example role that will work. Please note that this is a user-supplied list so there may be a few extraneous permissions that are not strictly required.\nFor Vsphere 5.5 the role needs the following privileges:\nDatastore.AllocateSpace Host.Config.AdvancedConfig Host.Config.NetService Host.Config.Network Network.Assign System.Anonymous System.Read System.View VApp.Import VirtualMachine.Config.AddNewDisk VirtualMachine.Config.AdvancedConfig VirtualMachine.Inventory.Delete \nand either (If reregister_vm is false):\nVirtualMachine.Provisioning.MarkAsTemplate \nor (if reregister_vm is true or unset):\nVirtualMachine.Inventory.Register VirtualMachine.Inventory.Unregister \nAnd this role must be authorized on the:\nCluster of the host The destination folder (not on Datastore, on the Vsphere logical view) The network to be assigned The destination datastore. \nSome users have reported that vSphere templates created from local VMWare builds get their boot order reset to cdrom only instead of the original boot order defined by the template. If this issue affects you, the solution is to set \"bios.hddOrder\": \"scsi0:0\" in your builder's vmx_data.\nPacker doesn't automatically do this for you because it causes strange upload behavior in certain versions of ovftool."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/post-processors/yandex-export",
  "text": "Yandex.Cloud Compute Image Exporter - Post-Processors | Packer\nType: yandex-export\nThe Yandex.Cloud Compute Image Exporter post-processor exports the resultant image from a yandex build as a qcow2 file to Yandex Object Storage.\nThe exporter uses the same Yandex.Cloud folder and authentication credentials as the yandex build that produced the image. A temporary VM is started in the folder using these credentials. The VM mounts the built image as a secondary disk, then dumps the image in qcow2 format. The VM then uploads the file to the provided Yandex Object Storage paths using the same credentials.\nAs such, assigned Service Account must have write permissions to the Yandex Object Storage paths. A new temporary static access keys from assigned Service Account used to upload image.\nAccess\ntoken (string) - OAuth token or IAM token to use to authenticate to Yandex.Cloud. Alternatively you may set value by environment variable YC_TOKEN.\nExport\npaths ([]string) - List of paths to Yandex Object Storage where exported image will be uploaded. Please be aware that use of space char inside path not supported. Also this param support build template function. Check available template data for Yandex builder. Paths to Yandex Object Storage where exported image will be uploaded.\nCommon\nfolder_id (string) - The folder ID that will be used to launch instances and store images. Alternatively you may set value by environment variable YC_FOLDER_ID. To use a different folder for looking up the source image or saving the target image to check options 'source_image_folder_id' and 'target_image_folder_id'.\nservice_account_id (string) - Service Account ID with proper permission to modify an instance, create and attach disk and make upload to specific Yandex Object Storage paths.\nAccess\nendpoint (string) - Non standard API endpoint. Default is api.cloud.yandex.net:443.\nservice_account_key_file (string) - Path to file with Service Account key in json format. This is an alternative method to authenticate to Yandex.Cloud. Alternatively you may set environment variable YC_SERVICE_ACCOUNT_KEY_FILE.\nmax_retries (int) - The maximum number of times an API request is being executed.\nExport\nssh_private_key_file (string) - Path to a PEM encoded private key file to use to authenticate with SSH. The ~ can be used in path and will be expanded to the home directory of current user. Login for attach: ubuntu\ntries (int) - Number of attempts to wait for export (must be greater than 0). Default: 1000\nCommon\nserial_log_file (string) - File path to save serial port output of the launched instance.\nstate_timeout (duration string | ex: \"1h5m2s\") - The time to wait for instance state changes. Defaults to 5m.\nInstance\ninstance_cores (int) - The number of cores available to the instance.\ninstance_gpus (int) - The number of GPU available to the instance.\ninstance_mem_gb (int) - The amount of memory available to the instance, specified in gigabytes.\ninstance_name (string) - The name assigned to the instance.\nplatform_id (string) - Identifier of the hardware platform configuration for the instance. This defaults to standard-v2.\nlabels (map[string]string) - Key/value pair labels to apply to the launched instance.\nmetadata (map[string]string) - Metadata applied to the launched instance.\nmetadata_from_file (map[string]string) - Metadata applied to the launched instance. The values in this map are the paths to the content files for the corresponding metadata keys.\npreemptible (bool) - Launch a preemptible instance. This defaults to false.\nDisk\ndisk_name (string) - The name of the disk, if unset the instance name will be used.\ndisk_size_gb (int) - The size of the disk in GB. This defaults to 10/100GB.\ndisk_type (string) - Specify disk type for the launched instance. Defaults to network-ssd.\ndisk_labels (map[string]string) - Key/value pair labels to apply to the disk.\nNetwork\nsubnet_id (string) - The Yandex VPC subnet id to use for the launched instance. Note, the zone of the subnet must match the zone in which the VM is launched.\nzone (string) - The name of the zone to launch the instance. This defaults to ru-central1-a.\nuse_ipv4_nat (bool) - If set to true, then launched instance will have external internet access.\nuse_ipv6 (bool) - Set to true to enable IPv6 for the instance being created. This defaults to false, or not enabled.\nNote: Usage of IPv6 will be available in the future.\nuse_internal_ip (bool) - If true, use the instance's internal IP address instead of its external IP during building.\nThe following example builds a Compute image in the folder with id b1g8jvfcgmitdrslcn86, with an Service Account whose keyfile is account.json. After the image build, a temporary VM will be created to export the image as a qcow2 file to s3://packer-export/my-exported-image.qcow2 and s3://packer-export/image-number-two.qcow2. keep_input_artifact is true, so the source Compute image won't be deleted after the export.\nIn order for this example to work, the service account associated with builder must have write access to both s3://packer-export/my-exported-image.qcow2 and s3://packer-export/image-number-two.qcow2 and get permission to modify temporary instance (create new disk, attach to instance, etc).\n{ \"builders\": [ { \"type\": \"yandex\", \"folder_id\": \"b1g8jvfcgmitdrslcn86\", \"subnet_id\": \"e9bp6l8sa4q39yourxzq\", \"zone\": \"ru-central1-a\", \"source_image_family\": \"ubuntu-1604-lts\", \"ssh_username\": \"ubuntu\", \"use_ipv4_nat\": true } ], \"post-processors\": [ { \"type\": \"yandex-export\", \"folder_id\": \"b1g8jvfcgmitdrslcn86\", \"subnet_id\": \"e9bp6l8sa4q39yourxzq\", \"service_account_id\": \"ajeu0363240rrnn7xgen\", \"paths\": [ \"s3://packer-export-bucket/my-exported-image.qcow2\", \"s3://packer-export-bucket/template-supported-get-{{build `ImageID` }}-right-here.qcow2\" ], \"keep_input_artifact\": true } ] }"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/post-processors/yandex-import",
  "text": "Yandex.Cloud Compute Image Import - Post-Processors | Packer\nThe Yandex.Cloud Compute Image Import post-processor create new Compute Image from a qcow2 file. As Compute service support image creation from Storage service object just before request to create its upload file into Storage service.\nAssigned Service Account must have write permissions to the Yandex Object Storage. A new temporary static access keys from assigned Service Account used to upload file.\n{ \"variables\": { \"token\": \"{{env `YC_TOKEN`}}\" }, \"sensitive-variables\": [\"token\"], \"builders\": [ { \"type\": \"file\", \"source\": \"xenial-server-cloudimg-amd64-disk1.img\", \"target\": \"test_artifact.qcow2\" } ], \"post-processors\": [ { \"type\": \"yandex-import\", \"token\": \"{{user `token`}}\", \"folder_id\": \"b1g8jvfcgmitdrslcn86\", \"service_account_id\": \"ajeui8kdvg8qs44fbrbr\", \"bucket\": \"bucket1\", \"image_name\": \"my-first-imported-image-{{isotime \\\"02-Jan-06-03-04-05\\\" | lower }}\", \"keep_input_artifact\": false } ] }"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.10.x/templates/legacy_json_templates/post-processors",
  "text": "Post-Processors - Templates | Packer\nNote: This page is about older-style JSON Packer templates. JSON templates are still supported by the Packer core, but new features added to the Packer core may not be implemented for JSON templates. We recommend you transition to HCL templates as soon as is convenient for you, in order to have the best possible experience with Packer. To help you upgrade your templates, we have written an hcl2_upgrade command command.\nThe post-processor section within a template configures any post-processing that will be done to images built by the builders. Examples of post-processing would be compressing files, uploading artifacts, etc.\nPost-processors are optional. If no post-processors are defined within a template, then no post-processing will be done to the image. The resulting artifact of a build is just the image outputted by the builder.\nThis documentation page will cover how to configure a post-processor in a template. The specific configuration options available for each post-processor, however, must be referenced from the documentation for that specific post-processor.\nWithin a template, a section of post-processor definitions looks like this:\n{ \"post-processors\": [ // ... one or more post-processor definitions here ] } \nFor each post-processor definition, Packer will take the result of each of the defined builders and send it through the post-processors. This means that if you have one post-processor defined and two builders defined in a template, the post-processor will run twice (once for each builder), by default. There are ways, which will be covered later, to control what builders post-processors apply to, if you wish. It is also possible to prevent a post-processor from running.\nPost-Processor Definition\nWithin the post-processors array in a template, there are three ways to define a post-processor. There are simple definitions, detailed definitions, and sequence definitions. Another way to think about this is that the \"simple\" and \"detailed\" definitions are shortcuts for the \"sequence\" definition.\nA simple definition is just a string; the name of the post-processor. An example is shown below. Simple definitions are used when no additional configuration is needed for the post-processor.\n{ \"post-processors\": [\"compress\"] } \nA detailed definition is a JSON object. It is very similar to a builder or provisioner definition. It contains a type field to denote the type of the post-processor, but may also contain additional configuration for the post-processor. A detailed definition is used when additional configuration is needed beyond simply the type for the post-processor. An example is shown below.\n{ \"post-processors\": [ { \"type\": \"compress\", \"format\": \"tar.gz\" } ] } \nA sequence definition is a JSON array comprised of other simple or detailed definitions. The post-processors defined in the array are run in order, with the artifact of each feeding into the next, and any intermediary artifacts being discarded. A sequence definition may not contain another sequence definition. Sequence definitions are used to chain together multiple post-processors. An example is shown below, where the artifact of a build is compressed then uploaded, but the compressed result is not kept.\nIt is very important that any post processors that need to be run in order, be sequenced!\n{ \"post-processors\": [ [\"compress\", { \"type\": \"upload\", \"endpoint\": \"http://example.com\" }] ] } \nAs you may be able to imagine, the simple and detailed definitions are simply shortcuts for a sequence definition of only one element.\nWhen using post-processors, the input artifact (coming from a builder or another post-processor) is discarded by default after the post-processor runs. This is because generally, you don't want the intermediary artifacts on the way to the final artifact created.\nIn some cases, however, you may want to keep the intermediary artifacts. You can tell Packer to keep these artifacts by setting the keep_input_artifact configuration to true. An example is shown below:\n{ \"post-processors\": [ { \"type\": \"compress\", \"keep_input_artifact\": true } ] } \nThis setting will only keep the input artifact to that specific post-processor. If you're specifying a sequence of post-processors, then all intermediaries are discarded by default except for the input artifacts to post-processors that explicitly state to keep the input artifact.\nNote: The intuitive reader may be wondering what happens if multiple post-processors are specified (not in a sequence). Does Packer require the configuration to keep the input artifact on all the post-processors? The answer is no, of course not. Packer is smart enough to figure out that at least one post-processor requested that the input be kept, so it will keep it around.\nYou can use the only or except fields to run a post-processor only with specific builds. These two fields do what you expect: only will only run the post-processor on the specified builds and except will run the post-processor on anything other than the specified builds. A sequence of post-processors will execute until a skipped post-processor.\nAn example of only being used is shown below, but the usage of except is effectively the same. only and except can only be specified on \"detailed\" fields. If you have a sequence of post-processors to run, only and except will affect that post-processor and stop the sequence.\nThe -except option can specifically skip a named post processor. The -only option ignores post-processors.\n([ { \"name\": \"vbox\", \"type\": \"vagrant\", \"only\": [\"virtualbox-iso\"] }, { \"type\": \"compress\" } ], [ \"compress\", { \"type\": \"upload\", \"endpoint\": \"http://example.com\" } ]) \nThe values within only or except are build names, not builder types. Name is a required block label in HCL, but in legacy JSON, build names default to the types of their builders (e.g. docker or amazon-ebs or virtualbox-iso), unless a specific name attribute is specified within the configuration."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.9.x/templates/legacy_json_templates/post-processors",
  "text": "Post-Processors - Templates | Packer\nNote: This page is about older-style JSON Packer templates. JSON templates are still supported by the Packer core, but new features added to the Packer core may not be implemented for JSON templates. We recommend you transition to HCL templates as soon as is convenient for you, in order to have the best possible experience with Packer. To help you upgrade your templates, we have written an hcl2_upgrade command command.\nThe post-processor section within a template configures any post-processing that will be done to images built by the builders. Examples of post-processing would be compressing files, uploading artifacts, etc.\nPost-processors are optional. If no post-processors are defined within a template, then no post-processing will be done to the image. The resulting artifact of a build is just the image outputted by the builder.\nThis documentation page will cover how to configure a post-processor in a template. The specific configuration options available for each post-processor, however, must be referenced from the documentation for that specific post-processor.\nWithin a template, a section of post-processor definitions looks like this:\n{ \"post-processors\": [ // ... one or more post-processor definitions here ] } \nFor each post-processor definition, Packer will take the result of each of the defined builders and send it through the post-processors. This means that if you have one post-processor defined and two builders defined in a template, the post-processor will run twice (once for each builder), by default. There are ways, which will be covered later, to control what builders post-processors apply to, if you wish. It is also possible to prevent a post-processor from running.\nPost-Processor Definition\nWithin the post-processors array in a template, there are three ways to define a post-processor. There are simple definitions, detailed definitions, and sequence definitions. Another way to think about this is that the \"simple\" and \"detailed\" definitions are shortcuts for the \"sequence\" definition.\nA simple definition is just a string; the name of the post-processor. An example is shown below. Simple definitions are used when no additional configuration is needed for the post-processor.\n{ \"post-processors\": [\"compress\"] } \nA detailed definition is a JSON object. It is very similar to a builder or provisioner definition. It contains a type field to denote the type of the post-processor, but may also contain additional configuration for the post-processor. A detailed definition is used when additional configuration is needed beyond simply the type for the post-processor. An example is shown below.\n{ \"post-processors\": [ { \"type\": \"compress\", \"format\": \"tar.gz\" } ] } \nA sequence definition is a JSON array comprised of other simple or detailed definitions. The post-processors defined in the array are run in order, with the artifact of each feeding into the next, and any intermediary artifacts being discarded. A sequence definition may not contain another sequence definition. Sequence definitions are used to chain together multiple post-processors. An example is shown below, where the artifact of a build is compressed then uploaded, but the compressed result is not kept.\nIt is very important that any post processors that need to be run in order, be sequenced!\n{ \"post-processors\": [ [\"compress\", { \"type\": \"upload\", \"endpoint\": \"http://example.com\" }] ] } \nAs you may be able to imagine, the simple and detailed definitions are simply shortcuts for a sequence definition of only one element.\nWhen using post-processors, the input artifact (coming from a builder or another post-processor) is discarded by default after the post-processor runs. This is because generally, you don't want the intermediary artifacts on the way to the final artifact created.\nIn some cases, however, you may want to keep the intermediary artifacts. You can tell Packer to keep these artifacts by setting the keep_input_artifact configuration to true. An example is shown below:\n{ \"post-processors\": [ { \"type\": \"compress\", \"keep_input_artifact\": true } ] } \nThis setting will only keep the input artifact to that specific post-processor. If you're specifying a sequence of post-processors, then all intermediaries are discarded by default except for the input artifacts to post-processors that explicitly state to keep the input artifact.\nNote: The intuitive reader may be wondering what happens if multiple post-processors are specified (not in a sequence). Does Packer require the configuration to keep the input artifact on all the post-processors? The answer is no, of course not. Packer is smart enough to figure out that at least one post-processor requested that the input be kept, so it will keep it around.\nYou can use the only or except fields to run a post-processor only with specific builds. These two fields do what you expect: only will only run the post-processor on the specified builds and except will run the post-processor on anything other than the specified builds. A sequence of post-processors will execute until a skipped post-processor.\nAn example of only being used is shown below, but the usage of except is effectively the same. only and except can only be specified on \"detailed\" fields. If you have a sequence of post-processors to run, only and except will affect that post-processor and stop the sequence.\nThe -except option can specifically skip a named post processor. The -only option ignores post-processors.\n([ { \"name\": \"vbox\", \"type\": \"vagrant\", \"only\": [\"virtualbox-iso\"] }, { \"type\": \"compress\" } ], [ \"compress\", { \"type\": \"upload\", \"endpoint\": \"http://example.com\" } ]) \nThe values within only or except are build names, not builder types. Name is a required block label in HCL, but in legacy JSON, build names default to the types of their builders (e.g. docker or amazon-ebs or virtualbox-iso), unless a specific name attribute is specified within the configuration."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.8.x/templates/legacy_json_templates/post-processors",
  "text": "Post-Processors - Templates | Packer\nNote: This page is about older-style JSON Packer templates. JSON templates are still supported by the Packer core, but new features added to the Packer core may not be implemented for JSON templates. We recommend you transition to HCL templates as soon as is convenient for you, in order to have the best possible experience with Packer. To help you upgrade your templates, we have written an hcl2_upgrade command command.\nThe post-processor section within a template configures any post-processing that will be done to images built by the builders. Examples of post-processing would be compressing files, uploading artifacts, etc.\nPost-processors are optional. If no post-processors are defined within a template, then no post-processing will be done to the image. The resulting artifact of a build is just the image outputted by the builder.\nThis documentation page will cover how to configure a post-processor in a template. The specific configuration options available for each post-processor, however, must be referenced from the documentation for that specific post-processor.\nWithin a template, a section of post-processor definitions looks like this:\n{ \"post-processors\": [ // ... one or more post-processor definitions here ] } \nFor each post-processor definition, Packer will take the result of each of the defined builders and send it through the post-processors. This means that if you have one post-processor defined and two builders defined in a template, the post-processor will run twice (once for each builder), by default. There are ways, which will be covered later, to control what builders post-processors apply to, if you wish. It is also possible to prevent a post-processor from running.\nPost-Processor Definition\nWithin the post-processors array in a template, there are three ways to define a post-processor. There are simple definitions, detailed definitions, and sequence definitions. Another way to think about this is that the \"simple\" and \"detailed\" definitions are shortcuts for the \"sequence\" definition.\nA simple definition is just a string; the name of the post-processor. An example is shown below. Simple definitions are used when no additional configuration is needed for the post-processor.\n{ \"post-processors\": [\"compress\"] } \nA detailed definition is a JSON object. It is very similar to a builder or provisioner definition. It contains a type field to denote the type of the post-processor, but may also contain additional configuration for the post-processor. A detailed definition is used when additional configuration is needed beyond simply the type for the post-processor. An example is shown below.\n{ \"post-processors\": [ { \"type\": \"compress\", \"format\": \"tar.gz\" } ] } \nA sequence definition is a JSON array comprised of other simple or detailed definitions. The post-processors defined in the array are run in order, with the artifact of each feeding into the next, and any intermediary artifacts being discarded. A sequence definition may not contain another sequence definition. Sequence definitions are used to chain together multiple post-processors. An example is shown below, where the artifact of a build is compressed then uploaded, but the compressed result is not kept.\nIt is very important that any post processors that need to be run in order, be sequenced!\n{ \"post-processors\": [ [\"compress\", { \"type\": \"upload\", \"endpoint\": \"http://example.com\" }] ] } \nAs you may be able to imagine, the simple and detailed definitions are simply shortcuts for a sequence definition of only one element.\nWhen using post-processors, the input artifact (coming from a builder or another post-processor) is discarded by default after the post-processor runs. This is because generally, you don't want the intermediary artifacts on the way to the final artifact created.\nIn some cases, however, you may want to keep the intermediary artifacts. You can tell Packer to keep these artifacts by setting the keep_input_artifact configuration to true. An example is shown below:\n{ \"post-processors\": [ { \"type\": \"compress\", \"keep_input_artifact\": true } ] } \nThis setting will only keep the input artifact to that specific post-processor. If you're specifying a sequence of post-processors, then all intermediaries are discarded by default except for the input artifacts to post-processors that explicitly state to keep the input artifact.\nNote: The intuitive reader may be wondering what happens if multiple post-processors are specified (not in a sequence). Does Packer require the configuration to keep the input artifact on all the post-processors? The answer is no, of course not. Packer is smart enough to figure out that at least one post-processor requested that the input be kept, so it will keep it around.\nYou can use the only or except fields to run a post-processor only with specific builds. These two fields do what you expect: only will only run the post-processor on the specified builds and except will run the post-processor on anything other than the specified builds. A sequence of post-processors will execute until a skipped post-processor.\nAn example of only being used is shown below, but the usage of except is effectively the same. only and except can only be specified on \"detailed\" fields. If you have a sequence of post-processors to run, only and except will affect that post-processor and stop the sequence.\nThe -except option can specifically skip a named post processor. The -only option ignores post-processors.\n([ { \"name\": \"vbox\", \"type\": \"vagrant\", \"only\": [\"virtualbox-iso\"] }, { \"type\": \"compress\" } ], [ \"compress\", { \"type\": \"upload\", \"endpoint\": \"http://example.com\" } ]) \nThe values within only or except are build names, not builder types. Name is a required block label in HCL, but in legacy JSON, build names default to the types of their builders (e.g. docker or amazon-ebs or virtualbox-iso), unless a specific name attribute is specified within the configuration."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/templates/legacy_json_templates/post-processors",
  "text": "Post-Processors - Templates | Packer\nNote: This page is about older-style JSON Packer templates. JSON templates are still supported by the Packer core, but new features added to the Packer core may not be implemented for JSON templates. We recommend you transition to HCL templates as soon as is convenient for you, in order to have the best possible experience with Packer. To help you upgrade your templates, we have written an hcl2_upgrade command command.\nThe post-processor section within a template configures any post-processing that will be done to images built by the builders. Examples of post-processing would be compressing files, uploading artifacts, etc.\nPost-processors are optional. If no post-processors are defined within a template, then no post-processing will be done to the image. The resulting artifact of a build is just the image outputted by the builder.\nThis documentation page will cover how to configure a post-processor in a template. The specific configuration options available for each post-processor, however, must be referenced from the documentation for that specific post-processor.\nWithin a template, a section of post-processor definitions looks like this:\n{ \"post-processors\": [ // ... one or more post-processor definitions here ] } \nFor each post-processor definition, Packer will take the result of each of the defined builders and send it through the post-processors. This means that if you have one post-processor defined and two builders defined in a template, the post-processor will run twice (once for each builder), by default. There are ways, which will be covered later, to control what builders post-processors apply to, if you wish. It is also possible to prevent a post-processor from running.\nPost-Processor Definition\nWithin the post-processors array in a template, there are three ways to define a post-processor. There are simple definitions, detailed definitions, and sequence definitions. Another way to think about this is that the \"simple\" and \"detailed\" definitions are shortcuts for the \"sequence\" definition.\nA simple definition is just a string; the name of the post-processor. An example is shown below. Simple definitions are used when no additional configuration is needed for the post-processor.\n{ \"post-processors\": [\"compress\"] } \nA detailed definition is a JSON object. It is very similar to a builder or provisioner definition. It contains a type field to denote the type of the post-processor, but may also contain additional configuration for the post-processor. A detailed definition is used when additional configuration is needed beyond simply the type for the post-processor. An example is shown below.\n{ \"post-processors\": [ { \"type\": \"compress\", \"format\": \"tar.gz\" } ] } \nA sequence definition is a JSON array comprised of other simple or detailed definitions. The post-processors defined in the array are run in order, with the artifact of each feeding into the next, and any intermediary artifacts being discarded. A sequence definition may not contain another sequence definition. Sequence definitions are used to chain together multiple post-processors. An example is shown below, where the artifact of a build is compressed then uploaded, but the compressed result is not kept.\nIt is very important that any post processors that need to be run in order, be sequenced!\n{ \"post-processors\": [ [\"compress\", { \"type\": \"upload\", \"endpoint\": \"http://example.com\" }] ] } \nAs you may be able to imagine, the simple and detailed definitions are simply shortcuts for a sequence definition of only one element.\nWhen using post-processors, the input artifact (coming from a builder or another post-processor) is discarded by default after the post-processor runs. This is because generally, you don't want the intermediary artifacts on the way to the final artifact created.\nIn some cases, however, you may want to keep the intermediary artifacts. You can tell Packer to keep these artifacts by setting the keep_input_artifact configuration to true. An example is shown below:\n{ \"post-processors\": [ { \"type\": \"compress\", \"keep_input_artifact\": true } ] } \nThis setting will only keep the input artifact to that specific post-processor. If you're specifying a sequence of post-processors, then all intermediaries are discarded by default except for the input artifacts to post-processors that explicitly state to keep the input artifact.\nNote: The intuitive reader may be wondering what happens if multiple post-processors are specified (not in a sequence). Does Packer require the configuration to keep the input artifact on all the post-processors? The answer is no, of course not. Packer is smart enough to figure out that at least one post-processor requested that the input be kept, so it will keep it around.\nYou can use the only or except fields to run a post-processor only with specific builds. These two fields do what you expect: only will only run the post-processor on the specified builds and except will run the post-processor on anything other than the specified builds. A sequence of post-processors will execute until a skipped post-processor.\nAn example of only being used is shown below, but the usage of except is effectively the same. only and except can only be specified on \"detailed\" fields. If you have a sequence of post-processors to run, only and except will affect that post-processor and stop the sequence.\nThe -except option can specifically skip a named post processor. The -only option ignores post-processors.\n([ { \"name\": \"vbox\", \"type\": \"vagrant\", \"only\": [\"virtualbox-iso\"] }, { \"type\": \"compress\" } ], [ \"compress\", { \"type\": \"upload\", \"endpoint\": \"http://example.com\" } ]) \nThe values within only or except are build names, not builder types. Name is a required block label in HCL, but in legacy JSON, build names default to the types of their builders (e.g. docker or amazon-ebs or virtualbox-iso), unless a specific name attribute is specified within the configuration."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vagrant/v1.1.2/components/builder/vagrant",
  "text": "Vagrant Builder (1.1.2) | Integrations | Packer\nThe Vagrant multi-component plugin can be used with HashiCorp Packer to create custom images.\nType: vagrant Artifact BuilderId: vagrant\nThe Vagrant builder is intended for building new boxes from already-existing boxes. Your source should be a URL or path to a .box file or a Vagrant Cloud box name such as hashicorp/precise64.\nPacker will not install vagrant, nor will it install the underlying virtualization platforms or extra providers; We expect when you run this builder that you have already installed what you need.\nBy default, this builder will initialize a new Vagrant workspace, launch your box from that workspace, provision it, call vagrant package to package it into a new box, and then destroy the original box. Please note that vagrant will not remove the box file from your system (we don't call vagrant box remove).\nYou can change the behavior so that the builder doesn't destroy the box by setting the teardown_method option. You can change the behavior so the builder doesn't package it (not all provisioners support the vagrant package command) by setting the skip package option. You can also change the behavior so that rather than initializing a new Vagrant workspace, you use an already defined one, by using global_id instead of source_box.\nPlease note that if you are using the Vagrant builder, then the Vagrant post-processor is unnecessary because the output of the Vagrant builder is already a Vagrant box; using that post-processor with the Vagrant builder will cause your build to fail. Similarly, since Vagrant boxes are already compressed, the Compress post-processor will not work with this builder.\nsource_path (string) - URL of the vagrant box to use, or the name of the vagrant box. hashicorp/precise64, ./mylocalbox.box and https://example.com/my-box.box are all valid source boxes. If your source is a .box file, whether locally or from a URL like the latter example above, you will also need to provide a box_name. This option is required, unless you set global_id. You may only set one or the other, not both.\nor\nglobal_id (string) - the global id of a Vagrant box already added to Vagrant on your system. You can find the global id of your Vagrant boxes using the command vagrant global-status; your global_id will be a 7-digit number and letter combination that you'll find in the leftmost column of the global-status output. If you choose to use global_id instead of source_box, Packer will skip the Vagrant initialize and add steps, and simply launch the box directly using the global id.\noutput_dir (string) - The directory to create that will contain your output box. We always create this directory and run from inside of it to prevent Vagrant init collisions. If unset, it will be set to packer- plus your buildname.\nchecksum (string) - The checksum for the .box file. The type of the checksum is specified within the checksum field as a prefix, ex: \"md5:{$checksum}\". The type of the checksum can also be omitted and Packer will try to infer it based on string length. Valid values are \"none\", \"{$checksum}\", \"md5:{$checksum}\", \"sha1:{$checksum}\", \"sha256:{$checksum}\", \"sha512:{$checksum}\" or \"file:{$path}\". Here is a list of valid checksum values:\nbox_name (string) - if your sourcebox is a boxfile that we need to add to Vagrant, this is the name to give it. If left blank, will default to \"packer\" plus your buildname.\ninsert_key (bool) - If true, Vagrant will automatically insert a keypair to use for SSH, replacing Vagrant's default insecure key inside the machine if detected. By default, Packer sets this to false.\nprovider (string) - The vagrant provider. This parameter is required when source_path have more than one provider, or when using vagrant-cloud post-processor. Defaults to unset.\nvagrantfile_template (string) - What vagrantfile to use\nteardown_method (string) - Whether to halt, suspend, or destroy the box when the build has completed. Defaults to \"halt\"\nbox_version (string) - What box version to use when initializing Vagrant.\ntemplate (string) - a path to a golang template for a vagrantfile. Our default template can be found here. The template variables available to you are {{ .BoxName }}, {{ .SyncedFolder }}, and {{.InsertKey}}, which correspond to the Packer options box_name, synced_folder, and insert_key. Alternatively, the template variable {{.DefaultTemplate}} is available for use if you wish to extend the default generated template.\nsynced_folder (string) - Path to the folder to be synced to the guest. The path can be absolute or relative to the directory Packer is being run from.\nskip_add (bool) - Don't call \"vagrant add\" to add the box to your local environment; this is necessary if you want to launch a box that is already added to your vagrant environment.\nadd_cacert (string) - Equivalent to setting the --cacert option in vagrant add; defaults to unset.\nadd_capath (string) - Equivalent to setting the --capath option in vagrant add; defaults to unset.\nadd_cert (string) - Equivalent to setting the --cert option in vagrant add; defaults to unset.\nadd_clean (bool) - Equivalent to setting the --clean flag in vagrant add; defaults to unset.\nadd_force (bool) - Equivalent to setting the --force flag in vagrant add; defaults to unset.\nadd_insecure (bool) - Equivalent to setting the --insecure flag in vagrant add; defaults to unset.\nskip_package (bool) - if true, Packer will not call vagrant package to package your base box into its own standalone .box file.\noutput_vagrantfile (string) - Output Vagrantfile\npackage_include ([]string) - Equivalent to setting the --include option in vagrant package; defaults to unset\nExample\nSample for hashicorp/precise64 with virtualbox provider.\n{ \"builders\": [ { \"communicator\": \"ssh\", \"source_path\": \"hashicorp/precise64\", \"provider\": \"virtualbox\", \"add_force\": true, \"type\": \"vagrant\" } ] } \nsource \"vagrant\" \"example\" { communicator = \"ssh\" source_path = \"hashicorp/precise64\" provider = \"virtualbox\" add_force = true } build { sources = [\"source.vagrant.example\"] } \nRegarding output directory and new box\nAfter Packer completes building and provisioning a new Vagrant Box file, it is worth noting that the new box file will need to be added to Vagrant. For a beginner to Packer and Vagrant, it may seem as if a simple 'vagrant up' in the output directory will run the the newly created Box. This is not the case.\nRather, create a new directory (to avoid Vagarant init collisions), add the new package.box to Vagrant and init. Then run vagrant up to bring up the new box created by Packer. You will now be able to connect to the new box with provisioned changes.\n'mkdir output2' 'cp package.box ./output2' 'vagrant box add new-box name-of-the-packer-box.box' 'vagrant init new-box' 'vagrant up' \nA note on SSH connections\nCurrently this builder only works for SSH connections, and automatically fills in all information needed for the SSH communicator using vagrant's ssh-config.\nIf you would like to connect via a different username or authentication method than is produced when you call vagrant ssh-config, then you must provide the\nssh_username and all other relevant authentication information (e.g. ssh_password or ssh_private_key_file)\nBy providing the ssh_username, you're telling Packer not to use the vagrant ssh config, except for determining the host and port for the virtual machine to connect to."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vagrant/v1.1.1/components/builder/vagrant",
  "text": "Vagrant Builder (1.1.1) | Integrations | Packer\nThe Vagrant multi-component plugin can be used with HashiCorp Packer to create custom images.\nType: vagrant Artifact BuilderId: vagrant\nThe Vagrant builder is intended for building new boxes from already-existing boxes. Your source should be a URL or path to a .box file or a Vagrant Cloud box name such as hashicorp/precise64.\nPacker will not install vagrant, nor will it install the underlying virtualization platforms or extra providers; We expect when you run this builder that you have already installed what you need.\nBy default, this builder will initialize a new Vagrant workspace, launch your box from that workspace, provision it, call vagrant package to package it into a new box, and then destroy the original box. Please note that vagrant will not remove the box file from your system (we don't call vagrant box remove).\nYou can change the behavior so that the builder doesn't destroy the box by setting the teardown_method option. You can change the behavior so the builder doesn't package it (not all provisioners support the vagrant package command) by setting the skip package option. You can also change the behavior so that rather than initializing a new Vagrant workspace, you use an already defined one, by using global_id instead of source_box.\nPlease note that if you are using the Vagrant builder, then the Vagrant post-processor is unnecessary because the output of the Vagrant builder is already a Vagrant box; using that post-processor with the Vagrant builder will cause your build to fail. Similarly, since Vagrant boxes are already compressed, the Compress post-processor will not work with this builder.\nsource_path (string) - URL of the vagrant box to use, or the name of the vagrant box. hashicorp/precise64, ./mylocalbox.box and https://example.com/my-box.box are all valid source boxes. If your source is a .box file, whether locally or from a URL like the latter example above, you will also need to provide a box_name. This option is required, unless you set global_id. You may only set one or the other, not both.\nor\nglobal_id (string) - the global id of a Vagrant box already added to Vagrant on your system. You can find the global id of your Vagrant boxes using the command vagrant global-status; your global_id will be a 7-digit number and letter combination that you'll find in the leftmost column of the global-status output. If you choose to use global_id instead of source_box, Packer will skip the Vagrant initialize and add steps, and simply launch the box directly using the global id.\noutput_dir (string) - The directory to create that will contain your output box. We always create this directory and run from inside of it to prevent Vagrant init collisions. If unset, it will be set to packer- plus your buildname.\nchecksum (string) - The checksum for the .box file. The type of the checksum is specified within the checksum field as a prefix, ex: \"md5:{$checksum}\". The type of the checksum can also be omitted and Packer will try to infer it based on string length. Valid values are \"none\", \"{$checksum}\", \"md5:{$checksum}\", \"sha1:{$checksum}\", \"sha256:{$checksum}\", \"sha512:{$checksum}\" or \"file:{$path}\". Here is a list of valid checksum values:\nbox_name (string) - if your sourcebox is a boxfile that we need to add to Vagrant, this is the name to give it. If left blank, will default to \"packer\" plus your buildname.\ninsert_key (bool) - If true, Vagrant will automatically insert a keypair to use for SSH, replacing Vagrant's default insecure key inside the machine if detected. By default, Packer sets this to false.\nprovider (string) - The vagrant provider. This parameter is required when source_path have more than one provider, or when using vagrant-cloud post-processor. Defaults to unset.\nvagrantfile_template (string) - What vagrantfile to use\nteardown_method (string) - Whether to halt, suspend, or destroy the box when the build has completed. Defaults to \"halt\"\nbox_version (string) - What box version to use when initializing Vagrant.\ntemplate (string) - a path to a golang template for a vagrantfile. Our default template can be found here. The template variables available to you are {{ .BoxName }}, {{ .SyncedFolder }}, and {{.InsertKey}}, which correspond to the Packer options box_name, synced_folder, and insert_key. Alternatively, the template variable {{.DefaultTemplate}} is available for use if you wish to extend the default generated template.\nsynced_folder (string) - Path to the folder to be synced to the guest. The path can be absolute or relative to the directory Packer is being run from.\nskip_add (bool) - Don't call \"vagrant add\" to add the box to your local environment; this is necessary if you want to launch a box that is already added to your vagrant environment.\nadd_cacert (string) - Equivalent to setting the --cacert option in vagrant add; defaults to unset.\nadd_capath (string) - Equivalent to setting the --capath option in vagrant add; defaults to unset.\nadd_cert (string) - Equivalent to setting the --cert option in vagrant add; defaults to unset.\nadd_clean (bool) - Equivalent to setting the --clean flag in vagrant add; defaults to unset.\nadd_force (bool) - Equivalent to setting the --force flag in vagrant add; defaults to unset.\nadd_insecure (bool) - Equivalent to setting the --insecure flag in vagrant add; defaults to unset.\nskip_package (bool) - if true, Packer will not call vagrant package to package your base box into its own standalone .box file.\noutput_vagrantfile (string) - Output Vagrantfile\npackage_include ([]string) - Equivalent to setting the --include option in vagrant package; defaults to unset\nExample\nSample for hashicorp/precise64 with virtualbox provider.\n{ \"builders\": [ { \"communicator\": \"ssh\", \"source_path\": \"hashicorp/precise64\", \"provider\": \"virtualbox\", \"add_force\": true, \"type\": \"vagrant\" } ] } \nsource \"vagrant\" \"example\" { communicator = \"ssh\" source_path = \"hashicorp/precise64\" provider = \"virtualbox\" add_force = true } build { sources = [\"source.vagrant.example\"] } \nRegarding output directory and new box\nAfter Packer completes building and provisioning a new Vagrant Box file, it is worth noting that the new box file will need to be added to Vagrant. For a beginner to Packer and Vagrant, it may seem as if a simple 'vagrant up' in the output directory will run the the newly created Box. This is not the case.\nRather, create a new directory (to avoid Vagarant init collisions), add the new package.box to Vagrant and init. Then run vagrant up to bring up the new box created by Packer. You will now be able to connect to the new box with provisioned changes.\n'mkdir output2' 'cp package.box ./output2' 'vagrant box add new-box name-of-the-packer-box.box' 'vagrant init new-box' 'vagrant up' \nA note on SSH connections\nCurrently this builder only works for SSH connections, and automatically fills in all information needed for the SSH communicator using vagrant's ssh-config.\nIf you would like to connect via a different username or authentication method than is produced when you call vagrant ssh-config, then you must provide the\nssh_username and all other relevant authentication information (e.g. ssh_password or ssh_private_key_file)\nBy providing the ssh_username, you're telling Packer not to use the vagrant ssh config, except for determining the host and port for the virtual machine to connect to."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vagrant/v1.1.0/components/builder/vagrant",
  "text": "Vagrant Builder (1.1.0) | Integrations | Packer\nThe Vagrant multi-component plugin can be used with HashiCorp Packer to create custom images.\nType: vagrant Artifact BuilderId: vagrant\nThe Vagrant builder is intended for building new boxes from already-existing boxes. Your source should be a URL or path to a .box file or a Vagrant Cloud box name such as hashicorp/precise64.\nPacker will not install vagrant, nor will it install the underlying virtualization platforms or extra providers; We expect when you run this builder that you have already installed what you need.\nBy default, this builder will initialize a new Vagrant workspace, launch your box from that workspace, provision it, call vagrant package to package it into a new box, and then destroy the original box. Please note that vagrant will not remove the box file from your system (we don't call vagrant box remove).\nYou can change the behavior so that the builder doesn't destroy the box by setting the teardown_method option. You can change the behavior so the builder doesn't package it (not all provisioners support the vagrant package command) by setting the skip package option. You can also change the behavior so that rather than initializing a new Vagrant workspace, you use an already defined one, by using global_id instead of source_box.\nPlease note that if you are using the Vagrant builder, then the Vagrant post-processor is unnecessary because the output of the Vagrant builder is already a Vagrant box; using that post-processor with the Vagrant builder will cause your build to fail. Similarly, since Vagrant boxes are already compressed, the Compress post-processor will not work with this builder.\nsource_path (string) - URL of the vagrant box to use, or the name of the vagrant box. hashicorp/precise64, ./mylocalbox.box and https://example.com/my-box.box are all valid source boxes. If your source is a .box file, whether locally or from a URL like the latter example above, you will also need to provide a box_name. This option is required, unless you set global_id. You may only set one or the other, not both.\nglobal_id (string) - the global id of a Vagrant box already added to Vagrant on your system. You can find the global id of your Vagrant boxes using the command vagrant global-status; your global_id will be a 7-digit number and letter combination that you'll find in the leftmost column of the global-status output. If you choose to use global_id instead of source_box, Packer will skip the Vagrant initialize and add steps, and simply launch the box directly using the global id.\noutput_dir (string) - The directory to create that will contain your output box. We always create this directory and run from inside of it to prevent Vagrant init collisions. If unset, it will be set to packer- plus your buildname.\nchecksum (string) - The checksum for the .box file. The type of the checksum is specified within the checksum field as a prefix, ex: \"md5:{$checksum}\". The type of the checksum can also be omitted and Packer will try to infer it based on string length. Valid values are \"none\", \"{$checksum}\", \"md5:{$checksum}\", \"sha1:{$checksum}\", \"sha256:{$checksum}\", \"sha512:{$checksum}\" or \"file:{$path}\". Here is a list of valid checksum values:\nbox_name (string) - if your sourcebox is a boxfile that we need to add to Vagrant, this is the name to give it. If left blank, will default to \"packer\" plus your buildname.\ninsert_key (bool) - If true, Vagrant will automatically insert a keypair to use for SSH, replacing Vagrant's default insecure key inside the machine if detected. By default, Packer sets this to false.\nprovider (string) - The vagrant provider. This parameter is required when source_path have more than one provider, or when using vagrant-cloud post-processor. Defaults to unset.\nvagrantfile_template (string) - What vagrantfile to use\nteardown_method (string) - Whether to halt, suspend, or destroy the box when the build has completed. Defaults to \"halt\"\nbox_version (string) - What box version to use when initializing Vagrant.\ntemplate (string) - a path to a golang template for a vagrantfile. Our default template can be found here. The template variables available to you are {{ .BoxName }}, {{ .SyncedFolder }}, and {{.InsertKey}}, which correspond to the Packer options box_name, synced_folder, and insert_key.\nsynced_folder (string) - Path to the folder to be synced to the guest. The path can be absolute or relative to the directory Packer is being run from.\nskip_add (bool) - Don't call \"vagrant add\" to add the box to your local environment; this is necessary if you want to launch a box that is already added to your vagrant environment.\nadd_cacert (string) - Equivalent to setting the --cacert option in vagrant add; defaults to unset.\nadd_capath (string) - Equivalent to setting the --capath option in vagrant add; defaults to unset.\nadd_cert (string) - Equivalent to setting the --cert option in vagrant add; defaults to unset.\nadd_clean (bool) - Equivalent to setting the --clean flag in vagrant add; defaults to unset.\nadd_force (bool) - Equivalent to setting the --force flag in vagrant add; defaults to unset.\nadd_insecure (bool) - Equivalent to setting the --insecure flag in vagrant add; defaults to unset.\nskip_package (bool) - if true, Packer will not call vagrant package to package your base box into its own standalone .box file.\noutput_vagrantfile (string) - Output Vagrantfile\npackage_include ([]string) - Equivalent to setting the --include option in vagrant package; defaults to unset\nSample for hashicorp/precise64 with virtualbox provider.\n{ \"builders\": [ { \"communicator\": \"ssh\", \"source_path\": \"hashicorp/precise64\", \"provider\": \"virtualbox\", \"add_force\": true, \"type\": \"vagrant\" } ] } \nsource \"vagrant\" \"example\" { communicator = \"ssh\" source_path = \"hashicorp/precise64\" provider = \"virtualbox\" add_force = true } build { sources = [\"source.vagrant.example\"] } \nRegarding output directory and new box\nAfter Packer completes building and provisioning a new Vagrant Box file, it is worth noting that the new box file will need to be added to Vagrant. For a beginner to Packer and Vagrant, it may seem as if a simple 'vagrant up' in the output directory will run the the newly created Box. This is not the case.\nRather, create a new directory (to avoid Vagarant init collisions), add the new package.box to Vagrant and init. Then run vagrant up to bring up the new box created by Packer. You will now be able to connect to the new box with provisioned changes.\n'mkdir output2' 'cp package.box ./output2' 'vagrant box add new-box name-of-the-packer-box.box' 'vagrant init new-box' 'vagrant up' \nA note on SSH connections\nCurrently this builder only works for SSH connections, and automatically fills in all information needed for the SSH communicator using vagrant's ssh-config.\nIf you would like to connect via a different username or authentication method than is produced when you call vagrant ssh-config, then you must provide the\nssh_username and all other relevant authentication information (e.g. ssh_password or ssh_private_key_file)\nBy providing the ssh_username, you're telling Packer not to use the vagrant ssh config, except for determining the host and port for the virtual machine to connect to."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vagrant/v1.0.3/components/builder/vagrant",
  "text": "Vagrant Builder (1.0.3) | Integrations | Packer\nThe Vagrant multi-component plugin can be used with HashiCorp Packer to create custom images.\nType: vagrant Artifact BuilderId: vagrant\nThe Vagrant builder is intended for building new boxes from already-existing boxes. Your source should be a URL or path to a .box file or a Vagrant Cloud box name such as hashicorp/precise64.\nPacker will not install vagrant, nor will it install the underlying virtualization platforms or extra providers; We expect when you run this builder that you have already installed what you need.\nBy default, this builder will initialize a new Vagrant workspace, launch your box from that workspace, provision it, call vagrant package to package it into a new box, and then destroy the original box. Please note that vagrant will not remove the box file from your system (we don't call vagrant box remove).\nYou can change the behavior so that the builder doesn't destroy the box by setting the teardown_method option. You can change the behavior so the builder doesn't package it (not all provisioners support the vagrant package command) by setting the skip package option. You can also change the behavior so that rather than initializing a new Vagrant workspace, you use an already defined one, by using global_id instead of source_box.\nPlease note that if you are using the Vagrant builder, then the Vagrant post-processor is unnecessary because the output of the Vagrant builder is already a Vagrant box; using that post-processor with the Vagrant builder will cause your build to fail. Similarly, since Vagrant boxes are already compressed, the Compress post-processor will not work with this builder.\nsource_path (string) - URL of the vagrant box to use, or the name of the vagrant box. hashicorp/precise64, ./mylocalbox.box and https://example.com/my-box.box are all valid source boxes. If your source is a .box file, whether locally or from a URL like the latter example above, you will also need to provide a box_name. This option is required, unless you set global_id. You may only set one or the other, not both.\nglobal_id (string) - the global id of a Vagrant box already added to Vagrant on your system. You can find the global id of your Vagrant boxes using the command vagrant global-status; your global_id will be a 7-digit number and letter combination that you'll find in the leftmost column of the global-status output. If you choose to use global_id instead of source_box, Packer will skip the Vagrant initialize and add steps, and simply launch the box directly using the global id.\noutput_dir (string) - The directory to create that will contain your output box. We always create this directory and run from inside of it to prevent Vagrant init collisions. If unset, it will be set to packer- plus your buildname.\nchecksum (string) - The checksum for the .box file. The type of the checksum is specified within the checksum field as a prefix, ex: \"md5:{$checksum}\". The type of the checksum can also be omitted and Packer will try to infer it based on string length. Valid values are \"none\", \"{$checksum}\", \"md5:{$checksum}\", \"sha1:{$checksum}\", \"sha256:{$checksum}\", \"sha512:{$checksum}\" or \"file:{$path}\". Here is a list of valid checksum values:\nbox_name (string) - if your sourcebox is a boxfile that we need to add to Vagrant, this is the name to give it. If left blank, will default to \"packer\" plus your buildname.\ninsert_key (bool) - If true, Vagrant will automatically insert a keypair to use for SSH, replacing Vagrant's default insecure key inside the machine if detected. By default, Packer sets this to false.\nprovider (string) - The vagrant provider. This parameter is required when source_path have more than one provider, or when using vagrant-cloud post-processor. Defaults to unset.\nvagrantfile_template (string) - What vagrantfile to use\nteardown_method (string) - Whether to halt, suspend, or destroy the box when the build has completed. Defaults to \"halt\"\nbox_version (string) - What box version to use when initializing Vagrant.\ntemplate (string) - a path to a golang template for a vagrantfile. Our default template can be found here. The template variables available to you are {{ .BoxName }}, {{ .SyncedFolder }}, and {{.InsertKey}}, which correspond to the Packer options box_name, synced_folder, and insert_key.\nsynced_folder (string) - Path to the folder to be synced to the guest. The path can be absolute or relative to the directory Packer is being run from.\nskip_add (bool) - Don't call \"vagrant add\" to add the box to your local environment; this is necessary if you want to launch a box that is already added to your vagrant environment.\nadd_cacert (string) - Equivalent to setting the --cacert option in vagrant add; defaults to unset.\nadd_capath (string) - Equivalent to setting the --capath option in vagrant add; defaults to unset.\nadd_cert (string) - Equivalent to setting the --cert option in vagrant add; defaults to unset.\nadd_clean (bool) - Equivalent to setting the --clean flag in vagrant add; defaults to unset.\nadd_force (bool) - Equivalent to setting the --force flag in vagrant add; defaults to unset.\nadd_insecure (bool) - Equivalent to setting the --insecure flag in vagrant add; defaults to unset.\nskip_package (bool) - if true, Packer will not call vagrant package to package your base box into its own standalone .box file.\noutput_vagrantfile (string) - Output Vagrantfile\npackage_include ([]string) - Equivalent to setting the --include option in vagrant package; defaults to unset\nSample for hashicorp/precise64 with virtualbox provider.\n{ \"builders\": [ { \"communicator\": \"ssh\", \"source_path\": \"hashicorp/precise64\", \"provider\": \"virtualbox\", \"add_force\": true, \"type\": \"vagrant\" } ] } \nsource \"vagrant\" \"example\" { communicator = \"ssh\" source_path = \"hashicorp/precise64\" provider = \"virtualbox\" add_force = true } build { sources = [\"source.vagrant.example\"] } \nRegarding output directory and new box\nAfter Packer completes building and provisioning a new Vagrant Box file, it is worth noting that the new box file will need to be added to Vagrant. For a beginner to Packer and Vagrant, it may seem as if a simple 'vagrant up' in the output directory will run the the newly created Box. This is not the case.\nRather, create a new directory (to avoid Vagarant init collisions), add the new package.box to Vagrant and init. Then run vagrant up to bring up the new box created by Packer. You will now be able to connect to the new box with provisioned changes.\n'mkdir output2' 'cp package.box ./output2' 'vagrant box add new-box name-of-the-packer-box.box' 'vagrant init new-box' 'vagrant up' \nA note on SSH connections\nCurrently this builder only works for SSH connections, and automatically fills in all information needed for the SSH communicator using vagrant's ssh-config.\nIf you would like to connect via a different username or authentication method than is produced when you call vagrant ssh-config, then you must provide the\nssh_username and all other relevant authentication information (e.g. ssh_password or ssh_private_key_file)\nBy providing the ssh_username, you're telling Packer not to use the vagrant ssh config, except for determining the host and port for the virtual machine to connect to."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vagrant/v1.0.0/components/builder/vagrant",
  "text": "Vagrant Builder (1.0.0) | Integrations | Packer\nPlease note that if you are using the Vagrant builder, then the Vagrant post-processor is unnecesary because the output of the Vagrant builder is already a Vagrant box; using that post-processor with the Vagrant builder will cause your build to fail. Similarly, since Vagrant boxes are already compressed, the Compress post-processor will not work with this builder.\ntemplate (string) - a path to a golang template for a vagrantfile. Our default template can be found here. The template variables available to you are {{ .BoxName }}, {{ .SyncedFolder }}, and {{.InsertKey}}, which correspond to the Packer options box_name, synced_folder, and insert_key."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vagrant/v1.1.2",
  "text": "Vagrant (v1.1.2) | Integrations | Packer\nThe Vagrant plugin integrates Packer with HashiCorp Vagrant, allowing you to use Packer to create development boxes.\nInstallation\nTo install this plugin add this code into your Packer configuration and run packer init\npacker { required_plugins { vagrant = { version = \"~> 1\" source = \"github.com/hashicorp/vagrant\" } } } \nAlternatively, you can use packer plugins install to manage installation of this plugin.\npacker plugins install github.com/hashicorp/vagrant \nComponents\nBuilders\nvagrant - The Vagrant builder is intended for building new boxes from already-existing boxes.\nPost-Processor\nvagrant - The Packer Vagrant post-processor takes a build and converts the artifact into a valid Vagrant box.\nvagrant-cloud - The Vagrant Cloud post-processor enables the upload of Vagrant boxes to Vagrant Cloud."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vagrant/v1.1.1",
  "text": "Vagrant (v1.1.1) | Integrations | Packer\nThe Vagrant plugin integrates Packer with HashiCorp Vagrant, allowing you to use Packer to create development boxes.\nInstallation\nTo install this plugin add this code into your Packer configuration and run packer init\npacker { required_plugins { vagrant = { version = \"~> 1\" source = \"github.com/hashicorp/vagrant\" } } } \nAlternatively, you can use packer plugins install to manage installation of this plugin.\npacker plugins install github.com/hashicorp/vagrant \nComponents\nBuilders\nvagrant - The Vagrant builder is intended for building new boxes from already-existing boxes.\nPost-Processor\nvagrant - The Packer Vagrant post-processor takes a build and converts the artifact into a valid Vagrant box.\nvagrant-cloud - The Vagrant Cloud post-processor enables the upload of Vagrant boxes to Vagrant Cloud."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vagrant/v1.1.0",
  "text": "Vagrant (v1.1.0) | Integrations | Packer\nThe Vagrant plugin integrates Packer with HashiCorp Vagrant, allowing you to use Packer to create development boxes.\nInstallation\nTo install this plugin add this code into your Packer configuration and run packer init\npacker { required_plugins { vagrant = { version = \"~> 1\" source = \"github.com/hashicorp/vagrant\" } } } \nAlternatively, you can use packer plugins install to manage installation of this plugin.\npacker plugins install github.com/hashicorp/vagrant \nComponents\nBuilders\nvagrant - The Vagrant builder is intended for building new boxes from already-existing boxes.\nPost-Processor\nvagrant - The Packer Vagrant post-processor takes a build and converts the artifact into a valid Vagrant box.\nvagrant-cloud - The Vagrant Cloud post-processor enables the upload of Vagrant boxes to Vagrant Cloud."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vagrant/v1.0.0",
  "text": "Vagrant (v1.0.0) | Integrations | Packer\nSign up\nTheme\nVagrant Plugin\nThe Vagrant plugin integrates Packer with HashiCorp Vagrant, allowing you to use Packer to create development boxes.\nbuilder - The Vagrant builder is intended for building new boxes from already-existing boxes.\npost-processor - The Packer Vagrant post-processor takes a build and converts the artifact into a valid Vagrant box.\nHCP Packer\nAutomate build management across your cloud providers\nTry HCP Packer for free"
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vagrant/v1.1.2/components/post-processor/vagrant-cloud",
  "text": "Vagrant Post-Processor (1.1.2) | Integrations | Packer\nType: vagrant-cloud Artifact BuilderId: pearkes.post-processor.vagrant-cloud\nVagrant Cloud hosts and serves boxes to Vagrant, allowing you to version and distribute boxes to an organization in a simple way.\nThe Vagrant Cloud post-processor enables the upload of Vagrant boxes to Vagrant Cloud. Currently, the Vagrant Cloud post-processor will accept and upload boxes supplied to it from the Vagrant or Artifice post-processors and the Vagrant builder.\nYou'll need to be familiar with Vagrant Cloud, have an upgraded account to enable box hosting, and be distributing your box via the shorthand name configuration.\nWorkflow\nIt's important to understand the workflow that using this post-processor enforces in order to take full advantage of Vagrant and Vagrant Cloud.\nThe use of this processor assume that you currently distribute, or plan to distribute, boxes via Vagrant Cloud. It also assumes you create Vagrant Boxes and deliver them to your team in some fashion.\nHere is an example workflow:\nYou use Packer to build a Vagrant Box for the virtualbox provider\nThe vagrant-cloud post-processor is configured to point to the box hashicorp/foobar on Vagrant Cloud via the box_tag configuration\nThe post-processor receives the box from the vagrant post-processor\nIt then creates the configured version, or verifies the existence of it, on Vagrant Cloud\nA provider matching the name of the Vagrant provider is then created\nThe box is uploaded to Vagrant Cloud\nThe upload is verified\nThe version is released and available to users of the box\nThe Vagrant Cloud box (hashicorp/foobar in this example) must already exist. Packer will not create the box automatically. If running Packer in automation, consider using the Vagrant Cloud API to create the Vagrant Cloud box if it doesn't already exist.\nConfiguration\nThe configuration allows you to specify the target box that you have access to on Vagrant Cloud, as well as authentication and version information.\nbox_tag (string) - The shorthand tag for your box that maps to Vagrant Cloud, for example hashicorp/precise64, which is short for vagrantcloud.com/hashicorp/precise64. This box must already exist in Vagrant Cloud. Packer will not create the box automatically.\nversion (string) - The version number, typically incrementing a previous version. The version string is validated based on Semantic Versioning. The string must match a pattern that could be semver, and doesn't validate that the version comes after your previous versions.\naccess_token (string) - Your access token for the Vagrant Cloud API. This can be generated on your tokens page. If not specified, the environment will be searched. First, VAGRANT_CLOUD_TOKEN is checked, and if nothing is found, finally ATLAS_TOKEN will be used. This is required unless you are using a private hosting solution (i.e. vagrant_cloud_url has been populated).\nvagrant_cloud_url (string) - Override the base URL for Vagrant Cloud. This is useful if you're using Vagrant Private Cloud in your own network. Defaults to https://vagrantcloud.com/api/v2. If this value is set to something other than the default then access_token can be left blank and no Authorization header will be added to requests sent by this post-processor.\narchitecture (string) - The architecture of the Vagrant box. This will be detected from the box if possible by default. Supported values: amd64, i386, arm, arm64, ppc64le, ppc64, mips64le, mips64, mipsle, mips, and s390x.\ndefault_architecture (string) - The architecture that should be flagged as the default architecture for this provider. See the Vagrant Cloud documentation for more information.\nno_release (boolean) - If set to true, does not release the version on Vagrant Cloud, making it active. You can manually release the version via the API or Web UI. Defaults to false.\ninsecure_skip_tls_verify (boolean) - If set to true and vagrant_cloud_url is set to something different than its default, it will set TLS InsecureSkipVerify to true. In other words, this will disable security checks of SSL. You may need to set this option to true if your host at vagrant_cloud_url is using a self-signed certificate.\nkeep_input_artifact (boolean) - When true, preserve the local box after uploading to Vagrant cloud. Defaults to true.\nversion_description (string) - Optional Markdown text used as a full-length and in-depth description of the version, typically for denoting changes introduced\nbox_download_url (string) - Optional URL for a self-hosted box. If this is set the box will not be uploaded to the Vagrant Cloud. This is a template engine. Therefore, you may use user variables and template functions in this field. The following extra variables are also available in this engine:\nArchitecture: The architecture of the Vagrant box\nbox_checksum (string) - Optional checksum for the provider .box file. The type of the checksum is specified within the checksum field as a prefix, ex: \"md5:{$checksum}\". Valid values are:\nnull or \"\"\n\"md5:{$checksum}\"\n\"sha1:{$checksum}\"\n\"sha256:{$checksum}\"\n\"sha512:{$checksum}\" See https://www.vagrantup.com/vagrant-cloud/api#arguments-7\nno_direct_upload (boolean) - When true, upload the box artifact through Vagrant Cloud instead of directly to the backend storage.\nUse with the Vagrant Post-Processor\nAn example configuration is shown below. Note the use of the post-processors block that wraps both the Vagrant and Vagrant Cloud post-processor blocks within the post-processor section. Chaining the post-processors together in this way tells Packer that the artifact produced by the Vagrant post-processor should be passed directly to the Vagrant Cloud Post-Processor. It also sets the order in which the post-processors should run.\n{ \"variables\": { \"cloud_token\": \"{{ env `VAGRANT_CLOUD_TOKEN` }}\", \"version\": \"1.0.{{timestamp}}\" \"architecture\": \"amd64\", }, \"post-processors\": [ { \"type\": \"shell-local\", \"inline\": [\"echo Doing stuff...\"] }, [ { \"type\": \"vagrant\", \"include\": [\"image.iso\"], \"vagrantfile_template\": \"vagrantfile.tpl\", \"output\": \"proxycore_{{.Provider}}.box\" }, { \"type\": \"vagrant-cloud\", \"box_tag\": \"hashicorp/precise64\", \"access_token\": \"{{user `cloud_token`}}\", \"version\": \"{{user `version`}}\", \"architecture\": \"{{user `architecture`}}\" } ] ] } \nbuild { sources = [\"source.null.autogenerated_1\"] post-processor \"shell-local\" { inline = [\"echo Doing stuff...\"] } post-processors { post-processor \"vagrant\" { include = [\"image.iso\"] output = \"proxycore_{{.Provider}}.box\" vagrantfile_template = \"vagrantfile.tpl\" } post-processor \"vagrant-cloud\" { access_token = \"${var.cloud_token}\" box_tag = \"hashicorp/precise64\" version = \"${local.version}\" architecture = \"${local.architecture}\" } } } \nUse with the Artifice Post-Processor\nAn example configuration is shown below. Note the use of the nested array that wraps both the Artifice and Vagrant Cloud post-processors within the post-processor section. Chaining the post-processors together in this way tells Packer that the artifact produced by the Artifice post-processor should be passed directly to the Vagrant Cloud Post-Processor. It also sets the order in which the post-processors should run.\nNote that the Vagrant box specified in the Artifice post-processor files array must end in the .box extension. It must also be the first file in the array. Additional files bundled by the Artifice post-processor will be ignored.\n{ \"variables\": { \"cloud_token\": \"{{ env `VAGRANT_CLOUD_TOKEN` }}\" }, \"builders\": [ { \"type\": \"null\", \"communicator\": \"none\" } ], \"post-processors\": [ { \"type\": \"shell-local\", \"inline\": [\"echo Doing stuff...\"] }, [ { \"type\": \"artifice\", \"files\": [\"./path/to/my.box\"] }, { \"type\": \"vagrant-cloud\", \"box_tag\": \"myorganisation/mybox\", \"access_token\": \"{{user `cloud_token`}}\", \"version\": \"0.1.0\", \"architecture\": \"amd64\" } ] ] } \nvariable \"cloud_token\" { type = string default = \"${env(\"VAGRANT_CLOUD_TOKEN\")}\" } source \"null\" \"autogenerated_1\" { communicator = \"none\" } build { sources = [\"source.null.autogenerated_1\"] post-processor \"shell-local\" { inline = [\"echo Doing stuff...\"] } post-processors { post-processor \"artifice\" { files = [\"./path/to/my.box\"] } post-processor \"vagrant-cloud\" { access_token = \"${var.cloud_token}\" box_tag = \"myorganisation/mybox\" version = \"0.1.0\" architecture = \"amd64\" } } }"
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vagrant/v1.1.1/components/post-processor/vagrant-cloud",
  "text": "Vagrant Post-Processor (1.1.1) | Integrations | Packer\nType: vagrant-cloud Artifact BuilderId: pearkes.post-processor.vagrant-cloud\nVagrant Cloud hosts and serves boxes to Vagrant, allowing you to version and distribute boxes to an organization in a simple way.\nThe Vagrant Cloud post-processor enables the upload of Vagrant boxes to Vagrant Cloud. Currently, the Vagrant Cloud post-processor will accept and upload boxes supplied to it from the Vagrant or Artifice post-processors and the Vagrant builder.\nYou'll need to be familiar with Vagrant Cloud, have an upgraded account to enable box hosting, and be distributing your box via the shorthand name configuration.\nWorkflow\nIt's important to understand the workflow that using this post-processor enforces in order to take full advantage of Vagrant and Vagrant Cloud.\nThe use of this processor assume that you currently distribute, or plan to distribute, boxes via Vagrant Cloud. It also assumes you create Vagrant Boxes and deliver them to your team in some fashion.\nHere is an example workflow:\nYou use Packer to build a Vagrant Box for the virtualbox provider\nThe vagrant-cloud post-processor is configured to point to the box hashicorp/foobar on Vagrant Cloud via the box_tag configuration\nThe post-processor receives the box from the vagrant post-processor\nIt then creates the configured version, or verifies the existence of it, on Vagrant Cloud\nA provider matching the name of the Vagrant provider is then created\nThe box is uploaded to Vagrant Cloud\nThe upload is verified\nThe version is released and available to users of the box\nThe Vagrant Cloud box (hashicorp/foobar in this example) must already exist. Packer will not create the box automatically. If running Packer in automation, consider using the Vagrant Cloud API to create the Vagrant Cloud box if it doesn't already exist.\nConfiguration\nThe configuration allows you to specify the target box that you have access to on Vagrant Cloud, as well as authentication and version information.\nbox_tag (string) - The shorthand tag for your box that maps to Vagrant Cloud, for example hashicorp/precise64, which is short for vagrantcloud.com/hashicorp/precise64. This box must already exist in Vagrant Cloud. Packer will not create the box automatically.\nversion (string) - The version number, typically incrementing a previous version. The version string is validated based on Semantic Versioning. The string must match a pattern that could be semver, and doesn't validate that the version comes after your previous versions.\naccess_token (string) - Your access token for the Vagrant Cloud API. This can be generated on your tokens page. If not specified, the environment will be searched. First, VAGRANT_CLOUD_TOKEN is checked, and if nothing is found, finally ATLAS_TOKEN will be used. This is required unless you are using a private hosting solution (i.e. vagrant_cloud_url has been populated).\nvagrant_cloud_url (string) - Override the base URL for Vagrant Cloud. This is useful if you're using Vagrant Private Cloud in your own network. Defaults to https://vagrantcloud.com/api/v2. If this value is set to something other than the default then access_token can be left blank and no Authorization header will be added to requests sent by this post-processor.\narchitecture (string) - The architecture of the Vagrant box. This will be detected from the box if possible by default. Supported values: amd64, i386, arm, arm64, ppc64le, ppc64, mips64le, mips64, mipsle, mips, and s390x.\ndefault_architecture (string) - The architecture that should be flagged as the default architecture for this provider. See the Vagrant Cloud documentation for more information.\nno_release (boolean) - If set to true, does not release the version on Vagrant Cloud, making it active. You can manually release the version via the API or Web UI. Defaults to false.\ninsecure_skip_tls_verify (boolean) - If set to true and vagrant_cloud_url is set to something different than its default, it will set TLS InsecureSkipVerify to true. In other words, this will disable security checks of SSL. You may need to set this option to true if your host at vagrant_cloud_url is using a self-signed certificate.\nkeep_input_artifact (boolean) - When true, preserve the local box after uploading to Vagrant cloud. Defaults to true.\nversion_description (string) - Optional Markdown text used as a full-length and in-depth description of the version, typically for denoting changes introduced\nbox_download_url (string) - Optional URL for a self-hosted box. If this is set the box will not be uploaded to the Vagrant Cloud. This is a template engine. Therefore, you may use user variables and template functions in this field. The following extra variables are also available in this engine:\nArchitecture: The architecture of the Vagrant box\nbox_checksum (string) - Optional checksum for the provider .box file. The type of the checksum is specified within the checksum field as a prefix, ex: \"md5:{$checksum}\". Valid values are:\nnull or \"\"\n\"md5:{$checksum}\"\n\"sha1:{$checksum}\"\n\"sha256:{$checksum}\"\n\"sha512:{$checksum}\" See https://www.vagrantup.com/vagrant-cloud/api#arguments-7\nno_direct_upload (boolean) - When true, upload the box artifact through Vagrant Cloud instead of directly to the backend storage.\nUse with the Vagrant Post-Processor\nAn example configuration is shown below. Note the use of the post-processors block that wraps both the Vagrant and Vagrant Cloud post-processor blocks within the post-processor section. Chaining the post-processors together in this way tells Packer that the artifact produced by the Vagrant post-processor should be passed directly to the Vagrant Cloud Post-Processor. It also sets the order in which the post-processors should run.\n{ \"variables\": { \"cloud_token\": \"{{ env `VAGRANT_CLOUD_TOKEN` }}\", \"version\": \"1.0.{{timestamp}}\" \"architecture\": \"amd64\", }, \"post-processors\": [ { \"type\": \"shell-local\", \"inline\": [\"echo Doing stuff...\"] }, [ { \"type\": \"vagrant\", \"include\": [\"image.iso\"], \"vagrantfile_template\": \"vagrantfile.tpl\", \"output\": \"proxycore_{{.Provider}}.box\" }, { \"type\": \"vagrant-cloud\", \"box_tag\": \"hashicorp/precise64\", \"access_token\": \"{{user `cloud_token`}}\", \"version\": \"{{user `version`}}\", \"architecture\": \"{{user `architecture`}}\" } ] ] } \nbuild { sources = [\"source.null.autogenerated_1\"] post-processor \"shell-local\" { inline = [\"echo Doing stuff...\"] } post-processors { post-processor \"vagrant\" { include = [\"image.iso\"] output = \"proxycore_{{.Provider}}.box\" vagrantfile_template = \"vagrantfile.tpl\" } post-processor \"vagrant-cloud\" { access_token = \"${var.cloud_token}\" box_tag = \"hashicorp/precise64\" version = \"${local.version}\" architecture = \"${local.architecture}\" } } } \nUse with the Artifice Post-Processor\nAn example configuration is shown below. Note the use of the nested array that wraps both the Artifice and Vagrant Cloud post-processors within the post-processor section. Chaining the post-processors together in this way tells Packer that the artifact produced by the Artifice post-processor should be passed directly to the Vagrant Cloud Post-Processor. It also sets the order in which the post-processors should run.\nNote that the Vagrant box specified in the Artifice post-processor files array must end in the .box extension. It must also be the first file in the array. Additional files bundled by the Artifice post-processor will be ignored.\n{ \"variables\": { \"cloud_token\": \"{{ env `VAGRANT_CLOUD_TOKEN` }}\" }, \"builders\": [ { \"type\": \"null\", \"communicator\": \"none\" } ], \"post-processors\": [ { \"type\": \"shell-local\", \"inline\": [\"echo Doing stuff...\"] }, [ { \"type\": \"artifice\", \"files\": [\"./path/to/my.box\"] }, { \"type\": \"vagrant-cloud\", \"box_tag\": \"myorganisation/mybox\", \"access_token\": \"{{user `cloud_token`}}\", \"version\": \"0.1.0\", \"architecture\": \"amd64\" } ] ] } \nvariable \"cloud_token\" { type = string default = \"${env(\"VAGRANT_CLOUD_TOKEN\")}\" } source \"null\" \"autogenerated_1\" { communicator = \"none\" } build { sources = [\"source.null.autogenerated_1\"] post-processor \"shell-local\" { inline = [\"echo Doing stuff...\"] } post-processors { post-processor \"artifice\" { files = [\"./path/to/my.box\"] } post-processor \"vagrant-cloud\" { access_token = \"${var.cloud_token}\" box_tag = \"myorganisation/mybox\" version = \"0.1.0\" architecture = \"amd64\" } } }"
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vagrant/v1.0.0/components/post-processor/vagrant-cloud",
  "text": "Vagrant Post-Processor (1.0.0) | Integrations | Packer\nType: vagrant-cloud Artifact BuilderId: pearkes.post-processor.vagrant-cloud\nWorkflow\nThe Vagrant Cloud box (hashicorp/foobar in this example) must already exist. Packer will not create the box automatically. If running Packer in automation, consider using the Vagrant Cloud API to create the Vagrant Cloud box if it doesn't already exist.\nbox_tag (string) - The shorthand tag for your box that maps to Vagrant Cloud, for example hashicorp/precise64, which is short for vagrantcloud.com/hashicorp/precise64. This box must already exist in Vagrant Cloud. Packer will not create the box automatically.\nvagrant_cloud_url (string) - Override the base URL for Vagrant Cloud. This is useful if you're using Vagrant Private Cloud in your own network. Defaults to https://vagrantcloud.com/api/v1. If this value is set to something other than the default then access_token can be left blank and no Authorization header will be added to requests sent by this post-processor.\nno_release (string) - If set to true, does not release the version on Vagrant Cloud, making it active. You can manually release the version via the API or Web UI. Defaults to false.\nversion_description (string) - Optional Markdown text used as a full-length and in-depth description of the version, typically for denoting changes introduced\nbox_download_url (string) - Optional URL for a self-hosted box. If this is set the box will not be uploaded to the Vagrant Cloud. This is a template engine. Therefore, you may use user variables and template functions in this field. The following extra variables are also available in this engine:\nbox_checksum (string) - Optional checksum for the provider .box file. The type of the checksum is specified within the checksum field as a prefix, ex: \"md5:{$checksum}\". Valid values are:\nnull or \"\"\n\"md5:{$checksum}\"\n\"sha1:{$checksum}\"\n\"sha256:{$checksum}\"\n\"sha512:{$checksum}\" See https://www.vagrantup.com/vagrant-cloud/api#arguments-7\nno_direct_upload (boolean) - When true, upload the box artifact through Vagrant Cloud instead of directly to the backend storage.\nAn example configuration is shown below. Note the use of the post-processors block that wraps both the Vagrant and Vagrant Cloud post-processor blocks within the post-processor section. Chaining the post-processors together in this way tells Packer that the artifact produced by the Vagrant post-processor should be passed directly to the Vagrant Cloud Post-Processor. It also sets the order in which the post-processors should run.\n{ \"variables\": { \"cloud_token\": \"{{ env `VAGRANT_CLOUD_TOKEN` }}\", \"version\": \"1.0.{{timestamp}}\" }, \"post-processors\": [ { \"type\": \"shell-local\", \"inline\": [\"echo Doing stuff...\"] }, [ { \"type\": \"vagrant\", \"include\": [\"image.iso\"], \"vagrantfile_template\": \"vagrantfile.tpl\", \"output\": \"proxycore_{{.Provider}}.box\" }, { \"type\": \"vagrant-cloud\", \"box_tag\": \"hashicorp/precise64\", \"access_token\": \"{{user `cloud_token`}}\", \"version\": \"{{user `version`}}\" } ] ] } \nbuild { sources = [\"source.null.autogenerated_1\"] post-processor \"shell-local\" { inline = [\"echo Doing stuff...\"] } post-processors { post-processor \"vagrant\" { include = [\"image.iso\"] output = \"proxycore_{{.Provider}}.box\" vagrantfile_template = \"vagrantfile.tpl\" } post-processor \"vagrant-cloud\" { access_token = \"${var.cloud_token}\" box_tag = \"hashicorp/precise64\" version = \"${local.version}\" } } } \n{ \"variables\": { \"cloud_token\": \"{{ env `VAGRANT_CLOUD_TOKEN` }}\" }, \"builders\": [ { \"type\": \"null\", \"communicator\": \"none\" } ], \"post-processors\": [ { \"type\": \"shell-local\", \"inline\": [\"echo Doing stuff...\"] }, [ { \"type\": \"artifice\", \"files\": [\"./path/to/my.box\"] }, { \"type\": \"vagrant-cloud\", \"box_tag\": \"myorganisation/mybox\", \"access_token\": \"{{user `cloud_token`}}\", \"version\": \"0.1.0\" } ] ] } \nvariable \"cloud_token\" { type = string default = \"${env(\"VAGRANT_CLOUD_TOKEN\")}\" } source \"null\" \"autogenerated_1\" { communicator = \"none\" } build { sources = [\"source.null.autogenerated_1\"] post-processor \"shell-local\" { inline = [\"echo Doing stuff...\"] } post-processors { post-processor \"artifice\" { files = [\"./path/to/my.box\"] } post-processor \"vagrant-cloud\" { access_token = \"${var.cloud_token}\" box_tag = \"myorganisation/mybox\" version = \"0.1.0\" } } }"
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vagrant/v1.1.0/components/post-processor/vagrant-cloud",
  "text": "Vagrant Post-Processor (1.1.0) | Integrations | Packer\nType: vagrant-cloud Artifact BuilderId: pearkes.post-processor.vagrant-cloud\nWorkflow\nvagrant_cloud_url (string) - Override the base URL for Vagrant Cloud. This is useful if you're using Vagrant Private Cloud in your own network. Defaults to https://vagrantcloud.com/api/v2. If this value is set to something other than the default then access_token can be left blank and no Authorization header will be added to requests sent by this post-processor.\narchitecture (string) - The architecture of the Vagrant box. This will be detected from the box if possible by default. Supported values: amd64, i386, arm, arm64, ppc64le, ppc64, mips64le, mips64, mipsle, mips, and s390x.\ndefault_architecture (string) - The architecture that should be flagged as the default architecture for this provider. See the Vagrant Cloud documentation for more information.\nno_release (boolean) - If set to true, does not release the version on Vagrant Cloud, making it active. You can manually release the version via the API or Web UI. Defaults to false.\nversion_description (string) - Optional Markdown text used as a full-length and in-depth description of the version, typically for denoting changes introduced\nbox_download_url (string) - Optional URL for a self-hosted box. If this is set the box will not be uploaded to the Vagrant Cloud. This is a template engine. Therefore, you may use user variables and template functions in this field. The following extra variables are also available in this engine:\nArchitecture: The architecture of the Vagrant box\nbox_checksum (string) - Optional checksum for the provider .box file. The type of the checksum is specified within the checksum field as a prefix, ex: \"md5:{$checksum}\". Valid values are:\nnull or \"\"\n\"md5:{$checksum}\"\n\"sha1:{$checksum}\"\n\"sha256:{$checksum}\"\n\"sha512:{$checksum}\" See https://www.vagrantup.com/vagrant-cloud/api#arguments-7\nAn example configuration is shown below. Note the use of the post-processors block that wraps both the Vagrant and Vagrant Cloud post-processor blocks within the post-processor section. Chaining the post-processors together in this way tells Packer that the artifact produced by the Vagrant post-processor should be passed directly to the Vagrant Cloud Post-Processor. It also sets the order in which the post-processors should run.\n{ \"variables\": { \"cloud_token\": \"{{ env `VAGRANT_CLOUD_TOKEN` }}\", \"version\": \"1.0.{{timestamp}}\" \"architecture\": \"amd64\", }, \"post-processors\": [ { \"type\": \"shell-local\", \"inline\": [\"echo Doing stuff...\"] }, [ { \"type\": \"vagrant\", \"include\": [\"image.iso\"], \"vagrantfile_template\": \"vagrantfile.tpl\", \"output\": \"proxycore_{{.Provider}}.box\" }, { \"type\": \"vagrant-cloud\", \"box_tag\": \"hashicorp/precise64\", \"access_token\": \"{{user `cloud_token`}}\", \"version\": \"{{user `version`}}\", \"architecture\": \"{{user `architecture`}}\" } ] ] } \nbuild { sources = [\"source.null.autogenerated_1\"] post-processor \"shell-local\" { inline = [\"echo Doing stuff...\"] } post-processors { post-processor \"vagrant\" { include = [\"image.iso\"] output = \"proxycore_{{.Provider}}.box\" vagrantfile_template = \"vagrantfile.tpl\" } post-processor \"vagrant-cloud\" { access_token = \"${var.cloud_token}\" box_tag = \"hashicorp/precise64\" version = \"${local.version}\" architecture = \"${local.architecture}\" } } } \n{ \"variables\": { \"cloud_token\": \"{{ env `VAGRANT_CLOUD_TOKEN` }}\" }, \"builders\": [ { \"type\": \"null\", \"communicator\": \"none\" } ], \"post-processors\": [ { \"type\": \"shell-local\", \"inline\": [\"echo Doing stuff...\"] }, [ { \"type\": \"artifice\", \"files\": [\"./path/to/my.box\"] }, { \"type\": \"vagrant-cloud\", \"box_tag\": \"myorganisation/mybox\", \"access_token\": \"{{user `cloud_token`}}\", \"version\": \"0.1.0\", \"architecture\": \"amd64\" } ] ] } \nvariable \"cloud_token\" { type = string default = \"${env(\"VAGRANT_CLOUD_TOKEN\")}\" } source \"null\" \"autogenerated_1\" { communicator = \"none\" } build { sources = [\"source.null.autogenerated_1\"] post-processor \"shell-local\" { inline = [\"echo Doing stuff...\"] } post-processors { post-processor \"artifice\" { files = [\"./path/to/my.box\"] } post-processor \"vagrant-cloud\" { access_token = \"${var.cloud_token}\" box_tag = \"myorganisation/mybox\" version = \"0.1.0\" architecture = \"amd64\" } } }"
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vagrant/v1.0.3/components/post-processor/vagrant-cloud",
  "text": "Vagrant Post-Processor (1.0.3) | Integrations | Packer\nvagrant_cloud_url (string) - Override the base URL for Vagrant Cloud. This is useful if you're using Vagrant Private Cloud in your own network. Defaults to https://vagrantcloud.com/api/v1. If this value is set to something other than the default then access_token can be left blank and no Authorization header will be added to requests sent by this post-processor.\nno_release (string) - If set to true, does not release the version on Vagrant Cloud, making it active. You can manually release the version via the API or Web UI. Defaults to false.\n{ \"variables\": { \"cloud_token\": \"{{ env `VAGRANT_CLOUD_TOKEN` }}\", \"version\": \"1.0.{{timestamp}}\" }, \"post-processors\": [ { \"type\": \"shell-local\", \"inline\": [\"echo Doing stuff...\"] }, [ { \"type\": \"vagrant\", \"include\": [\"image.iso\"], \"vagrantfile_template\": \"vagrantfile.tpl\", \"output\": \"proxycore_{{.Provider}}.box\" }, { \"type\": \"vagrant-cloud\", \"box_tag\": \"hashicorp/precise64\", \"access_token\": \"{{user `cloud_token`}}\", \"version\": \"{{user `version`}}\" } ] ] } \nbuild { sources = [\"source.null.autogenerated_1\"] post-processor \"shell-local\" { inline = [\"echo Doing stuff...\"] } post-processors { post-processor \"vagrant\" { include = [\"image.iso\"] output = \"proxycore_{{.Provider}}.box\" vagrantfile_template = \"vagrantfile.tpl\" } post-processor \"vagrant-cloud\" { access_token = \"${var.cloud_token}\" box_tag = \"hashicorp/precise64\" version = \"${local.version}\" } } } \n{ \"variables\": { \"cloud_token\": \"{{ env `VAGRANT_CLOUD_TOKEN` }}\" }, \"builders\": [ { \"type\": \"null\", \"communicator\": \"none\" } ], \"post-processors\": [ { \"type\": \"shell-local\", \"inline\": [\"echo Doing stuff...\"] }, [ { \"type\": \"artifice\", \"files\": [\"./path/to/my.box\"] }, { \"type\": \"vagrant-cloud\", \"box_tag\": \"myorganisation/mybox\", \"access_token\": \"{{user `cloud_token`}}\", \"version\": \"0.1.0\" } ] ] } \nvariable \"cloud_token\" { type = string default = \"${env(\"VAGRANT_CLOUD_TOKEN\")}\" } source \"null\" \"autogenerated_1\" { communicator = \"none\" } build { sources = [\"source.null.autogenerated_1\"] post-processor \"shell-local\" { inline = [\"echo Doing stuff...\"] } post-processors { post-processor \"artifice\" { files = [\"./path/to/my.box\"] } post-processor \"vagrant-cloud\" { access_token = \"${var.cloud_token}\" box_tag = \"myorganisation/mybox\" version = \"0.1.0\" } } }"
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vagrant/v1.1.2/components/post-processor/vagrant-registry",
  "text": "Packer | HashiCorp Developer\nThis page does not exist for version v1.1.2."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vagrant/v1.1.1/components/post-processor/vagrant-registry",
  "text": "Packer | HashiCorp Developer\nThis page does not exist for version v1.1.1."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vagrant/v1.0.3",
  "text": "Vagrant (v1.0.3) | Integrations | Packer\npacker { required_plugins { vagrant = { version = \"~> 1\" source = \"github.com/hashicorp/vagrant\" } } } \npacker plugins install github.com/hashicorp/vagrant \nvagrant - The Vagrant builder is intended for building new boxes from already-existing boxes.\nPost-Processor\nvagrant - The Packer Vagrant post-processor takes a build and converts the artifact into a valid Vagrant box.\nvagrant-cloud - The Vagrant Cloud post-processor enables the upload of Vagrant boxes to Vagrant Cloud."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vagrant/v1.1.0/components/post-processor/vagrant-registry",
  "text": "Packer | HashiCorp Developer\nThis page does not exist for version v1.1.0."
},
{
  "url": "https://developer.hashicorp.com/docs/post-processor/vagrant.mdx",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/docs/builder/vagrant.mdx",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vagrant/v1.0.3/components/post-processor/vagrant-registry",
  "text": "This page does not exist for version v1.0.3."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vagrant/v1.0.0/components/post-processor/vagrant-registry",
  "text": "This page does not exist for version v1.0.0."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.8.x/communicators/winrm",
  "text": "Communicators - Templates | Packer\nCommunicators are the mechanism Packer uses to upload files, execute scripts, etc. with the machine being created. The WinRM communicator uses the Windows Remote Management protocol to do this.\nThe WinRM communicator is not the default communicator, so you will always have to set the \"communicator\": \"winrm\", template option explicitly. In addition, you will almost always have to provide a pre-run script that enables and configures WinRM on the guest machine. This will generally be in the form of a PowerShell script or a batch file.\nIf you are building from a brand-new and unconfigured operating system image, you will need to provide this pre-run script as part of your Autounattend.xml file, required by Windows for automatic operating system installation. If you are building in a cloud or from a pre-installed image, your method for providing this pre-run script will vary based on the builder. Please refer to each builder's documentation for more information on how to supply the winrm configuration script.\nIf you are unfamiliar with how to use an autounattend file, take a look at our quick guides; knowing how to automatically initalize your operating system is critical for being able to successfully use Packer to build from an iso.\nBasics of WinRM Connection\nPlease note that WinRM is not a Packer-specific protocol. Microsoft has a great deal of documentation about WinRM. If you find after reading this guide that you are still not able to connect via WinRM, check the Microsoft documentation to make sure there isn't anything you're missing.\nThere are some steps that you will normally need to take in order for Packer to be able to connect via WinRM\nSet up a username and password that Packer to connect with.\nMake any necesary registry edits to enable remote execution (and remote execution with elevated privileges, if needed)\nStart WinRM, setting any config needed for allowing basic auth\nOpen ports 5985 and/or 5986 depending on how you're connecting\nlaunch WinRM and set it to automatically launch when the computer restarts\nIf necessary, generate a self-signed certificate or provide a real certificate to the WinRM listener.\nConfiguring WinRM in VMware\nIf you are configuring WinRM using an Autounattend.xml, the simplest way to set up WinRM is to put the configuration commands directly into the Autounattend file as shown here\nInstead of entering each line individually, you can also add a batch file to your autounattend that contains the commands for configuring winrm. Depending on your winrm setup, this could be a complex batch file, or a very simple one.\nBelow is an example of how we would call a batch file from inside the Autounattend file.\n<FirstLogonCommands> ... <SynchronousCommand wcm:action=\"add\"> <CommandLine>cmd.exe /c a:\\winrmConfig.bat</CommandLine> <Description>Configure WinRM</Description> <Order>3</Order> <RequiresUserInput>true</RequiresUserInput> </SynchronousCommand> ... </FirstLogonCommands> \nIt is also possible to call PowerShell scripts in a similar manner.\nThe winrmConfig.bat referenced above can be as simple as\nrem basic config for winrm cmd.exe /c winrm quickconfig -q rem allow unencrypted traffic, and configure auth to use basic username/password auth cmd.exe /c winrm set winrm/config/service @{AllowUnencrypted=\"true\"} cmd.exe /c winrm set winrm/config/service/auth @{Basic=\"true\"} rem update firewall rules to open the right port and to allow remote administration cmd.exe /c netsh advfirewall firewall set rule group=\"remote administration\" new enable=yes rem restart winrm cmd.exe /c net stop winrm cmd.exe /c net start winrm \nPlease note that the above batch file is extremely simplistic, and not secure. It is intended to be an example of the bare minimum configuration. Below, you'll find a more complicated example of a more secure WinRM configuration process.\nThis batch file will only work for HTTP connections, not HTTPS, but will enable you to connect using only the username and password created earlier in the Autounattend file. The above batchfile will allow you to connect using a very simple Packer config:\n\"communicator\": \"winrm\", \"winrm_username\": \"packeruser\", \"winrm_password\": \"SecretPassword\" \nA more complex example of a PowerShell script used for configuration can be seen below.\n# A Packer config that works with this example would be: # # # \"winrm_username\": \"Administrator\", # \"winrm_password\": \"SuperS3cr3t!!!\", # \"winrm_insecure\": true, # \"winrm_use_ssl\": true # # # Create username and password net user Administrator SuperS3cr3t!!! wmic useraccount where \"name='Administrator'\" set PasswordExpires=FALSE Set-ExecutionPolicy Unrestricted -Scope LocalMachine -Force -ErrorAction Ignore # Don't set this before Set-ExecutionPolicy as it throws an error $ErrorActionPreference = \"stop\" # Remove HTTP listener Remove-Item -Path WSMan:\\Localhost\\listener\\listener* -Recurse # Create a self-signed certificate to let ssl work $Cert = New-SelfSignedCertificate -CertstoreLocation Cert:\\LocalMachine\\My -DnsName \"packer\" New-Item -Path WSMan:\\LocalHost\\Listener -Transport HTTPS -Address * -CertificateThumbPrint $Cert.Thumbprint -Force # WinRM write-output \"Setting up WinRM\" write-host \"(host) setting up WinRM\" # Configure WinRM to allow unencrypted communication, and provide the # self-signed cert to the WinRM listener. cmd.exe /c winrm quickconfig -q cmd.exe /c winrm set \"winrm/config/service\" '@{AllowUnencrypted=\"true\"}' cmd.exe /c winrm set \"winrm/config/client\" '@{AllowUnencrypted=\"true\"}' cmd.exe /c winrm set \"winrm/config/service/auth\" '@{Basic=\"true\"}' cmd.exe /c winrm set \"winrm/config/client/auth\" '@{Basic=\"true\"}' cmd.exe /c winrm set \"winrm/config/service/auth\" '@{CredSSP=\"true\"}' cmd.exe /c winrm set \"winrm/config/listener?Address=*+Transport=HTTPS\" \"@{Port=`\"5986`\";Hostname=`\"packer`\";CertificateThumbprint=`\"$($Cert.Thumbprint)`\"}\" # Make sure appropriate firewall port openings exist cmd.exe /c netsh advfirewall firewall set rule group=\"remote administration\" new enable=yes cmd.exe /c netsh firewall add portopening TCP 5986 \"Port 5986\" # Restart WinRM, and set it so that it auto-launches on startup. cmd.exe /c net stop winrm cmd.exe /c sc config winrm start= auto cmd.exe /c net start winrm \nPlease note that having WinRM auto-launch on all start ups may not be the right choice for you, if you don't need the server to recieve WinRM connections in the future. Clean up after yourself and close unnecesary firewall ports at a final provisioning step to make sure your image is secure.\nConfiguring WinRM in the Cloud\nMost clouds allow you to provide a configuration script that runs when the instance is launched. In AWS, this is the user_data_file. In Google Cloud, this is provided using the windows-startup-script-cmd metadata tag. Example\nEssentially, these files are powershell or cmd scripts that configure winrm, without having to be wrapped in an Autounattend. Provide the script in the format requested by each cloud, and make sure you manually configure any firewall rules that the cloud doesn't allow you to manage internally. More specific details for each cloud can be found in the builder sections.\nThe above examples will work in cloud prep too, but may be overkill depending on how much preconfiguration the cloud has done for you."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.9.x/communicators/winrm",
  "text": "Communicators - Templates | Packer\nCommunicators are the mechanism Packer uses to upload files, execute scripts, etc. with the machine being created. The WinRM communicator uses the Windows Remote Management protocol to do this.\nThe WinRM communicator is not the default communicator, so you will always have to set the \"communicator\": \"winrm\", template option explicitly. In addition, you will almost always have to provide a pre-run script that enables and configures WinRM on the guest machine. This will generally be in the form of a PowerShell script or a batch file.\nIf you are building from a brand-new and unconfigured operating system image, you will need to provide this pre-run script as part of your Autounattend.xml file, required by Windows for automatic operating system installation. If you are building in a cloud or from a pre-installed image, your method for providing this pre-run script will vary based on the builder. Please refer to each builder's documentation for more information on how to supply the winrm configuration script.\nIf you are unfamiliar with how to use an autounattend file, take a look at our quick guides; knowing how to automatically initalize your operating system is critical for being able to successfully use Packer to build from an iso.\nBasics of WinRM Connection\nPlease note that WinRM is not a Packer-specific protocol. Microsoft has a great deal of documentation about WinRM. If you find after reading this guide that you are still not able to connect via WinRM, check the Microsoft documentation to make sure there isn't anything you're missing.\nThere are some steps that you will normally need to take in order for Packer to be able to connect via WinRM\nSet up a username and password that Packer to connect with.\nMake any necesary registry edits to enable remote execution (and remote execution with elevated privileges, if needed)\nStart WinRM, setting any config needed for allowing basic auth\nOpen ports 5985 and/or 5986 depending on how you're connecting\nlaunch WinRM and set it to automatically launch when the computer restarts\nIf necessary, generate a self-signed certificate or provide a real certificate to the WinRM listener.\nConfiguring WinRM in VMware\nIf you are configuring WinRM using an Autounattend.xml, the simplest way to set up WinRM is to put the configuration commands directly into the Autounattend file as shown here\nInstead of entering each line individually, you can also add a batch file to your autounattend that contains the commands for configuring winrm. Depending on your winrm setup, this could be a complex batch file, or a very simple one.\nBelow is an example of how we would call a batch file from inside the Autounattend file.\n<FirstLogonCommands> ... <SynchronousCommand wcm:action=\"add\"> <CommandLine>cmd.exe /c a:\\winrmConfig.bat</CommandLine> <Description>Configure WinRM</Description> <Order>3</Order> <RequiresUserInput>true</RequiresUserInput> </SynchronousCommand> ... </FirstLogonCommands> \nIt is also possible to call PowerShell scripts in a similar manner.\nThe winrmConfig.bat referenced above can be as simple as\nrem basic config for winrm cmd.exe /c winrm quickconfig -q rem allow unencrypted traffic, and configure auth to use basic username/password auth cmd.exe /c winrm set winrm/config/service @{AllowUnencrypted=\"true\"} cmd.exe /c winrm set winrm/config/service/auth @{Basic=\"true\"} rem update firewall rules to open the right port and to allow remote administration cmd.exe /c netsh advfirewall firewall set rule group=\"remote administration\" new enable=yes rem restart winrm cmd.exe /c net stop winrm cmd.exe /c net start winrm \nPlease note that the above batch file is extremely simplistic, and not secure. It is intended to be an example of the bare minimum configuration. Below, you'll find a more complicated example of a more secure WinRM configuration process.\nThis batch file will only work for HTTP connections, not HTTPS, but will enable you to connect using only the username and password created earlier in the Autounattend file. The above batchfile will allow you to connect using a very simple Packer config:\n\"communicator\": \"winrm\", \"winrm_username\": \"packeruser\", \"winrm_password\": \"SecretPassword\" \nA more complex example of a PowerShell script used for configuration can be seen below.\n# A Packer config that works with this example would be: # # # \"winrm_username\": \"Administrator\", # \"winrm_password\": \"SuperS3cr3t!!!\", # \"winrm_insecure\": true, # \"winrm_use_ssl\": true # # # Create username and password net user Administrator SuperS3cr3t!!! wmic useraccount where \"name='Administrator'\" set PasswordExpires=FALSE Set-ExecutionPolicy Unrestricted -Scope LocalMachine -Force -ErrorAction Ignore # Don't set this before Set-ExecutionPolicy as it throws an error $ErrorActionPreference = \"stop\" # Remove HTTP listener Remove-Item -Path WSMan:\\Localhost\\listener\\listener* -Recurse # Create a self-signed certificate to let ssl work $Cert = New-SelfSignedCertificate -CertstoreLocation Cert:\\LocalMachine\\My -DnsName \"packer\" New-Item -Path WSMan:\\LocalHost\\Listener -Transport HTTPS -Address * -CertificateThumbPrint $Cert.Thumbprint -Force # WinRM write-output \"Setting up WinRM\" write-host \"(host) setting up WinRM\" # Configure WinRM to allow unencrypted communication, and provide the # self-signed cert to the WinRM listener. cmd.exe /c winrm quickconfig -q cmd.exe /c winrm set \"winrm/config/service\" '@{AllowUnencrypted=\"true\"}' cmd.exe /c winrm set \"winrm/config/client\" '@{AllowUnencrypted=\"true\"}' cmd.exe /c winrm set \"winrm/config/service/auth\" '@{Basic=\"true\"}' cmd.exe /c winrm set \"winrm/config/client/auth\" '@{Basic=\"true\"}' cmd.exe /c winrm set \"winrm/config/service/auth\" '@{CredSSP=\"true\"}' cmd.exe /c winrm set \"winrm/config/listener?Address=*+Transport=HTTPS\" \"@{Port=`\"5986`\";Hostname=`\"packer`\";CertificateThumbprint=`\"$($Cert.Thumbprint)`\"}\" # Make sure appropriate firewall port openings exist cmd.exe /c netsh advfirewall firewall set rule group=\"remote administration\" new enable=yes cmd.exe /c netsh firewall add portopening TCP 5986 \"Port 5986\" # Restart WinRM, and set it so that it auto-launches on startup. cmd.exe /c net stop winrm cmd.exe /c sc config winrm start= auto cmd.exe /c net start winrm \nPlease note that having WinRM auto-launch on all start ups may not be the right choice for you, if you don't need the server to recieve WinRM connections in the future. Clean up after yourself and close unnecesary firewall ports at a final provisioning step to make sure your image is secure.\nConfiguring WinRM in the Cloud\nMost clouds allow you to provide a configuration script that runs when the instance is launched. In AWS, this is the user_data_file. In Google Cloud, this is provided using the windows-startup-script-cmd metadata tag. Example\nEssentially, these files are powershell or cmd scripts that configure winrm, without having to be wrapped in an Autounattend. Provide the script in the format requested by each cloud, and make sure you manually configure any firewall rules that the cloud doesn't allow you to manage internally. More specific details for each cloud can be found in the builder sections.\nThe above examples will work in cloud prep too, but may be overkill depending on how much preconfiguration the cloud has done for you."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.10.x/communicators/winrm",
  "text": "Communicators - Templates | Packer\nCommunicators are the mechanism Packer uses to upload files, execute scripts, etc. with the machine being created. The WinRM communicator uses the Windows Remote Management protocol to do this.\nThe WinRM communicator is not the default communicator, so you will always have to set the \"communicator\": \"winrm\", template option explicitly. In addition, you will almost always have to provide a pre-run script that enables and configures WinRM on the guest machine. This will generally be in the form of a PowerShell script or a batch file.\nIf you are building from a brand-new and unconfigured operating system image, you will need to provide this pre-run script as part of your Autounattend.xml file, required by Windows for automatic operating system installation. If you are building in a cloud or from a pre-installed image, your method for providing this pre-run script will vary based on the builder. Please refer to each builder's documentation for more information on how to supply the winrm configuration script.\nIf you are unfamiliar with how to use an autounattend file, take a look at our quick guides; knowing how to automatically initalize your operating system is critical for being able to successfully use Packer to build from an iso.\nBasics of WinRM Connection\nPlease note that WinRM is not a Packer-specific protocol. Microsoft has a great deal of documentation about WinRM. If you find after reading this guide that you are still not able to connect via WinRM, check the Microsoft documentation to make sure there isn't anything you're missing.\nThere are some steps that you will normally need to take in order for Packer to be able to connect via WinRM\nSet up a username and password that Packer to connect with.\nMake any necesary registry edits to enable remote execution (and remote execution with elevated privileges, if needed)\nStart WinRM, setting any config needed for allowing basic auth\nOpen ports 5985 and/or 5986 depending on how you're connecting\nlaunch WinRM and set it to automatically launch when the computer restarts\nIf necessary, generate a self-signed certificate or provide a real certificate to the WinRM listener.\nConfiguring WinRM in VMware\nIf you are configuring WinRM using an Autounattend.xml, the simplest way to set up WinRM is to put the configuration commands directly into the Autounattend file as shown here\nInstead of entering each line individually, you can also add a batch file to your autounattend that contains the commands for configuring winrm. Depending on your winrm setup, this could be a complex batch file, or a very simple one.\nBelow is an example of how we would call a batch file from inside the Autounattend file.\n<FirstLogonCommands> ... <SynchronousCommand wcm:action=\"add\"> <CommandLine>cmd.exe /c a:\\winrmConfig.bat</CommandLine> <Description>Configure WinRM</Description> <Order>3</Order> <RequiresUserInput>true</RequiresUserInput> </SynchronousCommand> ... </FirstLogonCommands> \nIt is also possible to call PowerShell scripts in a similar manner.\nThe winrmConfig.bat referenced above can be as simple as\nrem basic config for winrm cmd.exe /c winrm quickconfig -q rem allow unencrypted traffic, and configure auth to use basic username/password auth cmd.exe /c winrm set winrm/config/service @{AllowUnencrypted=\"true\"} cmd.exe /c winrm set winrm/config/service/auth @{Basic=\"true\"} rem update firewall rules to open the right port and to allow remote administration cmd.exe /c netsh advfirewall firewall set rule group=\"remote administration\" new enable=yes rem restart winrm cmd.exe /c net stop winrm cmd.exe /c net start winrm \nPlease note that the above batch file is extremely simplistic, and not secure. It is intended to be an example of the bare minimum configuration. Below, you'll find a more complicated example of a more secure WinRM configuration process.\nThis batch file will only work for HTTP connections, not HTTPS, but will enable you to connect using only the username and password created earlier in the Autounattend file. The above batchfile will allow you to connect using a very simple Packer config:\n\"communicator\": \"winrm\", \"winrm_username\": \"packeruser\", \"winrm_password\": \"SecretPassword\" \nA more complex example of a PowerShell script used for configuration can be seen below.\n# A Packer config that works with this example would be: # # # \"winrm_username\": \"Administrator\", # \"winrm_password\": \"SuperS3cr3t!!!\", # \"winrm_insecure\": true, # \"winrm_use_ssl\": true # # # Create username and password net user Administrator SuperS3cr3t!!! wmic useraccount where \"name='Administrator'\" set PasswordExpires=FALSE Set-ExecutionPolicy Unrestricted -Scope LocalMachine -Force -ErrorAction Ignore # Don't set this before Set-ExecutionPolicy as it throws an error $ErrorActionPreference = \"stop\" # Remove HTTP listener Remove-Item -Path WSMan:\\Localhost\\listener\\listener* -Recurse # Create a self-signed certificate to let ssl work $Cert = New-SelfSignedCertificate -CertstoreLocation Cert:\\LocalMachine\\My -DnsName \"packer\" New-Item -Path WSMan:\\LocalHost\\Listener -Transport HTTPS -Address * -CertificateThumbPrint $Cert.Thumbprint -Force # WinRM write-output \"Setting up WinRM\" write-host \"(host) setting up WinRM\" # Configure WinRM to allow unencrypted communication, and provide the # self-signed cert to the WinRM listener. cmd.exe /c winrm quickconfig -q cmd.exe /c winrm set \"winrm/config/service\" '@{AllowUnencrypted=\"true\"}' cmd.exe /c winrm set \"winrm/config/client\" '@{AllowUnencrypted=\"true\"}' cmd.exe /c winrm set \"winrm/config/service/auth\" '@{Basic=\"true\"}' cmd.exe /c winrm set \"winrm/config/client/auth\" '@{Basic=\"true\"}' cmd.exe /c winrm set \"winrm/config/service/auth\" '@{CredSSP=\"true\"}' cmd.exe /c winrm set \"winrm/config/listener?Address=*+Transport=HTTPS\" \"@{Port=`\"5986`\";Hostname=`\"packer`\";CertificateThumbprint=`\"$($Cert.Thumbprint)`\"}\" # Make sure appropriate firewall port openings exist cmd.exe /c netsh advfirewall firewall set rule group=\"remote administration\" new enable=yes cmd.exe /c netsh firewall add portopening TCP 5986 \"Port 5986\" # Restart WinRM, and set it so that it auto-launches on startup. cmd.exe /c net stop winrm cmd.exe /c sc config winrm start= auto cmd.exe /c net start winrm \nPlease note that having WinRM auto-launch on all start ups may not be the right choice for you, if you don't need the server to recieve WinRM connections in the future. Clean up after yourself and close unnecesary firewall ports at a final provisioning step to make sure your image is secure.\nConfiguring WinRM in the Cloud\nMost clouds allow you to provide a configuration script that runs when the instance is launched. In AWS, this is the user_data_file. In Google Cloud, this is provided using the windows-startup-script-cmd metadata tag. Example\nEssentially, these files are powershell or cmd scripts that configure winrm, without having to be wrapped in an Autounattend. Provide the script in the format requested by each cloud, and make sure you manually configure any firewall rules that the cloud doesn't allow you to manage internally. More specific details for each cloud can be found in the builder sections.\nThe above examples will work in cloud prep too, but may be overkill depending on how much preconfiguration the cloud has done for you."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/communicators/winrm",
  "text": "Communicators - Templates | Packer\nCommunicators are the mechanism Packer uses to upload files, execute scripts, etc. with the machine being created. The WinRM communicator uses the Windows Remote Management protocol to do this.\nThe WinRM communicator is not the default communicator, so you will always have to set the \"communicator\": \"winrm\", template option explicitly. In addition, you will almost always have to provide a pre-run script that enables and configures WinRM on the guest machine. This will generally be in the form of a powershell script or a batch file.\nIf you are building from a brand-new and unconfigured operating system image, you will need to provide this pre-run script as part of your Autounattend.xml file, required by Windows for automatic operating system installation. If you are building in a cloud or from a pre-installed image, your method for providing this pre-run script will vary based on the builder. Please refer to each builder's documentation for more information on how to supply the winrm configuration script.\nIf you are unfamiliar with how to use an autounattend file, take a look at our quick guides; knowing how to automatically initalize your operating system is critical for being able to successfully use Packer to build from an iso.\nBasics of WinRM Connection\nPlease note that WinRM is not a Packer-specific protocol. Microsoft has a great deal of documentation about WinRM. If you find after reading this guide that you are still not able to connect via WinRM, check the Microsoft documentation to make sure there isn't anything you're missing.\nThere are some steps that you will normally need to take in order for Packer to be able to connect via WinRM\nSet up a username and password that Packer to connect with.\nMake any necesary registry edits to enable remote execution (and remote execution with elevated privileges, if needed)\nStart WinRM, setting any config needed for allowing basic auth\nOpen ports 5985 and/or 5986 depending on how you're connecting\nlaunch WinRM and set it to automatically launch when the computer restarts\nIf necessary, generate a self-signed certificate or provide a real certificate to the WinRM listener.\nConfiguring WinRM in VMWare\nIf you are configuring WinRM using an Autounattend.xml, the simplest way to set up WinRM is to put the configuration commands directly into the Autounattend file as shown here\nInstead of entering each line individually, you can also add a batch file to your autounattend that contains the commands for configuring winrm. Depending on your winrm setup, this could be a complex batch file, or a very simple one.\nBelow is an example of how we would call a batch file from inside the Autounattend file.\n<FirstLogonCommands> ... <SynchronousCommand wcm:action=\"add\"> <CommandLine>cmd.exe /c a:\\winrmConfig.bat</CommandLine> <Description>Configure WinRM</Description> <Order>3</Order> <RequiresUserInput>true</RequiresUserInput> </SynchronousCommand> ... </FirstLogonCommands> \nIt is also possible to call powershell scripts in a similar manner.\nThe winrmConfig.bat referenced above can be as simple as\nrem basic config for winrm cmd.exe /c winrm quickconfig -q rem allow unencrypted traffic, and configure auth to use basic username/password auth cmd.exe /c winrm set winrm/config/service @{AllowUnencrypted=\"true\"} cmd.exe /c winrm set winrm/config/service/auth @{Basic=\"true\"} rem update firewall rules to open the right port and to allow remote administration cmd.exe /c netsh advfirewall firewall set rule group=\"remote administration\" new enable=yes rem restart winrm cmd.exe /c net stop winrm cmd.exe /c net start winrm \nPlease note that the above batch file is extremely simplistic, and not secure. It is intended to be an example of the bare minimum configuration. Below, you'll find a more complicated example of a more secure WinRM configuration process.\nThis batch file will only work for http connections, not https, but will enable you to connect using only the username and password created earlier in the Autounattend file. The above batchfile will allow you to connect using a very simple Packer config:\n\"communicator\": \"winrm\", \"winrm_username\": \"packeruser\", \"winrm_password\": \"SecretPassword\" \nA more complex example of a powershell script used for configuration can be seen below.\n# A Packer config that works with this example would be: # # # \"winrm_username\": \"Administrator\", # \"winrm_password\": \"SuperS3cr3t!!!\", # \"winrm_insecure\": true, # \"winrm_use_ssl\": true # # # Create username and password net user Administrator SuperS3cr3t!!! wmic useraccount where \"name='Administrator'\" set PasswordExpires=FALSE Set-ExecutionPolicy Unrestricted -Scope LocalMachine -Force -ErrorAction Ignore # Don't set this before Set-ExecutionPolicy as it throws an error $ErrorActionPreference = \"stop\" # Remove HTTP listener Remove-Item -Path WSMan:\\Localhost\\listener\\listener* -Recurse # Create a self-signed certificate to let ssl work $Cert = New-SelfSignedCertificate -CertstoreLocation Cert:\\LocalMachine\\My -DnsName \"packer\" New-Item -Path WSMan:\\LocalHost\\Listener -Transport HTTPS -Address * -CertificateThumbPrint $Cert.Thumbprint -Force # WinRM write-output \"Setting up WinRM\" write-host \"(host) setting up WinRM\" # Configure WinRM to allow unencrypted communication, and provide the # self-signed cert to the WinRM listener. cmd.exe /c winrm quickconfig -q cmd.exe /c winrm set \"winrm/config/service\" '@{AllowUnencrypted=\"true\"}' cmd.exe /c winrm set \"winrm/config/client\" '@{AllowUnencrypted=\"true\"}' cmd.exe /c winrm set \"winrm/config/service/auth\" '@{Basic=\"true\"}' cmd.exe /c winrm set \"winrm/config/client/auth\" '@{Basic=\"true\"}' cmd.exe /c winrm set \"winrm/config/service/auth\" '@{CredSSP=\"true\"}' cmd.exe /c winrm set \"winrm/config/listener?Address=*+Transport=HTTPS\" \"@{Port=`\"5986`\";Hostname=`\"packer`\";CertificateThumbprint=`\"$($Cert.Thumbprint)`\"}\" # Make sure appropriate firewall port openings exist cmd.exe /c netsh advfirewall firewall set rule group=\"remote administration\" new enable=yes cmd.exe /c netsh firewall add portopening TCP 5986 \"Port 5986\" # Restart WinRM, and set it so that it auto-launches on startup. cmd.exe /c net stop winrm cmd.exe /c sc config winrm start= auto cmd.exe /c net start winrm \nPlease note that having WinRM auto-launch on all start ups may not be the right choice for you, if you don't need the server to recieve WinRM connections in the future. Clean up after yourself and close unnecesary firewall ports at a final provisioning step to make sure your image is secure.\nConfiguring WinRM in the Cloud\nMost clouds allow you to provide a configuration script that runs when the instance is launched. In AWS, this is the user_data_file. In Google Cloud, this is provided using the windows-startup-script-cmd metadata tag. Example\nEssentially, these files are powershell or cmd scripts that configure winrm, without having to be wrapped in an Autounattend. Provide the script in the format requested by each cloud, and make sure you manually configure any firewall rules that the cloud doesn't allow you to manage internally. More specific details for each cloud can be found in the builder sections.\nThe above examples will work in cloud prep too, but may be overkill depending on how much preconfiguration the cloud has done for you."
},
{
  "url": "https://developer.hashicorp.com/packer/plugins/builders/googlecompute",
  "text": "Google Cloud Platform Builder | Integrations | Packer\nGoogle Cloud Platform\n@hashicorp\nThe googlecompute plugin can be used with HashiCorp Packer to create custom images on GCE.\nType: googlecompute Artifact BuilderId: packer.googlecompute\nThe googlecompute Packer builder is able to create images for use with Google Compute Engine (GCE) based on existing images.\nIt is possible to build images from scratch, but not with the googlecompute Packer builder. The process is recommended only for advanced users, please see Building GCE Images from Scratch and the Google Compute Import Post-Processor for more information.\nAuthentication\nTo authenticate with GCE, this builder supports everything the plugin does. To get more information on this, refer to the plugin's description page, under the authentication section.\nIn addition to the options listed here, a communicator can be configured for this builder.\nproject_id (string) - The project ID that will be used to launch instances and store images.\nsource_image (string) - The source image to use to create the new image from. You can also specify source_image_family instead. If both source_image and source_image_family are specified, source_image takes precedence. Example: \"debian-8-jessie-v20161027\"\nsource_image_family (string) - The source image family to use to create the new image from. The image family always returns its latest image that is not deprecated. Example: \"debian-8\".\nzone (string) - The zone in which to launch the instance used to create the image. Example: \"us-central1-a\"\naccelerator_type (string) - Full or partial URL of the guest accelerator type. GPU accelerators can only be used with \"on_host_maintenance\": \"TERMINATE\" option set. Example: \"projects/project_id/zones/europe-west1-b/acceleratorTypes/nvidia-tesla-k80\"\naccelerator_count (int64) - Number of guest accelerator cards to add to the launched instance.\naddress (string) - The name of a pre-allocated static external IP address. Note, must be the name and not the actual IP address.\ndisable_default_service_account (bool) - If true, the default service account will not be used if service_account_email is not specified. Set this value to true and omit service_account_email to provision a VM with no service account.\ndisk_name (string) - The name of the disk, if unset the instance name will be used.\ndisk_size (int64) - The size of the disk in GB. This defaults to 20, which is 20GB.\ndisk_type (string) - Type of disk used to back your instance, like pd-ssd or pd-standard. Defaults to pd-standard.\ndisk_encryption_key (*common.CustomerEncryptionKey) - Disk encryption key to apply to the created boot disk. Possible values:\nkmsKeyName - The name of the encryption key that is stored in Google Cloud KMS.\nRawKey: - A 256-bit customer-supplied encryption key, encodes in RFC 4648 base64.\nexamples:\n{ \"kmsKeyName\": \"projects/${project}/locations/${region}/keyRings/computeEngine/cryptoKeys/computeEngine/cryptoKeyVersions/4\" } \ndisk_encryption_key { kmsKeyName = \"projects/${var.project}/locations/${var.region}/keyRings/computeEngine/cryptoKeys/computeEngine/cryptoKeyVersions/4\" } \nRefer to the Customer Encryption Key section for more information on the contents of this block.\nenable_nested_virtualization (bool) - Create a instance with enabling nested virtualization.\nenable_secure_boot (bool) - Create a Shielded VM image with Secure Boot enabled. It helps ensure that the system only runs authentic software by verifying the digital signature of all boot components, and halting the boot process if signature verification fails. Details\nenable_vtpm (bool) - Create a Shielded VM image with virtual trusted platform module Measured Boot enabled. A vTPM is a virtualized trusted platform module, which is a specialized computer chip you can use to protect objects, like keys and certificates, that you use to authenticate access to your system. Details\nenable_integrity_monitoring (bool) - Integrity monitoring helps you understand and make decisions about the state of your VM instances. Note: integrity monitoring relies on having vTPM enabled. Details\ndisk_attachment ([]common.BlockDevice) - Extra disks to attach to the instance that will build the final image.\nYou may reference an existing external persistent disk, or you can configure a set of disks to be created before the instance is created, and will be deleted upon build completion.\nScratch (ephemeral) SSDs are always created at launch, and deleted when the instance is torn-down.\nNote: local SSDs are not supported on all machine types, refer to the docs for more information on that.\nRefer to the Extra Disk Attachments section for more information on this configuration type.\nskip_create_image (bool) - Skip creating the image. Useful for setting to true during a build test stage. Defaults to false.\nimage_architecture (string) - The architecture of the resulting image.\nDefaults to unset: GCE will use the origin image architecture.\nimage_name (string) - The unique name of the resulting image. Defaults to packer-{{timestamp}}.\nimage_description (string) - The description of the resulting image.\nimage_encryption_key (*common.CustomerEncryptionKey) - Image encryption key to apply to the created image. Possible values:\nkmsKeyName - The name of the encryption key that is stored in Google Cloud KMS.\nRawKey: - A 256-bit customer-supplied encryption key, encodes in RFC 4648 base64.\nexamples:\n{ \"kmsKeyName\": \"projects/${project}/locations/${region}/keyRings/computeEngine/cryptoKeys/computeEngine/cryptoKeyVersions/4\" } \nimage_encryption_key { kmsKeyName = \"projects/${var.project}/locations/${var.region}/keyRings/computeEngine/cryptoKeys/computeEngine/cryptoKeyVersions/4\" } \nRefer to the Customer Encryption Key section for more information on the contents of this block.\nimage_family (string) - The name of the image family to which the resulting image belongs. You can create disks by specifying an image family instead of a specific image name. The image family always returns its latest image that is not deprecated.\nimage_labels (map[string]string) - Key/value pair labels to apply to the created image.\nimage_licenses ([]string) - Licenses to apply to the created image.\nimage_guest_os_features ([]string) - Guest OS features to apply to the created image.\nimage_project_id (string) - The project ID to push the build image into. Defaults to project_id.\nimage_storage_locations ([]string) - Storage location, either regional or multi-regional, where snapshot content is to be stored and only accepts 1 value. Always defaults to a nearby regional or multi-regional location.\nmulti-regional example:\n{ \"image_storage_locations\": [\"us\"] } \nregional example:\n{ \"image_storage_locations\": [\"us-east1\"] } \ninstance_name (string) - A name to give the launched instance. Beware that this must be unique. Defaults to packer-{{uuid}}.\nlabels (map[string]string) - Key/value pair labels to apply to the launched instance.\nmachine_type (string) - The machine type. Defaults to \"e2-standard-2\".\nmetadata (map[string]string) - Metadata applied to the launched instance. All metadata configuration values are expected to be of type string. Google metadata options that take a value of TRUE or FALSE should be set as a string (i.e \"TRUE\" \"FALSE\" or \"true\" \"false\").\nmetadata_files (map[string]string) - Metadata applied to the launched instance. Values are files.\nmin_cpu_platform (string) - A Minimum CPU Platform for VM Instance. Availability and default CPU platforms vary across zones, based on the hardware available in each GCP zone. Details\nnetwork (string) - The Google Compute network id or URL to use for the launched instance. Defaults to \"default\". If the value is not a URL, it will be interpolated to projects/((network_project_id))/global/networks/((network)). This value is not required if a subnet is specified.\nnetwork_project_id (string) - The project ID for the network and subnetwork to use for launched instance. Defaults to project_id.\nomit_external_ip (bool) - If true, the instance will not have an external IP. use_internal_ip must be true if this property is true.\non_host_maintenance (string) - Sets Host Maintenance Option. Valid choices are MIGRATE and TERMINATE. Please see GCE Instance Scheduling Options, as not all machine_types support MIGRATE (i.e. machines with GPUs). If preemptible is true this can only be TERMINATE. If preemptible is false, it defaults to MIGRATE\npreemptible (bool) - If true, launch a preemptible instance.\nnode_affinity ([]common.NodeAffinity) - Sets a node affinity label for the launched instance (eg. for sole tenancy). Please see Provisioning VMs on sole-tenant nodes for more information.\nkey = \"workload\" operator = \"IN\" values = [\"packer\"] \nRefer to the Node Affinity for more information on affinities.\nstate_timeout (duration string | ex: \"1h5m2s\") - The time to wait for instance state changes. Defaults to \"5m\".\nregion (string) - The region in which to launch the instance. Defaults to the region hosting the specified zone.\nscopes ([]string) - The service account scopes for launched instance. Defaults to:\n[ \"https://www.googleapis.com/auth/userinfo.email\", \"https://www.googleapis.com/auth/compute\", \"https://www.googleapis.com/auth/devstorage.full_control\" ] \nservice_account_email (string) - The service account to be used for launched instance. Defaults to the project's default service account unless disable_default_service_account is true.\nsource_image_project_id ([]string) - A list of project IDs to search for the source image. Packer will search the first project ID in the list first, and fall back to the next in the list, until it finds the source image.\nstartup_script_file (string) - The path to a startup script to run on the launched instance from which the image will be made. When set, the contents of the startup script file will be added to the instance metadata under the \"startup_script\" metadata property. See Providing startup script contents directly for more details.\nWhen using startup_script_file the following rules apply:\nThe contents of the script file will overwrite the value of the \"startup_script\" metadata property at runtime.\nThe contents of the script file will be wrapped in Packer's startup script wrapper, unless wrap_startup_script is disabled. See wrap_startup_script for more details.\nNot supported by Windows instances. See Startup Scripts for Windows for more details.\nwindows_password_timeout (duration string | ex: \"1h5m2s\") - The time to wait for windows password to be retrieved. Defaults to \"3m\".\nwrap_startup_script (boolean) - For backwards compatibility this option defaults to \"true\" in the future it will default to \"false\". If \"true\", the contents of startup_script_file or \"startup_script\" in the instance metadata is wrapped in a Packer specific script that tracks the execution and completion of the provided startup script. The wrapper ensures that the builder will not continue until the startup script has been executed.\nThe use of the wrapped script file requires that the user or service account running the build has the compute.instance.Metadata role.\nsubnetwork (string) - The Google Compute subnetwork id or URL to use for the launched instance. Only required if the network has been created with custom subnetting. Note, the region of the subnetwork must match the region or zone in which the VM is launched. If the value is not a URL, it will be interpolated to projects/((network_project_id))/regions/((region))/subnetworks/((subnetwork))\ntags ([]string) - Assign network tags to apply firewall rules to VM instance.\nuse_internal_ip (bool) - If true, use the instance's internal IP instead of its external IP during building.\nuse_os_login (bool) - If true, OSLogin will be used to manage SSH access to the compute instance by dynamically importing a temporary SSH key to the Google account's login profile, and setting the enable-oslogin to TRUE in the instance metadata. Optionally, use_os_login can be used with an existing ssh_username and ssh_private_key_file if a SSH key has already been added to the Google account's login profile - See Adding SSH Keys.\nSSH keys can be added to an individual user account\n$ gcloud compute os-login ssh-keys add --key-file=/home/user/.ssh/my-key.pub $ gcloud compute os-login describe-profile PosixAccounts: - accountId: <project-id> gid: '34567890754' homeDirectory: /home/user_example_com ... primary: true uid: '2504818925' username: user_example_com sshPublicKeys: 000000000000000000000000000000000000000000000000000000000000000a: fingerprint: 000000000000000000000000000000000000000000000000000000000000000a \nOr SSH keys can be added to an associated service account\n$ gcloud auth activate-service-account --key-file=<path to service account credentials file (e.g account.json)> $ gcloud compute os-login ssh-keys add --key-file=/home/user/.ssh/my-key.pub $ gcloud compute os-login describe-profile PosixAccounts: - accountId: <project-id> gid: '34567890754' homeDirectory: /home/sa_000000000000000000000 ... primary: true uid: '2504818925' username: sa_000000000000000000000 sshPublicKeys: 000000000000000000000000000000000000000000000000000000000000000a: fingerprint: 000000000000000000000000000000000000000000000000000000000000000a \nwait_to_add_ssh_keys (duration string | ex: \"1h5m2s\") - The time to wait between the creation of the instance used to create the image, and the addition of SSH configuration, including SSH keys, to that instance. The delay is intended to protect packer from anything in the instance boot sequence that has potential to disrupt the creation of SSH configuration (e.g. SSH user creation, SSH key creation) on the instance. Note: All other instance metadata, including startup scripts, are still added to the instance during it's creation. Example value: 5m.\nuse_iap (bool) - Whether to use an IAP proxy. Prerequisites and limitations for using IAP:\nYou must manually enable the IAP API in the Google Cloud console.\nYou must have the gcloud sdk installed on the computer running Packer.\nIf you use a service account, you must add it to project level IAP permissions in https://console.cloud.google.com/security/iap. To do so, click \"project\" > \"SSH and TCP resources\" > \"All Tunnel Resources\" > \"Add Member\". Then add your service account and choose the role \"IAP-secured Tunnel User\" and add any conditions you may care about.\niap_localhost_port (int) - Which port to connect the local end of the IAM localhost proxy to. If left blank, Packer will choose a port for you from available ports.\niap_hashbang (string) - What \"hashbang\" to use to invoke script that sets up gcloud. Default: \"/bin/sh\"\niap_ext (string) - What file extension to use for script that sets up gcloud. Default: \".sh\"\niap_tunnel_launch_wait (int) - How long to wait, in seconds, before assuming a tunnel launch was successful. Defaults to 30 seconds for SSH or 40 seconds for WinRM.\nStartup Scripts\nStartup scripts can be a powerful tool for configuring the instance from which the image is made. The builder will wait for a startup script to terminate. A startup script can be provided via the startup_script_file or startup-script instance creation metadata field. Therefore, the build time will vary depending on the duration of the startup script. If startup_script_file is set, the startup-script metadata field will be overwritten. In other words, startup_script_file takes precedence.\nThe builder does check for a pass/fail/error signal from the startup script by tracking the startup-script-status metadata. Packer will check if this key is set to done and if it not set to done before the timeout, Packer will fail the build.\nWindows\nA Windows startup script can only be provided as a metadata field option. The builder will not wait for a Windows startup script to terminate. You have to ensure that it finishes before the instance shuts down. For a list of supported startup script keys refer to Using startup scripts on Windows\nmetadata = { sysprep-specialize-script-cmd = \"...\" } \nLogging\nStartup script logs can be copied to a Google Cloud Storage (GCS) location specified via the startup-script-log-dest instance creation metadata field. The GCS location must be writeable by the service account of the instance that Packer created.\nCommunicator Configuration\nNOTE: Guests using Windows with Win32-OpenSSH v9.1.0.0p1-Beta, scp (the default protocol for copying data) returns a a non-zero error code since the MOTW cannot be set, which cause any file transfer to fail. As a workaround you can override the transfer protocol with SFTP instead ssh_file_transfer_method = \"sftp\".\nTemporary SSH keypair\nWhen no ssh credentials are specified, Packer will generate a temporary SSH keypair for the instance. You can change the algorithm type and bits settings.\ntemporary_key_pair_type (string) - dsa | ecdsa | ed25519 | rsa ( the default )\nSpecifies the type of key to create. The possible values are 'dsa', 'ecdsa', 'ed25519', or 'rsa'.\nNOTE: DSA is deprecated and no longer recognized as secure, please consider other alternatives like RSA or ED25519.\ntemporary_key_pair_bits (int) - Specifies the number of bits in the key to create. For RSA keys, the minimum size is 1024 bits and the default is 4096 bits. Generally, 3072 bits is considered sufficient. DSA keys must be exactly 1024 bits as specified by FIPS 186-2. For ECDSA keys, bits determines the key length by selecting from one of three elliptic curve sizes: 256, 384 or 521 bits. Attempting to use bit lengths other than these three values for ECDSA keys will fail. Ed25519 keys have a fixed length and bits will be ignored.\nNOTE: DSA is deprecated and no longer recognized as secure as specified by FIPS 186-5, please consider other alternatives like RSA or ED25519.\nGotchas\nCentOS and recent Debian images have root ssh access disabled by default. Set ssh_username to any user, which will be created by packer with sudo access.\nThe machine type must have a scratch disk, which means you can't use an f1-micro or g1-small to build images.\nExtra disk attachments\nBlockDevice is a block device attachement/creation to an instance when building an image.\nThese can be defined using the disk_attachment block in the configuration.\nNote that this is an array, and therefore in HCL2 can be defined as multiple blocks, each one corresponding to a disk that will be attached to the instance you are booting.\nExample:\nsource \"googlecompute\" \"example\" { # Add whichever is necessary to build the image disk_attachment { volume_type = \"scratch\" volume_size = 375 } disk_attachment { volume_type = \"pd-standard\" volume_size = 25 interface_type = \"SCSI\" } } \nvolume_size (int) - Size of the volume to request, in gigabytes.\nThe size specified must be in range of the sizes for the chosen volume type.\nvolume_type (BlockDeviceType) - The volume type is the type of storage to reserve and attach to the instance being provisioned.\nThe following values are supported by this builder:\nscratch: local SSD data, always 375 GiB (default)\npd_standard: persistent, HDD-backed disk\npd_balanced: persistent, SSD-backed disk\npd_ssd: persistent, SSD-backed disk, with extra performance guarantees\npd_extreme: persistent, fastest SSD-backed disk, with custom IOPS\nhyperdisk-balanced: persistent hyperdisk volume, bootable\nhyperdisk-extreme: persistent hyperdisk volume, optimised for performance\nhyperdisk-ml: persistent, shareable, hyperdisk volume, highest throughput\nhyperdisk-throughput: persistent hyperdisk volume with flexible throughput\nFor details on the different types, refer to: https://cloud.google.com/compute/docs/disks#disk-types For more information on hyperdisk volumes, refer to: https://cloud.google.com/compute/docs/disks/hyperdisks#throughput\nattachment_mode (string) - How to attach the volume to the instance\nCan be either READ_ONLY or READ_WRITE (default).\ncreate_image (bool) - If true, an image will be created for this disk, instead of the boot disk.\nThis only applies to non-scratch disks, and can only be specified on one disk at a time.\ndevice_name (string) - The device name as exposed to the OS in the /dev/disk/by-id/google-* directory\nIf unspecified, the disk will have a default name in the form persistent-disk-x with 'x' being a number assigned by GCE\nThis field only applies to persistent disks, local SSDs will always be exposed as /dev/disk/by-id/google-local-nvme-ssd-x.\ndisk_encryption_key (CustomerEncryptionKey) - Disk encryption key to apply to the requested disk.\nPossible values:\nkmsKeyName - The name of the encryption key that is stored in Google Cloud KMS.\nRawKey: - A 256-bit customer-supplied encryption key, encodes in RFC 4648 base64.\nRefer to the Customer Encryption Key section for more information on the contents of this block.\ndisk_name (string) - Name of the disk to create. This only applies to non-scratch disks. If the disk is persistent, and not specified, Packer will generate a unique name for the disk.\nThe name must be 1-63 characters long and comply to the regexp 'a-z?'\ninterface_type (string) - The interface to use for attaching the disk. Can be either NVME or SCSI. Defaults to SCSI.\nThe available options depend on the type of disk, SEE: https://cloud.google.com/compute/docs/disks/persistent-disks#choose_an_interface\niops (int) - The requested IOPS for the disk.\nThis is only available for pd_extreme disks.\nkeep_device (bool) - Keep the device in the created disks after the instance is terminated. By default, the builder will remove the disks at the end of the build.\nThis cannot be used with 'scratch' volumes.\nreplica_zones ([]string) - The list of extra zones to replicate the disk into\nThe zone in which the instance is created will automatically be added to the zones in which the disk is replicated.\nsource_volume (string) - The URI of the volume to attach\nIf this is specified, it won't be deleted after the instance is shut-down.\n_ (string) - Zone is the zone in which to create the disk in.\nIt is not exposed since the parent config already specifies it and it will be set for the block device when preparing it.\nCustomer Encryption Key\nSpecifying a custom key allows you to use your own encryption keys to encrypt the data of the image you are creating.\nNote: you will need to reuse the same key later on when reusing the image.\nkmsKeyName (string) - KmsKeyName: The name of the encryption key that is stored in Google Cloud KMS.\nrawKey (string) - RawKey: Specifies a 256-bit customer-supplied encryption key, encoded in RFC 4648 base64 to either encrypt or decrypt this resource.\nNode Affinities\nNode affinity configuration allows you to restrict the nodes on which to run the instance that Packer will build the image from. This requires configuring sole-tenant node groups first.\nkey (string) - Key: Corresponds to the label key of Node resource.\noperator (string) - Operator: Defines the operation of node selection. Valid operators are IN for affinity and NOT_IN for anti-affinity.\nvalues ([]string) - Values: Corresponds to the label values of Node resource."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.10.x/templates/legacy_json_templates/builders",
  "text": "Builders - Templates | Packer\nWithin the template, the builders section contains an array of all the builders that Packer should use to generate machine images for the template.\nBuilders are responsible for creating machines and generating images from them for various platforms. For example, there are separate builders for EC2, VMware, VirtualBox, etc. Packer comes with many builders by default, and can also be extended to add new builders.\nThis documentation page will cover how to configure a builder in a template. The specific configuration options available for each builder, however, must be referenced from the documentation for that specific builder.\nWithin a template, a section of builder definitions looks like this:\n{ \"builders\": [ // ... one or more builder definitions here ] } \nA single builder definition maps to exactly one build. A builder definition is a JSON object that requires at least a type key. The type is the name of the builder that will be used to create a machine image for the build.\nIn addition to the type, other keys configure the builder itself. For example, the AWS builder requires an access_key, secret_key, and some other settings. These are placed directly within the builder definition.\nAn example builder definition is shown below, in this case configuring the AWS builder:\n{ \"type\": \"amazon-ebs\", \"access_key\": \"...\", \"secret_key\": \"...\" } \nEach build in Packer has a name. By default, the name is just the name of the builder being used. In general, this is good enough. Names only serve as an indicator in the output of what is happening. If you want, however, you can specify a custom name using the name key within the builder definition.\nThis is particularly useful if you have multiple builds defined that use the same underlying builder. In this case, you must specify a name for at least one of them since the names must be unique.\nEvery build is associated with a single communicator. Communicators are used to establish a connection for provisioning a remote machine (such as an AWS instance or local virtual machine).\nAll the examples for the various builders show some communicator (usually SSH), but the communicators are highly customizable so we recommend reading the communicator documentation."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/communicators/winrm",
  "text": "The WinRM communicator is not the default communicator, so you will always have to set the \"communicator\": \"winrm\", template option explicitly. In addition, you will almost always have to provide a pre-run script that enables and configures WinRM on the guest machine. This will generally be in the form of a powershell script or a batch file.\nConfiguring WinRM in VMWare\nIt is also possible to call powershell scripts in a similar manner.\nThis batch file will only work for http connections, not https, but will enable you to connect using only the username and password created earlier in the Autounattend file. The above batchfile will allow you to connect using a very simple Packer config:\nA more complex example of a powershell script used for configuration can be seen below."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/templates/legacy_json_templates/builders",
  "text": "Builders - Templates | Packer\nWithin the template, the builders section contains an array of all the builders that Packer should use to generate machine images for the template.\nBuilders are responsible for creating machines and generating images from them for various platforms. For example, there are separate builders for EC2, VMware, VirtualBox, etc. Packer comes with many builders by default, and can also be extended to add new builders.\nThis documentation page will cover how to configure a builder in a template. The specific configuration options available for each builder, however, must be referenced from the documentation for that specific builder.\nWithin a template, a section of builder definitions looks like this:\n{ \"builders\": [ // ... one or more builder definitions here ] } \nA single builder definition maps to exactly one build. A builder definition is a JSON object that requires at least a type key. The type is the name of the builder that will be used to create a machine image for the build.\nIn addition to the type, other keys configure the builder itself. For example, the AWS builder requires an access_key, secret_key, and some other settings. These are placed directly within the builder definition.\nAn example builder definition is shown below, in this case configuring the AWS builder:\n{ \"type\": \"amazon-ebs\", \"access_key\": \"...\", \"secret_key\": \"...\" } \nEach build in Packer has a name. By default, the name is just the name of the builder being used. In general, this is good enough. Names only serve as an indicator in the output of what is happening. If you want, however, you can specify a custom name using the name key within the builder definition.\nThis is particularly useful if you have multiple builds defined that use the same underlying builder. In this case, you must specify a name for at least one of them since the names must be unique.\nEvery build is associated with a single communicator. Communicators are used to establish a connection for provisioning a remote machine (such as an AWS instance or local virtual machine).\nAll the examples for the various builders show some communicator (usually SSH), but the communicators are highly customizable so we recommend reading the communicator documentation."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.9.x/templates/legacy_json_templates/builders",
  "text": "Builders - Templates | Packer\nWithin the template, the builders section contains an array of all the builders that Packer should use to generate machine images for the template.\nBuilders are responsible for creating machines and generating images from them for various platforms. For example, there are separate builders for EC2, VMware, VirtualBox, etc. Packer comes with many builders by default, and can also be extended to add new builders.\nThis documentation page will cover how to configure a builder in a template. The specific configuration options available for each builder, however, must be referenced from the documentation for that specific builder.\nWithin a template, a section of builder definitions looks like this:\n{ \"builders\": [ // ... one or more builder definitions here ] } \nA single builder definition maps to exactly one build. A builder definition is a JSON object that requires at least a type key. The type is the name of the builder that will be used to create a machine image for the build.\nIn addition to the type, other keys configure the builder itself. For example, the AWS builder requires an access_key, secret_key, and some other settings. These are placed directly within the builder definition.\nAn example builder definition is shown below, in this case configuring the AWS builder:\n{ \"type\": \"amazon-ebs\", \"access_key\": \"...\", \"secret_key\": \"...\" } \nEach build in Packer has a name. By default, the name is just the name of the builder being used. In general, this is good enough. Names only serve as an indicator in the output of what is happening. If you want, however, you can specify a custom name using the name key within the builder definition.\nThis is particularly useful if you have multiple builds defined that use the same underlying builder. In this case, you must specify a name for at least one of them since the names must be unique.\nEvery build is associated with a single communicator. Communicators are used to establish a connection for provisioning a remote machine (such as an AWS instance or local virtual machine).\nAll the examples for the various builders show some communicator (usually SSH), but the communicators are highly customizable so we recommend reading the communicator documentation."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.8.x/templates/legacy_json_templates/builders",
  "text": "Builders - Templates | Packer\nWithin the template, the builders section contains an array of all the builders that Packer should use to generate machine images for the template.\nBuilders are responsible for creating machines and generating images from them for various platforms. For example, there are separate builders for EC2, VMware, VirtualBox, etc. Packer comes with many builders by default, and can also be extended to add new builders.\nThis documentation page will cover how to configure a builder in a template. The specific configuration options available for each builder, however, must be referenced from the documentation for that specific builder.\nWithin a template, a section of builder definitions looks like this:\n{ \"builders\": [ // ... one or more builder definitions here ] } \nA single builder definition maps to exactly one build. A builder definition is a JSON object that requires at least a type key. The type is the name of the builder that will be used to create a machine image for the build.\nIn addition to the type, other keys configure the builder itself. For example, the AWS builder requires an access_key, secret_key, and some other settings. These are placed directly within the builder definition.\nAn example builder definition is shown below, in this case configuring the AWS builder:\n{ \"type\": \"amazon-ebs\", \"access_key\": \"...\", \"secret_key\": \"...\" } \nEach build in Packer has a name. By default, the name is just the name of the builder being used. In general, this is good enough. Names only serve as an indicator in the output of what is happening. If you want, however, you can specify a custom name using the name key within the builder definition.\nThis is particularly useful if you have multiple builds defined that use the same underlying builder. In this case, you must specify a name for at least one of them since the names must be unique.\nEvery build is associated with a single communicator. Communicators are used to establish a connection for provisioning a remote machine (such as an AWS instance or local virtual machine).\nAll the examples for the various builders show some communicator (usually SSH), but the communicators are highly customizable so we recommend reading the communicator documentation."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/communicators/winrm",
  "text": "The WinRM communicator is not the default communicator, so you will always have to set the \"communicator\": \"winrm\", template option explicitly. In addition, you will almost always have to provide a pre-run script that enables and configures WinRM on the guest machine. This will generally be in the form of a powershell script or a batch file.\nConfiguring WinRM in VMWare\nIt is also possible to call powershell scripts in a similar manner.\nThis batch file will only work for http connections, not https, but will enable you to connect using only the username and password created earlier in the Autounattend file. The above batchfile will allow you to connect using a very simple Packer config:\nA more complex example of a powershell script used for configuration can be seen below."
},
{
  "url": "https://developer.hashicorp.com/packer/guides/v1.10.x/hcl/component-object-spec",
  "text": "Generating code for config spec. | Packer\nAuto Generate the HCL2 code of a plugin\nFrom v1.5, Packer can be configured using HCL2. Because Packer has so many builders, provisioners, and post-processors, we created a on code generation tool to add the HCL2-enabling code more easily. You can use this code generator to create the HCL2 spec code of your custom plugin simply. It's a Go binary package made available through the Packer plugin SDK\nSay you want to configure the Config struct of a Builder in a package located in my/example-plugin/config.go. Here are some simple steps you can follow to make it HCL2 enabled:\nrun go install github.com/hashicorp/packer-plugin-sdk/cmd/packer-sdc@latest\nAdd //go:generate packer-sdc mapstructure-to-hcl2 -type Config at the top of config.go\nrun go generate ./my/example-plugin/...\nThis will generate a my/example-plugin/config.hcl2spec.go file containing the configuration fields of Config.\nMake sure that all the nested structs of Config are also auto generated the same way.\nNow we only need to make your Builder implement the interface by adding the following snippet:\nfunc (b *Builder) ConfigSpec() hcldec.ObjectSpec { return b.config.FlatMapstructure().HCL2Spec() } \nFrom now on every time you add or change a field of Config you will need to run the go generate command again.\nA good example of this is the Config struct of the amazon-ebs builder"
},
{
  "url": "https://developer.hashicorp.com/packer/intro/getting-started",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/plugins/v1.10.x/datasources/amazon/ami",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/plugins/v1.9.x/datasources/amazon/ami",
  "text": "This page does not exist for version v1.9.x."
},
{
  "url": "https://developer.hashicorp.com/packer/guides/v1.8.x/hcl/component-object-spec",
  "text": "Generating code for config spec. | Packer\nAuto Generate the HCL2 code of a plugin\nFrom v1.5, Packer can be configured using HCL2. Because Packer has so many builders, provisioners, and post-processors, we created a on code generation tool to add the HCL2-enabling code more easily. You can use this code generator to create the HCL2 spec code of your custom plugin simply. It's a Go binary package made available through the Packer plugin SDK\nSay you want to configure the Config struct of a Builder in a package located in my/example-plugin/config.go. Here are some simple steps you can follow to make it HCL2 enabled:\nrun go install github.com/hashicorp/packer-plugin-sdk/cmd/packer-sdc@latest\nAdd //go:generate packer-sdc mapstructure-to-hcl2 -type Config at the top of config.go\nrun go generate ./my/example-plugin/...\nThis will generate a my/example-plugin/config.hcl2spec.go file containing the configuration fields of Config.\nMake sure that all the nested structs of Config are also auto generated the same way.\nNow we only need to make your Builder implement the interface by adding the following snippet:\nfunc (b *Builder) ConfigSpec() hcldec.ObjectSpec { return b.config.FlatMapstructure().HCL2Spec() } \nFrom now on every time you add or change a field of Config you will need to run the go generate command again.\nA good example of this is the Config struct of the amazon-ebs builder"
},
{
  "url": "https://developer.hashicorp.com/packer/guides/v1.9.x/hcl/component-object-spec",
  "text": "Generating code for config spec. | Packer\nAuto Generate the HCL2 code of a plugin\nFrom v1.5, Packer can be configured using HCL2. Because Packer has so many builders, provisioners, and post-processors, we created a on code generation tool to add the HCL2-enabling code more easily. You can use this code generator to create the HCL2 spec code of your custom plugin simply. It's a Go binary package made available through the Packer plugin SDK\nSay you want to configure the Config struct of a Builder in a package located in my/example-plugin/config.go. Here are some simple steps you can follow to make it HCL2 enabled:\nrun go install github.com/hashicorp/packer-plugin-sdk/cmd/packer-sdc@latest\nAdd //go:generate packer-sdc mapstructure-to-hcl2 -type Config at the top of config.go\nrun go generate ./my/example-plugin/...\nThis will generate a my/example-plugin/config.hcl2spec.go file containing the configuration fields of Config.\nMake sure that all the nested structs of Config are also auto generated the same way.\nNow we only need to make your Builder implement the interface by adding the following snippet:\nfunc (b *Builder) ConfigSpec() hcldec.ObjectSpec { return b.config.FlatMapstructure().HCL2Spec() } \nFrom now on every time you add or change a field of Config you will need to run the go generate command again.\nA good example of this is the Config struct of the amazon-ebs builder"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/extending/plugins",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/other/debugging",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/datasources/amazon/ami",
  "text": "This page does not exist for version v1.7.x."
},
{
  "url": "https://developer.hashicorp.com/packer/guides/v1.7.x/hcl/component-object-spec",
  "text": "Generating code for config spec. | Packer\nAuto Generate the HCL2 code of a plugin\nFrom v1.5, Packer can be configured using HCL2. Because Packer has so many builders, provisioners, and post-processors, we created a on code generation tool to add the HCL2-enabling code more easily. You can use this code generator to create the HCL2 spec code of your custom plugin simply. It's a Go binary package made available through the Packer plugin SDK\nSay you want to configure the Config struct of a Builder in a package located in my/example-plugin/config.go. Here are some simple steps you can follow to make it HCL2 enabled:\nrun go install github.com/hashicorp/packer-plugin-sdk/cmd/packer-sdc@latest\nAdd //go:generate packer-sdc mapstructure-to-hcl2 -type Config at the top of config.go\nrun go generate ./my/example-plugin/...\nThis will generate a my/example-plugin/config.hcl2spec.go file containing the configuration fields of Config.\nMake sure that all the nested structs of Config are also auto generated the same way.\nNow we only need to make your Builder implement the interface by adding the following snippet:\nfunc (b *Builder) ConfigSpec() hcldec.ObjectSpec { return b.config.FlatMapstructure().HCL2Spec() } \nFrom now on every time you add or change a field of Config you will need to run the go generate command again.\nA good example of this is the Config struct of the amazon-ebs builder"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/datasources/amazon/secretsmanager",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/guides/v1.6.x/hcl/component-object-spec",
  "text": "From v1.5, Packer can be configured using HCL2. Because Packer has so many builders, provisioner & post-processors, we relied on code generation to iterate more easily. The good new is that you can benefit from this code generator to get the HCL2 spec code of your component simply. It's a Go binary package and is located in cmd/mapstructure-to-hcl2.\nrun go install github.com/hashicorp/packer/cmd/mapstructure-to-hcl2\nAdd //go:generate mapstructure-to-hcl2 -type Config at the top of config.go\nNow we only need to make Builder implement the interface by adding the following snippet:"
},
{
  "url": "https://developer.hashicorp.com/packer/guides/v1.5.x/hcl/component-object-spec",
  "text": "From v1.5, Packer can be configured using HCL2. Because Packer has so many builders, provisioner & post-processors, we relied on code generation to iterate more easily. The good new is that you can benefit from this code generator to get the HCL2 spec code of your component simply. It's a Go binary package and is located in cmd/mapstructure-to-hcl2.\nrun go install github.com/hashicorp/packer/cmd/mapstructure-to-hcl2\nAdd //go:generate mapstructure-to-hcl2 -type Config at the top of config.go\nNow we only need to make Builder implement the interface by adding the following snippet:"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/extending",
  "text": "Extending | Packer | HashiCorp Developer\nPacker is designed to be extensible. Because the surface area for workloads is infinite, Packer supports plugins for builders, provisioners, and post-processors. To learn more about the different customizations, please choose a link from the sidebar."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/extending/custom-builders",
  "text": "Custom Builders - Extending | Packer\nPacker Builders are the components of Packer responsible for creating a machine, bringing it to a point where it can be provisioned, and then turning that provisioned machine into some sort of machine image. Several builders are officially distributed with Packer itself, such as the AMI builder, the VMware builder, etc. However, it is possible to write custom builders using the Packer plugin interface, and this page documents how to do that.\nPrior to reading this page, it is assumed you have read the page on plugin development basics.\nWarning! This is an advanced topic. If you're new to Packer, we recommend getting a bit more comfortable before you dive into writing plugins.\nThe interface that must be implemented for a builder is the packer.Builder interface. It is reproduced below for reference. The actual interface in the source code contains some basic documentation as well explaining what each method should do.\ntype Builder interface { ConfigSpec() hcldec.ObjectSpec Prepare(...interface{}) ([]string, []string, error) Run(context.Context, ui Ui, hook Hook) (Artifact, error) } \nThe \"ConfigSpec\" Method\nThis method returns a hcldec.ObjectSpec, which is a spec necessary for using HCL2 templates with Packer. For information on how to use and implement this function, check our object spec docs\nThe \"Prepare\" Method\nThe Prepare method for each builder is called prior to any runs with the configuration that was given in the template. This is passed in as an array of interface{} types, but is generally map[string]interface{}. The prepare method is responsible for translating this configuration into an internal structure, validating it, and returning any errors.\nFor multiple parameters, they should be merged together into the final configuration, with later parameters overwriting any previous configuration. The exact semantics of the merge are left to the builder author.\nFor decoding the interface{} into a meaningful structure, the mapstructure library is recommended. Mapstructure will take an interface{} and decode it into an arbitrarily complex struct. If there are any errors, it generates very human friendly errors that can be returned directly from the prepare method.\nWhile it is not actively enforced, no side effects should occur from running the Prepare method. Specifically, don't create files, don't launch virtual machines, etc. Prepare's purpose is solely to configure the builder and validate the configuration.\nIn addition to normal configuration, Packer will inject a map[string]interface{} with a key of packer.DebugConfigKey set to boolean true if debug mode is enabled for the build. If this is set to true, then the builder should enable a debug mode which assists builder developers and advanced users to introspect what is going on during a build. During debug builds, parallelism is strictly disabled, so it is safe to request input from stdin and so on.\nThe \"Run\" Method\nRun is where all the interesting stuff happens. Run is executed, often in parallel for multiple builders, to actually build the machine, provision it, and create the resulting machine image, which is returned as an implementation of the packer.Artifact interface.\nThe Run method takes three parameters. These are all very useful. The packer.Ui object is used to send output to the console. packer.Hook is used to execute hooks, which are covered in more detail in the hook section below. And packer.Cache is used to store files between multiple Packer runs, and is covered in more detail in the cache section below.\nBecause builder runs are typically a complex set of many steps, the multistep helper is recommended to bring order to the complexity. Multistep is a library which allows you to separate your logic into multiple distinct \"steps\" and string them together. It fully supports cancellation mid-step and so on. Please check it out, it is how the built-in builders are all implemented.\nFinally, as a result of Run, an implementation of packer.Artifact should be returned. More details on creating a packer.Artifact are covered in the artifact section below. If something goes wrong during the build, an error can be returned, as well. Note that it is perfectly fine to produce no artifact and no error, although this is rare.\nCancellation\nThe Run method is often run in parallel.\nWith the \"Cancel\" Method ( up until packer 1.3 )\nThe Cancel method can be called at any time and requests cancellation of any builder run in progress. This method should block until the run actually stops. Not that the Cancel method will no longer be called since packer 1.4.0.\nContext cancellation ( from packer 1.4 )\nThe <-ctx.Done() can unblock at any time and signifies request for cancellation of any builder run in progress.\nCancels are most commonly triggered by external interrupts, such as the user pressing Ctrl-C. Packer will only exit once all the builders clean up, so it is important that you architect your builder in a way that it is quick to respond to these cancellations and clean up after itself.\nThe Run method is expected to return an implementation of the packer.Artifact interface. Each builder must create their own implementation. The interface has ample documentation to help you get started.\nThe only part of an artifact that may be confusing is the BuilderId method. This method must return an absolutely unique ID for the builder. In general, I follow the practice of making the ID contain my GitHub username and then the platform it is building for. For example, the builder ID of the VMware builder is \"hashicorp.vmware\" or something similar.\nPost-processors use the builder ID value in order to make some assumptions about the artifact results, so it is important it never changes.\nOther than the builder ID, the rest should be self-explanatory by reading the packer.Artifact interface documentation.\nPacker has built-in support for provisioning, but the moment when provisioning runs must be invoked by the builder itself, since only the builder knows when the machine is running and ready for communication.\nWhen the machine is ready to be provisioned, run the packer.HookProvision hook, making sure the communicator is not nil, since this is required for provisioners. An example of calling the hook is shown below:\nhook.Run(context.Context, packer.HookProvision, ui, comm, nil) \nAt this point, Packer will run the provisioners and no additional work is necessary.\nNote: Hooks are still undergoing thought around their general design and will likely change in a future version. They aren't fully \"baked\" yet, so they aren't documented here other than to tell you how to hook in provisioners.\nBuild variables\nPacker makes it possible to provide custom template engine variables to be shared with provisioners and post-processors using the build function.\nPart of the builder interface changes made in 1.5.0 was to make builder Prepare() methods return a list of custom variables which we call generated data. We use that list of variables to generate a custom placeholder map per builder that combines custom variables with the placeholder map of default build variables created by Packer. Here's an example snippet telling packer what will be made available by the builder:\nfunc (b *Builder) Prepare(raws ...interface{}) ([]string, []string, error) { // ... generatedData := []string{\"SourceImageName\"} return generatedData, warns, nil } \nReturning the custom variable name(s) within the generated_data placeholder is necessary for the template containing the build variable(s) to validate.\nOnce the placeholder is set, it's necessary to pass the variables' real values when calling the provisioner. This can be done as the example below:\nfunc (b *Builder) Run(ctx context.Context, ui packer.Ui, hook packer.Hook) (packer.Artifact, error) { // ... // Create map of custom variable generatedData := map[string]interface{}{\"SourceImageName\": \"the source image name value\"} // Pass map to provisioner hook.Run(context.Context, packer.HookProvision, ui, comm, generatedData) // ... } \nIn order to make these same variables and the Packer default ones also available to post-processor, it is necessary to add them to the Artifact returned by the builder. This can be done by adding an attribute of type map[string]interface{} to the Artifact and putting the generated data in it. The post-processor will access this data later via the Artifact's State method.\nThe Artifact code should be implemented similar to the below:\ntype Artifact struct { // ... // StateData should store data such as GeneratedData // to be shared with post-processors StateData map[string]interface{} } // ... func (a *Artifact) State(name string) interface{} { return a.StateData[name] } // ... \nThe builder should return the above Artifact containing the generated data and the code should be similar to the example snippet below:\nfunc (b *Builder) Run(ctx context.Context, ui packer.Ui, hook packer.Hook) (packer.Artifact, error) { // ... return &Artifact{ // ... StateData: map[string]interface{}{\"generated_data\": state.Get(\"generated_data\")}, }, nil } \nThe code above assigns the generated_data state to the StateData map with the key generated_data.\nHere some example of how this data will be used by post-processors:\nfunc (p *PostProcessor) PostProcess(ctx context.Context, ui packer.Ui, source packer.Artifact) (packer.Artifact, bool, bool, error) { generatedData := source.State(\"generated_data\") // generatedData will then be used for interpolation // ... } \nTo know more about the template engine build function, please refer to the template engine docs."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/extending/custom-post-processors",
  "text": "Custom Post-Processors - Extending | Packer\nPacker Post-processors are the components of Packer that transform one artifact into another, for example by compressing files, or uploading them.\nIn the compression example, the transformation would be taking an artifact with a set of files, compressing those files, and returning a new artifact with only a single file (the compressed archive). For the upload example, the transformation would be taking an artifact with some set of files, uploading those files, and returning an artifact with a single ID: the URL of the upload.\nPrior to reading this page, it is assumed you have read the page on plugin development basics.\nPost-processor plugins implement the packer.PostProcessor interface and are served using the plugin.ServePostProcessor function.\nWarning! This is an advanced topic. If you're new to Packer, we recommend getting a bit more comfortable before you dive into writing plugins.\nThe interface that must be implemented for a post-processor is the packer.PostProcessor interface. It is reproduced below for reference. The actual interface in the source code contains some basic documentation as well explaining what each method should do.\ntype PostProcessor interface { ConfigSpec() hcldec.ObjectSpec Configure(interface{}) error PostProcess(context.Context, Ui, Artifact) (a Artifact, keep, mustKeep bool, err error) } \nThe \"ConfigSpec\" Method\nThis method returns a hcldec.ObjectSpec, which is a spec necessary for using HCL2 templates with Packer. For information on how to use and implement this function, check our object spec docs\nThe \"Configure\" Method\nThe Configure method for each post-processor is called early in the build process to configure the post-processor. The configuration is passed in as a raw interface{}. The configure method is responsible for translating this configuration into an internal structure, validating it, and returning any errors.\nFor decoding the interface{} into a meaningful structure, the mapstructure library is recommended. Mapstructure will take an interface{} and decode it into an arbitrarily complex struct. If there are any errors, it generates very human-friendly errors that can be returned directly from the configure method.\nWhile it is not actively enforced, no side effects should occur from running the Configure method. Specifically, don't create files, don't create network connections, etc. Configure's purpose is solely to setup internal state and validate the configuration as much as possible.\nConfigure being run is not an indication that PostProcess will ever run. For example, packer validate will run Configure to verify the configuration validates, but will never actually run the build.\nThe \"PostProcess\" Method\nThe PostProcess method is where the real work goes. PostProcess is responsible for taking one packer.Artifact implementation, and transforming it into another. A PostProcess call can be cancelled at any moment. Cancellation is triggered when the done chan of the context struct (<-ctx.Done()) unblocks .\nWhen we say \"transform,\" we don't mean actually modifying the existing packer.Artifact value itself. We mean taking the contents of the artifact and creating a new artifact from that. For example, if we were creating a \"compress\" post-processor that is responsible for compressing files, the transformation would be taking the Files() from the original artifact, compressing them, and creating a new artifact with a single file: the compressed archive.\nThe result signature of this method is (Artifact, bool, bool, error). Each return value is explained below:\nArtifact - The newly created artifact if no errors occurred.\nbool - If keep true, the input artifact will forcefully be kept. By default, Packer typically deletes all input artifacts, since the user doesn't generally want intermediary artifacts. However, some post-processors depend on the previous artifact existing. If this is true, it forces packer to keep the artifact around.\nbool - If forceOverride is true, then any user input for keep_input_artifact is ignored and the artifact is either kept or discarded according to the value set in keep.\nerror - Non-nil if there was an error in any way. If this is the case, the other two return values are ignored."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/extending/custom-provisioners",
  "text": "Custom Provisioners - Extending | Packer\nPacker Provisioners are the components of Packer that install and configure software into a running machine prior to turning that machine into an image. An example of a provisioner is the shell provisioner, which runs shell scripts within the machines.\nPrior to reading this page, it is assumed you have read the page on plugin development basics.\nProvisioner plugins implement the packer.Provisioner interface and are served using the plugin.ServeProvisioner function.\nWarning! This is an advanced topic. If you're new to Packer, we recommend getting a bit more comfortable before you dive into writing plugins.\nThe interface that must be implemented for a provisioner is the packer.Provisioner interface. It is reproduced below for reference. The actual interface in the source code contains some basic documentation as well explaining what each method should do.\ntype Provisioner interface { ConfigSpec() hcldec.ObjectSpec Prepare(...interface{}) error Provision(context.Context, Ui, Communicator, map[string]interface{}) error } \nThe \"Prepare\" Method\nThe Prepare method for each provisioner is called prior to any runs with the configuration that was given in the template. This is passed in as an array of interface{} types, but is generally map[string]interface{}. The prepare method is responsible for translating this configuration into an internal structure, validating it, and returning any errors.\nFor multiple parameters, they should be merged together into the final configuration, with later parameters overwriting any previous configuration. The exact semantics of the merge are left to the builder author.\nFor decoding the interface{} into a meaningful structure, the mapstructure library is recommended. Mapstructure will take an interface{} and decode it into an arbitrarily complex struct. If there are any errors, it generates very human friendly errors that can be returned directly from the prepare method.\nWhile it is not actively enforced, no side effects should occur from running the Prepare method. Specifically, don't create files, don't launch virtual machines, etc. Prepare's purpose is solely to configure the builder and validate the configuration.\nThe Prepare method is called very early in the build process so that errors may be displayed to the user before anything actually happens.\nThe \"ConfigSpec\" Method\nThis method returns a hcldec.ObjectSpec, which is a spec necessary for using HCL2 templates with Packer. For information on how to use and implement this function, check our object spec docs\nThe \"Provision\" Method\nThe Provision method is called when a machine is running and ready to be provisioned. The provisioner should do its real work here.\nThe method takes two parameters: a packer.Ui and a packer.Communicator. The UI can be used to communicate with the user what is going on. The communicator is used to communicate with the running machine, and is guaranteed to be connected at this point.\nThe provision method should not return until provisioning is complete.\nThe map[string]interface{} provides users with build-specific information, like host and IP, provided by the build template engine. Provisioners may use this information however they please, or not use it.\nThe packer.Communicator parameter and interface is used to communicate with running machine. The machine may be local (in a virtual machine or container of some sort) or it may be remote (in a cloud). The communicator interface abstracts this away so that communication is the same overall.\nThe documentation around the code itself is really great as an overview of how to use the interface. You should begin by reading this. Once you have read it, you can see some example usage below:\n// Build the remote command. var cmd packer.RemoteCmd cmd.Command = \"echo foo\" // We care about stdout, so lets collect that into a buffer. Since // we don't set stderr, that will just be discarded. var stdout bytes.Buffer cmd.Stdout = &stdout // Start the command if err := comm.Start(&cmd); err != nil { panic(err) } // Wait for it to complete cmd.Wait() // Read the stdout! fmt.Printf(\"Command output: %s\", stdout.String())"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.10.x/commands/plugins",
  "text": "plugins Command | Packer | HashiCorp Developer\nThe plugins command groups subcommands for interacting with Packers' plugins.\n$ packer plugins -h Usage: packer plugins <subcommand> [options] [args] This command groups subcommands for interacting with Packer plugins. Related but not under the \"plugins\" command : - \"packer init <path>\" will install all plugins required by a config. Subcommands: install Install latest Packer plugin [matching version constraint] installed List all installed Packer plugin binaries remove Remove Packer plugins [matching a version] required List plugins required by a config \nRelated\npacker init will install all required plugins."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/other/core-configuration",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/commands/plugins",
  "text": "plugins Command | Packer | HashiCorp Developer\nThe plugins command groups subcommands for interacting with Packers' plugins.\n$ packer plugins -h Usage: packer plugins <subcommand> [options] [args] This command groups subcommands for interacting with Packer plugins. Related but not under the \"plugins\" command : - \"packer init <path>\" will install all plugins required by a config. Subcommands: install Install latest Packer plugin [matching version constraint] installed List all installed Packer plugin binaries remove Remove Packer plugins [matching a version] required List plugins required by a config \nRelated\npacker init will install all required plugins."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.10.x/commands/plugins/installed",
  "text": "plugins installed\nThe plugins installed subcommand lists installed Packer plugins\n$ packer plugins installed -h Usage: packer plugins installed This command lists all installed plugin binaries that match with the current OS and architecture. Packer's API version will be ignored. "
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.9.x/commands/plugins/installed",
  "text": "plugins installed\nThe plugins installed subcommand lists installed Packer plugins\n$ packer plugins installed -h Usage: packer plugins installed This command lists all installed plugin binaries that match with the current OS and architecture. Packer's API version will be ignored. "
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.8.x/commands/plugins/installed",
  "text": "plugins installed\nThe plugins installed subcommand lists installed Packer plugins\n$ packer plugins installed -h Usage: packer plugins installed This command lists all installed plugin binaries that match with the current OS and architecture. Packer's API version will be ignored. "
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/commands/plugins/installed",
  "text": "plugins installed\nThe plugins installed subcommand lists installed Packer plugins\n$ packer plugins installed -h Usage: packer plugins installed This command lists all installed plugin binaries that match with the current OS and architecture. Packer's API version will be ignored. "
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.10.x/commands/plugins/remove",
  "text": "The plugins remove subcommand removes a Packer plugin at a version constraint\n$ packer plugins remove -h Usage: packer plugins remove <plugin> [<version constraint>] This command will remove all Packer plugins matching the version constraint for the current OS and architecture. When the version is omitted all installed versions will be removed. Ex: packer plugins remove github.com/hashicorp/happycloud v1.2.3 "
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.9.x/commands/plugins/remove",
  "text": "The plugins remove subcommand removes a Packer plugin at a version constraint\n$ packer plugins remove -h Usage: packer plugins remove <plugin> [<version constraint>] This command will remove all Packer plugins matching the version constraint for the current OS and architecture. When the version is omitted all installed versions will be removed. Ex: packer plugins remove github.com/hashicorp/happycloud v1.2.3 "
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.8.x/commands/plugins/remove",
  "text": "The plugins remove subcommand removes a Packer plugin at a version constraint\n$ packer plugins remove -h Usage: packer plugins remove <plugin> [<version constraint>] This command will remove all Packer plugins matching the version constraint for the current OS and architecture. When the version is omitted all installed versions will be removed. Ex: packer plugins remove github.com/hashicorp/happycloud v1.2.3 "
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/commands/plugins/remove",
  "text": "The plugins remove subcommand removes a Packer plugin at a version constraint\n$ packer plugins remove -h Usage: packer plugins remove <plugin> [<version constraint>] This command will remove all Packer plugins matching the version constraint for the current OS and architecture. When the version is omitted all installed versions will be removed. Ex: packer plugins remove github.com/hashicorp/happycloud v1.2.3 "
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.10.x/commands/plugins/required",
  "text": "The plugins required command lists all plugins required by a Packer config and all the installed binaries that match the constraint. The first binary is the most up-to-date installed version and will be the one picked by Packer in a build.\n$ packer plugins required -h Usage: packer plugins required <path> This command will list every Packer plugin required by a Packer config, in packer.required_plugins blocks. All binaries matching the required version constrain and the current OS and Architecture will be listed. The most recent version (and the first of the list) will be the one picked by Packer during a build. Ex: packer plugins required require.pkr.hcl Ex: packer plugins required path/to/folder/ "
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.8.x/commands/plugins/required",
  "text": "The plugins required command lists all plugins required by a Packer config and all the installed binaries that match the constraint. The first binary is the most up-to-date installed version and will be the one picked by Packer in a build.\n$ packer plugins required -h Usage: packer plugins required <path> This command will list every Packer plugin required by a Packer config, in packer.required_plugins blocks. All binaries matching the required version constrain and the current OS and Architecture will be listed. The most recent version (and the first of the list) will be the one picked by Packer during a build. Ex: packer plugins required require.pkr.hcl Ex: packer plugins required path/to/folder/ "
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.9.x/commands/plugins/required",
  "text": "The plugins required command lists all plugins required by a Packer config and all the installed binaries that match the constraint. The first binary is the most up-to-date installed version and will be the one picked by Packer in a build.\n$ packer plugins required -h Usage: packer plugins required <path> This command will list every Packer plugin required by a Packer config, in packer.required_plugins blocks. All binaries matching the required version constrain and the current OS and Architecture will be listed. The most recent version (and the first of the list) will be the one picked by Packer during a build. Ex: packer plugins required require.pkr.hcl Ex: packer plugins required path/to/folder/ "
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/extending",
  "text": "Extending | Packer | HashiCorp Developer\nPacker is designed to be extensible. Because the surface area for workloads is infinite, Packer supports plugins for builders, provisioners, and post-processors. To learn more about the different customizations, please choose a link from the sidebar."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/extending/custom-builders",
  "text": "Custom Builders - Extending | Packer\nPacker Builders are the components of Packer responsible for creating a machine, bringing it to a point where it can be provisioned, and then turning that provisioned machine into some sort of machine image. Several builders are officially distributed with Packer itself, such as the AMI builder, the VMware builder, etc. However, it is possible to write custom builders using the Packer plugin interface, and this page documents how to do that.\nPrior to reading this page, it is assumed you have read the page on plugin development basics.\nWarning! This is an advanced topic. If you're new to Packer, we recommend getting a bit more comfortable before you dive into writing plugins.\nThe interface that must be implemented for a builder is the packer.Builder interface. It is reproduced below for reference. The actual interface in the source code contains some basic documentation as well explaining what each method should do.\ntype Builder interface { ConfigSpec() hcldec.ObjectSpec Prepare(...interface{}) ([]string, []string, error) Run(context.Context, ui Ui, hook Hook) (Artifact, error) } \nThe \"ConfigSpec\" Method\nThis method returns a hcldec.ObjectSpec, which is a spec necessary for using HCL2 templates with Packer. For information on how to use and implement this function, check our object spec docs\nThe \"Prepare\" Method\nThe Prepare method for each builder is called prior to any runs with the configuration that was given in the template. This is passed in as an array of interface{} types, but is generally map[string]interface{}. The prepare method is responsible for translating this configuration into an internal structure, validating it, and returning any errors.\nFor multiple parameters, they should be merged together into the final configuration, with later parameters overwriting any previous configuration. The exact semantics of the merge are left to the builder author.\nFor decoding the interface{} into a meaningful structure, the mapstructure library is recommended. Mapstructure will take an interface{} and decode it into an arbitrarily complex struct. If there are any errors, it generates very human friendly errors that can be returned directly from the prepare method.\nWhile it is not actively enforced, no side effects should occur from running the Prepare method. Specifically, don't create files, don't launch virtual machines, etc. Prepare's purpose is solely to configure the builder and validate the configuration.\nIn addition to normal configuration, Packer will inject a map[string]interface{} with a key of packer.DebugConfigKey set to boolean true if debug mode is enabled for the build. If this is set to true, then the builder should enable a debug mode which assists builder developers and advanced users to introspect what is going on during a build. During debug builds, parallelism is strictly disabled, so it is safe to request input from stdin and so on.\nThe \"Run\" Method\nRun is where all the interesting stuff happens. Run is executed, often in parallel for multiple builders, to actually build the machine, provision it, and create the resulting machine image, which is returned as an implementation of the packer.Artifact interface.\nThe Run method takes three parameters. These are all very useful. The packer.Ui object is used to send output to the console. packer.Hook is used to execute hooks, which are covered in more detail in the hook section below. And packer.Cache is used to store files between multiple Packer runs, and is covered in more detail in the cache section below.\nBecause builder runs are typically a complex set of many steps, the multistep helper is recommended to bring order to the complexity. Multistep is a library which allows you to separate your logic into multiple distinct \"steps\" and string them together. It fully supports cancellation mid-step and so on. Please check it out, it is how the built-in builders are all implemented.\nFinally, as a result of Run, an implementation of packer.Artifact should be returned. More details on creating a packer.Artifact are covered in the artifact section below. If something goes wrong during the build, an error can be returned, as well. Note that it is perfectly fine to produce no artifact and no error, although this is rare.\nCancellation\nThe Run method is often run in parallel.\nWith the \"Cancel\" Method ( up until packer 1.3 )\nThe Cancel method can be called at any time and requests cancellation of any builder run in progress. This method should block until the run actually stops. Not that the Cancel method will no longer be called since packer 1.4.0.\nContext cancellation ( from packer 1.4 )\nThe <-ctx.Done() can unblock at any time and signifies request for cancellation of any builder run in progress.\nCancels are most commonly triggered by external interrupts, such as the user pressing Ctrl-C. Packer will only exit once all the builders clean up, so it is important that you architect your builder in a way that it is quick to respond to these cancellations and clean up after itself.\nThe Run method is expected to return an implementation of the packer.Artifact interface. Each builder must create their own implementation. The interface has ample documentation to help you get started.\nThe only part of an artifact that may be confusing is the BuilderId method. This method must return an absolutely unique ID for the builder. In general, I follow the practice of making the ID contain my GitHub username and then the platform it is building for. For example, the builder ID of the VMware builder is \"hashicorp.vmware\" or something similar.\nPost-processors use the builder ID value in order to make some assumptions about the artifact results, so it is important it never changes.\nOther than the builder ID, the rest should be self-explanatory by reading the packer.Artifact interface documentation.\nPacker has built-in support for provisioning, but the moment when provisioning runs must be invoked by the builder itself, since only the builder knows when the machine is running and ready for communication.\nWhen the machine is ready to be provisioned, run the packer.HookProvision hook, making sure the communicator is not nil, since this is required for provisioners. An example of calling the hook is shown below:\nhook.Run(context.Context, packer.HookProvision, ui, comm, nil) \nAt this point, Packer will run the provisioners and no additional work is necessary.\nNote: Hooks are still undergoing thought around their general design and will likely change in a future version. They aren't fully \"baked\" yet, so they aren't documented here other than to tell you how to hook in provisioners.\nBuild variables\nPacker makes it possible to provide custom template engine variables to be shared with provisioners and post-processors using the build function.\nPart of the builder interface changes made in 1.5.0 was to make builder Prepare() methods return a list of custom variables which we call generated data. We use that list of variables to generate a custom placeholder map per builder that combines custom variables with the placeholder map of default build variables created by Packer. Here's an example snippet telling packer what will be made available by the builder:\nfunc (b *Builder) Prepare(raws ...interface{}) ([]string, []string, error) { // ... generatedData := []string{\"SourceImageName\"} return generatedData, warns, nil } \nReturning the custom variable name(s) within the generated_data placeholder is necessary for the template containing the build variable(s) to validate.\nOnce the placeholder is set, it's necessary to pass the variables' real values when calling the provisioner. This can be done as the example below:\nfunc (b *Builder) Run(ctx context.Context, ui packer.Ui, hook packer.Hook) (packer.Artifact, error) { // ... // Create map of custom variable generatedData := map[string]interface{}{\"SourceImageName\": \"the source image name value\"} // Pass map to provisioner hook.Run(context.Context, packer.HookProvision, ui, comm, generatedData) // ... } \nIn order to make these same variables and the Packer default ones also available to post-processor, it is necessary to add them to the Artifact returned by the builder. This can be done by adding an attribute of type map[string]interface{} to the Artifact and putting the generated data in it. The post-processor will access this data later via the Artifact's State method.\nThe Artifact code should be implemented similar to the below:\ntype Artifact struct { // ... // StateData should store data such as GeneratedData // to be shared with post-processors StateData map[string]interface{} } // ... func (a *Artifact) State(name string) interface{} { return a.StateData[name] } // ... \nThe builder should return the above Artifact containing the generated data and the code should be similar to the example snippet below:\nfunc (b *Builder) Run(ctx context.Context, ui packer.Ui, hook packer.Hook) (packer.Artifact, error) { // ... return &Artifact{ // ... StateData: map[string]interface{}{\"generated_data\": state.Get(\"generated_data\")}, }, nil } \nThe code above assigns the generated_data state to the StateData map with the key generated_data.\nHere some example of how this data will be used by post-processors:\nfunc (p *PostProcessor) PostProcess(ctx context.Context, ui packer.Ui, source packer.Artifact) (packer.Artifact, bool, bool, error) { generatedData := source.State(\"generated_data\") // generatedData will then be used for interpolation // ... } \nTo know more about the template engine build function, please refer to the template engine docs."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/commands/plugins/required",
  "text": "The plugins required command lists all plugins required by a Packer config and all the installed binaries that match the constraint. The first binary is the most up-to-date installed version and will be the one picked by Packer in a build.\n$ packer plugins required -h Usage: packer plugins required <path> This command will list every Packer plugin required by a Packer config, in packer.required_plugins blocks. All binaries matching the required version constrain and the current OS and Architecture will be listed. The most recent version (and the first of the list) will be the one picked by Packer during a build. Ex: packer plugins required require.pkr.hcl Ex: packer plugins required path/to/folder/ "
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/extending/custom-provisioners",
  "text": "Custom Provisioners - Extending | Packer\nPacker Provisioners are the components of Packer that install and configure software into a running machine prior to turning that machine into an image. An example of a provisioner is the shell provisioner, which runs shell scripts within the machines.\nProvisioner plugins implement the packer.Provisioner interface and are served using the plugin.ServeProvisioner function.\nThe interface that must be implemented for a provisioner is the packer.Provisioner interface. It is reproduced below for reference. The actual interface in the source code contains some basic documentation as well explaining what each method should do.\ntype Provisioner interface { ConfigSpec() hcldec.ObjectSpec Prepare(...interface{}) error Provision(context.Context, Ui, Communicator, map[string]interface{}) error } \nThe \"Prepare\" Method\nThe Prepare method for each provisioner is called prior to any runs with the configuration that was given in the template. This is passed in as an array of interface{} types, but is generally map[string]interface{}. The prepare method is responsible for translating this configuration into an internal structure, validating it, and returning any errors.\nFor multiple parameters, they should be merged together into the final configuration, with later parameters overwriting any previous configuration. The exact semantics of the merge are left to the builder author.\nFor decoding the interface{} into a meaningful structure, the mapstructure library is recommended. Mapstructure will take an interface{} and decode it into an arbitrarily complex struct. If there are any errors, it generates very human friendly errors that can be returned directly from the prepare method.\nWhile it is not actively enforced, no side effects should occur from running the Prepare method. Specifically, don't create files, don't launch virtual machines, etc. Prepare's purpose is solely to configure the builder and validate the configuration.\nThe Prepare method is called very early in the build process so that errors may be displayed to the user before anything actually happens.\nThe \"Provision\" Method\nThe Provision method is called when a machine is running and ready to be provisioned. The provisioner should do its real work here.\nThe method takes two parameters: a packer.Ui and a packer.Communicator. The UI can be used to communicate with the user what is going on. The communicator is used to communicate with the running machine, and is guaranteed to be connected at this point.\nThe provision method should not return until provisioning is complete.\nThe map[string]interface{} provides users with build-specific information, like host and IP, provided by the build template engine. Provisioners may use this information however they please, or not use it.\nThe packer.Communicator parameter and interface is used to communicate with running machine. The machine may be local (in a virtual machine or container of some sort) or it may be remote (in a cloud). The communicator interface abstracts this away so that communication is the same overall.\nThe documentation around the code itself is really great as an overview of how to use the interface. You should begin by reading this. Once you have read it, you can see some example usage below:\n// Build the remote command. var cmd packer.RemoteCmd cmd.Command = \"echo foo\" // We care about stdout, so lets collect that into a buffer. Since // we don't set stderr, that will just be discarded. var stdout bytes.Buffer cmd.Stdout = &stdout // Start the command if err := comm.Start(&cmd); err != nil { panic(err) } // Wait for it to complete cmd.Wait() // Read the stdout! fmt.Printf(\"Command output: %s\", stdout.String())"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.10.x/templates/legacy_json_templates/communicator",
  "text": "Communicators are the mechanism Packer uses to upload files, execute scripts, etc. with the machine being created.\nCommunicators are configured within the builder section.\nAll communicators have the following options:\nDepending on your builder, your communicator may not have all it needs in order to work \"out of the box\".\nIf you are building from a cloud image (for example, building on Amazon), there is a good chance that your cloud provider has already preconfigured SSH on the image for you, meaning that all you have to do is configure the communicator in the Packer template.\nHowever, if you are building from a brand-new and unconfigured operating system image, you will almost always have to perform some extra work to configure SSH on the guest machine. For most operating system distributions, this work will be performed by a boot_command that references a file which provides answers to the normally-interactive questions you get asked when installing an operating system. The name of this file varies by operating system; some common examples are the \"preseed\" file required by Debian, the \"kickstart\" file required by CentOS or the \"answer file\", also known as the Autounattend.xml file, required by Windows. For simplicity's sake, we'll refer to this file as the \"preseed\" file in the rest of the documentation.\nIf you are unfamiliar with how to use a preseed file for automatic bootstrapping of an image, please either take a look at our quick guides to image bootstrapping, or research automatic configuration for your specific guest operating system. Knowing how to automatically initalize your operating system is critical for being able to successfully use Packer.\nFor more details on how to use each communicator, visit the communicators page."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.5.x/extending/custom-post-processors",
  "text": "Custom Post-Processors - Extending | Packer\nPacker Post-processors are the components of Packer that transform one artifact into another, for example by compressing files, or uploading them.\nIn the compression example, the transformation would be taking an artifact with a set of files, compressing those files, and returning a new artifact with only a single file (the compressed archive). For the upload example, the transformation would be taking an artifact with some set of files, uploading those files, and returning an artifact with a single ID: the URL of the upload.\nPost-processor plugins implement the packer.PostProcessor interface and are served using the plugin.ServePostProcessor function.\nThe interface that must be implemented for a post-processor is the packer.PostProcessor interface. It is reproduced below for reference. The actual interface in the source code contains some basic documentation as well explaining what each method should do.\ntype PostProcessor interface { ConfigSpec() hcldec.ObjectSpec Configure(interface{}) error PostProcess(context.Context, Ui, Artifact) (a Artifact, keep, mustKeep bool, err error) } \nThe \"Configure\" Method\nThe Configure method for each post-processor is called early in the build process to configure the post-processor. The configuration is passed in as a raw interface{}. The configure method is responsible for translating this configuration into an internal structure, validating it, and returning any errors.\nFor decoding the interface{} into a meaningful structure, the mapstructure library is recommended. Mapstructure will take an interface{} and decode it into an arbitrarily complex struct. If there are any errors, it generates very human-friendly errors that can be returned directly from the configure method.\nWhile it is not actively enforced, no side effects should occur from running the Configure method. Specifically, don't create files, don't create network connections, etc. Configure's purpose is solely to setup internal state and validate the configuration as much as possible.\nConfigure being run is not an indication that PostProcess will ever run. For example, packer validate will run Configure to verify the configuration validates, but will never actually run the build.\nThe \"PostProcess\" Method\nThe PostProcess method is where the real work goes. PostProcess is responsible for taking one packer.Artifact implementation, and transforming it into another. A PostProcess call can be cancelled at any moment. Cancellation is triggered when the done chan of the context struct (<-ctx.Done()) unblocks .\nWhen we say \"transform,\" we don't mean actually modifying the existing packer.Artifact value itself. We mean taking the contents of the artifact and creating a new artifact from that. For example, if we were creating a \"compress\" post-processor that is responsible for compressing files, the transformation would be taking the Files() from the original artifact, compressing them, and creating a new artifact with a single file: the compressed archive.\nThe result signature of this method is (Artifact, bool, bool, error). Each return value is explained below:\nArtifact - The newly created artifact if no errors occurred.\nbool - If keep true, the input artifact will forcefully be kept. By default, Packer typically deletes all input artifacts, since the user doesn't generally want intermediary artifacts. However, some post-processors depend on the previous artifact existing. If this is true, it forces packer to keep the artifact around.\nbool - If forceOverride is true, then any user input for keep_input_artifact is ignored and the artifact is either kept or discarded according to the value set in keep.\nerror - Non-nil if there was an error in any way. If this is the case, the other two return values are ignored."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.9.x/templates/legacy_json_templates/communicator",
  "text": "Communicators are the mechanism Packer uses to upload files, execute scripts, etc. with the machine being created.\nCommunicators are configured within the builder section.\nAll communicators have the following options:\nDepending on your builder, your communicator may not have all it needs in order to work \"out of the box\".\nIf you are building from a cloud image (for example, building on Amazon), there is a good chance that your cloud provider has already preconfigured SSH on the image for you, meaning that all you have to do is configure the communicator in the Packer template.\nHowever, if you are building from a brand-new and unconfigured operating system image, you will almost always have to perform some extra work to configure SSH on the guest machine. For most operating system distributions, this work will be performed by a boot_command that references a file which provides answers to the normally-interactive questions you get asked when installing an operating system. The name of this file varies by operating system; some common examples are the \"preseed\" file required by Debian, the \"kickstart\" file required by CentOS or the \"answer file\", also known as the Autounattend.xml file, required by Windows. For simplicity's sake, we'll refer to this file as the \"preseed\" file in the rest of the documentation.\nIf you are unfamiliar with how to use a preseed file for automatic bootstrapping of an image, please either take a look at our quick guides to image bootstrapping, or research automatic configuration for your specific guest operating system. Knowing how to automatically initalize your operating system is critical for being able to successfully use Packer.\nFor more details on how to use each communicator, visit the communicators page."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.8.x/templates/legacy_json_templates/communicator",
  "text": "Communicators are the mechanism Packer uses to upload files, execute scripts, etc. with the machine being created.\nCommunicators are configured within the builder section.\nAll communicators have the following options:\nDepending on your builder, your communicator may not have all it needs in order to work \"out of the box\".\nIf you are building from a cloud image (for example, building on Amazon), there is a good chance that your cloud provider has already preconfigured SSH on the image for you, meaning that all you have to do is configure the communicator in the Packer template.\nHowever, if you are building from a brand-new and unconfigured operating system image, you will almost always have to perform some extra work to configure SSH on the guest machine. For most operating system distributions, this work will be performed by a boot_command that references a file which provides answers to the normally-interactive questions you get asked when installing an operating system. The name of this file varies by operating system; some common examples are the \"preseed\" file required by Debian, the \"kickstart\" file required by CentOS or the \"answer file\", also known as the Autounattend.xml file, required by Windows. For simplicity's sake, we'll refer to this file as the \"preseed\" file in the rest of the documentation.\nIf you are unfamiliar with how to use a preseed file for automatic bootstrapping of an image, please either take a look at our quick guides to image bootstrapping, or research automatic configuration for your specific guest operating system. Knowing how to automatically initalize your operating system is critical for being able to successfully use Packer.\nFor more details on how to use each communicator, visit the communicators page."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/templates/legacy_json_templates/communicator",
  "text": "Communicators are the mechanism Packer uses to upload files, execute scripts, etc. with the machine being created.\nCommunicators are configured within the builder section.\nAll communicators have the following options:\nDepending on your builder, your communicator may not have all it needs in order to work \"out of the box\".\nIf you are building from a cloud image (for example, building on Amazon), there is a good chance that your cloud provider has already preconfigured SSH on the image for you, meaning that all you have to do is configure the communicator in the Packer template.\nHowever, if you are building from a brand-new and unconfigured operating system image, you will almost always have to perform some extra work to configure SSH on the guest machine. For most operating system distributions, this work will be performed by a boot_command that references a file which provides answers to the normally-interactive questions you get asked when installing an operating system. The name of this file varies by operating system; some common examples are the \"preseed\" file required by Debian, the \"kickstart\" file required by CentOS or the \"answer file\", also known as the Autounattend.xml file, required by Windows. For simplicity's sake, we'll refer to this file as the \"preseed\" file in the rest of the documentation.\nIf you are unfamiliar with how to use a preseed file for automatic bootstrapping of an image, please either take a look at our quick guides to image bootstrapping, or research automatic configuration for your specific guest operating system. Knowing how to automatically initalize your operating system is critical for being able to successfully use Packer.\nFor more details on how to use each communicator, visit the communicators page."
},
{
  "url": "https://developer.hashicorp.com/packer/guides/v1.10.x/automatic-operating-system-installs",
  "text": "Automatic OS Installs | Packer\nIf you are building from a brand-new and unconfigured operating system image, Packer will need you to perform the operating system install before it can connect to and configure your image using its provisioners. Most operating system distributions have a mechanism for performing the normally-interactive installation in an automated way. For Debian operating systems, this is done using a preseed file; for Windows, it's done using an Autounattend.xml. We have compiled some simple guides here for common operating system distributions.\nThese guides are meant to give you a quick introduction to how to use automated installation answer files in order to perfom those installs; we don't mean to be a comprehensive guide on each operating system, but we hope to give you enough context to be able to more easily find any further information you need.\nRefer to the instructions for your operating system."
},
{
  "url": "https://developer.hashicorp.com/packer/plugins/v1.9.x/builders/vmware/iso",
  "text": "This page does not exist for version v1.9.x."
},
{
  "url": "https://developer.hashicorp.com/packer/guides/v1.9.x/automatic-operating-system-installs",
  "text": "Automatic OS Installs | Packer\nIf you are building from a brand-new and unconfigured operating system image, Packer will need you to perform the operating system install before it can connect to and configure your image using its provisioners. Most operating system distributions have a mechanism for performing the normally-interactive installation in an automated way. For Debian operating systems, this is done using a preseed file; for Windows, it's done using an Autounattend.xml. We have compiled some simple guides here for common operating system distributions.\nThese guides are meant to give you a quick introduction to how to use automated installation answer files in order to perfom those installs; we don't mean to be a comprehensive guide on each operating system, but we hope to give you enough context to be able to more easily find any further information you need.\nRefer to the instructions for your operating system."
},
{
  "url": "https://developer.hashicorp.com/packer/plugins/v1.10.x/builders/vmware/iso",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/BrandonRomano/azure",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/guides/v1.5.x/automatic-operating-system-installs",
  "text": "Automatic OS Installs | Packer\nIf you are building from a brand-new and unconfigured operating system image, Packer will need you to perform the operating system install before it can connect to and configure your image using its provisioners. Most operating system distributions have a mechanism for performing the normally-interactive installation in an automated way. For Debian operating systems, this is done using a preseed file; for Windows, it's done using an Autounattend.xml. We have compiled some simple guides here for common operating system distributions.\nThese guides are meant to give you a quick introduction to how to use automated installation answer files in order to perfom those installs; we don't mean to be a comprehensive guide on each operating system, but we hope to give you enough context to be able to more easily find any further information you need.\nPlease use the left-hand navigation to find instructions for the operating system that is relevant to you."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/builders/vmware-iso",
  "text": "This page does not exist for version v1.7.x."
},
{
  "url": "https://developer.hashicorp.com/packer/guides/v1.6.x/automatic-operating-system-installs",
  "text": "Automatic OS Installs | Packer\nIf you are building from a brand-new and unconfigured operating system image, Packer will need you to perform the operating system install before it can connect to and configure your image using its provisioners. Most operating system distributions have a mechanism for performing the normally-interactive installation in an automated way. For Debian operating systems, this is done using a preseed file; for Windows, it's done using an Autounattend.xml. We have compiled some simple guides here for common operating system distributions.\nThese guides are meant to give you a quick introduction to how to use automated installation answer files in order to perfom those installs; we don't mean to be a comprehensive guide on each operating system, but we hope to give you enough context to be able to more easily find any further information you need.\nPlease use the left-hand navigation to find instructions for the operating system that is relevant to you."
},
{
  "url": "https://developer.hashicorp.com/packer/guides/v1.7.x/automatic-operating-system-installs",
  "text": "Please use the left-hand navigation to find instructions for the operating system that is relevant to you."
},
{
  "url": "https://developer.hashicorp.com/packer/guides/v1.8.x/automatic-operating-system-installs",
  "text": "Refer to the instructions for your operating system."
},
{
  "url": "https://developer.hashicorp.com/packer/guides/automatic-operating-system-installs/autounattend_windows",
  "text": "Unattended Windows Installation | Packer\nUnattended Windows installation is done via \"Answer Files\", or \"Unattend files\".\nThese files are generally named \"autounattend.xml\". They are not Packer-specific tools, though we do make use of them.\nIf, after following this guide, you're still having issues getting an answer file working, We recommend you read the official documentation on answer files.\nThe guide here is hopefully enough to get you started, but isn't a replacement for the official documentation.\nIf you are installing the Windows Operating System from a mounted iso as part of your Packer build, you will need to use an Answer file. For example, you're building an image from scratch using the vmware-iso, virtualbox-iso, or hyperv-iso builders.\nIf you are not installing the operating system, you won't need to provide an answer file. If you are using a pre-built image in a cloud, you don't need to worry about Answer files.\nYou can either start from an example answer file from a known repo (take a look at the examples links below), or you can generate one using an answer file wizard by selecting New File > New Answer file on a Windows machine. A comprehensive list of all the options you can set in an answer file can be found here\nWindows will automatically look for an autounattend.xml file on mounted drives. Many users use the floppy_files option or a secondary mounted iso for providing the answer file to their iso builders.\nYou can also specify an unattend file to use by using the /unattend: option when running Windows Setup (setup.exe) in your boot_command.\nPacker needs the Answer File to handle any questions that would normally be answered interactively during a Windows installation.\nIf you want to be able to use provisioners, the Answer file must also contain a script that sets up SSH or WinRM so that Packer can connect to the instance.\nFinally, your Packer build will be much smoother if the Answer File handles or disables Windows updates rather than you trying to run them using a Packer provisioner. This is because the winrm communicator does not handle the disconnects caused by automatic reboots in Windows updates well, and the disconnections can fail a build.\nThe chef-maintained bento boxes are a great example of a Windows build that sets up openssh as part of the unattended installation so that Packer can connect using the SSH communicator. The functioning answer files for every modern Windows version can be found here.\nStefan Scherer's packer-windows repo is a great example of Windows builds that set up WinRM as part of the unattended installation so that Packer can connect using the winrm communicator:\n{ \"type\": \"virtualbox-iso\", \"guest_os_type\": \"Windows2008_64\", \"iso_url\": \"https://download.microsoft.com/download/7/5/E/75EC4E54-5B02-42D6-8879-D8D3A25FBEF7/7601.17514.101119-1850_x64fre_server_eval_en-us-GRMSXEVAL_EN_DVD.iso\", \"iso_checksum\": \"sha256:30832AD76CCFA4CE48CCB936EDEFE02079D42FB1DA32201BF9E3A880C8ED6312\", \"shutdown_command\": \"shutdown /s /t 10 /f /d p:4:1 /c Packer_Provisioning_Shutdown\", \"guest_additions_mode\": \"attach\", \"floppy_files\": [\"./scripts/Autounattend.xml\", \"./scripts/openssh.ps1\"], \"communicator\": \"winrm\", \"winrm_username\": \"vagrant\", \"winrm_password\": \"vagrant\" }"
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vmware/v1.1.0",
  "text": "VMware (v1.1.0) | Integrations | Packer\nThe Packer Plugin for VMware with to create virtual machine images for use with VMware products.\nThe Packer Plugin for VMware is a plugin that can be used to create virtual machine images for use with VMware desktop hypervisors (VMware Fusion Pro, VMware Workstation Pro, and VMware Workstation Player 1) and VMware vSphere Hypervisor 2.\nTo install this plugin, add following to your Packer configuration and run packer init.\npacker { required_plugins { vmware = { version = \"~> 1\" source = \"github.com/hashicorp/vmware\" } } } \npacker plugins install github.com/hashicorp/vmware \nThe plugin includes two builders which are able to create images, depending on your desired strategy.\nvmware-iso - This builder creates a virtual machine, installs an operating system from an ISO, provisions software within the operating system, and then exports the virtual machine as an image. This is best for those who want to start by creating an image.\nvmware-vmx - This builder imports an existing virtual machine (from a.vmx file), runs provisioners on the virtual machine, and then exports the virtual machine as an image. This is best for those who want to start from an existing virtual machine as the source. You can feed the artifact of this builder back into Packer to iterate on an image.\nKnown Issues\nVMware Workstation Player (Linux)\nYou may encounter issues due to dependencies and configuration requirements on VMware Workstation Player for Linux 1:\nDependencies\nAdd qemu-img. This command is available in the qemu package in Red Hat Enterprise Linux, Debian, and derivative distributions.\nAdd vmrun. This command is available from VMware Virtual Infrastructure eXtension (VIX) SDK.\nEdit the file /usr/lib/vmware-vix/vixwrapper-config.txt. The version specified in the fourth column must be changed to match the version in the third column of the output from the vmplayer -v command.\nFor detailed steps and troubleshooting, refer to this StackOverflow thread."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vmware/v1.0.8/components/builder/iso",
  "text": "VMware Builder (1.0.8) | Integrations | Packer\nThe Packer Plugin for VMware with to create virtual machine images for use with VMware products.\nType: vmware-iso Artifact BuilderId: mitchellh.vmware If remote_type is esx: Artifact BuilderId: mitchellh.vmware-esx\nThis VMware Packer builder is able to create VMware virtual machines from an ISO file as a source. It currently supports building virtual machines on hosts running VMware Fusion for OS X, VMware Workstation for Linux and Windows, and VMware Player on Linux. It can also build machines directly on VMware vSphere Hypervisor using SSH as opposed to the vSphere API.\nThe builder builds a virtual machine by creating a new virtual machine from scratch, booting it, installing an OS, provisioning software within the OS, then shutting it down. The result of the VMware builder is a directory containing all the files necessary to run the virtual machine.\nBasic Example\nHere is a basic example. This example is not functional. It will start the OS installer but then fail because we don't provide the preseed file for Ubuntu to self-install. Still, the example serves to show the basic configuration:\n{ \"type\": \"vmware-iso\", \"iso_url\": \"http://old-releases.ubuntu.com/releases/precise/ubuntu-12.04.2-server-amd64.iso\", \"iso_checksum\": \"md5:af5f788aee1b32c4b2634734309cc9e9\", \"ssh_username\": \"packer\", \"ssh_password\": \"packer\", \"shutdown_command\": \"shutdown -P now\" } \nsource \"vmware-iso\" \"basic-example\" { iso_url = \"http://old-releases.ubuntu.com/releases/precise/ubuntu-12.04.2-server-amd64.iso\" iso_checksum = \"md5:af5f788aee1b32c4b2634734309cc9e9\" ssh_username = \"packer\" ssh_password = \"packer\" shutdown_command = \"shutdown -P now\" } build { sources = [\"sources.vmware-iso.basic-example\"] } \nVMware-ISO Builder Configuration Reference\nThere are many configuration options available for the builder. In addition to the items listed here, you will want to look at the general configuration references for ISO, HTTP, Floppy, CD, Boot, Driver, Hardware, Output, Run, Shutdown, Communicator, Tools, vmx, Export, configuration references, which are necessary for this build to succeed and can be found further down the page.\ndisk_size (uint) - The size of the hard disk for the VM in megabytes. The builder uses expandable, not fixed-size virtual hard disks, so the actual file representing the disk will not use the full size unless it is full. By default this is set to 40000 (about 40 GB).\ncdrom_adapter_type (string) - The adapter type (or bus) that will be used by the cdrom device. This is chosen by default based on the disk adapter type. VMware tends to lean towards ide for the cdrom device unless sata is chosen for the disk adapter and so Packer attempts to mirror this logic. This field can be specified as either ide, sata, or scsi.\nguest_os_type (string) - The guest OS type being installed. This will be set in the VMware VMX. By default this is other. By specifying a more specific OS type, VMware may perform some optimizations or virtual hardware changes to better support the operating system running in the virtual machine. Valid values differ by platform and version numbers, and may not match other VMware API's representation of the guest OS names. Consult your platform for valid values.\nversion (string) - The vmx hardware version for the new virtual machine. Only the default value has been tested, any other value is experimental. Default value is 9.\nvm_name (string) - This is the name of the VMX file for the new virtual machine, without the file extension. By default this is packer-BUILDNAME, where \"BUILDNAME\" is the name of the build.\nvmx_disk_template_path (string) - VMX Disk Template Path\nvmx_template_path (string) - Path to a configuration template that defines the contents of the virtual machine VMX file for VMware. The engine has access to the template variables {{ .DiskNumber }} and {{ .DiskName }}.\nThis is for advanced users only as this can render the virtual machine non-functional. See below for more information. For basic VMX modifications, try vmx_data first.\nsnapshot_name (string) - This is the name of the initial snapshot created after provisioning and cleanup. if left blank, no initial snapshot will be created\nExtra Disk Configuration\ndisk_additional_size ([]uint) - The size(s) of any additional hard disks for the VM in megabytes. If this is not specified then the VM will only contain a primary hard disk. The builder uses expandable, not fixed-size virtual hard disks, so the actual file representing the disk will not use the full size unless it is full.\ndisk_adapter_type (string) - The adapter type of the VMware virtual disk to create. This option is for advanced usage, modify only if you know what you're doing. Some of the options you can specify are ide, sata, nvme or scsi (which uses the \"lsilogic\" scsi interface by default). If you specify another option, Packer will assume that you're specifying a scsi interface of that specified type. For more information, please consult Virtual Disk Manager User's Guide for desktop VMware clients. For ESXi, refer to the proper ESXi documentation.\nvmdk_name (string) - The filename of the virtual disk that'll be created, without the extension. This defaults to \"disk\".\ndisk_type_id (string) - The type of VMware virtual disk to create. This option is for advanced usage.\nFor desktop VMware clients:\nType IDDescription\n0\tGrowable virtual disk contained in a single file (monolithic sparse).\t\n1\tGrowable virtual disk split into 2GB files (split sparse).\t\n2\tPreallocated virtual disk contained in a single file (monolithic flat).\t\n3\tPreallocated virtual disk split into 2GB files (split flat).\t\n4\tPreallocated virtual disk compatible with ESX server (VMFS flat).\t\n5\tCompressed disk optimized for streaming.\t\nThe default is 1.\nFor ESXi, this defaults to zeroedthick. The available options for ESXi are: zeroedthick, eagerzeroedthick, thin. rdm:dev, rdmp:dev, 2gbsparse are not supported. Due to default disk compaction, when using zeroedthick or eagerzeroedthick set skip_compaction to true.\nFor more information, please consult the Virtual Disk Manager User's Guide for desktop VMware clients. For ESXi, refer to the proper ESXi documentation.\nA floppy can be made available for your build. This is most useful for unattended Windows installs, which look for an Autounattend.xml file on removable media. By default, no floppy will be attached. All files listed in this setting get placed into the root directory of the floppy and the floppy is attached as the first floppy device. The summary size of the listed files must not exceed 1.44 MB. The supported ways to move large files into the OS are using http_directory or the file provisioner.\nfloppy_files ([]string) - A list of files to place onto a floppy disk that is attached when the VM is booted. Currently, no support exists for creating sub-directories on the floppy. Wildcard characters (\\*, ?, and []) are allowed. Directory names are also allowed, which will add all the files found in the directory to the floppy.\nfloppy_dirs ([]string) - A list of directories to place onto the floppy disk recursively. This is similar to the floppy_files option except that the directory structure is preserved. This is useful for when your floppy disk includes drivers or if you just want to organize it's contents as a hierarchy. Wildcard characters (\\*, ?, and []) are allowed. The maximum summary size of all files in the listed directories are the same as in floppy_files.\nfloppy_files = [\"vendor-data\"] floppy_content = { \"meta-data\" = jsonencode(local.instance_data) \"user-data\" = templatefile(\"user-data\", { packages = [\"nginx\"] }) } floppy_label = \"cidata\" \nfloppy_label (string) - Floppy Label\nCD configuration\nShutdown configuration\nshutdown_command (string) - The command to use to gracefully shut down the machine once all provisioning is complete. By default this is an empty string, which tells Packer to just forcefully shut down the machine. This setting can be safely omitted if for example, a shutdown command to gracefully halt the machine is configured inside a provisioning script. If one or more scripts require a reboot it is suggested to leave this blank (since reboots may fail) and instead specify the final shutdown command in your last script.\nshutdown_timeout (duration string | ex: \"1h5m2s\") - The amount of time to wait after executing the shutdown_command for the virtual machine to actually shut down. If the machine doesn't shut down in this time it is considered an error. By default, the time out is \"5m\" (five minutes).\nDriver configuration\ncleanup_remote_cache (bool) - When set to true, Packer will cleanup the cache folder where the ISO file is stored during the build on the remote machine. By default, this is set to false.\nfusion_app_path (string) - Path to \"VMware Fusion.app\". By default this is /Applications/VMware Fusion.app but this setting allows you to customize this.\nremote_type (string) - The type of remote machine that will be used to build this VM rather than a local desktop product. The only value accepted for this currently is esx5. If this is not set, a desktop product will be used. By default, this is not set.\nremote_datastore (string) - The path to the datastore where the VM will be stored on the ESXi machine.\nremote_cache_datastore (string) - The path to the datastore where supporting files will be stored during the build on the remote machine.\nremote_cache_directory (string) - The path where the ISO and/or floppy files will be stored during the build on the remote machine. The path is relative to the remote_cache_datastore on the remote machine.\nremote_host (string) - The host of the remote machine used for access. This is only required if remote_type is enabled.\nremote_port (int) - The SSH port of the remote machine\nremote_username (string) - The SSH username used to access the remote machine.\nremote_password (string) - The SSH password for access to the remote machine.\nremote_private_key_file (string) - The SSH key for access to the remote machine.\nskip_validate_credentials (bool) - When Packer is preparing to run a remote esxi build, and export is not disable, by default it runs a no-op ovftool command to make sure that the remote_username and remote_password given are valid. If you set this flag to true, Packer will skip this validation. Default: false.\nHardware configuration\ncpus (int) - The number of cpus to use when building the VM.\nmemory (int) - The amount of memory to use when building the VM in megabytes.\ncores (int) - The number of cores per socket to use when building the VM. This corresponds to the cpuid.coresPerSocket option in the .vmx file.\nnetwork (string) - This is the network type that the virtual machine will be created with. This can be one of the generic values that map to a device such as hostonly, nat, or bridged. If the network is not one of these values, then it is assumed to be a VMware network device. (VMnet0..x)\nnetwork_adapter_type (string) - This is the ethernet adapter type the the virtual machine will be created with. By default the e1000 network adapter type will be used by Packer. For more information, please consult Choosing a network adapter for your virtual machine for desktop VMware clients. For ESXi, refer to the proper ESXi documentation.\nnetwork_name (string) - The custom name of the network. Sets the vmx value \"ethernet0.networkName\"\nsound (bool) - Specify whether to enable VMware's virtual soundcard device when building the VM. Defaults to false.\nusb (bool) - Enable VMware's USB bus when building the guest VM. Defaults to false. To enable usage of the XHCI bus for USB 3 (5 Gbit/s), one can use the vmx_data option to enable it by specifying true for the usb_xhci.present property.\nserial (string) - This specifies a serial port to add to the VM. It has a format of Type:option1,option2,.... The field Type can be one of the following values: FILE, DEVICE, PIPE, AUTO, or NONE.\nFILE:path(,yield) - Specifies the path to the local file to be used as the serial port.\nyield (bool) - This is an optional boolean that specifies whether the vm should yield the cpu when polling the port. By default, the builder will assume this as FALSE.\nDEVICE:path(,yield) - Specifies the path to the local device to be used as the serial port. If path is empty, then default to the first serial port.\nyield (bool) - This is an optional boolean that specifies whether the vm should yield the cpu when polling the port. By default, the builder will assume this as FALSE.\nPIPE:path,endpoint,host(,yield) - Specifies to use the named-pipe \"path\" as a serial port. This has a few options that determine how the VM should use the named-pipe.\nendpoint (string) - Chooses the type of the VM-end, which can be either a client or server.\nhost (string) - Chooses the type of the host-end, which can be either app (application) or vm (another virtual-machine).\nyield (bool) - This is an optional boolean that specifies whether the vm should yield the cpu when polling the port. By default, the builder will assume this as FALSE.\nAUTO:(yield) - Specifies to use auto-detection to determine the serial port to use. This has one option to determine how the VM should support the serial port.\nyield (bool) - This is an optional boolean that specifies whether the vm should yield the cpu when polling the port. By default, the builder will assume this as FALSE.\nNONE - Specifies to not use a serial port. (default)\nparallel (string) - This specifies a parallel port to add to the VM. It has the format of Type:option1,option2,.... Type can be one of the following values: FILE, DEVICE, AUTO, or NONE.\nFILE:path - Specifies the path to the local file to be used for the parallel port.\nDEVICE:path - Specifies the path to the local device to be used for the parallel port.\nAUTO:direction - Specifies to use auto-detection to determine the parallel port. Direction can be BI to specify bidirectional communication or UNI to specify unidirectional communication.\nNONE - Specifies to not use a parallel port. (default)\nOutput configuration\noutput_directory (string) - This is the path on your local machine (the one running Packer) to the directory where the resulting virtual machine will be created. This may be relative or absolute. If relative, the path is relative to the working directory when packer is executed.\nIf you are running a remote esx build, the output_dir is the path on your local machine (the machine running Packer) to which Packer will export the vm if you have \"skip_export\": false. If you want to manage the virtual machine's path on the remote datastore, use remote_output_dir.\nThis directory must not exist or be empty prior to running the builder.\nBy default this is output-BUILDNAME where \"BUILDNAME\" is the name of the build.\nremote_output_directory (string) - This is the directoy on your remote esx host where you will save your vm, relative to your remote_datastore.\nThis option's default value is your vm_name, and the final path of your vm will be vmfs/volumes/$remote_datastore/$vm_name/$vm_name.vmx where $remote_datastore and $vm_name match their corresponding template options\nFor example, setting \"remote_output_directory\": \"path/to/subdir will create a directory /vmfs/volumes/remote_datastore/path/to/subdir.\nPacker will not create the remote datastore for you; it must already exist. However, Packer will create all directories defined in the option that do not currently exist.\nThis option will be ignored unless you are building on a remote esx host.\nRun configuration\nNote: If vnc_over_websocket is set to true, any other VNC configuration will be ignored.\nheadless (bool) - Packer defaults to building VMware virtual machines by launching a GUI that shows the console of the machine being built. When this value is set to true, the machine will start without a console. For VMware machines, Packer will output VNC connection information in case you need to connect to the console to debug the build process. Some users have experienced issues where Packer cannot properly connect to a VM if it is headless; this appears to be a result of not ever having launched the VMWare GUI and accepting the evaluation license, or supplying a real license. If you experience this, launching VMWare and accepting the license should resolve your problem.\nvnc_bind_address (string) - The IP address that should be binded to for VNC. By default packer will use 127.0.0.1 for this. If you wish to bind to all interfaces use 0.0.0.0.\nvnc_port_min (int) - The minimum and maximum port to use for VNC access to the virtual machine. The builder uses VNC to type the initial boot_command. Because Packer generally runs in parallel, Packer uses a randomly chosen port in this range that appears available. By default this is 5900 to 6000. The minimum and maximum ports are inclusive.\nvnc_port_max (int) - VNC Port Max\nvnc_disable_password (bool) - Don't auto-generate a VNC password that is used to secure the VNC communication with the VM. This must be set to true if building on ESXi 6.5 and 6.7 with VNC enabled. Defaults to false.\nvnc_over_websocket (bool) - When set to true, Packer will connect to the remote VNC server over a websocket connection and any other VNC configuration option will be ignored. Remote builds using ESXi 6.7+ allows to connect to the VNC server only over websocket, for these the vnc_over_websocket must be set to true.\ninsecure_connection (bool) - Do not validate VNC over websocket server's TLS certificate. Defaults to false.\nTools configuration\ntools_upload_flavor (string) - The flavor of the VMware Tools ISO to upload into the VM. Valid values are darwin, linux, and windows. By default, this is empty, which means VMware tools won't be uploaded.\ntools_upload_path (string) - The path in the VM to upload the VMware tools. This only takes effect if tools_upload_flavor is non-empty. This is a configuration template that has a single valid variable: Flavor, which will be the value of tools_upload_flavor. By default the upload path is set to {{.Flavor}}.iso. This setting is not used when remote_type is esx5.\ntools_source_path (string) - The path on your local machine to fetch the vmware tools from. If this is not set but the tools_upload_flavor is set, then Packer will try to load the VMWare tools from the VMWare installation directory.\nVMX configuration\nvmx_data (map[string]string) - Arbitrary key/values to enter into the virtual machine VMX file. This is for advanced users who want to set properties that aren't yet supported by the builder.\nvmx_data_post (map[string]string) - Identical to vmx_data, except that it is run after the virtual machine is shutdown, and before the virtual machine is exported.\nvmx_remove_ethernet_interfaces (bool) - Remove all ethernet interfaces from the VMX file after building. This is for advanced users who understand the ramifications, but is useful for building Vagrant boxes since Vagrant will create ethernet interfaces when provisioning a box. Defaults to false.\ndisplay_name (string) - The name that will appear in your vSphere client, and will be used for the vmx basename. This will override the \"displayname\" value in your vmx file. It will also override the \"displayname\" if you have set it in the \"vmx_data\" Packer option. This option is useful if you are chaining vmx builds and want to make sure that the display name of each step in the chain is unique.\nExport configuration\nformat (string) - Either \"ovf\", \"ova\" or \"vmx\", this specifies the output format of the exported virtual machine. This defaults to \"ovf\" for remote (esx) builds, and \"vmx\" for local builds. Before using this option, you need to install ovftool. Since ovftool is only capable of password based authentication remote_password must be set when exporting the VM from a remote instance. If you are building locally, Packer will create a vmx and then export that vm to an ovf or ova. Packer will not delete the vmx and vmdk files; this is left up to the user if you don't want to keep those files.\novftool_options ([]string) - Extra options to pass to ovftool during export. Each item in the array is a new argument. The options --noSSLVerify, --skipManifestCheck, and --targetType are used by Packer for remote exports, and should not be passed to this argument. For ovf/ova exports from local builds, Packer does not automatically set any ovftool options.\nskip_export (bool) - Defaults to false. When true, Packer will not export the VM. This can be useful if the build output is not the resultant image, but created inside the VM.\nkeep_registered (bool) - Set this to true if you would like to keep a remotely-built VM registered with the remote ESXi server. If you do not need to export the vm, then also set skip_export: true in order to avoid unnecessarily using ovftool to export the vm. Defaults to false.\nskip_compaction (bool) - VMware-created disks are defragmented and compacted at the end of the build process using vmware-vdiskmanager or vmkfstools in ESXi. In certain rare cases, this might actually end up making the resulting disks slightly larger. If you find this to be the case, you can disable compaction using this configuration value. Defaults to false. Default to true for ESXi when disk_type_id is not explicitly defined and false otherwise.\nThe boot command \"typed\" character for character over a VNC connection to the machine, simulating a human actually typing the keyboard.\nKeystrokes are typed as separate key up/down events over VNC with a default 100ms delay. The delay alleviates issues with latency and CPU contention. You can tune this delay on a per-builder basis by specifying \"boot_key_interval\" in your Packer template.\nNote: for the HTTPIP to be resolved correctly, your VM's network configuration has to include a hostonly or nat type network interface. If you are using this feature, it is recommended to leave the default network configuration while you are building the VM, and use the vmx_data_post hook to modify the network configuration after the VM is done building.\ndisable_vnc (bool) - Whether to create a VNC connection or not. A boot_command cannot be used when this is true. Defaults to false.\nboot_key_interval (duration string | ex: \"1h5m2s\") - Time in ms to wait between each key press\nVMX Template\nThe heart of a VMware machine is the \"vmx\" file. This contains all the virtual hardware metadata necessary for the VM to function. Packer by default uses a safe, flexible VMX file. But for advanced users, this template can be customized. This allows Packer to build virtual machines of effectively any guest operating system type.\nThis is an advanced feature. Modifying the VMX template can easily cause your virtual machine to not boot properly. Please only modify the template if you know what you're doing.\nWithin the template, a handful of variables are available so that your template can continue working with the rest of the Packer machinery. Using these variables isn't required, however.\nName - The name of the virtual machine.\nGuestOS - The VMware-valid guest OS type.\nDiskName - The filename (without the suffix) of the main virtual disk.\nISOPath - The path to the ISO to use for the OS installation.\nVersion - The Hardware version VMWare will execute this vm under. Also known as the virtualhw.version.\nBuilding on a Remote vSphere Hypervisor\nIn addition to using the desktop products of VMware locally to build virtual machines, Packer can use a remote VMware Hypervisor to build the virtual machine.\nNote: Packer supports ESXi 5.1 and above.\n$ esxcli system settings advanced set -o /Net/GuestIPHack -i 1 \nWhen using a remote VMware Hypervisor, the builder still downloads the ISO and various files locally, and uploads these to the remote machine. Packer currently uses SSH to communicate to the ESXi machine rather than the vSphere API. If you want to use vSphere API, see the vsphere-iso builder.\nPacker also requires VNC to issue boot commands during a build, which may be disabled on some remote VMware Hypervisors. Please consult the appropriate documentation on how to update VMware Hypervisor's firewall to allow these connections. VNC can be disabled by not setting a boot_command and setting disable_vnc to true.\nPlease note that you should disable vMotion for the host you intend to run Packer builds on; a vMotion event will cause the Packer build to fail.\nTo use a remote VMware vSphere Hypervisor to build your virtual machine, fill in the required remote_* configurations:\nremote_type - This must be set to \"esx5\".\nremote_host - The host of the remote machine.\nAdditionally, there are some optional configurations that you'll likely have to modify as well:\nremote_port - The SSH port of the remote machine\nremote_datastore - The path to the datastore where the VM will be stored on the ESXi machine.\nremote_cache_datastore - The path to the datastore where supporting files will be stored during the build on the remote machine.\nremote_cache_directory - The path where the ISO and/or floppy files will be stored during the build on the remote machine. The path is relative to the remote_cache_datastore on the remote machine.\nremote_username - The SSH username used to access the remote machine.\nremote_password - The SSH password for access to the remote machine.\nremote_private_key_file - The SSH key for access to the remote machine.\nformat (string) - Either \"ovf\", \"ova\" or \"vmx\", this specifies the output format of the exported virtual machine. This defaults to \"ovf\". Before using this option, you need to install ovftool. This option currently only works when option remote_type is set to \"esx5\". Since ovftool is only capable of password based authentication remote_password must be set when exporting the VM.\nvnc_disable_password - This must be set to \"true\" when using VNC with ESXi 6.5 or 6.7.\nVNC port discovery\nPacker needs to decide on a port to use for VNC when building remotely. To find an open port, we try to connect to ports in the range of vnc_port_min to vnc_port_max. If we notice something is listening on a port in the range, we try to connect to the next one, and so on until we find a port that has nothing listening on it. If you have many clients building on the ESXi host, there might be competition for the VNC ports. You can adjust how long Packer waits for a connection timeout by setting PACKER_ESXI_VNC_PROBE_TIMEOUT. This defaults to 15 seconds. Set this shorter if VNC connections are refused, and set it longer if Packer can't find an open port. This is intended as an advanced configuration option. Please make sure your firewall settings are correct before adjusting.\nUsing a Floppy for Linux kickstart file or preseed\nDepending on your network configuration, it may be difficult to use packer's built-in HTTP server with ESXi. Instead, you can provide a kickstart or preseed file by attaching a floppy disk. An example below, based on RHEL:\n{ \"builders\": [ { \"type\": \"vmware-iso\", \"floppy_files\": [\"folder/ks.cfg\"], \"boot_command\": \"<tab> text ks=floppy <enter><wait>\" } ] } \nIt's also worth noting that ks=floppy has been deprecated. Later versions of the Anaconda installer (used in RHEL/CentOS 7 and Fedora) may require a different syntax to source a kickstart file from a mounted floppy image.\n{ \"builders\": [ { \"type\": \"vmware-iso\", \"floppy_files\": [\"folder/ks.cfg\"], \"boot_command\": \"<tab> inst.text inst.ks=hd:fd0:/ks.cfg <enter><wait>\" } ] }"
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vmware/v1.0.2/components/builder/iso",
  "text": "VMware Builder (1.0.2) | Integrations | Packer\nThe Packer Plugin for VMware with to create virtual machine images for use with VMware products.\nType: vmware-iso Artifact BuilderId: mitchellh.vmware If remote_type is esx: Artifact BuilderId: mitchellh.vmware-esx\nThis VMware Packer builder is able to create VMware virtual machines from an ISO file as a source. It currently supports building virtual machines on hosts running VMware Fusion for OS X, VMware Workstation for Linux and Windows, and VMware Player on Linux. It can also build machines directly on VMware vSphere Hypervisor using SSH as opposed to the vSphere API.\nThe builder builds a virtual machine by creating a new virtual machine from scratch, booting it, installing an OS, provisioning software within the OS, then shutting it down. The result of the VMware builder is a directory containing all the files necessary to run the virtual machine.\nBasic Example\nHere is a basic example. This example is not functional. It will start the OS installer but then fail because we don't provide the preseed file for Ubuntu to self-install. Still, the example serves to show the basic configuration:\n{ \"type\": \"vmware-iso\", \"iso_url\": \"http://old-releases.ubuntu.com/releases/precise/ubuntu-12.04.2-server-amd64.iso\", \"iso_checksum\": \"md5:af5f788aee1b32c4b2634734309cc9e9\", \"ssh_username\": \"packer\", \"ssh_password\": \"packer\", \"shutdown_command\": \"shutdown -P now\" } \nsource \"vmware-iso\" \"basic-example\" { iso_url = \"http://old-releases.ubuntu.com/releases/precise/ubuntu-12.04.2-server-amd64.iso\" iso_checksum = \"md5:af5f788aee1b32c4b2634734309cc9e9\" ssh_username = \"packer\" ssh_password = \"packer\" shutdown_command = \"shutdown -P now\" } build { sources = [\"sources.vmware-iso.basic-example\"] } \nVMware-ISO Builder Configuration Reference\nThere are many configuration options available for the builder. In addition to the items listed here, you will want to look at the general configuration references for ISO, HTTP, Floppy, CD, Boot, Driver, Hardware, Output, Run, Shutdown, Communicator, Tools, vmx, Export, configuration references, which are necessary for this build to succeed and can be found further down the page.\ndisk_size (uint) - The size of the hard disk for the VM in megabytes. The builder uses expandable, not fixed-size virtual hard disks, so the actual file representing the disk will not use the full size unless it is full. By default this is set to 40000 (about 40 GB).\ncdrom_adapter_type (string) - The adapter type (or bus) that will be used by the cdrom device. This is chosen by default based on the disk adapter type. VMware tends to lean towards ide for the cdrom device unless sata is chosen for the disk adapter and so Packer attempts to mirror this logic. This field can be specified as either ide, sata, or scsi.\nguest_os_type (string) - The guest OS type being installed. This will be set in the VMware VMX. By default this is other. By specifying a more specific OS type, VMware may perform some optimizations or virtual hardware changes to better support the operating system running in the virtual machine. Valid values differ by platform and version numbers, and may not match other VMware API's representation of the guest OS names. Consult your platform for valid values.\nversion (string) - The vmx hardware version for the new virtual machine. Only the default value has been tested, any other value is experimental. Default value is 9.\nvm_name (string) - This is the name of the VMX file for the new virtual machine, without the file extension. By default this is packer-BUILDNAME, where \"BUILDNAME\" is the name of the build.\nvmx_disk_template_path (string) - VMX Disk Template Path\nvmx_template_path (string) - Path to a configuration template that defines the contents of the virtual machine VMX file for VMware. The engine has access to the template variables {{ .DiskNumber }} and {{ .DiskName }}.\nThis is for advanced users only as this can render the virtual machine non-functional. See below for more information. For basic VMX modifications, try vmx_data first.\nsnapshot_name (string) - This is the name of the initial snapshot created after provisioning and cleanup. if left blank, no initial snapshot will be created\nExtra Disk Configuration\ndisk_additional_size ([]uint) - The size(s) of any additional hard disks for the VM in megabytes. If this is not specified then the VM will only contain a primary hard disk. The builder uses expandable, not fixed-size virtual hard disks, so the actual file representing the disk will not use the full size unless it is full.\ndisk_adapter_type (string) - The adapter type of the VMware virtual disk to create. This option is for advanced usage, modify only if you know what you're doing. Some of the options you can specify are ide, sata, nvme or scsi (which uses the \"lsilogic\" scsi interface by default). If you specify another option, Packer will assume that you're specifying a scsi interface of that specified type. For more information, please consult Virtual Disk Manager User's Guide for desktop VMware clients. For ESXi, refer to the proper ESXi documentation.\nvmdk_name (string) - The filename of the virtual disk that'll be created, without the extension. This defaults to \"disk\".\ndisk_type_id (string) - The type of VMware virtual disk to create. This option is for advanced usage.\nFor desktop VMware clients:\nType IDDescription\n0\tGrowable virtual disk contained in a single file (monolithic sparse).\t\n1\tGrowable virtual disk split into 2GB files (split sparse).\t\n2\tPreallocated virtual disk contained in a single file (monolithic flat).\t\n3\tPreallocated virtual disk split into 2GB files (split flat).\t\n4\tPreallocated virtual disk compatible with ESX server (VMFS flat).\t\n5\tCompressed disk optimized for streaming.\t\nThe default is 1.\nFor ESXi, this defaults to zeroedthick. The available options for ESXi are: zeroedthick, eagerzeroedthick, thin. rdm:dev, rdmp:dev, 2gbsparse are not supported. Due to default disk compaction, when using zeroedthick or eagerzeroedthick set skip_compaction to true.\nFor more information, please consult the Virtual Disk Manager User's Guide for desktop VMware clients. For ESXi, refer to the proper ESXi documentation.\n`wget http://{{ .HTTPIP }}:{{ .HTTPPort }}/foo/bar/preseed.cfg` \nA floppy can be made available for your build. This is most useful for unattended Windows installs, which look for an Autounattend.xml file on removable media. By default, no floppy will be attached. All files listed in this setting get placed into the root directory of the floppy and the floppy is attached as the first floppy device. The summary size of the listed files must not exceed 1.44 MB. The supported ways to move large files into the OS are using http_directory or the file provisioner.\nfloppy_files ([]string) - A list of files to place onto a floppy disk that is attached when the VM is booted. Currently, no support exists for creating sub-directories on the floppy. Wildcard characters (\\*, ?, and []) are allowed. Directory names are also allowed, which will add all the files found in the directory to the floppy.\nfloppy_dirs ([]string) - A list of directories to place onto the floppy disk recursively. This is similar to the floppy_files option except that the directory structure is preserved. This is useful for when your floppy disk includes drivers or if you just want to organize it's contents as a hierarchy. Wildcard characters (\\*, ?, and []) are allowed. The maximum summary size of all files in the listed directories are the same as in floppy_files.\nfloppy_files = [\"vendor-data\"] floppy_content = { \"meta-data\" = jsonencode(local.instance_data) \"user-data\" = templatefile(\"user-data\", { packages = [\"nginx\"] }) } floppy_label = \"cidata\" \nfloppy_label (string) - Floppy Label\nCD configuration\nShutdown configuration\nshutdown_command (string) - The command to use to gracefully shut down the machine once all provisioning is complete. By default this is an empty string, which tells Packer to just forcefully shut down the machine. This setting can be safely omitted if for example, a shutdown command to gracefully halt the machine is configured inside a provisioning script. If one or more scripts require a reboot it is suggested to leave this blank (since reboots may fail) and instead specify the final shutdown command in your last script.\nshutdown_timeout (duration string | ex: \"1h5m2s\") - The amount of time to wait after executing the shutdown_command for the virtual machine to actually shut down. If the machine doesn't shut down in this time it is considered an error. By default, the time out is \"5m\" (five minutes).\nDriver configuration\ncleanup_remote_cache (bool) - When set to true, Packer will cleanup the cache folder where the ISO file is stored during the build on the remote machine. By default, this is set to false.\nfusion_app_path (string) - Path to \"VMware Fusion.app\". By default this is /Applications/VMware Fusion.app but this setting allows you to customize this.\nremote_type (string) - The type of remote machine that will be used to build this VM rather than a local desktop product. The only value accepted for this currently is esx5. If this is not set, a desktop product will be used. By default, this is not set.\nremote_datastore (string) - The path to the datastore where the VM will be stored on the ESXi machine.\nremote_cache_datastore (string) - The path to the datastore where supporting files will be stored during the build on the remote machine.\nremote_cache_directory (string) - The path where the ISO and/or floppy files will be stored during the build on the remote machine. The path is relative to the remote_cache_datastore on the remote machine.\nremote_host (string) - The host of the remote machine used for access. This is only required if remote_type is enabled.\nremote_port (int) - The SSH port of the remote machine\nremote_username (string) - The SSH username used to access the remote machine.\nremote_password (string) - The SSH password for access to the remote machine.\nremote_private_key_file (string) - The SSH key for access to the remote machine.\nskip_validate_credentials (bool) - When Packer is preparing to run a remote esxi build, and export is not disable, by default it runs a no-op ovftool command to make sure that the remote_username and remote_password given are valid. If you set this flag to true, Packer will skip this validation. Default: false.\nHardware configuration\ncpus (int) - The number of cpus to use when building the VM.\nmemory (int) - The amount of memory to use when building the VM in megabytes.\ncores (int) - The number of cores per socket to use when building the VM. This corresponds to the cpuid.coresPerSocket option in the .vmx file.\nnetwork (string) - This is the network type that the virtual machine will be created with. This can be one of the generic values that map to a device such as hostonly, nat, or bridged. If the network is not one of these values, then it is assumed to be a VMware network device. (VMnet0..x)\nnetwork_adapter_type (string) - This is the ethernet adapter type the the virtual machine will be created with. By default the e1000 network adapter type will be used by Packer. For more information, please consult Choosing a network adapter for your virtual machine for desktop VMware clients. For ESXi, refer to the proper ESXi documentation.\nnetwork_name (string) - The custom name of the network. Sets the vmx value \"ethernet0.networkName\"\nsound (bool) - Specify whether to enable VMware's virtual soundcard device when building the VM. Defaults to false.\nusb (bool) - Enable VMware's USB bus when building the guest VM. Defaults to false. To enable usage of the XHCI bus for USB 3 (5 Gbit/s), one can use the vmx_data option to enable it by specifying true for the usb_xhci.present property.\nserial (string) - This specifies a serial port to add to the VM. It has a format of Type:option1,option2,.... The field Type can be one of the following values: FILE, DEVICE, PIPE, AUTO, or NONE.\nFILE:path(,yield) - Specifies the path to the local file to be used as the serial port.\nDEVICE:path(,yield) - Specifies the path to the local device to be used as the serial port. If path is empty, then default to the first serial port.\nPIPE:path,endpoint,host(,yield) - Specifies to use the named-pipe \"path\" as a serial port. This has a few options that determine how the VM should use the named-pipe.\nendpoint (string) - Chooses the type of the VM-end, which can be either a client or server.\nhost (string) - Chooses the type of the host-end, which can be either app (application) or vm (another virtual-machine).\nAUTO:(yield) - Specifies to use auto-detection to determine the serial port to use. This has one option to determine how the VM should support the serial port.\nNONE - Specifies to not use a serial port. (default)\nparallel (string) - This specifies a parallel port to add to the VM. It has the format of Type:option1,option2,.... Type can be one of the following values: FILE, DEVICE, AUTO, or NONE.\nFILE:path - Specifies the path to the local file to be used for the parallel port.\nDEVICE:path - Specifies the path to the local device to be used for the parallel port.\nAUTO:direction - Specifies to use auto-detection to determine the parallel port. Direction can be BI to specify bidirectional communication or UNI to specify unidirectional communication.\nNONE - Specifies to not use a parallel port. (default)\nOutput configuration\noutput_directory (string) - This is the path on your local machine (the one running Packer) to the directory where the resulting virtual machine will be created. This may be relative or absolute. If relative, the path is relative to the working directory when packer is executed.\nIf you are running a remote esx build, the output_dir is the path on your local machine (the machine running Packer) to which Packer will export the vm if you have \"skip_export\": false. If you want to manage the virtual machine's path on the remote datastore, use remote_output_dir.\nThis directory must not exist or be empty prior to running the builder.\nBy default this is output-BUILDNAME where \"BUILDNAME\" is the name of the build.\nremote_output_directory (string) - This is the directoy on your remote esx host where you will save your vm, relative to your remote_datastore.\nThis option's default value is your vm_name, and the final path of your vm will be vmfs/volumes/$remote_datastore/$vm_name/$vm_name.vmx where $remote_datastore and $vm_name match their corresponding template options\nFor example, setting \"remote_output_directory\": \"path/to/subdir will create a directory /vmfs/volumes/remote_datastore/path/to/subdir.\nPacker will not create the remote datastore for you; it must already exist. However, Packer will create all directories defined in the option that do not currently exist.\nThis option will be ignored unless you are building on a remote esx host.\nRun configuration\nNote: If vnc_over_websocket is set to true, any other VNC configuration will be ignored.\nheadless (bool) - Packer defaults to building VMware virtual machines by launching a GUI that shows the console of the machine being built. When this value is set to true, the machine will start without a console. For VMware machines, Packer will output VNC connection information in case you need to connect to the console to debug the build process. Some users have experienced issues where Packer cannot properly connect to a VM if it is headless; this appears to be a result of not ever having launched the VMWare GUI and accepting the evaluation license, or supplying a real license. If you experience this, launching VMWare and accepting the license should resolve your problem.\nvnc_bind_address (string) - The IP address that should be binded to for VNC. By default packer will use 127.0.0.1 for this. If you wish to bind to all interfaces use 0.0.0.0.\nvnc_port_min (int) - The minimum and maximum port to use for VNC access to the virtual machine. The builder uses VNC to type the initial boot_command. Because Packer generally runs in parallel, Packer uses a randomly chosen port in this range that appears available. By default this is 5900 to 6000. The minimum and maximum ports are inclusive.\nvnc_port_max (int) - VNC Port Max\nvnc_disable_password (bool) - Don't auto-generate a VNC password that is used to secure the VNC communication with the VM. This must be set to true if building on ESXi 6.5 and 6.7 with VNC enabled. Defaults to false.\nvnc_over_websocket (bool) - When set to true, Packer will connect to the remote VNC server over a websocket connection and any other VNC configuration option will be ignored. Remote builds using ESXi 6.7+ allows to connect to the VNC server only over websocket, for these the vnc_over_websocket must be set to true.\ninsecure_connection (bool) - Do not validate VNC over websocket server's TLS certificate. Defaults to false.\nTools configuration\ntools_upload_flavor (string) - The flavor of the VMware Tools ISO to upload into the VM. Valid values are darwin, linux, and windows. By default, this is empty, which means VMware tools won't be uploaded.\ntools_upload_path (string) - The path in the VM to upload the VMware tools. This only takes effect if tools_upload_flavor is non-empty. This is a configuration template that has a single valid variable: Flavor, which will be the value of tools_upload_flavor. By default the upload path is set to {{.Flavor}}.iso. This setting is not used when remote_type is esx5.\ntools_source_path (string) - The path on your local machine to fetch the vmware tools from. If this is not set but the tools_upload_flavor is set, then Packer will try to load the VMWare tools from the VMWare installation directory.\nVMX configuration\nvmx_data (map[string]string) - Arbitrary key/values to enter into the virtual machine VMX file. This is for advanced users who want to set properties that aren't yet supported by the builder.\nvmx_data_post (map[string]string) - Identical to vmx_data, except that it is run after the virtual machine is shutdown, and before the virtual machine is exported.\nvmx_remove_ethernet_interfaces (bool) - Remove all ethernet interfaces from the VMX file after building. This is for advanced users who understand the ramifications, but is useful for building Vagrant boxes since Vagrant will create ethernet interfaces when provisioning a box. Defaults to false.\ndisplay_name (string) - The name that will appear in your vSphere client, and will be used for the vmx basename. This will override the \"displayname\" value in your vmx file. It will also override the \"displayname\" if you have set it in the \"vmx_data\" Packer option. This option is useful if you are chaining vmx builds and want to make sure that the display name of each step in the chain is unique.\nExport configuration\nformat (string) - Either \"ovf\", \"ova\" or \"vmx\", this specifies the output format of the exported virtual machine. This defaults to \"ovf\" for remote (esx) builds, and \"vmx\" for local builds. Before using this option, you need to install ovftool. Since ovftool is only capable of password based authentication remote_password must be set when exporting the VM from a remote instance. If you are building locally, Packer will create a vmx and then export that vm to an ovf or ova. Packer will not delete the vmx and vmdk files; this is left up to the user if you don't want to keep those files.\novftool_options ([]string) - Extra options to pass to ovftool during export. Each item in the array is a new argument. The options --noSSLVerify, --skipManifestCheck, and --targetType are used by Packer for remote exports, and should not be passed to this argument. For ovf/ova exports from local builds, Packer does not automatically set any ovftool options.\nskip_export (bool) - Defaults to false. When true, Packer will not export the VM. This can be useful if the build output is not the resultant image, but created inside the VM.\nkeep_registered (bool) - Set this to true if you would like to keep a remotely-built VM registered with the remote ESXi server. If you do not need to export the vm, then also set skip_export: true in order to avoid unnecessarily using ovftool to export the vm. Defaults to false.\nskip_compaction (bool) - VMware-created disks are defragmented and compacted at the end of the build process using vmware-vdiskmanager or vmkfstools in ESXi. In certain rare cases, this might actually end up making the resulting disks slightly larger. If you find this to be the case, you can disable compaction using this configuration value. Defaults to false. Default to true for ESXi when disk_type_id is not explicitly defined and false otherwise.\nThe boot command \"typed\" character for character over a VNC connection to the machine, simulating a human actually typing the keyboard.\nKeystrokes are typed as separate key up/down events over VNC with a default 100ms delay. The delay alleviates issues with latency and CPU contention. You can tune this delay on a per-builder basis by specifying \"boot_key_interval\" in your Packer template.\nNote: for the HTTPIP to be resolved correctly, your VM's network configuration has to include a hostonly or nat type network interface. If you are using this feature, it is recommended to leave the default network configuration while you are building the VM, and use the vmx_data_post hook to modify the network configuration after the VM is done building.\ndisable_vnc (bool) - Whether to create a VNC connection or not. A boot_command cannot be used when this is true. Defaults to false.\nboot_key_interval (duration string | ex: \"1h5m2s\") - Time in ms to wait between each key press\nVMX Template\nThe heart of a VMware machine is the \"vmx\" file. This contains all the virtual hardware metadata necessary for the VM to function. Packer by default uses a safe, flexible VMX file. But for advanced users, this template can be customized. This allows Packer to build virtual machines of effectively any guest operating system type.\nThis is an advanced feature. Modifying the VMX template can easily cause your virtual machine to not boot properly. Please only modify the template if you know what you're doing.\nWithin the template, a handful of variables are available so that your template can continue working with the rest of the Packer machinery. Using these variables isn't required, however.\nName - The name of the virtual machine.\nGuestOS - The VMware-valid guest OS type.\nDiskName - The filename (without the suffix) of the main virtual disk.\nISOPath - The path to the ISO to use for the OS installation.\nVersion - The Hardware version VMWare will execute this vm under. Also known as the virtualhw.version.\nBuilding on a Remote vSphere Hypervisor\nIn addition to using the desktop products of VMware locally to build virtual machines, Packer can use a remote VMware Hypervisor to build the virtual machine.\nNote: Packer supports ESXi 5.1 and above.\n$ esxcli system settings advanced set -o /Net/GuestIPHack -i 1 \nWhen using a remote VMware Hypervisor, the builder still downloads the ISO and various files locally, and uploads these to the remote machine. Packer currently uses SSH to communicate to the ESXi machine rather than the vSphere API. If you want to use vSphere API, see the vsphere-iso builder.\nPacker also requires VNC to issue boot commands during a build, which may be disabled on some remote VMware Hypervisors. Please consult the appropriate documentation on how to update VMware Hypervisor's firewall to allow these connections. VNC can be disabled by not setting a boot_command and setting disable_vnc to true.\nPlease note that you should disable vMotion for the host you intend to run Packer builds on; a vMotion event will cause the Packer build to fail.\nTo use a remote VMware vSphere Hypervisor to build your virtual machine, fill in the required remote_* configurations:\nremote_type - This must be set to \"esx5\".\nremote_host - The host of the remote machine.\nAdditionally, there are some optional configurations that you'll likely have to modify as well:\nremote_port - The SSH port of the remote machine\nremote_datastore - The path to the datastore where the VM will be stored on the ESXi machine.\nremote_cache_datastore - The path to the datastore where supporting files will be stored during the build on the remote machine.\nremote_cache_directory - The path where the ISO and/or floppy files will be stored during the build on the remote machine. The path is relative to the remote_cache_datastore on the remote machine.\nremote_username - The SSH username used to access the remote machine.\nremote_password - The SSH password for access to the remote machine.\nremote_private_key_file - The SSH key for access to the remote machine.\nformat (string) - Either \"ovf\", \"ova\" or \"vmx\", this specifies the output format of the exported virtual machine. This defaults to \"ovf\". Before using this option, you need to install ovftool. This option currently only works when option remote_type is set to \"esx5\". Since ovftool is only capable of password based authentication remote_password must be set when exporting the VM.\nvnc_disable_password - This must be set to \"true\" when using VNC with ESXi 6.5 or 6.7.\nVNC port discovery\nPacker needs to decide on a port to use for VNC when building remotely. To find an open port, we try to connect to ports in the range of vnc_port_min to vnc_port_max. If we notice something is listening on a port in the range, we try to connect to the next one, and so on until we find a port that has nothing listening on it. If you have many clients building on the ESXi host, there might be competition for the VNC ports. You can adjust how long Packer waits for a connection timeout by setting PACKER_ESXI_VNC_PROBE_TIMEOUT. This defaults to 15 seconds. Set this shorter if VNC connections are refused, and set it longer if Packer can't find an open port. This is intended as an advanced configuration option. Please make sure your firewall settings are correct before adjusting.\nUsing a Floppy for Linux kickstart file or preseed\nDepending on your network configuration, it may be difficult to use packer's built-in HTTP server with ESXi. Instead, you can provide a kickstart or preseed file by attaching a floppy disk. An example below, based on RHEL:\n{ \"builders\": [ { \"type\": \"vmware-iso\", \"floppy_files\": [\"folder/ks.cfg\"], \"boot_command\": \"<tab> text ks=floppy <enter><wait>\" } ] } \nIt's also worth noting that ks=floppy has been deprecated. Later versions of the Anaconda installer (used in RHEL/CentOS 7 and Fedora) may require a different syntax to source a kickstart file from a mounted floppy image.\n{ \"builders\": [ { \"type\": \"vmware-iso\", \"floppy_files\": [\"folder/ks.cfg\"], \"boot_command\": \"<tab> inst.text inst.ks=hd:fd0:/ks.cfg <enter><wait>\" } ] }"
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vmware/v1.0.0/components/builder/iso",
  "text": "VMware Builder (1.0.0) | Integrations | Packer\nThe Packer Plugin for VMware with to create virtual machine images for use with VMware products.\nType: vmware-iso Artifact BuilderId: mitchellh.vmware If remote_type is esx: Artifact BuilderId: mitchellh.vmware-esx\nThis VMware Packer builder is able to create VMware virtual machines from an ISO file as a source. It currently supports building virtual machines on hosts running VMware Fusion for OS X, VMware Workstation for Linux and Windows, and VMware Player on Linux. It can also build machines directly on VMware vSphere Hypervisor using SSH as opposed to the vSphere API.\nThe builder builds a virtual machine by creating a new virtual machine from scratch, booting it, installing an OS, provisioning software within the OS, then shutting it down. The result of the VMware builder is a directory containing all the files necessary to run the virtual machine.\nBasic Example\nHere is a basic example. This example is not functional. It will start the OS installer but then fail because we don't provide the preseed file for Ubuntu to self-install. Still, the example serves to show the basic configuration:\n{ \"type\": \"vmware-iso\", \"iso_url\": \"http://old-releases.ubuntu.com/releases/precise/ubuntu-12.04.2-server-amd64.iso\", \"iso_checksum\": \"md5:af5f788aee1b32c4b2634734309cc9e9\", \"ssh_username\": \"packer\", \"ssh_password\": \"packer\", \"shutdown_command\": \"shutdown -P now\" } \nsource \"vmware-iso\" \"basic-example\" { iso_url = \"http://old-releases.ubuntu.com/releases/precise/ubuntu-12.04.2-server-amd64.iso\" iso_checksum = \"md5:af5f788aee1b32c4b2634734309cc9e9\" ssh_username = \"packer\" ssh_password = \"packer\" shutdown_command = \"shutdown -P now\" } build { sources = [\"sources.vmware-iso.basic-example\"] } \nVMware-ISO Builder Configuration Reference\nThere are many configuration options available for the builder. In addition to the items listed here, you will want to look at the general configuration references for ISO, HTTP, Floppy, CD, Boot, Driver, Hardware, Output, Run, Shutdown, Communicator, Tools, vmx, Export, configuration references, which are necessary for this build to succeed and can be found further down the page.\ndisk_size (uint) - The size of the hard disk for the VM in megabytes. The builder uses expandable, not fixed-size virtual hard disks, so the actual file representing the disk will not use the full size unless it is full. By default this is set to 40000 (about 40 GB).\ncdrom_adapter_type (string) - The adapter type (or bus) that will be used by the cdrom device. This is chosen by default based on the disk adapter type. VMware tends to lean towards ide for the cdrom device unless sata is chosen for the disk adapter and so Packer attempts to mirror this logic. This field can be specified as either ide, sata, or scsi.\nguest_os_type (string) - The guest OS type being installed. This will be set in the VMware VMX. By default this is other. By specifying a more specific OS type, VMware may perform some optimizations or virtual hardware changes to better support the operating system running in the virtual machine. Valid values differ by platform and version numbers, and may not match other VMware API's representation of the guest OS names. Consult your platform for valid values.\nversion (string) - The vmx hardware version for the new virtual machine. Only the default value has been tested, any other value is experimental. Default value is 9.\nvm_name (string) - This is the name of the VMX file for the new virtual machine, without the file extension. By default this is packer-BUILDNAME, where \"BUILDNAME\" is the name of the build.\nvmx_disk_template_path (string) - VMX Disk Template Path\nvmx_template_path (string) - Path to a configuration template that defines the contents of the virtual machine VMX file for VMware. The engine has access to the template variables {{ .DiskNumber }} and {{ .DiskName }}.\nThis is for advanced users only as this can render the virtual machine non-functional. See below for more information. For basic VMX modifications, try vmx_data first.\nsnapshot_name (string) - This is the name of the initial snapshot created after provisioning and cleanup. if left blank, no initial snapshot will be created\nExtra Disk Configuration\ndisk_additional_size ([]uint) - The size(s) of any additional hard disks for the VM in megabytes. If this is not specified then the VM will only contain a primary hard disk. The builder uses expandable, not fixed-size virtual hard disks, so the actual file representing the disk will not use the full size unless it is full.\ndisk_adapter_type (string) - The adapter type of the VMware virtual disk to create. This option is for advanced usage, modify only if you know what you're doing. Some of the options you can specify are ide, sata, nvme or scsi (which uses the \"lsilogic\" scsi interface by default). If you specify another option, Packer will assume that you're specifying a scsi interface of that specified type. For more information, please consult Virtual Disk Manager User's Guide for desktop VMware clients. For ESXi, refer to the proper ESXi documentation.\nvmdk_name (string) - The filename of the virtual disk that'll be created, without the extension. This defaults to \"disk\".\ndisk_type_id (string) - The type of VMware virtual disk to create. This option is for advanced usage.\nFor desktop VMware clients:\nType IDDescription\n0\tGrowable virtual disk contained in a single file (monolithic sparse).\t\n1\tGrowable virtual disk split into 2GB files (split sparse).\t\n2\tPreallocated virtual disk contained in a single file (monolithic flat).\t\n3\tPreallocated virtual disk split into 2GB files (split flat).\t\n4\tPreallocated virtual disk compatible with ESX server (VMFS flat).\t\n5\tCompressed disk optimized for streaming.\t\nThe default is 1.\nFor ESXi, this defaults to zeroedthick. The available options for ESXi are: zeroedthick, eagerzeroedthick, thin. rdm:dev, rdmp:dev, 2gbsparse are not supported. Due to default disk compaction, when using zeroedthick or eagerzeroedthick set skip_compaction to true.\nFor more information, please consult the Virtual Disk Manager User's Guide for desktop VMware clients. For ESXi, refer to the proper ESXi documentation.\n`wget http://{{ .HTTPIP }}:{{ .HTTPPort }}/foo/bar/preseed.cfg` \nA floppy can be made available for your build. This is most useful for unattended Windows installs, which look for an Autounattend.xml file on removable media. By default, no floppy will be attached. All files listed in this setting get placed into the root directory of the floppy and the floppy is attached as the first floppy device. The summary size of the listed files must not exceed 1.44 MB. The supported ways to move large files into the OS are using http_directory or the file provisioner.\nfloppy_files ([]string) - A list of files to place onto a floppy disk that is attached when the VM is booted. Currently, no support exists for creating sub-directories on the floppy. Wildcard characters (\\*, ?, and []) are allowed. Directory names are also allowed, which will add all the files found in the directory to the floppy.\nfloppy_dirs ([]string) - A list of directories to place onto the floppy disk recursively. This is similar to the floppy_files option except that the directory structure is preserved. This is useful for when your floppy disk includes drivers or if you just want to organize it's contents as a hierarchy. Wildcard characters (\\*, ?, and []) are allowed. The maximum summary size of all files in the listed directories are the same as in floppy_files.\nfloppy_files = [\"vendor-data\"] floppy_content = { \"meta-data\" = jsonencode(local.instance_data) \"user-data\" = templatefile(\"user-data\", { packages = [\"nginx\"] }) } floppy_label = \"cidata\" \nfloppy_label (string) - Floppy Label\nCD configuration\nShutdown configuration\nshutdown_command (string) - The command to use to gracefully shut down the machine once all provisioning is complete. By default this is an empty string, which tells Packer to just forcefully shut down the machine. This setting can be safely omitted if for example, a shutdown command to gracefully halt the machine is configured inside a provisioning script. If one or more scripts require a reboot it is suggested to leave this blank (since reboots may fail) and instead specify the final shutdown command in your last script.\nshutdown_timeout (duration string | ex: \"1h5m2s\") - The amount of time to wait after executing the shutdown_command for the virtual machine to actually shut down. If the machine doesn't shut down in this time it is considered an error. By default, the time out is \"5m\" (five minutes).\nDriver configuration\ncleanup_remote_cache (bool) - When set to true, Packer will cleanup the cache folder where the ISO file is stored during the build on the remote machine. By default, this is set to false.\nfusion_app_path (string) - Path to \"VMware Fusion.app\". By default this is /Applications/VMware Fusion.app but this setting allows you to customize this.\nremote_type (string) - The type of remote machine that will be used to build this VM rather than a local desktop product. The only value accepted for this currently is esx5. If this is not set, a desktop product will be used. By default, this is not set.\nremote_datastore (string) - The path to the datastore where the VM will be stored on the ESXi machine.\nremote_cache_datastore (string) - The path to the datastore where supporting files will be stored during the build on the remote machine.\nremote_cache_directory (string) - The path where the ISO and/or floppy files will be stored during the build on the remote machine. The path is relative to the remote_cache_datastore on the remote machine.\nremote_host (string) - The host of the remote machine used for access. This is only required if remote_type is enabled.\nremote_port (int) - The SSH port of the remote machine\nremote_username (string) - The SSH username used to access the remote machine.\nremote_password (string) - The SSH password for access to the remote machine.\nremote_private_key_file (string) - The SSH key for access to the remote machine.\nskip_validate_credentials (bool) - When Packer is preparing to run a remote esxi build, and export is not disable, by default it runs a no-op ovftool command to make sure that the remote_username and remote_password given are valid. If you set this flag to true, Packer will skip this validation. Default: false.\nHardware configuration\ncpus (int) - The number of cpus to use when building the VM.\nmemory (int) - The amount of memory to use when building the VM in megabytes.\ncores (int) - The number of cores per socket to use when building the VM. This corresponds to the cpuid.coresPerSocket option in the .vmx file.\nnetwork (string) - This is the network type that the virtual machine will be created with. This can be one of the generic values that map to a device such as hostonly, nat, or bridged. If the network is not one of these values, then it is assumed to be a VMware network device. (VMnet0..x)\nnetwork_adapter_type (string) - This is the ethernet adapter type the the virtual machine will be created with. By default the e1000 network adapter type will be used by Packer. For more information, please consult Choosing a network adapter for your virtual machine for desktop VMware clients. For ESXi, refer to the proper ESXi documentation.\nnetwork_name (string) - The custom name of the network. Sets the vmx value \"ethernet0.networkName\"\nsound (bool) - Specify whether to enable VMware's virtual soundcard device when building the VM. Defaults to false.\nusb (bool) - Enable VMware's USB bus when building the guest VM. Defaults to false. To enable usage of the XHCI bus for USB 3 (5 Gbit/s), one can use the vmx_data option to enable it by specifying true for the usb_xhci.present property.\nserial (string) - This specifies a serial port to add to the VM. It has a format of Type:option1,option2,.... The field Type can be one of the following values: FILE, DEVICE, PIPE, AUTO, or NONE.\nFILE:path(,yield) - Specifies the path to the local file to be used as the serial port.\nDEVICE:path(,yield) - Specifies the path to the local device to be used as the serial port. If path is empty, then default to the first serial port.\nPIPE:path,endpoint,host(,yield) - Specifies to use the named-pipe \"path\" as a serial port. This has a few options that determine how the VM should use the named-pipe.\nendpoint (string) - Chooses the type of the VM-end, which can be either a client or server.\nhost (string) - Chooses the type of the host-end, which can be either app (application) or vm (another virtual-machine).\nAUTO:(yield) - Specifies to use auto-detection to determine the serial port to use. This has one option to determine how the VM should support the serial port.\nNONE - Specifies to not use a serial port. (default)\nparallel (string) - This specifies a parallel port to add to the VM. It has the format of Type:option1,option2,.... Type can be one of the following values: FILE, DEVICE, AUTO, or NONE.\nFILE:path - Specifies the path to the local file to be used for the parallel port.\nDEVICE:path - Specifies the path to the local device to be used for the parallel port.\nAUTO:direction - Specifies to use auto-detection to determine the parallel port. Direction can be BI to specify bidirectional communication or UNI to specify unidirectional communication.\nNONE - Specifies to not use a parallel port. (default)\nOutput configuration\noutput_directory (string) - This is the path on your local machine (the one running Packer) to the directory where the resulting virtual machine will be created. This may be relative or absolute. If relative, the path is relative to the working directory when packer is executed.\nIf you are running a remote esx build, the output_dir is the path on your local machine (the machine running Packer) to which Packer will export the vm if you have \"skip_export\": false. If you want to manage the virtual machine's path on the remote datastore, use remote_output_dir.\nThis directory must not exist or be empty prior to running the builder.\nBy default this is output-BUILDNAME where \"BUILDNAME\" is the name of the build.\nremote_output_directory (string) - This is the directoy on your remote esx host where you will save your vm, relative to your remote_datastore.\nThis option's default value is your vm_name, and the final path of your vm will be vmfs/volumes/$remote_datastore/$vm_name/$vm_name.vmx where $remote_datastore and $vm_name match their corresponding template options\nFor example, setting \"remote_output_directory\": \"path/to/subdir will create a directory /vmfs/volumes/remote_datastore/path/to/subdir.\nPacker will not create the remote datastore for you; it must already exist. However, Packer will create all directories defined in the option that do not currently exist.\nThis option will be ignored unless you are building on a remote esx host.\nRun configuration\nNote: If vnc_over_websocket is set to true, any other VNC configuration will be ignored.\nheadless (bool) - Packer defaults to building VMware virtual machines by launching a GUI that shows the console of the machine being built. When this value is set to true, the machine will start without a console. For VMware machines, Packer will output VNC connection information in case you need to connect to the console to debug the build process. Some users have experienced issues where Packer cannot properly connect to a VM if it is headless; this appears to be a result of not ever having launched the VMWare GUI and accepting the evaluation license, or supplying a real license. If you experience this, launching VMWare and accepting the license should resolve your problem.\nvnc_bind_address (string) - The IP address that should be binded to for VNC. By default packer will use 127.0.0.1 for this. If you wish to bind to all interfaces use 0.0.0.0.\nvnc_port_min (int) - The minimum and maximum port to use for VNC access to the virtual machine. The builder uses VNC to type the initial boot_command. Because Packer generally runs in parallel, Packer uses a randomly chosen port in this range that appears available. By default this is 5900 to 6000. The minimum and maximum ports are inclusive.\nvnc_port_max (int) - VNC Port Max\nvnc_disable_password (bool) - Don't auto-generate a VNC password that is used to secure the VNC communication with the VM. This must be set to true if building on ESXi 6.5 and 6.7 with VNC enabled. Defaults to false.\nvnc_over_websocket (bool) - When set to true, Packer will connect to the remote VNC server over a websocket connection and any other VNC configuration option will be ignored. Remote builds using ESXi 6.7+ allows to connect to the VNC server only over websocket, for these the vnc_over_websocket must be set to true.\ninsecure_connection (bool) - Do not validate VNC over websocket server's TLS certificate. Defaults to false.\nTools configuration\ntools_upload_flavor (string) - The flavor of the VMware Tools ISO to upload into the VM. Valid values are darwin, linux, and windows. By default, this is empty, which means VMware tools won't be uploaded.\ntools_upload_path (string) - The path in the VM to upload the VMware tools. This only takes effect if tools_upload_flavor is non-empty. This is a configuration template that has a single valid variable: Flavor, which will be the value of tools_upload_flavor. By default the upload path is set to {{.Flavor}}.iso. This setting is not used when remote_type is esx5.\ntools_source_path (string) - The path on your local machine to fetch the vmware tools from. If this is not set but the tools_upload_flavor is set, then Packer will try to load the VMWare tools from the VMWare installation directory.\nVMX configuration\nvmx_data (map[string]string) - Arbitrary key/values to enter into the virtual machine VMX file. This is for advanced users who want to set properties that aren't yet supported by the builder.\nvmx_data_post (map[string]string) - Identical to vmx_data, except that it is run after the virtual machine is shutdown, and before the virtual machine is exported.\nvmx_remove_ethernet_interfaces (bool) - Remove all ethernet interfaces from the VMX file after building. This is for advanced users who understand the ramifications, but is useful for building Vagrant boxes since Vagrant will create ethernet interfaces when provisioning a box. Defaults to false.\ndisplay_name (string) - The name that will appear in your vSphere client, and will be used for the vmx basename. This will override the \"displayname\" value in your vmx file. It will also override the \"displayname\" if you have set it in the \"vmx_data\" Packer option. This option is useful if you are chaining vmx builds and want to make sure that the display name of each step in the chain is unique.\nExport configuration\nformat (string) - Either \"ovf\", \"ova\" or \"vmx\", this specifies the output format of the exported virtual machine. This defaults to \"ovf\" for remote (esx) builds, and \"vmx\" for local builds. Before using this option, you need to install ovftool. Since ovftool is only capable of password based authentication remote_password must be set when exporting the VM from a remote instance. If you are building locally, Packer will create a vmx and then export that vm to an ovf or ova. Packer will not delete the vmx and vmdk files; this is left up to the user if you don't want to keep those files.\novftool_options ([]string) - Extra options to pass to ovftool during export. Each item in the array is a new argument. The options --noSSLVerify, --skipManifestCheck, and --targetType are used by Packer for remote exports, and should not be passed to this argument. For ovf/ova exports from local builds, Packer does not automatically set any ovftool options.\nskip_export (bool) - Defaults to false. When true, Packer will not export the VM. This can be useful if the build output is not the resultant image, but created inside the VM.\nkeep_registered (bool) - Set this to true if you would like to keep a remotely-built VM registered with the remote ESXi server. If you do not need to export the vm, then also set skip_export: true in order to avoid unnecessarily using ovftool to export the vm. Defaults to false.\nskip_compaction (bool) - VMware-created disks are defragmented and compacted at the end of the build process using vmware-vdiskmanager or vmkfstools in ESXi. In certain rare cases, this might actually end up making the resulting disks slightly larger. If you find this to be the case, you can disable compaction using this configuration value. Defaults to false. Default to true for ESXi when disk_type_id is not explicitly defined and false otherwise.\nThe boot command \"typed\" character for character over a VNC connection to the machine, simulating a human actually typing the keyboard.\nKeystrokes are typed as separate key up/down events over VNC with a default 100ms delay. The delay alleviates issues with latency and CPU contention. You can tune this delay on a per-builder basis by specifying \"boot_key_interval\" in your Packer template.\nNote: for the HTTPIP to be resolved correctly, your VM's network configuration has to include a hostonly or nat type network interface. If you are using this feature, it is recommended to leave the default network configuration while you are building the VM, and use the vmx_data_post hook to modify the network configuration after the VM is done building.\ndisable_vnc (bool) - Whether to create a VNC connection or not. A boot_command cannot be used when this is true. Defaults to false.\nboot_key_interval (duration string | ex: \"1h5m2s\") - Time in ms to wait between each key press\nVMX Template\nThe heart of a VMware machine is the \"vmx\" file. This contains all the virtual hardware metadata necessary for the VM to function. Packer by default uses a safe, flexible VMX file. But for advanced users, this template can be customized. This allows Packer to build virtual machines of effectively any guest operating system type.\nThis is an advanced feature. Modifying the VMX template can easily cause your virtual machine to not boot properly. Please only modify the template if you know what you're doing.\nWithin the template, a handful of variables are available so that your template can continue working with the rest of the Packer machinery. Using these variables isn't required, however.\nName - The name of the virtual machine.\nGuestOS - The VMware-valid guest OS type.\nDiskName - The filename (without the suffix) of the main virtual disk.\nISOPath - The path to the ISO to use for the OS installation.\nVersion - The Hardware version VMWare will execute this vm under. Also known as the virtualhw.version.\nBuilding on a Remote vSphere Hypervisor\nIn addition to using the desktop products of VMware locally to build virtual machines, Packer can use a remote VMware Hypervisor to build the virtual machine.\nNote: Packer supports ESXi 5.1 and above.\n$ esxcli system settings advanced set -o /Net/GuestIPHack -i 1 \nWhen using a remote VMware Hypervisor, the builder still downloads the ISO and various files locally, and uploads these to the remote machine. Packer currently uses SSH to communicate to the ESXi machine rather than the vSphere API. If you want to use vSphere API, see the vsphere-iso builder.\nPacker also requires VNC to issue boot commands during a build, which may be disabled on some remote VMware Hypervisors. Please consult the appropriate documentation on how to update VMware Hypervisor's firewall to allow these connections. VNC can be disabled by not setting a boot_command and setting disable_vnc to true.\nPlease note that you should disable vMotion for the host you intend to run Packer builds on; a vMotion event will cause the Packer build to fail.\nTo use a remote VMware vSphere Hypervisor to build your virtual machine, fill in the required remote_* configurations:\nremote_type - This must be set to \"esx5\".\nremote_host - The host of the remote machine.\nAdditionally, there are some optional configurations that you'll likely have to modify as well:\nremote_port - The SSH port of the remote machine\nremote_datastore - The path to the datastore where the VM will be stored on the ESXi machine.\nremote_cache_datastore - The path to the datastore where supporting files will be stored during the build on the remote machine.\nremote_cache_directory - The path where the ISO and/or floppy files will be stored during the build on the remote machine. The path is relative to the remote_cache_datastore on the remote machine.\nremote_username - The SSH username used to access the remote machine.\nremote_password - The SSH password for access to the remote machine.\nremote_private_key_file - The SSH key for access to the remote machine.\nformat (string) - Either \"ovf\", \"ova\" or \"vmx\", this specifies the output format of the exported virtual machine. This defaults to \"ovf\". Before using this option, you need to install ovftool. This option currently only works when option remote_type is set to \"esx5\". Since ovftool is only capable of password based authentication remote_password must be set when exporting the VM.\nvnc_disable_password - This must be set to \"true\" when using VNC with ESXi 6.5 or 6.7.\nVNC port discovery\nPacker needs to decide on a port to use for VNC when building remotely. To find an open port, we try to connect to ports in the range of vnc_port_min to vnc_port_max. If we notice something is listening on a port in the range, we try to connect to the next one, and so on until we find a port that has nothing listening on it. If you have many clients building on the ESXi host, there might be competition for the VNC ports. You can adjust how long Packer waits for a connection timeout by setting PACKER_ESXI_VNC_PROBE_TIMEOUT. This defaults to 15 seconds. Set this shorter if VNC connections are refused, and set it longer if Packer can't find an open port. This is intended as an advanced configuration option. Please make sure your firewall settings are correct before adjusting.\nUsing a Floppy for Linux kickstart file or preseed\nDepending on your network configuration, it may be difficult to use packer's built-in HTTP server with ESXi. Instead, you can provide a kickstart or preseed file by attaching a floppy disk. An example below, based on RHEL:\n{ \"builders\": [ { \"type\": \"vmware-iso\", \"floppy_files\": [\"folder/ks.cfg\"], \"boot_command\": \"<tab> text ks=floppy <enter><wait>\" } ] } \nIt's also worth noting that ks=floppy has been deprecated. Later versions of the Anaconda installer (used in RHEL/CentOS 7 and Fedora) may require a different syntax to source a kickstart file from a mounted floppy image.\n{ \"builders\": [ { \"type\": \"vmware-iso\", \"floppy_files\": [\"folder/ks.cfg\"], \"boot_command\": \"<tab> inst.text inst.ks=hd:fd0:/ks.cfg <enter><wait>\" } ] }"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.9.x/templates/hcl_templates/functions/conversion/can",
  "text": "can - Functions - Configuration Language | Packer\ncan evaluates the given expression and returns a boolean value indicating whether the expression produced a result without any errors.\nThis is a special function that is able to catch errors produced when evaluating its argument. For most situations where you could use can it's better to use try instead, because it allows for more concise definition of fallback values for failing expressions.\nThe can function can only catch and handle dynamic errors resulting from access to data that isn't known until runtime. It will not catch errors relating to expressions that can be proven to be invalid for any input, such as a malformed resource reference.\nWarning: The can function is intended only for simple tests in variable validation rules. Although it can technically accept any sort of expression and be used elsewhere in the configuration, we recommend against using it in other contexts. For error handling elsewhere in the configuration, prefer to use try.\n> local.foo { \"bar\" = \"baz\" } > can(local.foo.bar) true > can(local.foo.boop) false \nThe can function will not catch errors relating to constructs that are provably invalid even before dynamic expression evaluation, such as a malformed reference or a reference to a top-level object that has not been declared:\n> can(local.nonexist) Error: Reference to undeclared local value A local value with the name \"nonexist\" has not been declared. \ntry, which tries evaluating a sequence of expressions and returns the result of the first one that succeeds."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.10.x/templates/hcl_templates/functions/conversion/can",
  "text": "can - Functions - Configuration Language | Packer\ncan evaluates the given expression and returns a boolean value indicating whether the expression produced a result without any errors.\nThis is a special function that is able to catch errors produced when evaluating its argument. For most situations where you could use can it's better to use try instead, because it allows for more concise definition of fallback values for failing expressions.\nThe can function can only catch and handle dynamic errors resulting from access to data that isn't known until runtime. It will not catch errors relating to expressions that can be proven to be invalid for any input, such as a malformed resource reference.\nWarning: The can function is intended only for simple tests in variable validation rules. Although it can technically accept any sort of expression and be used elsewhere in the configuration, we recommend against using it in other contexts. For error handling elsewhere in the configuration, prefer to use try.\n> local.foo { \"bar\" = \"baz\" } > can(local.foo.bar) true > can(local.foo.boop) false \nThe can function will not catch errors relating to constructs that are provably invalid even before dynamic expression evaluation, such as a malformed reference or a reference to a top-level object that has not been declared:\n> can(local.nonexist) Error: Reference to undeclared local value A local value with the name \"nonexist\" has not been declared. \ntry, which tries evaluating a sequence of expressions and returns the result of the first one that succeeds."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.8.x/templates/hcl_templates/functions/conversion/can",
  "text": "can - Functions - Configuration Language | Packer\ncan evaluates the given expression and returns a boolean value indicating whether the expression produced a result without any errors.\nThis is a special function that is able to catch errors produced when evaluating its argument. For most situations where you could use can it's better to use try instead, because it allows for more concise definition of fallback values for failing expressions.\nThe can function can only catch and handle dynamic errors resulting from access to data that isn't known until runtime. It will not catch errors relating to expressions that can be proven to be invalid for any input, such as a malformed resource reference.\nWarning: The can function is intended only for simple tests in variable validation rules. Although it can technically accept any sort of expression and be used elsewhere in the configuration, we recommend against using it in other contexts. For error handling elsewhere in the configuration, prefer to use try.\n> local.foo { \"bar\" = \"baz\" } > can(local.foo.bar) true > can(local.foo.boop) false \nThe can function will not catch errors relating to constructs that are provably invalid even before dynamic expression evaluation, such as a malformed reference or a reference to a top-level object that has not been declared:\n> can(local.nonexist) Error: Reference to undeclared local value A local value with the name \"nonexist\" has not been declared. \ntry, which tries evaluating a sequence of expressions and returns the result of the first one that succeeds."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/templates/hcl_templates/functions/conversion/convert",
  "text": "convert - Functions - Configuration Language | Packer\nconvert converts a value or an expression to a given type.\nExplicit type conversions are rarely necessary in HCL because it will convert types automatically where required. Use the explicit type conversion functions only to normalize types returned in outputs.\nOnly numbers and strings containing decimal representations of numbers can be converted to number. All other values will produce an error.\nOnly boolean values and the exact strings \"true\" and \"false\" can be converted to boolean. All other values will produce an error.\nOnly the primitive types (string, number, and bool) can be converted to string. All other values will produce an error.\nconvert(value, type_constraint)\n> convert(3, string) \"3\" > convert(\"3\", number) 3 > convert(\"false\", bool) false > convert(false, string) \"false\" > convert([\"a\", \"b\", 3], list) [ \"a\", \"b\", \"3\", ] > convert([\"c\", \"b\", \"b\"], set) [ \"b\", \"c\", ] > convert({\"a\" = \"foo\", \"b\" = true}, map) { \"a\" = \"foo\" \"b\" = \"true\" }"
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/vmware/v1.0.11/components/builder/iso",
  "text": "VMware Builder (1.0.11) | Integrations | Packer\nType: vmware-iso Artifact BuilderId: mitchellh.vmware If remote_type is esx: Artifact BuilderId: mitchellh.vmware-esx\nThis VMware Packer builder is able to create VMware virtual machines from an ISO file as a source. It currently supports building virtual machines on hosts running VMware Fusion for OS X, VMware Workstation for Linux and Windows, and VMware Player on Linux. It can also build machines directly on VMware vSphere Hypervisor using SSH as opposed to the vSphere API.\nThe builder builds a virtual machine by creating a new virtual machine from scratch, booting it, installing an OS, provisioning software within the OS, then shutting it down. The result of the VMware builder is a directory containing all the files necessary to run the virtual machine.\nBasic Example\nHere is a basic example. This example is not functional. It will start the OS installer but then fail because we don't provide the preseed file for Ubuntu to self-install. Still, the example serves to show the basic configuration:\n{ \"type\": \"vmware-iso\", \"iso_url\": \"http://old-releases.ubuntu.com/releases/precise/ubuntu-12.04.2-server-amd64.iso\", \"iso_checksum\": \"md5:af5f788aee1b32c4b2634734309cc9e9\", \"ssh_username\": \"packer\", \"ssh_password\": \"packer\", \"shutdown_command\": \"shutdown -P now\" } \nsource \"vmware-iso\" \"basic-example\" { iso_url = \"http://old-releases.ubuntu.com/releases/precise/ubuntu-12.04.2-server-amd64.iso\" iso_checksum = \"md5:af5f788aee1b32c4b2634734309cc9e9\" ssh_username = \"packer\" ssh_password = \"packer\" shutdown_command = \"shutdown -P now\" } build { sources = [\"sources.vmware-iso.basic-example\"] } \nVMware-ISO Builder Configuration Reference\nThere are many configuration options available for the builder. In addition to the items listed here, you will want to look at the general configuration references for ISO, HTTP, Floppy, CD, Boot, Driver, Hardware, Output, Run, Shutdown, Communicator, Tools, vmx, Export, configuration references, which are necessary for this build to succeed and can be found further down the page.\ndisk_size (uint) - The size of the hard disk for the VM in megabytes. The builder uses expandable, not fixed-size virtual hard disks, so the actual file representing the disk will not use the full size unless it is full. By default this is set to 40000 (about 40 GB).\ncdrom_adapter_type (string) - The adapter type (or bus) that will be used by the cdrom device. This is chosen by default based on the disk adapter type. VMware tends to lean towards ide for the cdrom device unless sata is chosen for the disk adapter and so Packer attempts to mirror this logic. This field can be specified as either ide, sata, or scsi.\nguest_os_type (string) - The guest OS type being installed. This will be set in the VMware VMX. By default this is other. By specifying a more specific OS type, VMware may perform some optimizations or virtual hardware changes to better support the operating system running in the virtual machine. Valid values differ by platform and version numbers, and may not match other VMware API's representation of the guest OS names. Consult your platform for valid values.\nversion (string) - The vmx hardware version for the new virtual machine. Only the default value has been tested, any other value is experimental. Default value is 9.\nvm_name (string) - This is the name of the VMX file for the new virtual machine, without the file extension. By default this is packer-BUILDNAME, where \"BUILDNAME\" is the name of the build.\nvmx_disk_template_path (string) - VMX Disk Template Path\nvmx_template_path (string) - Path to a configuration template that defines the contents of the virtual machine VMX file for VMware. The engine has access to the template variables {{ .DiskNumber }} and {{ .DiskName }}.\nThis is for advanced users only as this can render the virtual machine non-functional. See below for more information. For basic VMX modifications, try vmx_data first.\nsnapshot_name (string) - This is the name of the initial snapshot created after provisioning and cleanup. if left blank, no initial snapshot will be created\nExtra Disk Configuration\ndisk_additional_size ([]uint) - The size(s) of any additional hard disks for the VM in megabytes. If this is not specified then the VM will only contain a primary hard disk. The builder uses expandable, not fixed-size virtual hard disks, so the actual file representing the disk will not use the full size unless it is full.\ndisk_adapter_type (string) - The adapter type of the VMware virtual disk to create. This option is for advanced usage, modify only if you know what you're doing. Some of the options you can specify are ide, sata, nvme or scsi (which uses the \"lsilogic\" scsi interface by default). If you specify another option, Packer will assume that you're specifying a scsi interface of that specified type. For more information, please consult Virtual Disk Manager User's Guide for desktop VMware clients. For ESXi, refer to the proper ESXi documentation.\nvmdk_name (string) - The filename of the virtual disk that'll be created, without the extension. This defaults to \"disk\".\ndisk_type_id (string) - The type of VMware virtual disk to create. This option is for advanced usage.\nFor desktop VMware clients:\nType IDDescription\n0\tGrowable virtual disk contained in a single file (monolithic sparse).\t\n1\tGrowable virtual disk split into 2GB files (split sparse).\t\n2\tPreallocated virtual disk contained in a single file (monolithic flat).\t\n3\tPreallocated virtual disk split into 2GB files (split flat).\t\n4\tPreallocated virtual disk compatible with ESX server (VMFS flat).\t\n5\tCompressed disk optimized for streaming.\t\nThe default is 1.\nFor ESXi, this defaults to zeroedthick. The available options for ESXi are: zeroedthick, eagerzeroedthick, thin. rdm:dev, rdmp:dev, 2gbsparse are not supported. Due to default disk compaction, when using zeroedthick or eagerzeroedthick set skip_compaction to true.\nFor more information, please consult the Virtual Disk Manager User's Guide for desktop VMware clients. For ESXi, refer to the proper ESXi documentation.\nA floppy can be made available for your build. This is most useful for unattended Windows installs, which look for an Autounattend.xml file on removable media. By default, no floppy will be attached. All files listed in this setting get placed into the root directory of the floppy and the floppy is attached as the first floppy device. The summary size of the listed files must not exceed 1.44 MB. The supported ways to move large files into the OS are using http_directory or the file provisioner.\nfloppy_files ([]string) - A list of files to place onto a floppy disk that is attached when the VM is booted. Currently, no support exists for creating sub-directories on the floppy. Wildcard characters (\\*, ?, and []) are allowed. Directory names are also allowed, which will add all the files found in the directory to the floppy.\nfloppy_dirs ([]string) - A list of directories to place onto the floppy disk recursively. This is similar to the floppy_files option except that the directory structure is preserved. This is useful for when your floppy disk includes drivers or if you just want to organize it's contents as a hierarchy. Wildcard characters (\\*, ?, and []) are allowed. The maximum summary size of all files in the listed directories are the same as in floppy_files.\nfloppy_files = [\"vendor-data\"] floppy_content = { \"meta-data\" = jsonencode(local.instance_data) \"user-data\" = templatefile(\"user-data\", { packages = [\"nginx\"] }) } floppy_label = \"cidata\" \nfloppy_label (string) - Floppy Label\nCD configuration\nShutdown configuration\nshutdown_command (string) - The command to use to gracefully shut down the machine once all provisioning is complete. By default this is an empty string, which tells Packer to just forcefully shut down the machine. This setting can be safely omitted if for example, a shutdown command to gracefully halt the machine is configured inside a provisioning script. If one or more scripts require a reboot it is suggested to leave this blank (since reboots may fail) and instead specify the final shutdown command in your last script.\nshutdown_timeout (duration string | ex: \"1h5m2s\") - The amount of time to wait after executing the shutdown_command for the virtual machine to actually shut down. If the machine doesn't shut down in this time it is considered an error. By default, the time out is \"5m\" (five minutes).\nDriver configuration\ncleanup_remote_cache (bool) - When set to true, Packer will cleanup the cache folder where the ISO file is stored during the build on the remote machine. By default, this is set to false.\nfusion_app_path (string) - Path to \"VMware Fusion.app\". By default this is /Applications/VMware Fusion.app but this setting allows you to customize this.\nremote_type (string) - The type of remote machine that will be used to build this VM rather than a local desktop product. The only value accepted for this currently is esx5. If this is not set, a desktop product will be used. By default, this is not set.\nremote_datastore (string) - The path to the datastore where the VM will be stored on the ESXi machine.\nremote_cache_datastore (string) - The path to the datastore where supporting files will be stored during the build on the remote machine.\nremote_cache_directory (string) - The path where the ISO and/or floppy files will be stored during the build on the remote machine. The path is relative to the remote_cache_datastore on the remote machine.\nremote_host (string) - The host of the remote machine used for access. This is only required if remote_type is enabled.\nremote_port (int) - The SSH port of the remote machine\nremote_username (string) - The SSH username used to access the remote machine.\nremote_password (string) - The SSH password for access to the remote machine.\nremote_private_key_file (string) - The SSH key for access to the remote machine.\nskip_validate_credentials (bool) - When Packer is preparing to run a remote esxi build, and export is not disable, by default it runs a no-op ovftool command to make sure that the remote_username and remote_password given are valid. If you set this flag to true, Packer will skip this validation. Default: false.\nHardware configuration\ncpus (int) - The number of cpus to use when building the VM.\nmemory (int) - The amount of memory to use when building the VM in megabytes.\ncores (int) - The number of cores per socket to use when building the VM. This corresponds to the cpuid.coresPerSocket option in the .vmx file.\nnetwork (string) - This is the network type that the virtual machine will be created with. This can be one of the generic values that map to a device such as hostonly, nat, or bridged. If the network is not one of these values, then it is assumed to be a VMware network device. (VMnet0..x)\nnetwork_adapter_type (string) - This is the ethernet adapter type the the virtual machine will be created with. By default the e1000 network adapter type will be used by Packer. For more information, please consult Choosing a network adapter for your virtual machine for desktop VMware clients. For ESXi, refer to the proper ESXi documentation.\nnetwork_name (string) - The custom name of the network. Sets the vmx value \"ethernet0.networkName\"\nsound (bool) - Specify whether to enable VMware's virtual soundcard device when building the VM. Defaults to false.\nusb (bool) - Enable VMware's USB bus when building the guest VM. Defaults to false. To enable usage of the XHCI bus for USB 3 (5 Gbit/s), one can use the vmx_data option to enable it by specifying true for the usb_xhci.present property.\nserial (string) - This specifies a serial port to add to the VM. It has a format of Type:option1,option2,.... The field Type can be one of the following values: FILE, DEVICE, PIPE, AUTO, or NONE.\nFILE:path(,yield) - Specifies the path to the local file to be used as the serial port.\nDEVICE:path(,yield) - Specifies the path to the local device to be used as the serial port. If path is empty, then default to the first serial port.\nPIPE:path,endpoint,host(,yield) - Specifies to use the named-pipe \"path\" as a serial port. This has a few options that determine how the VM should use the named-pipe.\nendpoint (string) - Chooses the type of the VM-end, which can be either a client or server.\nhost (string) - Chooses the type of the host-end, which can be either app (application) or vm (another virtual-machine).\nAUTO:(yield) - Specifies to use auto-detection to determine the serial port to use. This has one option to determine how the VM should support the serial port.\nNONE - Specifies to not use a serial port. (default)\nparallel (string) - This specifies a parallel port to add to the VM. It has the format of Type:option1,option2,.... Type can be one of the following values: FILE, DEVICE, AUTO, or NONE.\nFILE:path - Specifies the path to the local file to be used for the parallel port.\nDEVICE:path - Specifies the path to the local device to be used for the parallel port.\nAUTO:direction - Specifies to use auto-detection to determine the parallel port. Direction can be BI to specify bidirectional communication or UNI to specify unidirectional communication.\nNONE - Specifies to not use a parallel port. (default)\nOutput configuration\noutput_directory (string) - This is the path on your local machine (the one running Packer) to the directory where the resulting virtual machine will be created. This may be relative or absolute. If relative, the path is relative to the working directory when packer is executed.\nIf you are running a remote esx build, the output_dir is the path on your local machine (the machine running Packer) to which Packer will export the vm if you have \"skip_export\": false. If you want to manage the virtual machine's path on the remote datastore, use remote_output_dir.\nThis directory must not exist or be empty prior to running the builder.\nBy default this is output-BUILDNAME where \"BUILDNAME\" is the name of the build.\nremote_output_directory (string) - This is the directoy on your remote esx host where you will save your vm, relative to your remote_datastore.\nThis option's default value is your vm_name, and the final path of your vm will be vmfs/volumes/$remote_datastore/$vm_name/$vm_name.vmx where $remote_datastore and $vm_name match their corresponding template options\nFor example, setting \"remote_output_directory\": \"path/to/subdir will create a directory /vmfs/volumes/remote_datastore/path/to/subdir.\nPacker will not create the remote datastore for you; it must already exist. However, Packer will create all directories defined in the option that do not currently exist.\nThis option will be ignored unless you are building on a remote esx host.\nRun configuration\nNote: If vnc_over_websocket is set to true, any other VNC configuration will be ignored.\nheadless (bool) - Packer defaults to building VMware virtual machines by launching a GUI that shows the console of the machine being built. When this value is set to true, the machine will start without a console. For VMware machines, Packer will output VNC connection information in case you need to connect to the console to debug the build process. Some users have experienced issues where Packer cannot properly connect to a VM if it is headless; this appears to be a result of not ever having launched the VMWare GUI and accepting the evaluation license, or supplying a real license. If you experience this, launching VMWare and accepting the license should resolve your problem.\nvnc_bind_address (string) - The IP address that should be binded to for VNC. By default packer will use 127.0.0.1 for this. If you wish to bind to all interfaces use 0.0.0.0.\nvnc_port_min (int) - The minimum and maximum port to use for VNC access to the virtual machine. The builder uses VNC to type the initial boot_command. Because Packer generally runs in parallel, Packer uses a randomly chosen port in this range that appears available. By default this is 5900 to 6000. The minimum and maximum ports are inclusive.\nvnc_port_max (int) - VNC Port Max\nvnc_disable_password (bool) - Don't auto-generate a VNC password that is used to secure the VNC communication with the VM. This must be set to true if building on ESXi 6.5 and 6.7 with VNC enabled. Defaults to false.\nvnc_over_websocket (bool) - When set to true, Packer will connect to the remote VNC server over a websocket connection and any other VNC configuration option will be ignored. Remote builds using ESXi 6.7+ allows to connect to the VNC server only over websocket, for these the vnc_over_websocket must be set to true.\ninsecure_connection (bool) - Do not validate VNC over websocket server's TLS certificate. Defaults to false.\nTools configuration\ntools_upload_flavor (string) - The flavor of the VMware Tools ISO to upload into the VM. Valid values are darwin, linux, and windows. By default, this is empty, which means VMware tools won't be uploaded.\ntools_upload_path (string) - The path in the VM to upload the VMware tools. This only takes effect if tools_upload_flavor is non-empty. This is a configuration template that has a single valid variable: Flavor, which will be the value of tools_upload_flavor. By default the upload path is set to {{.Flavor}}.iso. This setting is not used when remote_type is esx5.\ntools_source_path (string) - The path on your local machine to fetch the vmware tools from. If this is not set but the tools_upload_flavor is set, then Packer will try to load the VMWare tools from the VMWare installation directory.\nVMX configuration\nvmx_data (map[string]string) - Arbitrary key/values to enter into the virtual machine VMX file. This is for advanced users who want to set properties that aren't yet supported by the builder.\nvmx_data_post (map[string]string) - Identical to vmx_data, except that it is run after the virtual machine is shutdown, and before the virtual machine is exported.\nvmx_remove_ethernet_interfaces (bool) - Remove all ethernet interfaces from the VMX file after building. This is for advanced users who understand the ramifications, but is useful for building Vagrant boxes since Vagrant will create ethernet interfaces when provisioning a box. Defaults to false.\ndisplay_name (string) - The name that will appear in your vSphere client, and will be used for the vmx basename. This will override the \"displayname\" value in your vmx file. It will also override the \"displayname\" if you have set it in the \"vmx_data\" Packer option. This option is useful if you are chaining vmx builds and want to make sure that the display name of each step in the chain is unique.\nExport configuration\nformat (string) - Either \"ovf\", \"ova\" or \"vmx\", this specifies the output format of the exported virtual machine. This defaults to \"ovf\" for remote (esx) builds, and \"vmx\" for local builds. Before using this option, you need to install ovftool. Since ovftool is only capable of password based authentication remote_password must be set when exporting the VM from a remote instance. If you are building locally, Packer will create a vmx and then export that vm to an ovf or ova. Packer will not delete the vmx and vmdk files; this is left up to the user if you don't want to keep those files.\novftool_options ([]string) - Extra options to pass to ovftool during export. Each item in the array is a new argument. The options --noSSLVerify, --skipManifestCheck, and --targetType are used by Packer for remote exports, and should not be passed to this argument. For ovf/ova exports from local builds, Packer does not automatically set any ovftool options.\nskip_export (bool) - Defaults to false. When true, Packer will not export the VM. This can be useful if the build output is not the resultant image, but created inside the VM.\nkeep_registered (bool) - Set this to true if you would like to keep a remotely-built VM registered with the remote ESXi server. If you do not need to export the vm, then also set skip_export: true in order to avoid unnecessarily using ovftool to export the vm. Defaults to false.\nskip_compaction (bool) - VMware-created disks are defragmented and compacted at the end of the build process using vmware-vdiskmanager or vmkfstools in ESXi. In certain rare cases, this might actually end up making the resulting disks slightly larger. If you find this to be the case, you can disable compaction using this configuration value. Defaults to false. Default to true for ESXi when disk_type_id is not explicitly defined and false otherwise.\ntemporary_key_pair_type (string) - dsa | ecdsa | ed25519 | rsa ( the default )\nSpecifies the type of key to create. The possible values are 'dsa', 'ecdsa', 'ed25519', or 'rsa'.\nNOTE: DSA is deprecated and no longer recognized as secure, please consider other alternatives like RSA or ED25519.\ntemporary_key_pair_bits (int) - Specifies the number of bits in the key to create. For RSA keys, the minimum size is 1024 bits and the default is 4096 bits. Generally, 3072 bits is considered sufficient. DSA keys must be exactly 1024 bits as specified by FIPS 186-2. For ECDSA keys, bits determines the key length by selecting from one of three elliptic curve sizes: 256, 384 or 521 bits. Attempting to use bit lengths other than these three values for ECDSA keys will fail. Ed25519 keys have a fixed length and bits will be ignored.\nNOTE: DSA is deprecated and no longer recognized as secure as specified by FIPS 186-5, please consider other alternatives like RSA or ED25519.\nThe boot command \"typed\" character for character over a VNC connection to the machine, simulating a human actually typing the keyboard.\nKeystrokes are typed as separate key up/down events over VNC with a default 100ms delay. The delay alleviates issues with latency and CPU contention. You can tune this delay on a per-builder basis by specifying \"boot_key_interval\" in your Packer template.\nNote: for the HTTPIP to be resolved correctly, your VM's network configuration has to include a hostonly or nat type network interface. If you are using this feature, it is recommended to leave the default network configuration while you are building the VM, and use the vmx_data_post hook to modify the network configuration after the VM is done building.\ndisable_vnc (bool) - Whether to create a VNC connection or not. A boot_command cannot be used when this is true. Defaults to false.\nboot_key_interval (duration string | ex: \"1h5m2s\") - Time in ms to wait between each key press\nVMX Template\nThe heart of a VMware machine is the \"vmx\" file. This contains all the virtual hardware metadata necessary for the VM to function. Packer by default uses a safe, flexible VMX file. But for advanced users, this template can be customized. This allows Packer to build virtual machines of effectively any guest operating system type.\nThis is an advanced feature. Modifying the VMX template can easily cause your virtual machine to not boot properly. Please only modify the template if you know what you're doing.\nWithin the template, a handful of variables are available so that your template can continue working with the rest of the Packer machinery. Using these variables isn't required, however.\nName - The name of the virtual machine.\nGuestOS - The VMware-valid guest OS type.\nDiskName - The filename (without the suffix) of the main virtual disk.\nISOPath - The path to the ISO to use for the OS installation.\nVersion - The Hardware version VMWare will execute this vm under. Also known as the virtualhw.version.\nBuilding on a Remote vSphere Hypervisor\nIn addition to using the desktop products of VMware locally to build virtual machines, Packer can use a remote VMware Hypervisor to build the virtual machine.\nNote: Packer supports ESXi 5.1 and above.\n$ esxcli system settings advanced set -o /Net/GuestIPHack -i 1 \nWhen using a remote VMware Hypervisor, the builder still downloads the ISO and various files locally, and uploads these to the remote machine. Packer currently uses SSH to communicate to the ESXi machine rather than the vSphere API. If you want to use vSphere API, see the vsphere-iso builder.\nPacker also requires VNC to issue boot commands during a build, which may be disabled on some remote VMware Hypervisors. Please consult the appropriate documentation on how to update VMware Hypervisor's firewall to allow these connections. VNC can be disabled by not setting a boot_command and setting disable_vnc to true.\nPlease note that you should disable vMotion for the host you intend to run Packer builds on; a vMotion event will cause the Packer build to fail.\nTo use a remote VMware vSphere Hypervisor to build your virtual machine, fill in the required remote_* configurations:\nremote_type - This must be set to \"esx5\".\nremote_host - The host of the remote machine.\nAdditionally, there are some optional configurations that you'll likely have to modify as well:\nremote_port - The SSH port of the remote machine\nremote_datastore - The path to the datastore where the VM will be stored on the ESXi machine.\nremote_cache_datastore - The path to the datastore where supporting files will be stored during the build on the remote machine.\nremote_cache_directory - The path where the ISO and/or floppy files will be stored during the build on the remote machine. The path is relative to the remote_cache_datastore on the remote machine.\nremote_username - The SSH username used to access the remote machine.\nremote_password - The SSH password for access to the remote machine.\nremote_private_key_file - The SSH key for access to the remote machine.\nformat (string) - Either \"ovf\", \"ova\" or \"vmx\", this specifies the output format of the exported virtual machine. This defaults to \"ovf\". Before using this option, you need to install ovftool. This option currently only works when option remote_type is set to \"esx5\". Since ovftool is only capable of password based authentication remote_password must be set when exporting the VM.\nvnc_disable_password - This must be set to \"true\" when using VNC with ESXi 6.5 or 6.7.\nVNC port discovery\nPacker needs to decide on a port to use for VNC when building remotely. To find an open port, we try to connect to ports in the range of vnc_port_min to vnc_port_max. If we notice something is listening on a port in the range, we try to connect to the next one, and so on until we find a port that has nothing listening on it. If you have many clients building on the ESXi host, there might be competition for the VNC ports. You can adjust how long Packer waits for a connection timeout by setting PACKER_ESXI_VNC_PROBE_TIMEOUT. This defaults to 15 seconds. Set this shorter if VNC connections are refused, and set it longer if Packer can't find an open port. This is intended as an advanced configuration option. Please make sure your firewall settings are correct before adjusting.\nUsing a Floppy for Linux kickstart file or preseed\nDepending on your network configuration, it may be difficult to use packer's built-in HTTP server with ESXi. Instead, you can provide a kickstart or preseed file by attaching a floppy disk. An example below, based on RHEL:\n{ \"builders\": [ { \"type\": \"vmware-iso\", \"floppy_files\": [\"folder/ks.cfg\"], \"boot_command\": \"<tab> text ks=floppy <enter><wait>\" } ] } \nIt's also worth noting that ks=floppy has been deprecated. Later versions of the Anaconda installer (used in RHEL/CentOS 7 and Fedora) may require a different syntax to source a kickstart file from a mounted floppy image.\n{ \"builders\": [ { \"type\": \"vmware-iso\", \"floppy_files\": [\"folder/ks.cfg\"], \"boot_command\": \"<tab> inst.text inst.ks=hd:fd0:/ks.cfg <enter><wait>\" } ] } \nSSH key pair automation\nThe VMware builders can inject the current SSH key pair's public key into the template using the SSHPublicKey template engine. This is the SSH public key as a line in OpenSSH authorized_keys format.\nWhen a private key is provided using ssh_private_key_file, the key's corresponding public key can be accessed using the above engine.\nIf ssh_password and ssh_private_key_file are not specified, Packer will automatically generate en ephemeral key pair. The key pair's public key can be accessed using the template engine.\nFor example, the public key can be provided in the boot command as a URL encoded string by appending | urlquery to the variable:\n\"boot_command\": [ \"<up><wait><tab> text ks=http://{{ .HTTPIP }}:{{ .HTTPPort }}/ks.cfg PACKER_USER={{ user `username` }} PACKER_AUTHORIZED_KEY={{ .SSHPublicKey | urlquery }}<enter>\" ] \nboot_command = [ \"<up><wait><tab> text ks=http://{{ .HTTPIP }}:{{ .HTTPPort }}/ks.cfg PACKER_USER={{ user `username` }} PACKER_AUTHORIZED_KEY={{ .SSHPublicKey | urlquery }}<enter>\" ] \nA kickstart could then leverage those fields from the kernel command line by decoding the URL-encoded public key:\n%post # Newly created users need the file/folder framework for SSH key authentication. umask 0077 mkdir /etc/skel/.ssh touch /etc/skel/.ssh/authorized_keys # Loop over the command line. Set interesting variables. for x in $(cat /proc/cmdline) do case $x in PACKER_USER=*) PACKER_USER=\"${x#*=}\" ;; PACKER_AUTHORIZED_KEY=*) # URL decode $encoded into $PACKER_AUTHORIZED_KEY encoded=$(echo \"${x#*=}\" | tr '+' ' ') printf -v PACKER_AUTHORIZED_KEY '%b' \"${encoded//%/\\\\x}\" ;; esac done # Create/configure packer user, if any. if [ -n \"$PACKER_USER\" ] then useradd $PACKER_USER echo \"%$PACKER_USER ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers.d/$PACKER_USER [ -n \"$PACKER_AUTHORIZED_KEY\" ] && echo $PACKER_AUTHORIZED_KEY >> $(eval echo ~\"$PACKER_USER\")/.ssh/authorized_keys fi %end"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/templates/hcl_templates/functions/conversion/can",
  "text": "can - Functions - Configuration Language | Packer\ncan evaluates the given expression and returns a boolean value indicating whether the expression produced a result without any errors.\nThis is a special function that is able to catch errors produced when evaluating its argument. For most situations where you could use can it's better to use try instead, because it allows for more concise definition of fallback values for failing expressions.\nThe can function can only catch and handle dynamic errors resulting from access to data that isn't known until runtime. It will not catch errors relating to expressions that can be proven to be invalid for any input, such as a malformed resource reference.\nWarning: The can function is intended only for simple tests in variable validation rules. Although it can technically accept any sort of expression and be used elsewhere in the configuration, we recommend against using it in other contexts. For error handling elsewhere in the configuration, prefer to use try.\n> local.foo { \"bar\" = \"baz\" } > can(local.foo.bar) true > can(local.foo.boop) false \nThe can function will not catch errors relating to constructs that are provably invalid even before dynamic expression evaluation, such as a malformed reference or a reference to a top-level object that has not been declared:\n> can(local.nonexist) Error: Reference to undeclared local value A local value with the name \"nonexist\" has not been declared. \ntry, which tries evaluating a sequence of expressions and returns the result of the first one that succeeds."
},
{
  "url": "https://developer.hashicorp.com/packer/guides/v1.5.x/hcl/from-json-v1",
  "text": "Transforming Packer v1 files for Packer v1.5.0 | Packer\nNote: Starting from version 1.5.0 Packer can read HCL2 files.\nSupport for HCL2 is currently in Beta, which means the feature you are looking for may still need some polishing. If you find problems, please check the list of open and resolved HCL2 issues on the Packer Issue Tracker. For any strange behavior not already captured please create a new issue so that we can fix it!\nWe will soon provide a programatic way to transpose a v1 buildfile to a v1.5 HCL file. In the meantime we will show how to manually do it.\nThe following file :\n{ \"builders\": [ { \"ami_name\": \"packer-test\", \"region\": \"us-east-1\", \"instance_type\": \"t2.micro\", \"source_ami_filter\": { \"filters\": { \"virtualization-type\": \"hvm\", \"name\": \"ubuntu/images/*ubuntu-xenial-16.04-amd64-server-*\", \"root-device-type\": \"ebs\" }, \"owners\": [\"amazon\"], \"most_recent\": true }, \"ssh_username\": \"ubuntu\", \"type\": \"amazon-ebs\" } ], \"provisioners\": [ { \"type\": \"shell\", \"inline\": [\"sleep 5\"] } ] } \n# the source block is what was defined in the builders section and represents a # reusable way to start a machine. You build your images from that source. All # sources have a 1:1 correspondance to what currently is a builder. The # argument name (ie: ami_name) must be unquoted and can be set using the equal # sign operator (=). source \"amazon-ebs\" \"example\" { ami_name = \"packer-test\" region = \"us-east-1\" instance_type = \"t2.micro\" source_ami_filter { filters { virtualization-type = \"hvm\" name = \"ubuntu/images/*ubuntu-xenial-16.04-amd64-server-*\" root-device-type = \"ebs\" } owners = [\"amazon\"] most_recent = true } communicator = \"ssh\" ssh_username = \"ubuntu\" } # A build starts sources and runs provisioning steps on those sources. build { sources = [ # there can be multiple sources per build \"source.amazon-ebs.example\" ] # All provisioners and post-processors have a 1:1 correspondence to their # current layout. The argument name (ie: inline) must to be unquoted # and can be set using the equal sign operator (=). provisioner \"shell\" { inline = [\"sleep 5\"] } # post-processors work too, example: `post-processor \"shell-local\" {}`. } \n1:1 correspondence of components ... except :\nAll fields of builders, provisioners and post-processors have a 1:1 correspondance except for the following:\nbuilders:\naws ami_block_device_mappings\naws launch_block_device_mappings\naws run_volume_tags\nalicloud image_disk_mappings\nosc omi_block_device_mappings\nosc launch_block_device_mappings\nproxmox network_adapters\nproxmox disks\ntencentcloud data_disks\nucloud image_copy_to_mappings\nprovisioner:\nconverge module_dirs\npost-processor:\nalicloud-import image_disk_mappings\nOne could think that these are defined as \"arrays of blocks\" - they are in fact repeatable blocks with the same identifier. For example:\n\"builders\": [ { \"type\": \"amazon-ebs\", \"launch_block_device_mappings\": [ { \"device_name\": \"/dev/xvda\", \"volume_size\": \"20\", \"volume_type\": \"gp2\", \"delete_on_termination\": \"true\" }, { \"device_name\": \"/dev/xvdf\", \"volume_size\": \"500\", \"volume_type\": \"gp2\", \"delete_on_termination\": \"true\", \"encrypted\": true } ], } \nsource \"amazon-ebs\" \"example\" { launch_block_device_mappings { device_name = \"/dev/xvda\" volume_size = 20 volume_type = \"gp2\" delete_on_termination = true } launch_block_device_mappings { device_name = \"/dev/xvdf\" volume_size = 500 volume_type = \"gp2\" delete_on_termination = true encrypted = true } \nThere is soon going to be a PR to drop the s at the end of these fields.\nDeprecation\nThe current layout of buildfiles will be supported until we and the community love the new format. Only then the v1 format will be carefully deprecated.\nNote: The HCL parsing library can read JSON and if it is your configuration format of predilection, you will still be able to do it. You will have to tweak a few things in order to use future versions of Packer that have deprecated the current format. Sorry about that! Because the HCL reading code is generated from the JSON parsing settings; every builder, provisioner and post-processor setting should look and work the same. A config file transposer is currently in the making."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/templates/hcl_templates/functions/conversion/try",
  "text": "try - Functions - Configuration Language | Packer\nlocals { raw_value = yamldecode(\"${path.folder}/example.yaml\") normalized_value = { name = tostring(try(local.raw_value.name, null)) groups = try(local.raw_value.groups, []) } } \nWith the above local value expressions, configuration elsewhere in the folder can refer to local.normalized_value attributes without the need to repeatedly check for and handle absent attributes that would otherwise produce errors.\nWe strongly suggest using try only in special local values whose expressions perform normalization, so that the error handling is confined to a single location in the folder and the rest of the folder can just use straightforward references to the normalized structure and thus be more readable for future maintainers."
},
{
  "url": "https://developer.hashicorp.com/packer/guides/v1.6.x/hcl/from-json-v1",
  "text": "Transforming Packer v1 files for Packer v1.5.0 | Packer\nNote: Starting from version 1.5.0 Packer can read HCL2 files.\nSupport for HCL2 is currently in Beta, which means the feature you are looking for may still need some polishing. If you find problems, please check the Packer Issue Tracker for a list of supported features and open issues. For any strange behavior not already captured please create a new issue so that we can fix it!\nAs of v1.6.4, Packer provides a tool to help you convert legacy JSON files to HCL2 files. To run it, you can use the hcl2_upgrade command.\nfor example,\npacker hcl2_upgrade mytemplate.json \nwill convert your packer template to a new HCL2 file in your current working directory named mytemplate.json.pkr.hcl. It is not a perfect converter yet; please open an issue if you find a problem with the conversion. Packer will not destroy your legacy json template, so this is not a risky command to call.\nFollowing is an explanation of how to manually upgrade a JSON template to an HCL2 template.\nThe following file :\n{ \"builders\": [ { \"ami_name\": \"packer-test\", \"region\": \"us-east-1\", \"instance_type\": \"t2.micro\", \"source_ami_filter\": { \"filters\": { \"virtualization-type\": \"hvm\", \"name\": \"ubuntu/images/*ubuntu-xenial-16.04-amd64-server-*\", \"root-device-type\": \"ebs\" }, \"owners\": [\"amazon\"], \"most_recent\": true }, \"ssh_username\": \"ubuntu\", \"type\": \"amazon-ebs\" } ], \"provisioners\": [ { \"type\": \"shell\", \"inline\": [\"sleep 5\"] } ] } \n# the source block is what was defined in the builders section and represents a # reusable way to start a machine. You build your images from that source. All # sources have a 1:1 correspondance to what currently is a builder. The # argument name (ie: ami_name) must be unquoted and can be set using the equal # sign operator (=). source \"amazon-ebs\" \"example\" { ami_name = \"packer-test\" region = \"us-east-1\" instance_type = \"t2.micro\" source_ami_filter { filters = { virtualization-type = \"hvm\" name = \"ubuntu/images/*ubuntu-xenial-16.04-amd64-server-*\" root-device-type = \"ebs\" } owners = [\"amazon\"] most_recent = true } communicator = \"ssh\" ssh_username = \"ubuntu\" } # A build starts sources and runs provisioning steps on those sources. build { sources = [ # there can be multiple sources per build \"source.amazon-ebs.example\" ] # All provisioners and post-processors have a 1:1 correspondence to their # current layout. The argument name (ie: inline) must to be unquoted # and can be set using the equal sign operator (=). provisioner \"shell\" { inline = [\"sleep 5\"] } # post-processors work too, example: `post-processor \"shell-local\" {}`. } \n1:1 correspondence of components ... except :\nAll fields of builders, provisioners and post-processors have a 1:1 correspondance except for the following:\nbuilders:\naws ami_block_device_mappings\naws launch_block_device_mappings\naws run_volume_tags\nalicloud image_disk_mappings\nosc omi_block_device_mappings\nosc launch_block_device_mappings\nproxmox network_adapters\nproxmox disks\ntencentcloud data_disks\nucloud image_copy_to_mappings\nprovisioner:\nconverge module_dirs\npost-processor:\nalicloud-import image_disk_mappings\nOne could think that these are defined as \"arrays of blocks\" - they are in fact repeatable blocks with the same identifier. For example:\n\"builders\": [ { \"type\": \"amazon-ebs\", \"launch_block_device_mappings\": [ { \"device_name\": \"/dev/xvda\", \"volume_size\": \"20\", \"volume_type\": \"gp2\", \"delete_on_termination\": \"true\" }, { \"device_name\": \"/dev/xvdf\", \"volume_size\": \"500\", \"volume_type\": \"gp2\", \"delete_on_termination\": \"true\", \"encrypted\": true } ], } ] \nsource \"amazon-ebs\" \"example\" { launch_block_device_mappings { device_name = \"/dev/xvda\" volume_size = 20 volume_type = \"gp2\" delete_on_termination = true } launch_block_device_mappings { device_name = \"/dev/xvdf\" volume_size = 500 volume_type = \"gp2\" delete_on_termination = true encrypted = true } } \nThere is soon going to be a PR to drop the s at the end of these fields.\nDeprecation\nAs we become more confident in the new templates, we may begin to add new features that are HCL2-only; one of our major motivations to moving to the new template format is that HCL2 provides us with the flexibility to implement some features which would be very difficult to add to the legacy JSON templates.\nHowever, the Packer team will continue to support the main functionality of the current \"legacy JSON\" packer templates alongside the new HCL2 templates until we and the community love the new templates. Only then the v1 format will be deprecated. We do not anticipate this happening until late 2021 at the earliest."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.10.x/templates/legacy_json_templates/user-variables",
  "text": "User Variables - Templates | Packer\nUser variables allow your templates to be further configured with variables from the command-line, environment variables, Vault, or files. This lets you parameterize your templates so that you can keep secret tokens, environment-specific data, and other types of information out of your templates. This maximizes the portability of the template.\nUsing user variables expects you to know how configuration templates work. If you don't know how configuration templates work yet, please read that page first.\nIn order to set a user variable, you must define it either within the variables section within your template, or using the command-line -var or -var-file flags.\nEven if you want a user variable to default to an empty string, it is best to explicitly define it. This explicitness helps reduce the time it takes for newcomers to understand what can be modified using variables in your template.\nThe variables section is a key/value mapping of the user variable name to a default value. A default value can be the empty string. An example is shown below:\n{ \"variables\": { \"aws_access_key\": \"\", \"aws_secret_key\": \"\" }, \"builders\": [ { \"type\": \"amazon-ebs\", \"access_key\": \"{{user `aws_access_key`}}\", \"secret_key\": \"{{user `aws_secret_key`}}\" // ... } ] } \nIn the above example, the template defines two user variables: aws_access_key and aws_secret_key. They default to empty values. Later, the variables are used within the builder we defined in order to configure the actual keys for the Amazon builder.\nIf the default value is null, then the user variable will be required. This means that the user must specify a value for this variable or template validation will fail.\nUser variables are used by calling the {{user}} function in the form of {{user 'variable'}}. This function can be used in any value but type within the template: in builders, provisioners, anywhere outside the variables section. User variables are available globally within the rest of the template.\nEnvironment variables can be used within your template using user variables. The env function is available only within the default value of a user variable, allowing you to default a user variable to an environment variable. An example is shown below:\n{ \"variables\": { \"my_secret\": \"{{env `MY_SECRET`}}\" } } \nThis will default \"my_secret\" to be the value of the \"MY_SECRET\" environment variable (or an empty string if it does not exist).\nWhy can't I use environment variables elsewhere? User variables are the single source of configurable input to a template. We felt that having environment variables used anywhere in a template would confuse the user about the possible inputs to a template. By allowing environment variables only within default values for user variables, user variables remain as the single source of input to a template that a user can easily discover using packer inspect.\nWhy can't I use ~ for home variable? ~ is an special variable that is evaluated by shell during a variable expansion. As Packer doesn't run inside a shell, it won't expand ~.\nConsul keys can be used within your template using the consul_key function. This function is available only within the default value of a user variable, for reasons similar to environment variables above.\n{ \"variables\": { \"soft_versions\": \"{{ consul_key `my_image/softs_versions/next` }}\" } } \nThis will default soft_versions to the value of the key my_image/softs_versions/next in consul.\nThe configuration for consul (address, tokens, ...) must be specified as environment variables, as specified in the Documentation.\nSecrets can be read from Vault and used within your template as user variables. the vault function is available only within the default value of a user variable, allowing you to default a user variable to a vault secret.\nAn example of using a v2 kv engine:\nIf you store a value in vault using vault kv put secret/hello foo=world, you can access it using the following template engine:\n{ \"variables\": { \"my_secret\": \"{{ vault `/secret/data/hello` `foo`}}\" } } \nwhich will assign \"my_secret\": \"world\"\nAn example of using a v1 kv engine:\nIf you store a value in vault using:\nvault secrets enable -version=1 -path=secrets kv vault kv put secrets/hello foo=world \nYou can access it using the following template engine:\n{ \"variables\": { \"VAULT_SECRETY_SECRET\": \"{{ vault `secrets/hello` `foo`}}\" } } \nThis example accesses the Vault path secret/data/foo and returns the value stored at the key bar, storing it as \"my_secret\".\nIn order for this to work, you must set the environment variables VAULT_TOKEN and VAULT_ADDR to valid values.\nThe api tool we use allows for more custom configuration of the Vault client via environment variables.\nThe full list of available environment variables is:\n\"VAULT_ADDR\" \"VAULT_AGENT_ADDR\" \"VAULT_CACERT\" \"VAULT_CAPATH\" \"VAULT_CLIENT_CERT\" \"VAULT_CLIENT_KEY\" \"VAULT_CLIENT_TIMEOUT\" \"VAULT_SKIP_VERIFY\" \"VAULT_NAMESPACE\" \"VAULT_TLS_SERVER_NAME\" \"VAULT_WRAP_TTL\" \"VAULT_MAX_RETRIES\" \"VAULT_TOKEN\" \"VAULT_MFA\" \"VAULT_RATE_LIMIT\" \nand detailed documentation for usage of each of those variables can be found here.\nSecrets can be read from AWS Secrets Manager and used within your template as user variables. The aws_secretsmanager function is available only within the default value of a user variable, allowing you to default a user variable to an AWS Secrets Manager secret.\nPlaintext Secrets\n{ \"variables\": { \"password\": \"{{ aws_secretsmanager `globalpassword` }}\" } } \nIn the example above it is assumed that the secret globalpassword is not stored as a key pair but as a single non-JSON string value. Which the aws_secretsmanager function will return as a raw string.\nSingle Key Secrets\n{ \"variables\": { \"password\": \"{{ aws_secretsmanager `sample/app/password` }}\" } } \nIn the example above it is assumed that only one key is stored in sample/app/password if there are multiple keys stored in it then you need to indicate the specific key you want to fetch as shown below.\nMultiple Key Secrets\n{ \"variables\": { \"db_password\": \"{{ aws_secretsmanager `sample/app/passwords` `db` }}\", \"api_key\": \"{{ aws_secretsmanager `sample/app/passwords` `api_key` }}\" } } \nIn order to use this function you have to configure valid AWS credentials using one of the following methods:\nEnvironment Variables\nCLI Configuration Files\nContainer Credentials\nInstance Profile Credentials\nSome templates call for array values. You can use template variables for these, too. For example, the amazon-ebs builder has a configuration parameter called ami_regions, which takes an array of regions that it will copy the AMI to. You can parameterize this by using a variable that is a list of regions, joined by a ,. For example:\n{ \"variables\": { \"destination_regions\": \"us-west-1,us-west-2\" }, \"builders\": [ { \"ami_name\": \"packer-qs-{{timestamp}}\", \"instance_type\": \"t2.micro\", \"region\": \"us-east-1\", \"source_ami_filter\": { \"filters\": { \"name\": \"*ubuntu-xenial-16.04-amd64-server-*\", \"root-device-type\": \"ebs\", \"virtualization-type\": \"hvm\" }, \"most_recent\": true, \"owners\": [\"099720109477\"] }, \"ami_regions\": \"{{user `destination_regions`}}\", \"ssh_username\": \"ubuntu\", \"type\": \"amazon-ebs\" } ] } \nNow that we covered how to define and use user variables within a template, the next important point is how to actually set these variables. Packer exposes two methods for setting user variables: from the command line or from a file.\nFrom the Command Line\nTo set user variables from the command line, the -var flag is used as a parameter to packer build (and some other commands). Continuing our example above, we could build our template using the command below. The command is split across multiple lines for readability, but can of course be a single line.\n$ packer build \\ -var 'aws_access_key=foo' \\ -var 'aws_secret_key=bar' \\ template.json \nAs you can see, the -var flag can be specified multiple times in order to set multiple variables. Also, variables set later on the command-line override any earlier set variable of the same name.\nwarning If you are calling Packer from cmd.exe, you should double-quote your variables rather than single-quoting them. For example:\npacker build -var \"aws_secret_key=foo\" template.json\nFrom a File\nVariables can also be set from an external JSON file. The -var-file flag reads a file containing a key/value mapping of variables to values and sets those variables. An example JSON file may look like this:\n{ \"aws_access_key\": \"foo\", \"aws_secret_key\": \"bar\" } \nIt is a single JSON object where the keys are variables and the values are the variable values. Assuming this file is in variables.json, we can build our template using the following command:\nOn Linux : $ packer build -var-file=variables.json template.json On Windows : packer build -var-file variables.json template.json \nThe -var-file flag can be specified multiple times and variables from multiple files will be read and applied. As you'd expect, variables read from files specified later override a variable set earlier.\nCombining the -var and -var-file flags together also works how you'd expect. Variables set later in the command override variables set earlier. So, for example, in the following command with the above variables.json file:\n$ packer build \\ -var 'aws_access_key=bar' \\ -var-file=variables.json \\ -var 'aws_secret_key=baz' \\ template.json \nResults in the following variables:\nVariableValue\naws_access_key\tfoo\t\naws_secret_key\tbaz\t\nIf you use the environment to set a variable that is sensitive, you probably don't want that variable printed to the Packer logs. You can make sure that sensitive variables won't get printed to the logs by adding them to the \"sensitive-variables\" list within the Packer template:\n{ \"variables\": { \"my_secret\": \"{{env `MY_SECRET`}}\", \"not_a_secret\": \"plaintext\", \"foo\": \"bar\" }, \"sensitive-variables\": [\"my_secret\", \"foo\"], ... } \nThe above snippet of code will function exactly the same as if you did not set \"sensitive-variables\", except that the Packer UI and logs will replace all instances of \"bar\" and of whatever the value of \"my_secret\" is with <sensitive>. This allows you to be confident that you are not printing secrets in plaintext to our logs by accident.\nThere is no specific syntax in Packer templates for making a provisioner step conditional, depending on the value of a variable. However, you may be able to do this by referencing the variable within a command that you execute. For example, here is how to make a shell-local provisioner only run if the do_nexpose_scan variable is non-empty.\n{ \"type\": \"shell-local\", \"command\": \"if [ ! -z \\\"{{user `do_nexpose_scan`}}\\\" ]; then python -u trigger_nexpose_scan.py; fi\" } \nIn order to use $HOME variable, you can create a home variable in Packer:\n{ \"variables\": { \"home\": \"{{env `HOME`}}\" } } \nAnd this will be available to be used in the rest of the template, i.e.:\n{ \"builders\": [ { \"type\": \"google\", \"account_file\": \"{{ user `home` }}/.secrets/gcp-{{ user `env` }}.json\" } ] }"
},
{
  "url": "https://developer.hashicorp.com/packer/plugins/v1.9.x/builders/amazon",
  "text": "This page does not exist for version v1.9.x."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.8.x/templates/legacy_json_templates/user-variables",
  "text": "User Variables - Templates | Packer\nUser variables allow your templates to be further configured with variables from the command-line, environment variables, Vault, or files. This lets you parameterize your templates so that you can keep secret tokens, environment-specific data, and other types of information out of your templates. This maximizes the portability of the template.\nUsing user variables expects you to know how configuration templates work. If you don't know how configuration templates work yet, please read that page first.\nIn order to set a user variable, you must define it either within the variables section within your template, or using the command-line -var or -var-file flags.\nEven if you want a user variable to default to an empty string, it is best to explicitly define it. This explicitness helps reduce the time it takes for newcomers to understand what can be modified using variables in your template.\nThe variables section is a key/value mapping of the user variable name to a default value. A default value can be the empty string. An example is shown below:\n{ \"variables\": { \"aws_access_key\": \"\", \"aws_secret_key\": \"\" }, \"builders\": [ { \"type\": \"amazon-ebs\", \"access_key\": \"{{user `aws_access_key`}}\", \"secret_key\": \"{{user `aws_secret_key`}}\" // ... } ] } \nIn the above example, the template defines two user variables: aws_access_key and aws_secret_key. They default to empty values. Later, the variables are used within the builder we defined in order to configure the actual keys for the Amazon builder.\nIf the default value is null, then the user variable will be required. This means that the user must specify a value for this variable or template validation will fail.\nUser variables are used by calling the {{user}} function in the form of {{user 'variable'}}. This function can be used in any value but type within the template: in builders, provisioners, anywhere outside the variables section. User variables are available globally within the rest of the template.\nEnvironment variables can be used within your template using user variables. The env function is available only within the default value of a user variable, allowing you to default a user variable to an environment variable. An example is shown below:\n{ \"variables\": { \"my_secret\": \"{{env `MY_SECRET`}}\" } } \nThis will default \"my_secret\" to be the value of the \"MY_SECRET\" environment variable (or an empty string if it does not exist).\nWhy can't I use environment variables elsewhere? User variables are the single source of configurable input to a template. We felt that having environment variables used anywhere in a template would confuse the user about the possible inputs to a template. By allowing environment variables only within default values for user variables, user variables remain as the single source of input to a template that a user can easily discover using packer inspect.\nWhy can't I use ~ for home variable? ~ is an special variable that is evaluated by shell during a variable expansion. As Packer doesn't run inside a shell, it won't expand ~.\nConsul keys can be used within your template using the consul_key function. This function is available only within the default value of a user variable, for reasons similar to environment variables above.\n{ \"variables\": { \"soft_versions\": \"{{ consul_key `my_image/softs_versions/next` }}\" } } \nThis will default soft_versions to the value of the key my_image/softs_versions/next in consul.\nThe configuration for consul (address, tokens, ...) must be specified as environment variables, as specified in the Documentation.\nSecrets can be read from Vault and used within your template as user variables. the vault function is available only within the default value of a user variable, allowing you to default a user variable to a vault secret.\nAn example of using a v2 kv engine:\nIf you store a value in vault using vault kv put secret/hello foo=world, you can access it using the following template engine:\n{ \"variables\": { \"my_secret\": \"{{ vault `/secret/data/hello` `foo`}}\" } } \nwhich will assign \"my_secret\": \"world\"\nAn example of using a v1 kv engine:\nIf you store a value in vault using:\nvault secrets enable -version=1 -path=secrets kv vault kv put secrets/hello foo=world \nYou can access it using the following template engine:\n{ \"variables\": { \"VAULT_SECRETY_SECRET\": \"{{ vault `secrets/hello` `foo`}}\" } } \nThis example accesses the Vault path secret/data/foo and returns the value stored at the key bar, storing it as \"my_secret\".\nIn order for this to work, you must set the environment variables VAULT_TOKEN and VAULT_ADDR to valid values.\nThe api tool we use allows for more custom configuration of the Vault client via environment variables.\nThe full list of available environment variables is:\n\"VAULT_ADDR\" \"VAULT_AGENT_ADDR\" \"VAULT_CACERT\" \"VAULT_CAPATH\" \"VAULT_CLIENT_CERT\" \"VAULT_CLIENT_KEY\" \"VAULT_CLIENT_TIMEOUT\" \"VAULT_SKIP_VERIFY\" \"VAULT_NAMESPACE\" \"VAULT_TLS_SERVER_NAME\" \"VAULT_WRAP_TTL\" \"VAULT_MAX_RETRIES\" \"VAULT_TOKEN\" \"VAULT_MFA\" \"VAULT_RATE_LIMIT\" \nand detailed documentation for usage of each of those variables can be found here.\nSecrets can be read from AWS Secrets Manager and used within your template as user variables. The aws_secretsmanager function is available only within the default value of a user variable, allowing you to default a user variable to an AWS Secrets Manager secret.\nPlaintext Secrets\n{ \"variables\": { \"password\": \"{{ aws_secretsmanager `globalpassword` }}\" } } \nIn the example above it is assumed that the secret globalpassword is not stored as a key pair but as a single non-JSON string value. Which the aws_secretsmanager function will return as a raw string.\nSingle Key Secrets\n{ \"variables\": { \"password\": \"{{ aws_secretsmanager `sample/app/password` }}\" } } \nIn the example above it is assumed that only one key is stored in sample/app/password if there are multiple keys stored in it then you need to indicate the specific key you want to fetch as shown below.\nMultiple Key Secrets\n{ \"variables\": { \"db_password\": \"{{ aws_secretsmanager `sample/app/passwords` `db` }}\", \"api_key\": \"{{ aws_secretsmanager `sample/app/passwords` `api_key` }}\" } } \nIn order to use this function you have to configure valid AWS credentials using one of the following methods:\nEnvironment Variables\nCLI Configuration Files\nContainer Credentials\nInstance Profile Credentials\nSome templates call for array values. You can use template variables for these, too. For example, the amazon-ebs builder has a configuration parameter called ami_regions, which takes an array of regions that it will copy the AMI to. You can parameterize this by using a variable that is a list of regions, joined by a ,. For example:\n{ \"variables\": { \"destination_regions\": \"us-west-1,us-west-2\" }, \"builders\": [ { \"ami_name\": \"packer-qs-{{timestamp}}\", \"instance_type\": \"t2.micro\", \"region\": \"us-east-1\", \"source_ami_filter\": { \"filters\": { \"name\": \"*ubuntu-xenial-16.04-amd64-server-*\", \"root-device-type\": \"ebs\", \"virtualization-type\": \"hvm\" }, \"most_recent\": true, \"owners\": [\"099720109477\"] }, \"ami_regions\": \"{{user `destination_regions`}}\", \"ssh_username\": \"ubuntu\", \"type\": \"amazon-ebs\" } ] } \nNow that we covered how to define and use user variables within a template, the next important point is how to actually set these variables. Packer exposes two methods for setting user variables: from the command line or from a file.\nFrom the Command Line\nTo set user variables from the command line, the -var flag is used as a parameter to packer build (and some other commands). Continuing our example above, we could build our template using the command below. The command is split across multiple lines for readability, but can of course be a single line.\n$ packer build \\ -var 'aws_access_key=foo' \\ -var 'aws_secret_key=bar' \\ template.json \nAs you can see, the -var flag can be specified multiple times in order to set multiple variables. Also, variables set later on the command-line override any earlier set variable of the same name.\nwarning If you are calling Packer from cmd.exe, you should double-quote your variables rather than single-quoting them. For example:\npacker build -var \"aws_secret_key=foo\" template.json\nFrom a File\nVariables can also be set from an external JSON file. The -var-file flag reads a file containing a key/value mapping of variables to values and sets those variables. An example JSON file may look like this:\n{ \"aws_access_key\": \"foo\", \"aws_secret_key\": \"bar\" } \nIt is a single JSON object where the keys are variables and the values are the variable values. Assuming this file is in variables.json, we can build our template using the following command:\nOn Linux : $ packer build -var-file=variables.json template.json On Windows : packer build -var-file variables.json template.json \nThe -var-file flag can be specified multiple times and variables from multiple files will be read and applied. As you'd expect, variables read from files specified later override a variable set earlier.\nCombining the -var and -var-file flags together also works how you'd expect. Variables set later in the command override variables set earlier. So, for example, in the following command with the above variables.json file:\n$ packer build \\ -var 'aws_access_key=bar' \\ -var-file=variables.json \\ -var 'aws_secret_key=baz' \\ template.json \nResults in the following variables:\nVariableValue\naws_access_key\tfoo\t\naws_secret_key\tbaz\t\nIf you use the environment to set a variable that is sensitive, you probably don't want that variable printed to the Packer logs. You can make sure that sensitive variables won't get printed to the logs by adding them to the \"sensitive-variables\" list within the Packer template:\n{ \"variables\": { \"my_secret\": \"{{env `MY_SECRET`}}\", \"not_a_secret\": \"plaintext\", \"foo\": \"bar\" }, \"sensitive-variables\": [\"my_secret\", \"foo\"], ... } \nThe above snippet of code will function exactly the same as if you did not set \"sensitive-variables\", except that the Packer UI and logs will replace all instances of \"bar\" and of whatever the value of \"my_secret\" is with <sensitive>. This allows you to be confident that you are not printing secrets in plaintext to our logs by accident.\nThere is no specific syntax in Packer templates for making a provisioner step conditional, depending on the value of a variable. However, you may be able to do this by referencing the variable within a command that you execute. For example, here is how to make a shell-local provisioner only run if the do_nexpose_scan variable is non-empty.\n{ \"type\": \"shell-local\", \"command\": \"if [ ! -z \\\"{{user `do_nexpose_scan`}}\\\" ]; then python -u trigger_nexpose_scan.py; fi\" } \nIn order to use $HOME variable, you can create a home variable in Packer:\n{ \"variables\": { \"home\": \"{{env `HOME`}}\" } } \nAnd this will be available to be used in the rest of the template, i.e.:\n{ \"builders\": [ { \"type\": \"google\", \"account_file\": \"{{ user `home` }}/.secrets/gcp-{{ user `env` }}.json\" } ] }"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.9.x/templates/legacy_json_templates/user-variables",
  "text": "User Variables - Templates | Packer\nUser variables allow your templates to be further configured with variables from the command-line, environment variables, Vault, or files. This lets you parameterize your templates so that you can keep secret tokens, environment-specific data, and other types of information out of your templates. This maximizes the portability of the template.\nUsing user variables expects you to know how configuration templates work. If you don't know how configuration templates work yet, please read that page first.\nIn order to set a user variable, you must define it either within the variables section within your template, or using the command-line -var or -var-file flags.\nEven if you want a user variable to default to an empty string, it is best to explicitly define it. This explicitness helps reduce the time it takes for newcomers to understand what can be modified using variables in your template.\nThe variables section is a key/value mapping of the user variable name to a default value. A default value can be the empty string. An example is shown below:\n{ \"variables\": { \"aws_access_key\": \"\", \"aws_secret_key\": \"\" }, \"builders\": [ { \"type\": \"amazon-ebs\", \"access_key\": \"{{user `aws_access_key`}}\", \"secret_key\": \"{{user `aws_secret_key`}}\" // ... } ] } \nIn the above example, the template defines two user variables: aws_access_key and aws_secret_key. They default to empty values. Later, the variables are used within the builder we defined in order to configure the actual keys for the Amazon builder.\nIf the default value is null, then the user variable will be required. This means that the user must specify a value for this variable or template validation will fail.\nUser variables are used by calling the {{user}} function in the form of {{user 'variable'}}. This function can be used in any value but type within the template: in builders, provisioners, anywhere outside the variables section. User variables are available globally within the rest of the template.\nEnvironment variables can be used within your template using user variables. The env function is available only within the default value of a user variable, allowing you to default a user variable to an environment variable. An example is shown below:\n{ \"variables\": { \"my_secret\": \"{{env `MY_SECRET`}}\" } } \nThis will default \"my_secret\" to be the value of the \"MY_SECRET\" environment variable (or an empty string if it does not exist).\nWhy can't I use environment variables elsewhere? User variables are the single source of configurable input to a template. We felt that having environment variables used anywhere in a template would confuse the user about the possible inputs to a template. By allowing environment variables only within default values for user variables, user variables remain as the single source of input to a template that a user can easily discover using packer inspect.\nWhy can't I use ~ for home variable? ~ is an special variable that is evaluated by shell during a variable expansion. As Packer doesn't run inside a shell, it won't expand ~.\nConsul keys can be used within your template using the consul_key function. This function is available only within the default value of a user variable, for reasons similar to environment variables above.\n{ \"variables\": { \"soft_versions\": \"{{ consul_key `my_image/softs_versions/next` }}\" } } \nThis will default soft_versions to the value of the key my_image/softs_versions/next in consul.\nThe configuration for consul (address, tokens, ...) must be specified as environment variables, as specified in the Documentation.\nSecrets can be read from Vault and used within your template as user variables. the vault function is available only within the default value of a user variable, allowing you to default a user variable to a vault secret.\nAn example of using a v2 kv engine:\nIf you store a value in vault using vault kv put secret/hello foo=world, you can access it using the following template engine:\n{ \"variables\": { \"my_secret\": \"{{ vault `/secret/data/hello` `foo`}}\" } } \nwhich will assign \"my_secret\": \"world\"\nAn example of using a v1 kv engine:\nIf you store a value in vault using:\nvault secrets enable -version=1 -path=secrets kv vault kv put secrets/hello foo=world \nYou can access it using the following template engine:\n{ \"variables\": { \"VAULT_SECRETY_SECRET\": \"{{ vault `secrets/hello` `foo`}}\" } } \nThis example accesses the Vault path secret/data/foo and returns the value stored at the key bar, storing it as \"my_secret\".\nIn order for this to work, you must set the environment variables VAULT_TOKEN and VAULT_ADDR to valid values.\nThe api tool we use allows for more custom configuration of the Vault client via environment variables.\nThe full list of available environment variables is:\n\"VAULT_ADDR\" \"VAULT_AGENT_ADDR\" \"VAULT_CACERT\" \"VAULT_CAPATH\" \"VAULT_CLIENT_CERT\" \"VAULT_CLIENT_KEY\" \"VAULT_CLIENT_TIMEOUT\" \"VAULT_SKIP_VERIFY\" \"VAULT_NAMESPACE\" \"VAULT_TLS_SERVER_NAME\" \"VAULT_WRAP_TTL\" \"VAULT_MAX_RETRIES\" \"VAULT_TOKEN\" \"VAULT_MFA\" \"VAULT_RATE_LIMIT\" \nand detailed documentation for usage of each of those variables can be found here.\nSecrets can be read from AWS Secrets Manager and used within your template as user variables. The aws_secretsmanager function is available only within the default value of a user variable, allowing you to default a user variable to an AWS Secrets Manager secret.\nPlaintext Secrets\n{ \"variables\": { \"password\": \"{{ aws_secretsmanager `globalpassword` }}\" } } \nIn the example above it is assumed that the secret globalpassword is not stored as a key pair but as a single non-JSON string value. Which the aws_secretsmanager function will return as a raw string.\nSingle Key Secrets\n{ \"variables\": { \"password\": \"{{ aws_secretsmanager `sample/app/password` }}\" } } \nIn the example above it is assumed that only one key is stored in sample/app/password if there are multiple keys stored in it then you need to indicate the specific key you want to fetch as shown below.\nMultiple Key Secrets\n{ \"variables\": { \"db_password\": \"{{ aws_secretsmanager `sample/app/passwords` `db` }}\", \"api_key\": \"{{ aws_secretsmanager `sample/app/passwords` `api_key` }}\" } } \nIn order to use this function you have to configure valid AWS credentials using one of the following methods:\nEnvironment Variables\nCLI Configuration Files\nContainer Credentials\nInstance Profile Credentials\nSome templates call for array values. You can use template variables for these, too. For example, the amazon-ebs builder has a configuration parameter called ami_regions, which takes an array of regions that it will copy the AMI to. You can parameterize this by using a variable that is a list of regions, joined by a ,. For example:\n{ \"variables\": { \"destination_regions\": \"us-west-1,us-west-2\" }, \"builders\": [ { \"ami_name\": \"packer-qs-{{timestamp}}\", \"instance_type\": \"t2.micro\", \"region\": \"us-east-1\", \"source_ami_filter\": { \"filters\": { \"name\": \"*ubuntu-xenial-16.04-amd64-server-*\", \"root-device-type\": \"ebs\", \"virtualization-type\": \"hvm\" }, \"most_recent\": true, \"owners\": [\"099720109477\"] }, \"ami_regions\": \"{{user `destination_regions`}}\", \"ssh_username\": \"ubuntu\", \"type\": \"amazon-ebs\" } ] } \nNow that we covered how to define and use user variables within a template, the next important point is how to actually set these variables. Packer exposes two methods for setting user variables: from the command line or from a file.\nFrom the Command Line\nTo set user variables from the command line, the -var flag is used as a parameter to packer build (and some other commands). Continuing our example above, we could build our template using the command below. The command is split across multiple lines for readability, but can of course be a single line.\n$ packer build \\ -var 'aws_access_key=foo' \\ -var 'aws_secret_key=bar' \\ template.json \nAs you can see, the -var flag can be specified multiple times in order to set multiple variables. Also, variables set later on the command-line override any earlier set variable of the same name.\nwarning If you are calling Packer from cmd.exe, you should double-quote your variables rather than single-quoting them. For example:\npacker build -var \"aws_secret_key=foo\" template.json\nFrom a File\nVariables can also be set from an external JSON file. The -var-file flag reads a file containing a key/value mapping of variables to values and sets those variables. An example JSON file may look like this:\n{ \"aws_access_key\": \"foo\", \"aws_secret_key\": \"bar\" } \nIt is a single JSON object where the keys are variables and the values are the variable values. Assuming this file is in variables.json, we can build our template using the following command:\nOn Linux : $ packer build -var-file=variables.json template.json On Windows : packer build -var-file variables.json template.json \nThe -var-file flag can be specified multiple times and variables from multiple files will be read and applied. As you'd expect, variables read from files specified later override a variable set earlier.\nCombining the -var and -var-file flags together also works how you'd expect. Variables set later in the command override variables set earlier. So, for example, in the following command with the above variables.json file:\n$ packer build \\ -var 'aws_access_key=bar' \\ -var-file=variables.json \\ -var 'aws_secret_key=baz' \\ template.json \nResults in the following variables:\nVariableValue\naws_access_key\tfoo\t\naws_secret_key\tbaz\t\nIf you use the environment to set a variable that is sensitive, you probably don't want that variable printed to the Packer logs. You can make sure that sensitive variables won't get printed to the logs by adding them to the \"sensitive-variables\" list within the Packer template:\n{ \"variables\": { \"my_secret\": \"{{env `MY_SECRET`}}\", \"not_a_secret\": \"plaintext\", \"foo\": \"bar\" }, \"sensitive-variables\": [\"my_secret\", \"foo\"], ... } \nThe above snippet of code will function exactly the same as if you did not set \"sensitive-variables\", except that the Packer UI and logs will replace all instances of \"bar\" and of whatever the value of \"my_secret\" is with <sensitive>. This allows you to be confident that you are not printing secrets in plaintext to our logs by accident.\nThere is no specific syntax in Packer templates for making a provisioner step conditional, depending on the value of a variable. However, you may be able to do this by referencing the variable within a command that you execute. For example, here is how to make a shell-local provisioner only run if the do_nexpose_scan variable is non-empty.\n{ \"type\": \"shell-local\", \"command\": \"if [ ! -z \\\"{{user `do_nexpose_scan`}}\\\" ]; then python -u trigger_nexpose_scan.py; fi\" } \nIn order to use $HOME variable, you can create a home variable in Packer:\n{ \"variables\": { \"home\": \"{{env `HOME`}}\" } } \nAnd this will be available to be used in the rest of the template, i.e.:\n{ \"builders\": [ { \"type\": \"google\", \"account_file\": \"{{ user `home` }}/.secrets/gcp-{{ user `env` }}.json\" } ] }"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/templates/legacy_json_templates/user-variables",
  "text": "User Variables - Templates | Packer\nUser variables allow your templates to be further configured with variables from the command-line, environment variables, Vault, or files. This lets you parameterize your templates so that you can keep secret tokens, environment-specific data, and other types of information out of your templates. This maximizes the portability of the template.\nUsing user variables expects you to know how configuration templates work. If you don't know how configuration templates work yet, please read that page first.\nIn order to set a user variable, you must define it either within the variables section within your template, or using the command-line -var or -var-file flags.\nEven if you want a user variable to default to an empty string, it is best to explicitly define it. This explicitness helps reduce the time it takes for newcomers to understand what can be modified using variables in your template.\nThe variables section is a key/value mapping of the user variable name to a default value. A default value can be the empty string. An example is shown below:\n{ \"variables\": { \"aws_access_key\": \"\", \"aws_secret_key\": \"\" }, \"builders\": [ { \"type\": \"amazon-ebs\", \"access_key\": \"{{user `aws_access_key`}}\", \"secret_key\": \"{{user `aws_secret_key`}}\" // ... } ] } \nIn the above example, the template defines two user variables: aws_access_key and aws_secret_key. They default to empty values. Later, the variables are used within the builder we defined in order to configure the actual keys for the Amazon builder.\nIf the default value is null, then the user variable will be required. This means that the user must specify a value for this variable or template validation will fail.\nUser variables are used by calling the {{user}} function in the form of {{user 'variable'}}. This function can be used in any value but type within the template: in builders, provisioners, anywhere outside the variables section. User variables are available globally within the rest of the template.\nEnvironment variables can be used within your template using user variables. The env function is available only within the default value of a user variable, allowing you to default a user variable to an environment variable. An example is shown below:\n{ \"variables\": { \"my_secret\": \"{{env `MY_SECRET`}}\" } } \nThis will default \"my_secret\" to be the value of the \"MY_SECRET\" environment variable (or an empty string if it does not exist).\nWhy can't I use environment variables elsewhere? User variables are the single source of configurable input to a template. We felt that having environment variables used anywhere in a template would confuse the user about the possible inputs to a template. By allowing environment variables only within default values for user variables, user variables remain as the single source of input to a template that a user can easily discover using packer inspect.\nWhy can't I use ~ for home variable? ~ is an special variable that is evaluated by shell during a variable expansion. As Packer doesn't run inside a shell, it won't expand ~.\nConsul keys can be used within your template using the consul_key function. This function is available only within the default value of a user variable, for reasons similar to environment variables above.\n{ \"variables\": { \"soft_versions\": \"{{ consul_key `my_image/softs_versions/next` }}\" } } \nThis will default soft_versions to the value of the key my_image/softs_versions/next in consul.\nThe configuration for consul (address, tokens, ...) must be specified as environment variables, as specified in the Documentation.\nSecrets can be read from Vault and used within your template as user variables. the vault function is available only within the default value of a user variable, allowing you to default a user variable to a vault secret.\nAn example of using a v2 kv engine:\nIf you store a value in vault using vault kv put secret/hello foo=world, you can access it using the following template engine:\n{ \"variables\": { \"my_secret\": \"{{ vault `/secret/data/hello` `foo`}}\" } } \nwhich will assign \"my_secret\": \"world\"\nAn example of using a v1 kv engine:\nIf you store a value in vault using:\nvault secrets enable -version=1 -path=secrets kv vault kv put secrets/hello foo=world \nYou can access it using the following template engine:\n{ \"variables\": { \"VAULT_SECRETY_SECRET\": \"{{ vault `secrets/hello` `foo`}}\" } } \nThis example accesses the Vault path secret/data/foo and returns the value stored at the key bar, storing it as \"my_secret\".\nIn order for this to work, you must set the environment variables VAULT_TOKEN and VAULT_ADDR to valid values.\nThe api tool we use allows for more custom configuration of the Vault client via environment variables.\nThe full list of available environment variables is:\n\"VAULT_ADDR\" \"VAULT_AGENT_ADDR\" \"VAULT_CACERT\" \"VAULT_CAPATH\" \"VAULT_CLIENT_CERT\" \"VAULT_CLIENT_KEY\" \"VAULT_CLIENT_TIMEOUT\" \"VAULT_SKIP_VERIFY\" \"VAULT_NAMESPACE\" \"VAULT_TLS_SERVER_NAME\" \"VAULT_WRAP_TTL\" \"VAULT_MAX_RETRIES\" \"VAULT_TOKEN\" \"VAULT_MFA\" \"VAULT_RATE_LIMIT\" \nand detailed documentation for usage of each of those variables can be found here.\nSecrets can be read from AWS Secrets Manager and used within your template as user variables. The aws_secretsmanager function is available only within the default value of a user variable, allowing you to default a user variable to an AWS Secrets Manager secret.\nPlaintext Secrets\n{ \"variables\": { \"password\": \"{{ aws_secretsmanager `globalpassword` }}\" } } \nIn the example above it is assumed that the secret globalpassword is not stored as a key pair but as a single non-JSON string value. Which the aws_secretsmanager function will return as a raw string.\nSingle Key Secrets\n{ \"variables\": { \"password\": \"{{ aws_secretsmanager `sample/app/password` }}\" } } \nIn the example above it is assumed that only one key is stored in sample/app/password if there are multiple keys stored in it then you need to indicate the specific key you want to fetch as shown below.\nMultiple Key Secrets\n{ \"variables\": { \"db_password\": \"{{ aws_secretsmanager `sample/app/passwords` `db` }}\", \"api_key\": \"{{ aws_secretsmanager `sample/app/passwords` `api_key` }}\" } } \nIn order to use this function you have to configure valid AWS credentials using one of the following methods:\nEnvironment Variables\nCLI Configuration Files\nContainer Credentials\nInstance Profile Credentials\nSome templates call for array values. You can use template variables for these, too. For example, the amazon-ebs builder has a configuration parameter called ami_regions, which takes an array of regions that it will copy the AMI to. You can parameterize this by using a variable that is a list of regions, joined by a ,. For example:\n{ \"variables\": { \"destination_regions\": \"us-west-1,us-west-2\" }, \"builders\": [ { \"ami_name\": \"packer-qs-{{timestamp}}\", \"instance_type\": \"t2.micro\", \"region\": \"us-east-1\", \"source_ami_filter\": { \"filters\": { \"name\": \"*ubuntu-xenial-16.04-amd64-server-*\", \"root-device-type\": \"ebs\", \"virtualization-type\": \"hvm\" }, \"most_recent\": true, \"owners\": [\"099720109477\"] }, \"ami_regions\": \"{{user `destination_regions`}}\", \"ssh_username\": \"ubuntu\", \"type\": \"amazon-ebs\" } ] } \nNow that we covered how to define and use user variables within a template, the next important point is how to actually set these variables. Packer exposes two methods for setting user variables: from the command line or from a file.\nFrom the Command Line\nTo set user variables from the command line, the -var flag is used as a parameter to packer build (and some other commands). Continuing our example above, we could build our template using the command below. The command is split across multiple lines for readability, but can of course be a single line.\n$ packer build \\ -var 'aws_access_key=foo' \\ -var 'aws_secret_key=bar' \\ template.json \nAs you can see, the -var flag can be specified multiple times in order to set multiple variables. Also, variables set later on the command-line override any earlier set variable of the same name.\nwarning If you are calling Packer from cmd.exe, you should double-quote your variables rather than single-quoting them. For example:\npacker build -var \"aws_secret_key=foo\" template.json\nFrom a File\nVariables can also be set from an external JSON file. The -var-file flag reads a file containing a key/value mapping of variables to values and sets those variables. An example JSON file may look like this:\n{ \"aws_access_key\": \"foo\", \"aws_secret_key\": \"bar\" } \nIt is a single JSON object where the keys are variables and the values are the variable values. Assuming this file is in variables.json, we can build our template using the following command:\nOn Linux : $ packer build -var-file=variables.json template.json On Windows : packer build -var-file variables.json template.json \nThe -var-file flag can be specified multiple times and variables from multiple files will be read and applied. As you'd expect, variables read from files specified later override a variable set earlier.\nCombining the -var and -var-file flags together also works how you'd expect. Variables set later in the command override variables set earlier. So, for example, in the following command with the above variables.json file:\n$ packer build \\ -var 'aws_access_key=bar' \\ -var-file=variables.json \\ -var 'aws_secret_key=baz' \\ template.json \nResults in the following variables:\nVariableValue\naws_access_key\tfoo\t\naws_secret_key\tbaz\t\nIf you use the environment to set a variable that is sensitive, you probably don't want that variable printed to the Packer logs. You can make sure that sensitive variables won't get printed to the logs by adding them to the \"sensitive-variables\" list within the Packer template:\n{ \"variables\": { \"my_secret\": \"{{env `MY_SECRET`}}\", \"not_a_secret\": \"plaintext\", \"foo\": \"bar\" }, \"sensitive-variables\": [\"my_secret\", \"foo\"], ... } \nThe above snippet of code will function exactly the same as if you did not set \"sensitive-variables\", except that the Packer UI and logs will replace all instances of \"bar\" and of whatever the value of \"my_secret\" is with <sensitive>. This allows you to be confident that you are not printing secrets in plaintext to our logs by accident.\nThere is no specific syntax in Packer templates for making a provisioner step conditional, depending on the value of a variable. However, you may be able to do this by referencing the variable within a command that you execute. For example, here is how to make a shell-local provisioner only run if the do_nexpose_scan variable is non-empty.\n{ \"type\": \"shell-local\", \"command\": \"if [ ! -z \\\"{{user `do_nexpose_scan`}}\\\" ]; then python -u trigger_nexpose_scan.py; fi\" } \nIn order to use $HOME variable, you can create a home variable in Packer:\n{ \"variables\": { \"home\": \"{{env `HOME`}}\" } } \nAnd this will be available to be used in the rest of the template, i.e.:\n{ \"builders\": [ { \"type\": \"google\", \"account_file\": \"{{ user `home` }}/.secrets/gcp-{{ user `env` }}.json\" } ] }"
},
{
  "url": "https://developer.hashicorp.com/packer/plugins/v1.10.x/builders/amazon",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.7.x/builders/amazon",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/basics/terminology",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/builders/vmware/iso",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/builders/vsphere-iso",
  "text": "This page does not exist for version v1.6.x."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/builders/vmware/vsphere-iso",
  "text": "VSphere ISO - Builders | Packer\nPacker Builder for VMware vSphere\nType: vsphere-iso\nIt uses the official vCenter API, and does not require ESXi host modification\nThis builder is supported for vSphere version 6.5 and greater. Builds on lower versions may work, but some configuration options may throw errors because they do not exist in the older versions of the vSphere API.\nwget http://{{ .HTTPIP }}:{{ .HTTPPort }}/foo/bar/preseed.cfg\nvcenter_server (string) - vCenter server hostname.\ninsecure_connection (bool) - Do not validate vCenter server's TLS certificate. Defaults to false.\ndatacenter (string) - VMware datacenter name. Required if there is more than one datacenter in vCenter.\nvideo_ram (int64) - Amount of video memory in KB.\ncluster (string) - ESXi cluster where target VM is created. See the Working With Clusters And Hosts section above for more details.\nresource_pool (string) - VMWare resource pool. If not set, it will look for the root resource pool of the host or cluster. If a root resource is not found, it will then look for a default resource pool.\ndatastore (string) - VMWare datastore. Required if host is a cluster, or if host has multiple datastores.\nset_host_for_datastore_uploads (bool) - Set this to true if packer should the host for uploading files to the datastore. Defaults to false.\nshutdown_command (string) - Specify a VM guest shutdown command. This command will be executed using the communicator. Otherwise the VMware guest tools are used to gracefully shutdown the VM guest.\nfile:http://releases.ubuntu.com/20.04/MD5SUMS\nvm_version (uint) - Set VM hardware version. Defaults to the most current VM hardware version supported by vCenter. See VMWare article 1003746 for the full list of supported VM hardware versions.\nnetwork_adapters ([]NIC) - Network adapters\ndisk_controller_type ([]string) - Set VM disk controller type. Example lsilogic, pvscsi, nvme, or scsi. Use a list to define additional controllers. Defaults to lsilogic. See SCSI, SATA, and NVMe Storage Controller Conditions, Limitations, and Compatibility for additional details.\nDefines a Network Adapter\nYou may optionally export an ovf from VSphere to the instance running Packer.\nname (string) - name of the ovf. defaults to the name of the VM\nforce (bool) - overwrite ovf if it exists\nimages (bool) - include iso and img image files that are attached to the VM\nmanifest (string) - generate manifest using sha1, sha256, sha512. Defaults to 'sha256'. Use 'none' for no manifest.\ndescription (string) - Description of the library item that will be created. This option is not used when importing OVF templates. Defaults to \"Packer imported vm_name VM template\".\nconfiguration_parameters (map[string]string) - configuration_parameters is a direct passthrough to the VSphere API's ConfigSpec: https://pubs.vmware.com/vi3/sdk/ReferenceGuide/vim.vm.ConfigSpec.html\nssh_ciphers ([]string) - This overrides the value of ciphers supported by default by golang. The default value is [ \"aes128-gcm@openssh.com\", \"chacha20-poly1305@openssh.com\", \"aes128-ctr\", \"aes192-ctr\", \"aes256-ctr\", ]\nssh_key_exchange_algorithms ([]string) - If set, Packer will override the value of key exchange (kex) altorighms supported by default by golang. Acceptable values include: \"curve25519-sha256@libssh.org\", \"ecdh-sha2-nistp256\", \"ecdh-sha2-nistp384\", \"ecdh-sha2-nistp521\", \"diffie-hellman-group14-sha1\", and \"diffie-hellman-group1-sha1\".\nssh_timeout (duration string | ex: \"1h5m2s\") - The time to wait for SSH to become available. Packer uses this to determine when the machine has booted so this is usually quite long. Example value: 10m.\nssh_handshake_attempts (int) - The number of handshakes to attempt with SSH once it can connect. This defaults to 10.\nssh_remote_tunnels ([]string) -\nssh_local_tunnels ([]string) -\nVM folder (this object and children):\nVirtual machine -> Inventory Virtual machine -> Configuration Virtual machine -> Interaction Virtual machine -> Snapshot management Virtual machine -> Provisioning \nIndividual privileges are listed in https://github.com/jetbrains-infra/packer-builder-vsphere/issues/97#issuecomment-436063235.\nResource pool, host, or cluster (this object):\nResource -> Assign virtual machine to resource pool \nHost in clusters without DRS (this object):\nDatastore (this object):\nDatastore -> Allocate space Datastore -> Browse datastore Datastore -> Low level file operations \nNetwork (this object):\nNetwork -> Assign network \nDistributed switch (this object):\nFor floppy image upload:\nDatacenter (this object):\nDatastore -> Low level file operations \nHost (this object):\nHost -> Configuration -> System Management"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/builders/vmware/vmx",
  "text": "VMware VMX - Builders | Packer\nType: vmware-vmx\nThis VMware Packer builder is able to create VMware virtual machines from an existing VMware virtual machine (a VMX file). It currently supports building virtual machines on hosts running VMware Fusion Professional for OS X, VMware Workstation for Linux and Windows, and VMware Player on Linux.\nThe builder builds a virtual machine by cloning the VMX file using the clone capabilities introduced in VMware Fusion Professional 6, Workstation 10, and Player 6. After cloning the VM, it provisions software within the new machine, shuts it down, and compacts the disks. The resulting folder contains a new VMware virtual machine.\nHere is an example. This example is fully functional as long as the source path points to a real VMX file with the proper settings:\n{ \"type\": \"vmware-vmx\", \"source_path\": \"/path/to/a/vm.vmx\", \"ssh_username\": \"root\", \"ssh_password\": \"root\", \"shutdown_command\": \"shutdown -P now\" } \nThere are many configuration options available for the VMware builder. They are organized below into two categories: required and optional. Within each category, the available options are alphabetized and described.\nThere are many configuration options available for the builder. In addition to the items listed here, you will want to look at the general configuration references for HTTP, Floppy, Boot, Driver, Output, Run, Shutdown, Communicator, Tools, vmx, Export, configuration references, which are necessary for this build to succeed and can be found further down the page.\nsource_path (string) - Path to the source VMX file to clone. If remote_type is enabled then this specifies a path on the remote_host.\nlinked (bool) - By default Packer creates a 'full' clone of the virtual machine specified in source_path. The resultant virtual machine is fully independant from the parent it was cloned from.\nSetting linked to true instead causes Packer to create the virtual machine as a 'linked' clone. Linked clones use and require ongoing access to the disks of the parent virtual machine. The benefit of a linked clone is that the clones virtual disk is typically very much smaller than would be the case for a full clone. Additionally, the cloned virtual machine can also be created much faster. Creating a linked clone will typically only be of benefit in some advanced build scenarios. Most users will wish to create a full clone instead. Defaults to false.\nwget http://{{ .HTTPIP }}:{{ .HTTPPort }}/foo/bar/preseed.cfg\nssh_ciphers ([]string) - This overrides the value of ciphers supported by default by golang. The default value is [ \"aes128-gcm@openssh.com\", \"chacha20-poly1305@openssh.com\", \"aes128-ctr\", \"aes192-ctr\", \"aes256-ctr\", ]\nssh_key_exchange_algorithms ([]string) - If set, Packer will override the value of key exchange (kex) altorighms supported by default by golang. Acceptable values include: \"curve25519-sha256@libssh.org\", \"ecdh-sha2-nistp256\", \"ecdh-sha2-nistp384\", \"ecdh-sha2-nistp521\", \"diffie-hellman-group14-sha1\", and \"diffie-hellman-group1-sha1\".\nssh_timeout (duration string | ex: \"1h5m2s\") - The time to wait for SSH to become available. Packer uses this to determine when the machine has booted so this is usually quite long. Example value: 10m.\nssh_handshake_attempts (int) - The number of handshakes to attempt with SSH once it can connect. This defaults to 10.\nssh_remote_tunnels ([]string) -\nssh_local_tunnels ([]string) -\n{ \"builders\": [ { \"type\": \"vmware-vmx\", \"boot_key_interval\": \"10ms\", ... } ] } \nExample boot command. This is actually a working boot command used to start an Ubuntu 12.04 installer:\n[ \"<esc><esc><enter><wait>\", \"/install/vmlinuz noapic \", \"preseed/url=http://{{ .HTTPIP }}:{{ .HTTPPort }}/preseed.cfg \", \"debian-installer=en_US auto locale=en_US kbd-chooser/method=us \", \"hostname={{ .Name }} \", \"fb=false debconf/frontend=noninteractive \", \"keyboard-configuration/modelcode=SKIP keyboard-configuration/layout=USA \", \"keyboard-configuration/variant=USA console-setup/ask_detect=false \", \"initrd=/install/initrd.gz -- <enter>\" ] "
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/builders/vmware/vsphere-clone",
  "text": "VSphere Clone - Builders | Packer\nType: vsphere-clone\nThis builder clones VMs from existing templates.\nIt uses the official vCenter API, and does not require ESXi host modification\nThis builder is supported for vSphere version 6.5 and greater. Builds on lower versions may work, but some configuration options may throw errors because they do not exist in the older versions of the vSphere API.\nThere are many configuration options available for this builder. In addition to the items listed here, you will want to look at the general configuration references for Hardware, Output, Boot, Run, Shutdown, Communicator, Export, configuration references, which are necessary for this build to succeed and can be found further down the page.\ncontent_library_destination (*common.ContentLibraryDestinationConfig) - Configuration for importing a VM template or OVF template to a Content Library. The template will not be imported if no Content Library Import Configuration is specified. The import doesn't work if convert_to_template is set to true.\ncustomize (*CustomizeConfig) - Customize the cloned VM to configure host, network, or licensing settings. See the customization options.\nClone Configuration\ntemplate (string) - Name of source VM. Path is optional.\nlinked_clone (bool) - Create VM as a linked clone from latest snapshot. Defaults to false.\nmac_address (string) - Sets a custom Mac Address to the network adapter. If set, the network must be also specified.\nvapp (vAppConfig) - Set the vApp Options to a virtual machine. See the vApp Options Configuration to know the available options and how to use it.\ndisk_controller_type ([]string) - Set VM disk controller type. Example lsilogic, pvscsi, nvme, or scsi. Use a list to define additional controllers. Defaults to lsilogic. See SCSI, SATA, and NVMe Storage Controller Conditions, Limitations, and Compatibility for additional details.\nWhen cloning a VM, the storage configuration can be used to add additional storage and disk controllers. The resulting VM will contain the origin VM storage and disk controller plus the new configured ones.\nvApp Options Configuration\nproperties (map[string]string) - Set values for the available vApp Properties to supply configuration parameters to a virtual machine cloned from a template that came from an imported OVF or OVA file.\nNote: The only supported usage path for vApp properties is for existing user-configurable keys. These generally come from an existing template that was created from an imported OVF or OVA file. You cannot set values for vApp properties on virtual machines created from scratch, virtual machines lacking a vApp configuration, or on property keys that do not exist.\nExample of usage:\n\"vapp\": { \"properties\": { \"hostname\": \"{{ user `hostname`}}\", \"user-data\": \"{{ env `USERDATA`}}\" } } \nA user-data field requires the content of a yaml file to be encoded with base64. This can be done via environment variable: export USERDATA=$(gzip -c9 <userdata.yaml | { base64 -w0 2>/dev/null || base64; })\nconfiguration_parameters (map[string]string) - configuration_parameters is a direct passthrough to the VSphere API's ConfigSpec: https://pubs.vmware.com/vi3/sdk/ReferenceGuide/vim.vm.ConfigSpec.html\nCustomization\nA cloned virtual machine can be customized to configure host, network, or licensing settings.\nTo perform virtual machine customization as a part of the clone process, specify the customize block with the respective customization options. Windows guests are customized using Sysprep, which will result in the machine SID being reset. Before using customization, check that your source VM meets the requirements for guest OS customization on vSphere. See the customization example for a usage synopsis.\nThe settings for customize are as follows:\nlinux_options (*LinuxOptions) - Settings to Linux guest OS customization. See Linux customization settings.\nwindows_sysprep_file (string) - Supply your own sysprep.xml file to allow full control of the customization process out-of-band of vSphere.\nnetwork_interface (NetworkInterfaces) - Configure network interfaces on a per-interface basis that should matched up to the network adapters present in the VM. To use DHCP, declare an empty network_interface for each adapter being configured. This field is required. See Network interface settings.\nNetwork Interface Settings\ndns_server_list ([]string) - Network interface-specific DNS server settings for Windows operating systems. Ignored on Linux and possibly other operating systems - for those systems, please see the global DNS settings section.\ndns_domain (string) - Network interface-specific DNS search domain for Windows operating systems. Ignored on Linux and possibly other operating systems - for those systems, please see the global DNS settings section.\nipv4_address (string) - The IPv4 address assigned to this network adapter. If left blank or not included, DHCP is used.\nipv4_netmask (int) - The IPv4 subnet mask, in bits (example: 24 for 255.255.255.0).\nipv6_address (string) - The IPv6 address assigned to this network adapter. If left blank or not included, auto-configuration is used.\nipv6_netmask (int) - The IPv6 subnet mask, in bits (example: 32).\nGlobal Routing Settings\nThe settings here must match the IP/mask of at least one network_interface supplied to customization.\nipv4_gateway (string) - The IPv4 default gateway when using network_interface customization on the virtual machine.\nipv6_gateway (string) - The IPv6 default gateway when using network_interface customization on the virtual machine.\nGlobal DNS Settings\nThe following settings configure DNS globally, generally for Linux systems. For Windows systems, this is done per-interface, see network interface settings.\ndns_server_list ([]string) - The list of DNS servers to configure on a virtual machine.\ndns_suffix_list ([]string) - A list of DNS search domains to add to the DNS configuration on the virtual machine.\nLinux Customization Settings\ndomain (string) - The domain name for this machine. This, along with host_name, make up the FQDN of this virtual machine.\nhost_name (string) - The host name for this machine. This, along with domain, make up the FQDN of this virtual machine.\nhw_clock_utc (boolean) - Tells the operating system that the hardware clock is set to UTC. Default: true.\ntime_zone (string) - Sets the time zone. The default is UTC.\nCustomization Example\n\"customize\": { \"linux_options\": { \"host_name\": \"packer-test\", \"domain\": \"test.internal\" }, \"network_interface\": { \"ipv4_address\": \"10.0.0.10\", \"ipv4_netmask\": \"24\" }, \"ipv4_gateway\": \"10.0.0.1\", \"dns_server_list\": [\"10.0.0.18\"] } \nBoot configuration\nwget http://{{ .HTTPIP }}:{{ .HTTPPort }}/foo/bar/preseed.cfg\nvcenter_server (string) - vCenter server hostname.\ninsecure_connection (bool) - Do not validate vCenter server's TLS certificate. Defaults to false.\ndatacenter (string) - VMware datacenter name. Required if there is more than one datacenter in vCenter.\nvideo_ram (int64) - Amount of video memory in KB.\ncluster (string) - ESXi cluster where target VM is created. See the Working With Clusters And Hosts section above for more details.\nresource_pool (string) - VMWare resource pool. If not set, it will look for the root resource pool of the host or cluster. If a root resource is not found, it will then look for a default resource pool.\ndatastore (string) - VMWare datastore. Required if host is a cluster, or if host has multiple datastores.\nset_host_for_datastore_uploads (bool) - Set this to true if packer should the host for uploading files to the datastore. Defaults to false.\nshutdown_command (string) - Specify a VM guest shutdown command. This command will be executed using the communicator. Otherwise the VMware guest tools are used to gracefully shutdown the VM guest.\nssh_ciphers ([]string) - This overrides the value of ciphers supported by default by golang. The default value is [ \"aes128-gcm@openssh.com\", \"chacha20-poly1305@openssh.com\", \"aes128-ctr\", \"aes192-ctr\", \"aes256-ctr\", ]\nssh_key_exchange_algorithms ([]string) - If set, Packer will override the value of key exchange (kex) altorighms supported by default by golang. Acceptable values include: \"curve25519-sha256@libssh.org\", \"ecdh-sha2-nistp256\", \"ecdh-sha2-nistp384\", \"ecdh-sha2-nistp521\", \"diffie-hellman-group14-sha1\", and \"diffie-hellman-group1-sha1\".\nssh_timeout (duration string | ex: \"1h5m2s\") - The time to wait for SSH to become available. Packer uses this to determine when the machine has booted so this is usually quite long. Example value: 10m.\nssh_handshake_attempts (int) - The number of handshakes to attempt with SSH once it can connect. This defaults to 10.\nssh_remote_tunnels ([]string) -\nssh_local_tunnels ([]string) -\nssh_keypair_name (string) - If specified, this is the key that will be used for SSH with the machine. The key must match a key pair name loaded up into the remote. By default, this is blank, and Packer will generate a temporary keypair unless ssh_password is used. ssh_private_key_file or ssh_agent_auth must be specified when ssh_keypair_name is utilized.\nssh_agent_auth (bool) - If true, the local SSH agent will be used to authenticate connections to the source instance. No temporary keypair will be created, and the values of ssh_password and ssh_private_key_file will be ignored. The environment variable SSH_AUTH_SOCK must be set for this option to work properly.\nNOTE: Packer uses vApp Options to inject ssh public keys to the Virtual Machine. The temporary_key_pair_name will only work if the template being cloned contains the vApp property public-keys. If using ssh_private_key_file, provide the public key via configuration_parameters or vApp Options Configuration whenever the guestinto.userdata is available. See VMware Guestinfo datasource for more information about the key.\nYou may optionally export an ovf from VSphere to the instance running Packer.\nname (string) - name of the ovf. defaults to the name of the VM\nforce (bool) - overwrite ovf if it exists\nimages (bool) - include iso and img image files that are attached to the VM\nmanifest (string) - generate manifest using sha1, sha256, sha512. Defaults to 'sha256'. Use 'none' for no manifest.\ndescription (string) - Description of the library item that will be created. This option is not used when importing OVF templates. Defaults to \"Packer imported vm_name VM template\".\nMinimal example of usage:\nVM folder (this object and children):\nVirtual machine -> Inventory Virtual machine -> Configuration Virtual machine -> Interaction Virtual machine -> Snapshot management Virtual machine -> Provisioning \nIndividual privileges are listed in https://github.com/jetbrains-infra/packer-builder-vsphere/issues/97#issuecomment-436063235.\nResource pool, host, or cluster (this object):\nResource -> Assign virtual machine to resource pool \nHost in clusters without DRS (this object):\nDatastore (this object):\nDatastore -> Allocate space Datastore -> Browse datastore Datastore -> Low level file operations \nNetwork (this object):\nNetwork -> Assign network \nDistributed switch (this object):\nFor floppy image upload:\nDatacenter (this object):\nDatastore -> Low level file operations \nHost (this object):\nHost -> Configuration -> System Management"
},
{
  "url": "https://developer.hashicorp.com/packer/docs/builders/amazon/chroot",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/from-1.5/contextual-variables",
  "text": "Contextual Variables - HCL Configuration Language | Packer\nNote: This page is about HCL2 in Packer 1.5 and later. HCL2 support for Packer is still in Beta. Please see the Packer Issue Tracker for a list of supported features. For the old-style stable configuration language see template docs. You can now transform your JSON file into an HCL2 config file using the hcl2_upgrade command.\nIt is possible to access the name and type of your source from provisioners and post-processors:\nsource \"null\" \"first-example\" { communicator = \"none\" } build { name = \"roles\" source \"null.first-example\" { name = \"consul\" } source \"null.first-example\" { name = \"nomad\" } source \"null.first-example\" { name = \"vault\" } sources = [\"null.first-example\"] provisioner \"shell-local\" { inline = [\"echo ${source.name} and ${source.type}\"] } } # This will echo something like: # # roles.null.consul: consul and null # roles.null.nomad: nomad and null # roles.null.vault: vault and null # roles.null.first-example: first-example and null \nBuild variables will allow you to access connection information and basic instance state information for a builder. All special build variables are stored in the build variable:\nsource \"null\" \"first-example\" { communicator = \"none\" } build { name = \"my-build-name\" sources = [\"null.first-example\"] provisioner \"shell-local\" { environment_vars = [\"TESTVAR=${build.PackerRunUUID}\"] inline = [\"echo source.name is ${source.name}.\", \"echo build.name is ${build.name}.\", \"echo build.PackerRunUUID is $TESTVAR\"] } } \nHere is the list of available build variables:\nname Represents the name of the build block being run. This is different than the name of the source block being run.\nID: Represents the vm being provisioned. For example, in Amazon it is the instance id; in digitalocean, it is the droplet id; in Vmware, it is the vm name.\nHost, Port, User and Password: The host, port, user, and password that Packer uses to access the machine. Useful for using the shell local provisioner to run Ansible or Inspec against the provisioned instance.\nConnType: Type of communicator being used. For example, for SSH communicator this will be \"ssh\".\nPackerRunUUID: Current build's unique id. Can be used to specify build artifacts. An example of that, is when multiple builds runs at the same time producing the same artifact. It's possible to differentiate these artifacts by naming them with the builds' unique ids.\nPackerHTTPIP, PackerHTTPPort, and PackerHTTPAddr: HTTP IP, port, and address of the file server Packer creates to serve items in the \"http\" dir to the vm. The HTTP address is displayed in the format IP:PORT.\nSSHPublicKey and SSHPrivateKey: The public and private key that Packer uses to connect to the instance. These are unique to the SSH communicator and are unset when using other communicators. SSHPublicKey and SSHPrivateKey can have escape sequences and special characters so their output should be single quoted to avoid surprises. For example:\nprovisioner \"shell\" { inline = [\"echo '${build.SSHPrivateKey}' > /tmp/packer-session.pem\"] } \nFor backwards compatibility, WinRMPassword is also available through this engine, though it is no different than using the more general Password.\nAll build variables are valid to use with any of the HCL2 functions. Example of using upper to upper case the build ID:\npost-processor \"shell-local\" { inline = [\"echo ${upper(build.ID)}\"] } \nFor builder-specific builder variables, please also refer to the builder docs:\nAmazon EC2: chroot, EBS Volume, EBS, EBS Surrogate, Instance.\nThe HCL2 Special Build Variables is in beta; please report any issues or requests on the Packer issue tracker on GitHub.\nThis variable is set to the Packer version currently running.\nsource \"null\" \"first-example\" { communicator = \"none\" } build { sources = [\"null.first-example\"] provisioner \"shell-local\" { inline = [\"echo packer_version is '${packer.version}'\"] } } \nIf you are running a development version of Packer, the version variable will contain the released version number, dev flag, and current commit.\nPACKER_LOG=0 packer build packer_version_demo.pkr.hcl null.first-example: output will be in this color. ==> null.first-example: Running local shell script: /var/folders/8t/0yb5q0_x6mb2jldqq_vjn3lr0000gn/T/packer-shell083160352 null.first-example: packer_version is 1.6.5-dev (a69392129+CHANGES) \nIf you are running a released version of Packer, the version variable will contain the released version number only:\nPACKER_LOG=0 packer build packer_version_demo.pkr.hcl null.first-example: output will be in this color. ==> null.first-example: Running local shell script: /var/folders/8t/0yb5q0_x6mb2jldqq_vjn3lr0000gn/T/packer-shell718995312 null.first-example: packer_version is 1.6.5 \nMake sure to wrap your variable in single quotes in order to escape the string that is returned; if you are running a dev version of packer the parenthesis may through off your shell escaping otherwise."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/v1.6.x/configuration/from-1.5/expressions",
  "text": "This page does not exist for version v1.6.x."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/digitalocean/digitalocean/v0.0.0/components/builder/digitalocean",
  "text": "This page does not exist for version v0.0.0."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/hashicorp/cloudstack/v1.0.2",
  "text": "CloudStack (v1.0.2) | Integrations | Packer\nThe cloudstack plugin can be used with HashiCorp Packer to create custom images on Apache CloudStack.\nCommunity\nThe Cloudstack Packer plugin provides a cloudstack builder that is able to create new templates for use with CloudStack. The builder takes either an ISO or an existing template as its source, runs any provisioning necessary on the instance after launching it and then creates a new template from that instance.\nTo install this plugin, copy and paste this code into your Packer configuration, then run packer init.\npacker { required_plugins { cloudstack = { source = \"github.com/hashicorp/cloudstack\" version = \"~> 1\" } } } \n$ packer plugins install github.com/hashicorp/cloudstack \nThe Cloudstack plugin allows Packer to interface with cloudstack\ncloudstack - Creates new templates for use with CloudStack. The builder takes either an ISO or an existing template as its source, runs any provisioning necessary on the instance after launching it and then creates a new template from that instance."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/digitalocean/digitalocean/v1.3.0/components/post-processor/digitalocean-import",
  "text": "This page does not exist for version v1.3.0."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/digitalocean/digitalocean/v1.3.0/components/builder/digitalocean",
  "text": "DigitalOcean Builder (1.3.0) | Integrations | Packer\nDigitalOcean\n@digitalocean\nThe DigitalOcean plugin can be used with HashiCorp Packer to create custom images for DigitalOcean.\nPartner\nUpdated 7 months ago\nType: digitalocean Artifact BuilderId: pearkes.digitalocean\nThe digitalocean Packer builder is able to create new images for use with DigitalOcean. The builder takes a source image, runs any provisioning necessary on the image after launching it, then snapshots it into a reusable image. This reusable image can then be used as the foundation of new servers that are launched within DigitalOcean.\nThe builder does not manage images. Once it creates an image, it is up to you to use it or delete it.\nThere are many configuration options available for the builder. They are segmented below into two categories: required and optional parameters. Within each category, the available configuration keys are alphabetized.\napi_token (string) - The client TOKEN to use to access your account. It can also be specified via environment variable DIGITALOCEAN_TOKEN, DIGITALOCEAN_ACCESS_TOKEN, or DIGITALOCEAN_API_TOKEN if set. DIGITALOCEAN_API_TOKEN will be deprecated in a future release in favor of DIGITALOCEAN_TOKEN or DIGITALOCEAN_ACCESS_TOKEN.\nregion (string) - The name (or slug) of the region to launch the droplet in. Consequently, this is the region where the snapshot will be available. See https://docs.digitalocean.com/reference/api/api-reference/#operation/list_all_regions for the accepted region names/slugs.\nsize (string) - The name (or slug) of the droplet size to use. See https://docs.digitalocean.com/reference/api/api-reference/#operation/list_all_sizes for the accepted size names/slugs.\nimage (string) - The name (or slug) of the base image to use. This is the image that will be used to launch a new droplet and provision it. See https://docs.digitalocean.com/reference/api/api-reference/#operation/get_images_list for details on how to get a list of the accepted image names/slugs.\napi_url (string) - Non standard api endpoint URL. Set this if you are using a DigitalOcean API compatible service. It can also be specified via environment variable DIGITALOCEAN_API_URL.\nhttp_retry_max (*int) - The maximum number of retries for requests that fail with a 429 or 500-level error. The default value is 5. Set to 0 to disable reties.\nhttp_retry_wait_max (*float64) - The maximum wait time (in seconds) between failed API requests. Default: 30.0\nhttp_retry_wait_min (*float64) - The minimum wait time (in seconds) between failed API requests. Default: 1.0\nprivate_networking (bool) - Set to true to enable private networking for the droplet being created. This defaults to false, or not enabled.\nmonitoring (bool) - Set to true to enable monitoring for the droplet being created. This defaults to false, or not enabled.\ndroplet_agent (*bool) - A boolean indicating whether to install the DigitalOcean agent used for providing access to the Droplet web console in the control panel. By default, the agent is installed on new Droplets but installation errors (i.e. OS not supported) are ignored. To prevent it from being installed, set to false. To make installation errors fatal, explicitly set it to true.\nipv6 (bool) - Set to true to enable ipv6 for the droplet being created. This defaults to false, or not enabled.\nsnapshot_name (string) - The name of the resulting snapshot that will appear in your account. Defaults to packer-{{timestamp}} (see configuration templates for more info).\nsnapshot_regions ([]string) - Additional regions that resulting snapshot should be distributed to.\nwait_snapshot_transfer (*bool) - When true, Packer will block until all snapshot transfers have been completed and report errors. When false, Packer will initiate the snapshot transfers and exit successfully without waiting for completion. Defaults to true.\ntransfer_timeout (duration string | ex: \"1h5m2s\") - How long to wait for a snapshot to be transferred to an additional region before timing out. The default transfer timeout is \"30m\" (valid time units include s for seconds, m for minutes, and h for hours).\nstate_timeout (duration string | ex: \"1h5m2s\") - The time to wait, as a duration string, for a droplet to enter a desired state (such as \"active\") before timing out. The default state timeout is \"6m\".\nsnapshot_timeout (duration string | ex: \"1h5m2s\") - How long to wait for the Droplet snapshot to complete before timing out. The default snapshot timeout is \"60m\" (valid time units include s for seconds, m for minutes, and h for hours).\ndroplet_name (string) - The name assigned to the droplet. DigitalOcean sets the hostname of the machine to this value.\nuser_data (string) - User data to launch with the Droplet. Packer will not automatically wait for a user script to finish before shutting down the instance this must be handled in a provisioner.\nuser_data_file (string) - Path to a file that will be used for the user data when launching the Droplet.\ntags ([]string) - Tags to apply to the droplet when it is created\nvpc_uuid (string) - UUID of the VPC which the droplet will be created in. Before using this, private_networking should be enabled.\nconnect_with_private_ip (bool) - Wheter the communicators should use private IP or not (public IP in that case). If the droplet is or going to be accessible only from the local network because it is at behind a firewall, then communicators should use the private IP instead of the public IP. Before using this, private_networking should be enabled.\nssh_key_id (int) - The ID of an existing SSH key on the DigitalOcean account. This should be used in conjunction with ssh_private_key_file.\nHere is a basic example. It is completely valid as soon as you enter your own access tokens:\nsource \"digitalocean\" \"example\" { api_token = \"YOUR API KEY\" image = \"ubuntu-22-04-x64\" region = \"nyc3\" size = \"s-1vcpu-1gb\" ssh_username = \"root\" } build { sources = [\"source.digitalocean.example\"] } \n{ \"type\": \"digitalocean\", \"api_token\": \"YOUR API KEY\", \"image\": \"ubuntu-22-04-x64\", \"region\": \"nyc3\", \"size\": \"s-1vcpu-1gb\", \"ssh_username\": \"root\" } \nCommunicator Config\nIn addition to the builder options, a communicator can be configured for this builder."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/digitalocean/digitalocean/v1.3.1/components/builder/digitalocean",
  "text": "DigitalOcean Builder (1.3.1) | Integrations | Packer\nDigitalOcean\n@digitalocean\nThe DigitalOcean plugin can be used with HashiCorp Packer to create custom images for DigitalOcean.\nPartner\nUpdated 7 months ago\nType: digitalocean Artifact BuilderId: pearkes.digitalocean\nThe digitalocean Packer builder is able to create new images for use with DigitalOcean. The builder takes a source image, runs any provisioning necessary on the image after launching it, then snapshots it into a reusable image. This reusable image can then be used as the foundation of new servers that are launched within DigitalOcean.\nThe builder does not manage images. Once it creates an image, it is up to you to use it or delete it.\nThere are many configuration options available for the builder. They are segmented below into two categories: required and optional parameters. Within each category, the available configuration keys are alphabetized.\napi_token (string) - The client TOKEN to use to access your account. It can also be specified via environment variable DIGITALOCEAN_TOKEN, DIGITALOCEAN_ACCESS_TOKEN, or DIGITALOCEAN_API_TOKEN if set. DIGITALOCEAN_API_TOKEN will be deprecated in a future release in favor of DIGITALOCEAN_TOKEN or DIGITALOCEAN_ACCESS_TOKEN.\nregion (string) - The name (or slug) of the region to launch the droplet in. Consequently, this is the region where the snapshot will be available. See https://docs.digitalocean.com/reference/api/api-reference/#operation/list_all_regions for the accepted region names/slugs.\nsize (string) - The name (or slug) of the droplet size to use. See https://docs.digitalocean.com/reference/api/api-reference/#operation/list_all_sizes for the accepted size names/slugs.\nimage (string) - The name (or slug) of the base image to use. This is the image that will be used to launch a new droplet and provision it. See https://docs.digitalocean.com/reference/api/api-reference/#operation/get_images_list for details on how to get a list of the accepted image names/slugs.\napi_url (string) - Non standard api endpoint URL. Set this if you are using a DigitalOcean API compatible service. It can also be specified via environment variable DIGITALOCEAN_API_URL.\nhttp_retry_max (*int) - The maximum number of retries for requests that fail with a 429 or 500-level error. The default value is 5. Set to 0 to disable reties.\nhttp_retry_wait_max (*float64) - The maximum wait time (in seconds) between failed API requests. Default: 30.0\nhttp_retry_wait_min (*float64) - The minimum wait time (in seconds) between failed API requests. Default: 1.0\nprivate_networking (bool) - Set to true to enable private networking for the droplet being created. This defaults to false, or not enabled.\nmonitoring (bool) - Set to true to enable monitoring for the droplet being created. This defaults to false, or not enabled.\ndroplet_agent (*bool) - A boolean indicating whether to install the DigitalOcean agent used for providing access to the Droplet web console in the control panel. By default, the agent is installed on new Droplets but installation errors (i.e. OS not supported) are ignored. To prevent it from being installed, set to false. To make installation errors fatal, explicitly set it to true.\nipv6 (bool) - Set to true to enable ipv6 for the droplet being created. This defaults to false, or not enabled.\nsnapshot_name (string) - The name of the resulting snapshot that will appear in your account. Defaults to packer-{{timestamp}} (see configuration templates for more info).\nsnapshot_regions ([]string) - Additional regions that resulting snapshot should be distributed to.\nwait_snapshot_transfer (*bool) - When true, Packer will block until all snapshot transfers have been completed and report errors. When false, Packer will initiate the snapshot transfers and exit successfully without waiting for completion. Defaults to true.\ntransfer_timeout (duration string | ex: \"1h5m2s\") - How long to wait for a snapshot to be transferred to an additional region before timing out. The default transfer timeout is \"30m\" (valid time units include s for seconds, m for minutes, and h for hours).\nstate_timeout (duration string | ex: \"1h5m2s\") - The time to wait, as a duration string, for a droplet to enter a desired state (such as \"active\") before timing out. The default state timeout is \"6m\".\nsnapshot_timeout (duration string | ex: \"1h5m2s\") - How long to wait for the Droplet snapshot to complete before timing out. The default snapshot timeout is \"60m\" (valid time units include s for seconds, m for minutes, and h for hours).\ndroplet_name (string) - The name assigned to the droplet. DigitalOcean sets the hostname of the machine to this value.\nuser_data (string) - User data to launch with the Droplet. Packer will not automatically wait for a user script to finish before shutting down the instance this must be handled in a provisioner.\nuser_data_file (string) - Path to a file that will be used for the user data when launching the Droplet.\ntags ([]string) - Tags to apply to the droplet when it is created\nvpc_uuid (string) - UUID of the VPC which the droplet will be created in. Before using this, private_networking should be enabled.\nconnect_with_private_ip (bool) - Wheter the communicators should use private IP or not (public IP in that case). If the droplet is or going to be accessible only from the local network because it is at behind a firewall, then communicators should use the private IP instead of the public IP. Before using this, private_networking should be enabled.\nssh_key_id (int) - The ID of an existing SSH key on the DigitalOcean account. This should be used in conjunction with ssh_private_key_file.\nHere is a basic example. It is completely valid as soon as you enter your own access tokens:\nsource \"digitalocean\" \"example\" { api_token = \"YOUR API KEY\" image = \"ubuntu-22-04-x64\" region = \"nyc3\" size = \"s-1vcpu-1gb\" ssh_username = \"root\" } build { sources = [\"source.digitalocean.example\"] } \n{ \"type\": \"digitalocean\", \"api_token\": \"YOUR API KEY\", \"image\": \"ubuntu-22-04-x64\", \"region\": \"nyc3\", \"size\": \"s-1vcpu-1gb\", \"ssh_username\": \"root\" } \nCommunicator Config\nIn addition to the builder options, a communicator can be configured for this builder.\nNOTE: Guests using Windows with Win32-OpenSSH v9.1.0.0p1-Beta, scp (the default protocol for copying data) returns a a non-zero error code since the MOTW cannot be set, which cause any file transfer to fail. As a workaround you can override the transfer protocol with SFTP instead ssh_file_transfer_method = \"sftp\"."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/digitalocean/digitalocean/v1.3.0/components/data-source/digitalocean-image",
  "text": "This page does not exist for version v1.3.0."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/digitalocean/digitalocean/v0.0.0/components/data-source/digitalocean-image",
  "text": "This page does not exist for version v0.0.0."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/digitalocean/digitalocean/v0.0.0/components/post-processor/digitalocean-import",
  "text": "This page does not exist for version v0.0.0."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/digitalocean/digitalocean/v1.2.2/components/post-processor/digitalocean-import",
  "text": "This page does not exist for version v1.2.2."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/digitalocean/digitalocean/v1.3.1/components/post-processor/digitalocean-import",
  "text": "DigitalOcean Post-Processor (1.3.1) | Integrations | Packer\n@digitalocean\nThe DigitalOcean plugin can be used with HashiCorp Packer to create custom images for DigitalOcean.\nPartner\nUpdated 7 months ago\nType: digitalocean-import Artifact BuilderId: packer.post-processor.digitalocean-import\nThe Packer DigitalOcean Import post-processor is used to import images created by other Packer builders to DigitalOcean.\nNote: Users looking to create custom images, and reusable snapshots, directly on DigitalOcean can use the DigitalOcean builder without this post-processor.\nHow Does it Work?\nThe import process operates uploading a temporary copy of the image to DigitalOcean Spaces and then importing it as a custom image via the DigialOcean API. The temporary copy in Spaces can be discarded after the import is complete.\nFor information about the requirements to use an image for a DigitalOcean Droplet, see DigitalOcean's Custom Images documentation.\napi_token (string) - A personal access token used to communicate with the DigitalOcean v2 API. This may also be set using the DIGITALOCEAN_TOKEN or DIGITALOCEAN_ACCESS_TOKEN environmental variables.\nspaces_key (string) - The access key used to communicate with Spaces. This may also be set using the DIGITALOCEAN_SPACES_ACCESS_KEY environmental variable.\nspaces_secret (string) - The secret key used to communicate with Spaces. This may also be set using the DIGITALOCEAN_SPACES_SECRET_KEY environmental variable.\nspaces_region (string) - The name of the region, such as nyc3, in which to upload the image to Spaces.\nspace_name (string) - The name of the specific Space where the image file will be copied to for import. This Space must exist when the post-processor is run.\nimage_name (string) - The name to be used for the resulting DigitalOcean custom image.\nimage_regions ([]string) - A list of DigitalOcean regions, such as nyc3, where the resulting image will be available for use in creating Droplets.\napi_token (string) - A personal access token used to communicate with the DigitalOcean v2 API. This may also be set using the DIGITALOCEAN_TOKEN or DIGITALOCEAN_ACCESS_TOKEN environmental variables.\nspaces_key (string) - The access key used to communicate with Spaces. This may also be set using the DIGITALOCEAN_SPACES_ACCESS_KEY environmental variable.\nspaces_secret (string) - The secret key used to communicate with Spaces. This may also be set using the DIGITALOCEAN_SPACES_SECRET_KEY environmental variable.\nspaces_region (string) - The name of the region, such as nyc3, in which to upload the image to Spaces.\nspace_name (string) - The name of the specific Space where the image file will be copied to for import. This Space must exist when the post-processor is run.\nimage_name (string) - The name to be used for the resulting DigitalOcean custom image.\nimage_regions ([]string) - A list of DigitalOcean regions, such as nyc3, where the resulting image will be available for use in creating Droplets.\n{ \"type\": \"digitalocean-import\", \"api_token\": \"{{user `token`}}\", \"spaces_key\": \"{{user `key`}}\", \"spaces_secret\": \"{{user `secret`}}\", \"spaces_region\": \"nyc3\", \"space_name\": \"import-bucket\", \"image_name\": \"ubuntu-18.10-minimal-amd64\", \"image_description\": \"Packer import {{timestamp}}\", \"image_regions\": [\"nyc3\", \"nyc2\"], \"image_tags\": [\"custom\", \"packer\"] } \npost-processor \"digitalocean-import\" { api_token = \"{{user `token`}}\" spaces_key = \"{{user `key`}}\" spaces_secret = \"{{user `secret`}}\" spaces_region = \"nyc3\" space_name = \"import-bucket\" image_name = \"ubuntu-18.10-minimal-amd64\" image_description = \"Packer import {{timestamp}}\" image_regions = [\"nyc3\", \"nyc2\"] image_tags = [\"custom\", \"packer\"] }"
},
{
  "url": "https://developer.hashicorp.com/docs/templates/legacy_json_templates/communicator",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/digitalocean/digitalocean/v1.4.0",
  "text": "DigitalOcean (v1.4.0) | Integrations | Packer\n@digitalocean\nThe DigitalOcean plugin can be used with HashiCorp Packer to create custom images for DigitalOcean.\nPartner\nUpdated 7 months ago\nThe DigitalOcean Packer plugin provides a builder for building images in DigitalOcean, and a post-processor for importing already-existing images into DigitalOcean.\nTo install this plugin, copy and paste this code into your Packer configuration, then run packer init.\npacker { required_plugins { digitalocean = { version = \">= 1.0.4\" source = \"github.com/digitalocean/digitalocean\" } } } \n$ packer plugins install github.com/digitalocean/digitalocean \ndigitalocean - The builder takes a source image, runs any provisioning necessary on the image after launching it, then snapshots it into a reusable image. This reusable image can then be used as the foundation of new servers that are launched within DigitalOcean.\nData Sources\ndigitalocean-image - The DigitalOcean image data source is used look up the ID of an existing DigitalOcean image for use as a builder source.\nPost-processors\ndigitalocean-import - The digitalocean-import post-processor is used to import images to DigitalOcean"
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/digitalocean/digitalocean/v1.2.2/components/builder/digitalocean",
  "text": "DigitalOcean Builder (1.2.2) | Integrations | Packer\nType: digitalocean Artifact BuilderId: pearkes.digitalocean\nThe digitalocean Packer builder is able to create new images for use with DigitalOcean. The builder takes a source image, runs any provisioning necessary on the image after launching it, then snapshots it into a reusable image. This reusable image can then be used as the foundation of new servers that are launched within DigitalOcean.\nThe builder does not manage images. Once it creates an image, it is up to you to use it or delete it.\nThere are many configuration options available for the builder. They are segmented below into two categories: required and optional parameters. Within each category, the available configuration keys are alphabetized.\napi_token (string) - The client TOKEN to use to access your account. It can also be specified via environment variable DIGITALOCEAN_TOKEN, DIGITALOCEAN_ACCESS_TOKEN, or DIGITALOCEAN_API_TOKEN if set. DIGITALOCEAN_API_TOKEN will be deprecated in a future release in favor of DIGITALOCEAN_TOKEN or DIGITALOCEAN_ACCESS_TOKEN.\nregion (string) - The name (or slug) of the region to launch the droplet in. Consequently, this is the region where the snapshot will be available. See https://docs.digitalocean.com/reference/api/api-reference/#operation/list_all_regions for the accepted region names/slugs.\nsize (string) - The name (or slug) of the droplet size to use. See https://docs.digitalocean.com/reference/api/api-reference/#operation/list_all_sizes for the accepted size names/slugs.\nimage (string) - The name (or slug) of the base image to use. This is the image that will be used to launch a new droplet and provision it. See https://docs.digitalocean.com/reference/api/api-reference/#operation/get_images_list for details on how to get a list of the accepted image names/slugs.\napi_url (string) - Non standard api endpoint URL. Set this if you are using a DigitalOcean API compatible service. It can also be specified via environment variable DIGITALOCEAN_API_URL.\nhttp_retry_max (*int) - The maximum number of retries for requests that fail with a 429 or 500-level error. The default value is 5. Set to 0 to disable reties.\nhttp_retry_wait_max (*float64) - The maximum wait time (in seconds) between failed API requests. Default: 30.0\nhttp_retry_wait_min (*float64) - The minimum wait time (in seconds) between failed API requests. Default: 1.0\nprivate_networking (bool) - Set to true to enable private networking for the droplet being created. This defaults to false, or not enabled.\nmonitoring (bool) - Set to true to enable monitoring for the droplet being created. This defaults to false, or not enabled.\ndroplet_agent (*bool) - A boolean indicating whether to install the DigitalOcean agent used for providing access to the Droplet web console in the control panel. By default, the agent is installed on new Droplets but installation errors (i.e. OS not supported) are ignored. To prevent it from being installed, set to false. To make installation errors fatal, explicitly set it to true.\nipv6 (bool) - Set to true to enable ipv6 for the droplet being created. This defaults to false, or not enabled.\nsnapshot_name (string) - The name of the resulting snapshot that will appear in your account. Defaults to packer-{{timestamp}} (see configuration templates for more info).\nsnapshot_regions ([]string) - The regions of the resulting snapshot that will appear in your account.\nstate_timeout (duration string | ex: \"1h5m2s\") - The time to wait, as a duration string, for a droplet to enter a desired state (such as \"active\") before timing out. The default state timeout is \"6m\".\nsnapshot_timeout (duration string | ex: \"1h5m2s\") - How long to wait for an image to be published to the shared image gallery before timing out. If your Packer build is failing on the Publishing to Shared Image Gallery step with the error Original Error: context deadline exceeded, but the image is present when you check your Azure dashboard, then you probably need to increase this timeout from its default of \"60m\" (valid time units include s for seconds, m for minutes, and h for hours.)\ndroplet_name (string) - The name assigned to the droplet. DigitalOcean sets the hostname of the machine to this value.\nuser_data (string) - User data to launch with the Droplet. Packer will not automatically wait for a user script to finish before shutting down the instance this must be handled in a provisioner.\nuser_data_file (string) - Path to a file that will be used for the user data when launching the Droplet.\ntags ([]string) - Tags to apply to the droplet when it is created\nvpc_uuid (string) - UUID of the VPC which the droplet will be created in. Before using this, private_networking should be enabled.\nconnect_with_private_ip (bool) - Wheter the communicators should use private IP or not (public IP in that case). If the droplet is or going to be accessible only from the local network because it is at behind a firewall, then communicators should use the private IP instead of the public IP. Before using this, private_networking should be enabled.\nssh_key_id (int) - The ID of an existing SSH key on the DigitalOcean account. This should be used in conjunction with ssh_private_key_file.\nHere is a basic example. It is completely valid as soon as you enter your own access tokens:\nsource \"digitalocean\" \"example\" { api_token = \"YOUR API KEY\" image = \"ubuntu-16-04-x64\" region = \"nyc3\" size = \"512mb\" ssh_username = \"root\" } build { sources = [\"source.digitalocean.example\"] } \n{ \"type\": \"digitalocean\", \"api_token\": \"YOUR API KEY\", \"image\": \"ubuntu-16-04-x64\", \"region\": \"nyc3\", \"size\": \"512mb\", \"ssh_username\": \"root\" } \nCommunicator Config\nIn addition to the builder options, a communicator can be configured for this builder."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/digitalocean/digitalocean/v1.2.2/components/data-source/digitalocean-image",
  "text": "This page does not exist for version v1.2.2."
},
{
  "url": "https://developer.hashicorp.com/packer/integrations/digitalocean/digitalocean/v1.3.1/components/data-source/digitalocean-image",
  "text": "This page does not exist for version v1.3.1."
},
{
  "url": "https://developer.hashicorp.com/packer/docs/builders/amazon/ebsvolume",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/builders/amazon/ebssurrogate",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/configuration/from-1.5/expressions",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/builders/amazon/instance",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/builders/amazon/ebs",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/builders/vmware-iso",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/builders/amazon-ebs",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/[productSlug]/integrations/[organizationSlug]/[integrationSlug]/builders/arm.mdx",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/builders/amazon-instance",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/builders/amazon-chroot",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/builders/amazon-ebssurrogate",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/builders/alicloud-ecs",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/builders/cloudstack",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/builders/amazon-ebsvolume",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/builders/docker",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/builders/digitalocean",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/builders/hetzner-cloud",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/builders/hyperone",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/builders/googlecompute",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/builders/lxc",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/builders/linode",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/builders/lxd",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/builders/oneandone",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/packer/docs/builders/ncloud",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/endswith",
  "text": "endswith - Functions - Configuration Language | Terraform\nSkip to main content\nSign up\nTheme\nendswith Function\nendswith takes two values: a string to check and a suffix string. The function returns true if the first string ends with that exact suffix.\nExamples\n> endswith(\"hello world\", \"world\") true > endswith(\"hello world\", \"hello\") false \nRelated Functions\nstartswith takes two values: a string to check and a prefix string. The function returns true if the string begins with that exact prefix.\nWe use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/formatlist",
  "text": "formatlist - Functions - Configuration Language | Terraform\nformatlist produces a list of strings by formatting a number of other values according to a specification string.\nformatlist(spec, values...) \nThe specification string uses the same syntax as format.\nThe given values can be a mixture of list and non-list arguments. Any given lists must be the same length, which decides the length of the resulting list.\nThe list arguments are iterated together in order by index, while the non-list arguments are used repeatedly for each iteration. The format string is evaluated once per element of the list arguments.\n> formatlist(\"Hello, %s!\", [\"Valentina\", \"Ander\", \"Olivia\", \"Sam\"]) [ \"Hello, Valentina!\", \"Hello, Ander!\", \"Hello, Olivia!\", \"Hello, Sam!\", ] > formatlist(\"%s, %s!\", \"Salutations\", [\"Valentina\", \"Ander\", \"Olivia\", \"Sam\"]) [ \"Salutations, Valentina!\", \"Salutations, Ander!\", \"Salutations, Olivia!\", \"Salutations, Sam!\", ] \nformat defines the specification syntax used by this function and produces a single string as its result."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/indent",
  "text": "indent - Functions - Configuration Language | Terraform\nindent function reference\nThis topic provides reference information about the indent function. You can use the indent function to add indentation to the beginning of each line, except the first, in a multi-line string.\nThe indent function adds a specified number of spaces to the beginning of each line in a multi-line string, except for the first line. You can use the indent function to help ensure that complex strings are properly formatted, consistent, and readable. The function can be especially useful when you work with YAML, JSON, Kubernetes, or other formats that require complex, structured text.\nUse the indent function with the following syntax:\nindent(num_spaces, string) \nThe first argument is numeric. It specifies the number of spaces you want to add to each line except the first.\nThe second argument is a string. It specifies the multi-line string to which you want to add spaces.\nIn the following example, the indent function adds two spaces to the beginning of each line of the description variable to make it easier to read:\noutput \"formatted_description\" { value = indent(2, var.description) }"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/join",
  "text": "join - Functions - Configuration Language | Terraform\njoin produces a string by concatenating all of the elements of the specified list of strings with the specified separator.\n> join(\"-\", [\"foo\", \"bar\", \"baz\"]) \"foo-bar-baz\" > join(\", \", [\"foo\", \"bar\", \"baz\"]) foo, bar, baz > join(\", \", [\"foo\"]) foo"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/lower",
  "text": "lower - Functions - Configuration Language | Terraform\nlower Function\nlower converts all cased letters in the given string to lowercase.\n> lower(\"HELLO\") hello > lower(\"!\") ! \nThis function uses Unicode's definition of letters and of upper- and lowercase.\nupper converts letters in a string to uppercase.\ntitle converts the first letter of each word in a string to uppercase."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/regex",
  "text": "regex - Functions - Configuration Language | Terraform\nregex applies a regular expression to a string and returns the matching substrings.\nThe return type of regex depends on the capture groups, if any, in the pattern:\nIf the pattern has no capture groups at all, the result is a single string covering the substring matched by the pattern as a whole.\nIf the pattern has one or more unnamed capture groups, the result is a list of the captured substrings in the same order as the definition of the capture groups.\nIf the pattern has one or more named capture groups, the result is a map of the captured substrings, using the capture group names as map keys.\nIt's not valid to mix both named and unnamed capture groups in the same pattern.\nIf the given pattern does not match at all, the regex raises an error. To test whether a given pattern matches a string, use regexall and test that the result has length greater than zero.\nThe pattern is a string containing a mixture of literal characters and special matching operators as described in the following table. Note that when giving a regular expression pattern as a literal quoted string in the Terraform language, the quoted string itself already uses backslash \\ as an escape character for the string, so any backslashes intended to be recognized as part of the pattern must be escaped as \\\\.\nSequenceMatches\n.\tAny character except newline\t\n[xyz]\tAny character listed between the brackets (x, y, and z in this example)\t\n[a-z]\tAny character between a and z, inclusive\t\n[^xyz]\tThe opposite of [xyz]\t\n\\d\tASCII digits (0 through 9, inclusive)\t\n\\D\tAnything except ASCII digits\t\n\\s\tASCII spaces (space, tab, newline, carriage return, form feed)\t\n\\S\tAnything except ASCII spaces\t\n\\w\tThe same as [0-9A-Za-z_]\t\n\\W\tAnything except the characters matched by \\w\t\n[[:alnum:]]\tThe same as [0-9A-Za-z]\t\n[[:alpha:]]\tThe same as [A-Za-z]\t\n[[:ascii:]]\tAny ASCII character\t\n[[:blank:]]\tASCII tab or space\t\n[[:cntrl:]]\tASCII/Unicode control characters\t\n[[:digit:]]\tThe same as [0-9]\t\n[[:graph:]]\tAll \"graphical\" (printable) ASCII characters\t\n[[:lower:]]\tThe same as [a-z]\t\n[[:print:]]\tThe same as [[:graph:]]\t\n[[:punct:]]\tThe same as [!-/:-@[-`{-~]\t\n[[:space:]]\tThe same as [\\t\\n\\v\\f\\r ]\t\n[[:upper:]]\tThe same as [A-Z]\t\n[[:word:]]\tThe same as \\w\t\n[[:xdigit:]]\tThe same as [0-9A-Fa-f]\t\n\\pN\tUnicode character class by using single-letter class names (\"N\" in this example)\t\n\\p{Greek}\tUnicode character class by unicode name (\"Greek\" in this example)\t\n\\PN\tThe opposite of \\pN\t\n\\P{Greek}\tThe opposite of \\p{Greek}\t\nxy\tx followed immediately by y\t\nx&#124;y\teither x or y, preferring x\t\nx*\tzero or more x, preferring more\t\nx*?\tzero or more x, preferring fewer\t\nx+\tone or more x, preferring more\t\nx+?\tone or more x, preferring fewer\t\nx?\tzero or one x, preferring one\t\nx??\tzero or one x, preferring zero\t\nx{n,m}\tbetween n and m repetitions of x, preferring more\t\nx{n,m}?\tbetween n and m repetitions of x, preferring fewer\t\nx{n,}\tat least n repetitions of x, preferring more\t\nx{n,}?\tat least n repetitions of x, preferring fewer\t\nx{n}\texactly n repetitions of x\t\n(x)\tunnamed capture group for sub-pattern x\t\n(?P<name>x)\tnamed capture group, named name, for sub-pattern x\t\n(?:x)\tnon-capturing sub-pattern x\t\n\\*\tLiteral * for any punctuation character *\t\n\\Q...\\E\tLiteral ... for any text ... as long as it does not include literally \\E\t\nIn addition to the above matching operators that consume the characters they match, there are some additional operators that only match, but consume no characters. These are \"zero-width\" matching operators:\nSequenceMatches\n^\tAt the beginning of the given string\t\n$\tAt the end of the given string\t\n\\A\tAt the beginning of the given string\t\n\\z\tAt the end of the given string\t\n\\b\tAt an ASCII word boundary (transition between \\w and either \\W, \\A or \\z, or vice-versa)\t\n\\B\tNot at an ASCII word boundary\t\nTerraform uses the RE2 regular expression language. This engine does not support all of the features found in some other regular expression engines; in particular, it does not support backreferences.\nSome of the matching behaviors described above can be modified by setting matching flags, activated using either the (?flags) operator (to activate within the current sub-pattern) or the (?flags:x) operator (to match x with the modified flags). Each flag is a single letter, and multiple flags can be set at once by listing multiple letters in the flags position. The available flags are listed in the table below:\nFlagMeaning\ni\tCase insensitive: a literal letter in the pattern matches both lowercase and uppercase versions of that letter\t\nm\tThe ^ and $ operators also match the beginning and end of lines within the string, marked by newline characters; behavior of \\A and \\z is unchanged\t\ns\tThe . operator also matches newline\t\nU\tThe meaning of presence or absense ? after a repetition operator is inverted. For example, x* is interpreted like x*? and vice-versa.\t\n> regex(\"[a-z]+\", \"53453453.345345aaabbbccc23454\") aaabbbccc > regex(\"(\\\\d\\\\d\\\\d\\\\d)-(\\\\d\\\\d)-(\\\\d\\\\d)\", \"2019-02-01\") [ \"2019\", \"02\", \"01\", ] > regex(\"^(?:(?P<scheme>[^:/?#]+):)?(?://(?P<authority>[^/?#]*))?\", \"https://terraform.io/docs/\") { \"authority\" = \"terraform.io\" \"scheme\" = \"https\" } > regex(\"[a-z]+\", \"53453453.34534523454\") Error: Error in function call Call to function \"regex\" failed: pattern did not match any part of the given string. \nregexall searches for potentially multiple matches of a given pattern in a string.\nreplace replaces a substring of a string with another string, optionally matching using the same regular expression syntax as regex.\nIf Terraform already has a more specialized function to parse the syntax you are trying to match, prefer to use that function instead. Regular expressions can be hard to read and can obscure your intent, making a configuration harder to read and understand."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/regexall",
  "text": "regexall - Functions - Configuration Language | Terraform\nregexall applies a regular expression to a string and returns a list of all matches.\nregexall(pattern, string) \nregexall is a variant of regex and uses the same pattern syntax. For any given input to regex, regexall returns a list of whatever type regex would've returned, with one element per match. That is:\nIf the pattern has no capture groups at all, the result is a list of strings.\nIf the pattern has one or more unnamed capture groups, the result is a list of lists.\nIf the pattern has one or more named capture groups, the result is a list of maps.\nregexall can also be used to test whether a particular string matches a given pattern, by testing whether the length of the resulting list of matches is greater than zero.\n> regexall(\"[a-z]+\", \"1234abcd5678efgh9\") [ \"abcd\", \"efgh\", ] > length(regexall(\"[a-z]+\", \"1234abcd5678efgh9\")) 2 > length(regexall(\"[a-z]+\", \"123456789\")) > 0 false \nregex searches for a single match of a given pattern, and returns an error if no match is found.\nIf Terraform already has a more specialized function to parse the syntax you are trying to match, prefer to use that function instead. Regular expressions can be hard to read and can obscure your intent, making a configuration harder to read and understand."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/replace",
  "text": "replace - Functions - Configuration Language | Terraform\nreplace Function\nreplace searches a given string for another given substring, and replaces each occurrence with a given replacement string.\nreplace(string, substring, replacement) \nIf substring is wrapped in forward slashes, it is treated as a regular expression, using the same pattern syntax as regex. If using a regular expression for the substring argument, the replacement string can incorporate captured strings from the input by using an $n sequence, where n is the index or name of a capture group.\n> replace(\"1 + 2 + 3\", \"+\", \"-\") 1 - 2 - 3 > replace(\"hello world\", \"/w.*d/\", \"everybody\") hello everybody \nregex searches a given string for a substring matching a given regular expression pattern."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/split",
  "text": "split - Functions - Configuration Language | Terraform\nsplit produces a list by dividing a given string at all occurrences of a given separator.\n> split(\",\", \"foo,bar,baz\") [ \"foo\", \"bar\", \"baz\", ] > split(\",\", \"foo\") [ \"foo\", ] > split(\",\", \"\") [ \"\", ]"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/startswith",
  "text": "startswith - Functions - Configuration Language | Terraform\nstartswith Function\nstartswith takes two values: a string to check and a prefix string. The function returns true if the string begins with that exact prefix.\nstartswith(string, prefix) \n> startswith(\"hello world\", \"hello\") true > startswith(\"hello world\", \"world\") false \nendswith takes two values: a string to check and a suffix string. The function returns true if the first string ends with that exact suffix."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/strcontains",
  "text": "strcontains - Functions - Configuration Language | Terraform\nstrcontains Function\nstrcontains function checks whether a substring is within another string.\nstrcontains(string, substr) \n> strcontains(\"hello world\", \"wor\") true > strcontains(\"hello world\", \"wod\") false"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/strrev",
  "text": "strrev - Functions - Configuration Language | Terraform\nstrrev reverses the characters in a string. Note that the characters are treated as Unicode characters (in technical terms, Unicode grapheme cluster boundaries are respected).\nExamples\n> strrev(\"hello\") olleh > strrev(\"a \")  a \nRelated Functions\nreverse reverses a sequence."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/title",
  "text": "title - Functions - Configuration Language | Terraform\ntitle Function\ntitle converts the first letter of each word in the given string to uppercase.\n> title(\"hello world\") Hello World \nThis function uses Unicode's definition of letters and of upper- and lowercase.\nupper converts all letters in a string to uppercase.\nlower converts all letters in a string to lowercase."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/trim",
  "text": "trim - Functions - Configuration Language | Terraform\ntrim Function\ntrim removes the specified set of characters from the start and end of the given string.\ntrim(string, str_character_set) \nEvery occurrence of a character in the second argument is removed from the start and end of the string specified in the first argument.\n> trim(\"?!hello?!\", \"!?\") \"hello\" > trim(\"foobar\", \"far\") \"oob\" > trim(\" hello! world.! \", \"! \") \"hello! world.\" \ntrimprefix removes a word from the start of a string.\ntrimsuffix removes a word from the end of a string.\ntrimspace removes all types of whitespace from both the start and the end of a string."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/trimprefix",
  "text": "trimprefix - Functions - Configuration Language | Terraform\ntrimprefix Function\ntrimprefix removes the specified prefix from the start of the given string. If the string does not start with the prefix, the string is returned unchanged.\nExamples\n> trimprefix(\"helloworld\", \"hello\") world > trimprefix(\"helloworld\", \"cat\") helloworld \nRelated Functions\ntrim removes characters at the start and end of a string.\ntrimsuffix removes a word from the end of a string.\ntrimspace removes all types of whitespace from both the start and the end of a string."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/trimsuffix",
  "text": "trimsuffix - Functions - Configuration Language | Terraform\nSign up\nTheme\ntrimsuffix Function\ntrimsuffix removes the specified suffix from the end of the given string.\nExamples\n> trimsuffix(\"helloworld\", \"world\") hello \nRelated Functions\ntrim removes characters at the start and end of a string.\ntrimprefix removes a word from the start of a string.\ntrimspace removes all types of whitespace from both the start and the end of a string."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/trimspace",
  "text": "trimspace - Functions - Configuration Language | Terraform\ntrimspace removes any space characters from the start and end of the given string.\nThis function follows the Unicode definition of \"space\", which includes regular spaces, tabs, newline characters, and various other space-like characters.\n> trimspace(\" hello\\n\\n\") hello \nchomp removes just line ending characters from the end of a string."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/chomp",
  "text": "chomp - Functions - Configuration Language | Terraform\nchomp removes newline characters at the end of a string.\nThis can be useful if, for example, the string was read from a file that has a newline character at the end.\n> chomp(\"hello\\n\") hello > chomp(\"hello\\r\\n\") hello > chomp(\"hello\\n\\n\") hello \ntrimspace, which removes all types of whitespace from both the start and the end of a string."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.10.x/functions/upper",
  "text": "upper - Functions - Configuration Language | Terraform\nupper Function\nupper converts all cased letters in the given string to uppercase.\n> upper(\"hello\") HELLO > upper(\"!\") ! \nThis function uses Unicode's definition of letters and of upper- and lowercase.\nlower converts letters in a string to lowercase.\ntitle converts the first letter of each word in a string to uppercase."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/endswith",
  "text": "endswith - Functions - Configuration Language | Terraform\nendswith Function\nendswith takes two values: a string to check and a suffix string. The function returns true if the first string ends with that exact suffix.\n> endswith(\"hello world\", \"world\") true > endswith(\"hello world\", \"hello\") false \nstartswith takes two values: a string to check and a prefix string. The function returns true if the string begins with that exact prefix."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/formatlist",
  "text": "formatlist - Functions - Configuration Language | Terraform\nformatlist produces a list of strings by formatting a number of other values according to a specification string.\nformatlist(spec, values...) \nThe specification string uses the same syntax as format.\nThe given values can be a mixture of list and non-list arguments. Any given lists must be the same length, which decides the length of the resulting list.\nThe list arguments are iterated together in order by index, while the non-list arguments are used repeatedly for each iteration. The format string is evaluated once per element of the list arguments.\n> formatlist(\"Hello, %s!\", [\"Valentina\", \"Ander\", \"Olivia\", \"Sam\"]) [ \"Hello, Valentina!\", \"Hello, Ander!\", \"Hello, Olivia!\", \"Hello, Sam!\", ] > formatlist(\"%s, %s!\", \"Salutations\", [\"Valentina\", \"Ander\", \"Olivia\", \"Sam\"]) [ \"Salutations, Valentina!\", \"Salutations, Ander!\", \"Salutations, Olivia!\", \"Salutations, Sam!\", ] \nformat defines the specification syntax used by this function and produces a single string as its result."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/indent",
  "text": "indent - Functions - Configuration Language | Terraform\nindent Function\nindent adds a given number of spaces to the beginnings of all but the first line in a given multi-line string.\nindent(num_spaces, string) \nThis function is useful for inserting a multi-line string into an already-indented context in another string:\n> \" items: ${indent(2, \"[\\n foo,\\n bar,\\n]\\n\")}\" items: [ foo, bar, ] \nThe first line of the string is not indented so that, as above, it can be placed after an introduction sequence that has already begun the line."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/join",
  "text": "join - Functions - Configuration Language | Terraform\njoin produces a string by concatenating all of the elements of the specified list of strings with the specified separator.\n> join(\"-\", [\"foo\", \"bar\", \"baz\"]) \"foo-bar-baz\" > join(\", \", [\"foo\", \"bar\", \"baz\"]) foo, bar, baz > join(\", \", [\"foo\"]) foo"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/lower",
  "text": "lower - Functions - Configuration Language | Terraform\nlower Function\nlower converts all cased letters in the given string to lowercase.\n> lower(\"HELLO\") hello > lower(\"!\") ! \nThis function uses Unicode's definition of letters and of upper- and lowercase.\nupper converts letters in a string to uppercase.\ntitle converts the first letter of each word in a string to uppercase."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/regexall",
  "text": "regexall - Functions - Configuration Language | Terraform\nregexall applies a regular expression to a string and returns a list of all matches.\nregexall(pattern, string) \nregexall is a variant of regex and uses the same pattern syntax. For any given input to regex, regexall returns a list of whatever type regex would've returned, with one element per match. That is:\nIf the pattern has no capture groups at all, the result is a list of strings.\nIf the pattern has one or more unnamed capture groups, the result is a list of lists.\nIf the pattern has one or more named capture groups, the result is a list of maps.\nregexall can also be used to test whether a particular string matches a given pattern, by testing whether the length of the resulting list of matches is greater than zero.\n> regexall(\"[a-z]+\", \"1234abcd5678efgh9\") [ \"abcd\", \"efgh\", ] > length(regexall(\"[a-z]+\", \"1234abcd5678efgh9\")) 2 > length(regexall(\"[a-z]+\", \"123456789\")) > 0 false \nregex searches for a single match of a given pattern, and returns an error if no match is found.\nIf Terraform already has a more specialized function to parse the syntax you are trying to match, prefer to use that function instead. Regular expressions can be hard to read and can obscure your intent, making a configuration harder to read and understand."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/regex",
  "text": "regex - Functions - Configuration Language | Terraform\nregex applies a regular expression to a string and returns the matching substrings.\nThe return type of regex depends on the capture groups, if any, in the pattern:\nIf the pattern has no capture groups at all, the result is a single string covering the substring matched by the pattern as a whole.\nIf the pattern has one or more unnamed capture groups, the result is a list of the captured substrings in the same order as the definition of the capture groups.\nIf the pattern has one or more named capture groups, the result is a map of the captured substrings, using the capture group names as map keys.\nIt's not valid to mix both named and unnamed capture groups in the same pattern.\nIf the given pattern does not match at all, the regex raises an error. To test whether a given pattern matches a string, use regexall and test that the result has length greater than zero.\nThe pattern is a string containing a mixture of literal characters and special matching operators as described in the following table. Note that when giving a regular expression pattern as a literal quoted string in the Terraform language, the quoted string itself already uses backslash \\ as an escape character for the string, so any backslashes intended to be recognized as part of the pattern must be escaped as \\\\.\nSequenceMatches\n.\tAny character except newline\t\n[xyz]\tAny character listed between the brackets (x, y, and z in this example)\t\n[a-z]\tAny character between a and z, inclusive\t\n[^xyz]\tThe opposite of [xyz]\t\n\\d\tASCII digits (0 through 9, inclusive)\t\n\\D\tAnything except ASCII digits\t\n\\s\tASCII spaces (space, tab, newline, carriage return, form feed)\t\n\\S\tAnything except ASCII spaces\t\n\\w\tThe same as [0-9A-Za-z_]\t\n\\W\tAnything except the characters matched by \\w\t\n[[:alnum:]]\tThe same as [0-9A-Za-z]\t\n[[:alpha:]]\tThe same as [A-Za-z]\t\n[[:ascii:]]\tAny ASCII character\t\n[[:blank:]]\tASCII tab or space\t\n[[:cntrl:]]\tASCII/Unicode control characters\t\n[[:digit:]]\tThe same as [0-9]\t\n[[:graph:]]\tAll \"graphical\" (printable) ASCII characters\t\n[[:lower:]]\tThe same as [a-z]\t\n[[:print:]]\tThe same as [[:graph:]]\t\n[[:punct:]]\tThe same as [!-/:-@[-`{-~]\t\n[[:space:]]\tThe same as [\\t\\n\\v\\f\\r ]\t\n[[:upper:]]\tThe same as [A-Z]\t\n[[:word:]]\tThe same as \\w\t\n[[:xdigit:]]\tThe same as [0-9A-Fa-f]\t\n\\pN\tUnicode character class by using single-letter class names (\"N\" in this example)\t\n\\p{Greek}\tUnicode character class by unicode name (\"Greek\" in this example)\t\n\\PN\tThe opposite of \\pN\t\n\\P{Greek}\tThe opposite of \\p{Greek}\t\nxy\tx followed immediately by y\t\nx&#124;y\teither x or y, preferring x\t\nx*\tzero or more x, preferring more\t\nx*?\tzero or more x, preferring fewer\t\nx+\tone or more x, preferring more\t\nx+?\tone or more x, preferring fewer\t\nx?\tzero or one x, preferring one\t\nx??\tzero or one x, preferring zero\t\nx{n,m}\tbetween n and m repetitions of x, preferring more\t\nx{n,m}?\tbetween n and m repetitions of x, preferring fewer\t\nx{n,}\tat least n repetitions of x, preferring more\t\nx{n,}?\tat least n repetitions of x, preferring fewer\t\nx{n}\texactly n repetitions of x\t\n(x)\tunnamed capture group for sub-pattern x\t\n(?P<name>x)\tnamed capture group, named name, for sub-pattern x\t\n(?:x)\tnon-capturing sub-pattern x\t\n\\*\tLiteral * for any punctuation character *\t\n\\Q...\\E\tLiteral ... for any text ... as long as it does not include literally \\E\t\nIn addition to the above matching operators that consume the characters they match, there are some additional operators that only match, but consume no characters. These are \"zero-width\" matching operators:\nSequenceMatches\n^\tAt the beginning of the given string\t\n$\tAt the end of the given string\t\n\\A\tAt the beginning of the given string\t\n\\z\tAt the end of the given string\t\n\\b\tAt an ASCII word boundary (transition between \\w and either \\W, \\A or \\z, or vice-versa)\t\n\\B\tNot at an ASCII word boundary\t\nTerraform uses the RE2 regular expression language. This engine does not support all of the features found in some other regular expression engines; in particular, it does not support backreferences.\nSome of the matching behaviors described above can be modified by setting matching flags, activated using either the (?flags) operator (to activate within the current sub-pattern) or the (?flags:x) operator (to match x with the modified flags). Each flag is a single letter, and multiple flags can be set at once by listing multiple letters in the flags position. The available flags are listed in the table below:\nFlagMeaning\ni\tCase insensitive: a literal letter in the pattern matches both lowercase and uppercase versions of that letter\t\nm\tThe ^ and $ operators also match the beginning and end of lines within the string, marked by newline characters; behavior of \\A and \\z is unchanged\t\ns\tThe . operator also matches newline\t\nU\tThe meaning of presence or absense ? after a repetition operator is inverted. For example, x* is interpreted like x*? and vice-versa.\t\n> regex(\"[a-z]+\", \"53453453.345345aaabbbccc23454\") aaabbbccc > regex(\"(\\\\d\\\\d\\\\d\\\\d)-(\\\\d\\\\d)-(\\\\d\\\\d)\", \"2019-02-01\") [ \"2019\", \"02\", \"01\", ] > regex(\"^(?:(?P<scheme>[^:/?#]+):)?(?://(?P<authority>[^/?#]*))?\", \"https://terraform.io/docs/\") { \"authority\" = \"terraform.io\" \"scheme\" = \"https\" } > regex(\"[a-z]+\", \"53453453.34534523454\") Error: Error in function call Call to function \"regex\" failed: pattern did not match any part of the given string. \nregexall searches for potentially multiple matches of a given pattern in a string.\nreplace replaces a substring of a string with another string, optionally matching using the same regular expression syntax as regex.\nIf Terraform already has a more specialized function to parse the syntax you are trying to match, prefer to use that function instead. Regular expressions can be hard to read and can obscure your intent, making a configuration harder to read and understand."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/replace",
  "text": "replace - Functions - Configuration Language | Terraform\nreplace Function\nreplace searches a given string for another given substring, and replaces each occurrence with a given replacement string.\nreplace(string, substring, replacement) \nIf substring is wrapped in forward slashes, it is treated as a regular expression, using the same pattern syntax as regex. If using a regular expression for the substring argument, the replacement string can incorporate captured strings from the input by using an $n sequence, where n is the index or name of a capture group.\n> replace(\"1 + 2 + 3\", \"+\", \"-\") 1 - 2 - 3 > replace(\"hello world\", \"/w.*d/\", \"everybody\") hello everybody \nregex searches a given string for a substring matching a given regular expression pattern."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/startswith",
  "text": "startswith - Functions - Configuration Language | Terraform\nstartswith Function\nstartswith takes two values: a string to check and a prefix string. The function returns true if the string begins with that exact prefix.\nstartswith(string, prefix) \n> startswith(\"hello world\", \"hello\") true > startswith(\"hello world\", \"world\") false \nendswith takes two values: a string to check and a suffix string. The function returns true if the first string ends with that exact suffix."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/split",
  "text": "split - Functions - Configuration Language | Terraform\nsplit produces a list by dividing a given string at all occurrences of a given separator.\n> split(\",\", \"foo,bar,baz\") [ \"foo\", \"bar\", \"baz\", ] > split(\",\", \"foo\") [ \"foo\", ] > split(\",\", \"\") [ \"\", ]"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/strcontains",
  "text": "strcontains - Functions - Configuration Language | Terraform\nstrcontains Function\nstrcontains function checks whether a substring is within another string.\nstrcontains(string, substr) \n> strcontains(\"hello world\", \"wor\") true > strcontains(\"hello world\", \"wod\") false"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/strrev",
  "text": "strrev - Functions - Configuration Language | Terraform\nstrrev reverses the characters in a string. Note that the characters are treated as Unicode characters (in technical terms, Unicode grapheme cluster boundaries are respected).\n> strrev(\"hello\") olleh > strrev(\"a \")  a \nreverse reverses a sequence."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/title",
  "text": "title - Functions - Configuration Language | Terraform\ntitle Function\ntitle converts the first letter of each word in the given string to uppercase.\n> title(\"hello world\") Hello World \nupper converts all letters in a string to uppercase.\nlower converts all letters in a string to lowercase."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/trim",
  "text": "trim - Functions - Configuration Language | Terraform\ntrim Function\ntrim removes the specified set of characters from the start and end of the given string.\ntrim(string, str_character_set) \nEvery occurrence of a character in the second argument is removed from the start and end of the string specified in the first argument.\n> trim(\"?!hello?!\", \"!?\") \"hello\" > trim(\"foobar\", \"far\") \"oob\" > trim(\" hello! world.! \", \"! \") \"hello! world.\" \ntrimprefix removes a word from the start of a string.\ntrimsuffix removes a word from the end of a string.\ntrimspace removes all types of whitespace from both the start and the end of a string."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/trimprefix",
  "text": "trimprefix - Functions - Configuration Language | Terraform\ntrimprefix Function\ntrimprefix removes the specified prefix from the start of the given string. If the string does not start with the prefix, the string is returned unchanged.\n> trimprefix(\"helloworld\", \"hello\") world > trimprefix(\"helloworld\", \"cat\") helloworld \ntrim removes characters at the start and end of a string.\ntrimsuffix removes a word from the end of a string."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/trimsuffix",
  "text": "trimsuffix - Functions - Configuration Language | Terraform\nSign up\nTheme\ntrimsuffix Function\ntrimsuffix removes the specified suffix from the end of the given string.\n> trimsuffix(\"helloworld\", \"world\") hello \ntrim removes characters at the start and end of a string.\ntrimprefix removes a word from the start of a string."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/upper",
  "text": "upper - Functions - Configuration Language | Terraform\nupper Function\nupper converts all cased letters in the given string to uppercase.\n> upper(\"hello\") HELLO > upper(\"!\") ! \nlower converts letters in a string to lowercase.\ntitle converts the first letter of each word in a string to uppercase."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/trimspace",
  "text": "trimspace - Functions - Configuration Language | Terraform\ntrimspace removes any space characters from the start and end of the given string.\nThis function follows the Unicode definition of \"space\", which includes regular spaces, tabs, newline characters, and various other space-like characters.\n> trimspace(\" hello\\n\\n\") hello \nchomp removes just line ending characters from the end of a string."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.7.x/functions/formatdate",
  "text": "formatdate - Functions - Configuration Language | Terraform\nformatdate converts a timestamp into a different time format.\nformatdate(spec, timestamp) \nIn the Terraform language, timestamps are conventionally represented as strings using RFC 3339 \"Date and Time format\" syntax. formatdate requires the timestamp argument to be a string conforming to this syntax.\n> formatdate(\"DD MMM YYYY hh:mm ZZZ\", \"2018-01-02T23:12:01Z\") 02 Jan 2018 23:12 UTC > formatdate(\"EEEE, DD-MMM-YY hh:mm:ss ZZZ\", \"2018-01-02T23:12:01Z\") Tuesday, 02-Jan-18 23:12:01 UTC > formatdate(\"EEE, DD MMM YYYY hh:mm:ss ZZZ\", \"2018-01-02T23:12:01-08:00\") Tue, 02 Jan 2018 23:12:01 -0800 > formatdate(\"MMM DD, YYYY\", \"2018-01-02T23:12:01Z\") Jan 02, 2018 > formatdate(\"HH:mmaa\", \"2018-01-02T23:12:01Z\") 11:12pm \nThe format specification is a string that includes formatting sequences from the following table. This function is intended for producing common machine-oriented timestamp formats such as those defined in RFC822, RFC850, and RFC1123. It is not suitable for truly human-oriented date formatting because it is not locale-aware. In particular, it can produce month and day names only in English.\nThe specification may contain the following sequences:\nSequenceResult\nYYYY\tFour (or more) digit year, like \"2006\".\t\nYY\tThe year modulo 100, zero padded to at least two digits, like \"06\".\t\nMMMM\tEnglish month name unabbreviated, like \"January\".\t\nMMM\tEnglish month name abbreviated to three letters, like \"Jan\".\t\nMM\tMonth number zero-padded to two digits, like \"01\" for January.\t\nM\tMonth number with no padding, like \"1\" for January.\t\nDD\tDay of month number zero-padded to two digits, like \"02\".\t\nD\tDay of month number with no padding, like \"2\".\t\nEEEE\tEnglish day of week name unabbreviated, like \"Monday\".\t\nEEE\tEnglish day of week name abbreviated to three letters, like \"Mon\".\t\nhh\t24-hour number zero-padded to two digits, like \"02\".\t\nh\t24-hour number unpadded, like \"2\".\t\nHH\t12-hour number zero-padded to two digits, like \"02\".\t\nH\t12-hour number unpadded, like \"2\".\t\nAA\tHour AM/PM marker in uppercase, like \"AM\".\t\naa\tHour AM/PM marker in lowercase, like \"am\".\t\nmm\tMinute within hour zero-padded to two digits, like \"05\".\t\nm\tMinute within hour unpadded, like \"5\".\t\nss\tSecond within minute zero-padded to two digits, like \"09\".\t\ns\tSecond within minute, like \"9\".\t\nZZZZZ\tTimezone offset with colon separating hours and minutes, like \"-08:00\".\t\nZZZZ\tTimezone offset with just sign and digit, like \"-0800\".\t\nZZZ\tLike ZZZZ but with a special case \"UTC\" for UTC.\t\nZ\tLike ZZZZZ but with a special case \"Z\" for UTC.\t\nAny non-letter characters, such as punctuation, are reproduced verbatim in the output. To include literal letters in the format string, enclose them in single quotes '. To include a literal quote, escape it by doubling the quotes.\n> formatdate(\"h'h'mm\", \"2018-01-02T23:12:01-08:00\") 23h12 > formatdate(\"H 'o''clock'\", \"2018-01-02T23:12:01-08:00\") 11 o'clock \nThis format specification syntax is intended to make it easy for a reader to guess which format will result even if they are not experts on the syntax. Therefore there are no predefined shorthands for common formats, but format strings for various RFC-specified formats are given below to be copied into your configuration as needed:\nRFC 822 and RFC RFC 2822: \"DD MMM YYYY hh:mm ZZZ\"\nRFC 850: \"EEEE, DD-MMM-YY hh:mm:ss ZZZ\"\nRFC 1123: \"EEE, DD MMM YYYY hh:mm:ss ZZZ\"\nRFC 3339: \"YYYY-MM-DD'T'hh:mm:ssZ\" (but this is also the input format, so such a conversion is redundant.)\nformat is a more general formatting function for arbitrary data.\ntimestamp returns the current date and time in a format suitable for input to formatdate."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/chomp",
  "text": "chomp - Functions - Configuration Language | Terraform\nchomp removes newline characters at the end of a string.\nThis can be useful if, for example, the string was read from a file that has a newline character at the end.\n> chomp(\"hello\\n\") hello > chomp(\"hello\\r\\n\") hello > chomp(\"hello\\n\\n\") hello \ntrimspace, which removes all types of whitespace from both the start and the end of a string."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/endswith",
  "text": "endswith - Functions - Configuration Language | Terraform\nendswith Function\n> endswith(\"hello world\", \"world\") true > endswith(\"hello world\", \"hello\") false "
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/formatlist",
  "text": "formatlist - Functions - Configuration Language | Terraform\nformatlist produces a list of strings by formatting a number of other values according to a specification string.\nformatlist(spec, values...) \nThe specification string uses the same syntax as format.\nThe given values can be a mixture of list and non-list arguments. Any given lists must be the same length, which decides the length of the resulting list.\nThe list arguments are iterated together in order by index, while the non-list arguments are used repeatedly for each iteration. The format string is evaluated once per element of the list arguments.\n> formatlist(\"Hello, %s!\", [\"Valentina\", \"Ander\", \"Olivia\", \"Sam\"]) [ \"Hello, Valentina!\", \"Hello, Ander!\", \"Hello, Olivia!\", \"Hello, Sam!\", ] > formatlist(\"%s, %s!\", \"Salutations\", [\"Valentina\", \"Ander\", \"Olivia\", \"Sam\"]) [ \"Salutations, Valentina!\", \"Salutations, Ander!\", \"Salutations, Olivia!\", \"Salutations, Sam!\", ] \nformat defines the specification syntax used by this function and produces a single string as its result."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/indent",
  "text": "indent - Functions - Configuration Language | Terraform\nindent Function\nindent adds a given number of spaces to the beginnings of all but the first line in a given multi-line string.\nindent(num_spaces, string) \nThis function is useful for inserting a multi-line string into an already-indented context in another string:\n> \" items: ${indent(2, \"[\\n foo,\\n bar,\\n]\\n\")}\" items: [ foo, bar, ] \nThe first line of the string is not indented so that, as above, it can be placed after an introduction sequence that has already begun the line."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/join",
  "text": "join - Functions - Configuration Language | Terraform\njoin produces a string by concatenating all of the elements of the specified list of strings with the specified separator.\n> join(\"-\", [\"foo\", \"bar\", \"baz\"]) \"foo-bar-baz\" > join(\", \", [\"foo\", \"bar\", \"baz\"]) foo, bar, baz > join(\", \", [\"foo\"]) foo"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/lower",
  "text": "lower - Functions - Configuration Language | Terraform\nlower Function\nlower converts all cased letters in the given string to lowercase.\n> lower(\"HELLO\") hello > lower(\"!\") ! \nupper converts letters in a string to uppercase."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/regex",
  "text": "regex - Functions - Configuration Language | Terraform\nregex applies a regular expression to a string and returns the matching substrings.\nThe return type of regex depends on the capture groups, if any, in the pattern:\nIf the pattern has no capture groups at all, the result is a single string covering the substring matched by the pattern as a whole.\nIf the pattern has one or more unnamed capture groups, the result is a list of the captured substrings in the same order as the definition of the capture groups.\nIf the pattern has one or more named capture groups, the result is a map of the captured substrings, using the capture group names as map keys.\nIt's not valid to mix both named and unnamed capture groups in the same pattern.\nIf the given pattern does not match at all, the regex raises an error. To test whether a given pattern matches a string, use regexall and test that the result has length greater than zero.\nThe pattern is a string containing a mixture of literal characters and special matching operators as described in the following table. Note that when giving a regular expression pattern as a literal quoted string in the Terraform language, the quoted string itself already uses backslash \\ as an escape character for the string, so any backslashes intended to be recognized as part of the pattern must be escaped as \\\\.\n.\tAny character except newline\t\n[xyz]\tAny character listed between the brackets (x, y, and z in this example)\t\n[a-z]\tAny character between a and z, inclusive\t\n[^xyz]\tThe opposite of [xyz]\t\n\\d\tASCII digits (0 through 9, inclusive)\t\n\\D\tAnything except ASCII digits\t\n\\s\tASCII spaces (space, tab, newline, carriage return, form feed)\t\n\\S\tAnything except ASCII spaces\t\n\\w\tThe same as [0-9A-Za-z_]\t\n\\W\tAnything except the characters matched by \\w\t\n[[:alnum:]]\tThe same as [0-9A-Za-z]\t\n[[:alpha:]]\tThe same as [A-Za-z]\t\n[[:ascii:]]\tAny ASCII character\t\n[[:blank:]]\tASCII tab or space\t\n[[:cntrl:]]\tASCII/Unicode control characters\t\n[[:digit:]]\tThe same as [0-9]\t\n[[:graph:]]\tAll \"graphical\" (printable) ASCII characters\t\n[[:lower:]]\tThe same as [a-z]\t\n[[:print:]]\tThe same as [[:graph:]]\t\n[[:punct:]]\tThe same as [!-/:-@[-`{-~]\t\n[[:space:]]\tThe same as [\\t\\n\\v\\f\\r ]\t\n[[:upper:]]\tThe same as [A-Z]\t\n[[:word:]]\tThe same as \\w\t\n[[:xdigit:]]\tThe same as [0-9A-Fa-f]\t\n\\pN\tUnicode character class by using single-letter class names (\"N\" in this example)\t\n\\p{Greek}\tUnicode character class by unicode name (\"Greek\" in this example)\t\n\\PN\tThe opposite of \\pN\t\n\\P{Greek}\tThe opposite of \\p{Greek}\t\nxy\tx followed immediately by y\t\nx&#124;y\teither x or y, preferring x\t\nx*\tzero or more x, preferring more\t\nx*?\tzero or more x, preferring fewer\t\nx+\tone or more x, preferring more\t\nx+?\tone or more x, preferring fewer\t\nx?\tzero or one x, preferring one\t\nx??\tzero or one x, preferring zero\t\nx{n,m}\tbetween n and m repetitions of x, preferring more\t\nx{n,m}?\tbetween n and m repetitions of x, preferring fewer\t\nx{n,}\tat least n repetitions of x, preferring more\t\nx{n,}?\tat least n repetitions of x, preferring fewer\t\nx{n}\texactly n repetitions of x\t\n(x)\tunnamed capture group for sub-pattern x\t\n(?P<name>x)\tnamed capture group, named name, for sub-pattern x\t\n(?:x)\tnon-capturing sub-pattern x\t\n\\*\tLiteral * for any punctuation character *\t\n\\Q...\\E\tLiteral ... for any text ... as long as it does not include literally \\E\t\nIn addition to the above matching operators that consume the characters they match, there are some additional operators that only match, but consume no characters. These are \"zero-width\" matching operators:\n^\tAt the beginning of the given string\t\n$\tAt the end of the given string\t\n\\A\tAt the beginning of the given string\t\n\\z\tAt the end of the given string\t\n\\b\tAt an ASCII word boundary (transition between \\w and either \\W, \\A or \\z, or vice-versa)\t\n\\B\tNot at an ASCII word boundary\t\nTerraform uses the RE2 regular expression language. This engine does not support all of the features found in some other regular expression engines; in particular, it does not support backreferences.\nSome of the matching behaviors described above can be modified by setting matching flags, activated using either the (?flags) operator (to activate within the current sub-pattern) or the (?flags:x) operator (to match x with the modified flags). Each flag is a single letter, and multiple flags can be set at once by listing multiple letters in the flags position. The available flags are listed in the table below:\nFlagMeaning\ni\tCase insensitive: a literal letter in the pattern matches both lowercase and uppercase versions of that letter\t\nm\tThe ^ and $ operators also match the beginning and end of lines within the string, marked by newline characters; behavior of \\A and \\z is unchanged\t\ns\tThe . operator also matches newline\t\nU\tThe meaning of presence or absense ? after a repetition operator is inverted. For example, x* is interpreted like x*? and vice-versa.\t\n> regex(\"[a-z]+\", \"53453453.345345aaabbbccc23454\") aaabbbccc > regex(\"(\\\\d\\\\d\\\\d\\\\d)-(\\\\d\\\\d)-(\\\\d\\\\d)\", \"2019-02-01\") [ \"2019\", \"02\", \"01\", ] > regex(\"^(?:(?P<scheme>[^:/?#]+):)?(?://(?P<authority>[^/?#]*))?\", \"https://terraform.io/docs/\") { \"authority\" = \"terraform.io\" \"scheme\" = \"https\" } > regex(\"[a-z]+\", \"53453453.34534523454\") Error: Error in function call Call to function \"regex\" failed: pattern did not match any part of the given string. \nregexall searches for potentially multiple matches of a given pattern in a string.\nreplace replaces a substring of a string with another string, optionally matching using the same regular expression syntax as regex."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/regexall",
  "text": "regexall - Functions - Configuration Language | Terraform\nregexall applies a regular expression to a string and returns a list of all matches.\nregexall(pattern, string) \nregexall is a variant of regex and uses the same pattern syntax. For any given input to regex, regexall returns a list of whatever type regex would've returned, with one element per match. That is:\nIf the pattern has no capture groups at all, the result is a list of strings.\nIf the pattern has one or more unnamed capture groups, the result is a list of lists.\nIf the pattern has one or more named capture groups, the result is a list of maps.\nregexall can also be used to test whether a particular string matches a given pattern, by testing whether the length of the resulting list of matches is greater than zero.\n> regexall(\"[a-z]+\", \"1234abcd5678efgh9\") [ \"abcd\", \"efgh\", ] > length(regexall(\"[a-z]+\", \"1234abcd5678efgh9\")) 2 > length(regexall(\"[a-z]+\", \"123456789\")) > 0 false \nregex searches for a single match of a given pattern, and returns an error if no match is found."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/replace",
  "text": "replace - Functions - Configuration Language | Terraform\nreplace Function\nreplace searches a given string for another given substring, and replaces each occurrence with a given replacement string.\nreplace(string, substring, replacement) \nIf substring is wrapped in forward slashes, it is treated as a regular expression, using the same pattern syntax as regex. If using a regular expression for the substring argument, the replacement string can incorporate captured strings from the input by using an $n sequence, where n is the index or name of a capture group.\n> replace(\"1 + 2 + 3\", \"+\", \"-\") 1 - 2 - 3 > replace(\"hello world\", \"/w.*d/\", \"everybody\") hello everybody \nregex searches a given string for a substring matching a given regular expression pattern."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/split",
  "text": "split - Functions - Configuration Language | Terraform\nsplit produces a list by dividing a given string at all occurrences of a given separator.\n> split(\",\", \"foo,bar,baz\") [ \"foo\", \"bar\", \"baz\", ] > split(\",\", \"foo\") [ \"foo\", ] > split(\",\", \"\") [ \"\", ]"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/strcontains",
  "text": "strcontains - Functions - Configuration Language | Terraform\nstrcontains Function\nstrcontains function checks whether a substring is within another string.\nstrcontains(string, substr) \n> strcontains(\"hello world\", \"wor\") true > strcontains(\"hello world\", \"wod\") false"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/strrev",
  "text": "strrev - Functions - Configuration Language | Terraform\nstrrev reverses the characters in a string. Note that the characters are treated as Unicode characters (in technical terms, Unicode grapheme cluster boundaries are respected).\n> strrev(\"hello\") olleh > strrev(\"a \")  a \nreverse reverses a sequence."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/title",
  "text": "title - Functions - Configuration Language | Terraform\ntitle Function\ntitle converts the first letter of each word in the given string to uppercase.\n> title(\"hello world\") Hello World \nupper converts all letters in a string to uppercase.\nlower converts all letters in a string to lowercase."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/startswith",
  "text": "startswith - Functions - Configuration Language | Terraform\nstartswith Function\nstartswith(string, prefix) \n> startswith(\"hello world\", \"hello\") true > startswith(\"hello world\", \"world\") false "
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/trim",
  "text": "trim - Functions - Configuration Language | Terraform\ntrim Function\ntrim removes the specified set of characters from the start and end of the given string.\ntrim(string, str_character_set) \nEvery occurrence of a character in the second argument is removed from the start and end of the string specified in the first argument.\n> trim(\"?!hello?!\", \"!?\") \"hello\" > trim(\"foobar\", \"far\") \"oob\" > trim(\" hello! world.! \", \"! \") \"hello! world.\" "
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/trimprefix",
  "text": "trimprefix - Functions - Configuration Language | Terraform\ntrimprefix Function\ntrimprefix removes the specified prefix from the start of the given string. If the string does not start with the prefix, the string is returned unchanged.\n> trimprefix(\"helloworld\", \"hello\") world > trimprefix(\"helloworld\", \"cat\") helloworld "
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/trimsuffix",
  "text": "trimsuffix - Functions - Configuration Language | Terraform\nSign up\nTheme\ntrimsuffix Function\ntrimsuffix removes the specified suffix from the end of the given string.\n> trimsuffix(\"helloworld\", \"world\") hello "
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/trimspace",
  "text": "trimspace - Functions - Configuration Language | Terraform\ntrimspace removes any space characters from the start and end of the given string.\nThis function follows the Unicode definition of \"space\", which includes regular spaces, tabs, newline characters, and various other space-like characters.\n> trimspace(\" hello\\n\\n\") hello \nchomp removes just line ending characters from the end of a string."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/upper",
  "text": "upper - Functions - Configuration Language | Terraform\nupper Function\nupper converts all cased letters in the given string to uppercase.\n> upper(\"hello\") HELLO > upper(\"!\") ! \nlower converts letters in a string to lowercase."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.8.x/functions/formatdate",
  "text": "formatdate - Functions - Configuration Language | Terraform\nformatdate converts a timestamp into a different time format.\nformatdate(spec, timestamp) \nIn the Terraform language, timestamps are conventionally represented as strings using RFC 3339 \"Date and Time format\" syntax. formatdate requires the timestamp argument to be a string conforming to this syntax.\n> formatdate(\"DD MMM YYYY hh:mm ZZZ\", \"2018-01-02T23:12:01Z\") 02 Jan 2018 23:12 UTC > formatdate(\"EEEE, DD-MMM-YY hh:mm:ss ZZZ\", \"2018-01-02T23:12:01Z\") Tuesday, 02-Jan-18 23:12:01 UTC > formatdate(\"EEE, DD MMM YYYY hh:mm:ss ZZZ\", \"2018-01-02T23:12:01-08:00\") Tue, 02 Jan 2018 23:12:01 -0800 > formatdate(\"MMM DD, YYYY\", \"2018-01-02T23:12:01Z\") Jan 02, 2018 > formatdate(\"HH:mmaa\", \"2018-01-02T23:12:01Z\") 11:12pm \nThe format specification is a string that includes formatting sequences from the following table. This function is intended for producing common machine-oriented timestamp formats such as those defined in RFC822, RFC850, and RFC1123. It is not suitable for truly human-oriented date formatting because it is not locale-aware. In particular, it can produce month and day names only in English.\nThe specification may contain the following sequences:\nSequenceResult\nYYYY\tFour (or more) digit year, like \"2006\".\t\nYY\tThe year modulo 100, zero padded to at least two digits, like \"06\".\t\nMMMM\tEnglish month name unabbreviated, like \"January\".\t\nMMM\tEnglish month name abbreviated to three letters, like \"Jan\".\t\nMM\tMonth number zero-padded to two digits, like \"01\" for January.\t\nM\tMonth number with no padding, like \"1\" for January.\t\nDD\tDay of month number zero-padded to two digits, like \"02\".\t\nD\tDay of month number with no padding, like \"2\".\t\nEEEE\tEnglish day of week name unabbreviated, like \"Monday\".\t\nEEE\tEnglish day of week name abbreviated to three letters, like \"Mon\".\t\nhh\t24-hour number zero-padded to two digits, like \"02\".\t\nh\t24-hour number unpadded, like \"2\".\t\nHH\t12-hour number zero-padded to two digits, like \"02\".\t\nH\t12-hour number unpadded, like \"2\".\t\nAA\tHour AM/PM marker in uppercase, like \"AM\".\t\naa\tHour AM/PM marker in lowercase, like \"am\".\t\nmm\tMinute within hour zero-padded to two digits, like \"05\".\t\nm\tMinute within hour unpadded, like \"5\".\t\nss\tSecond within minute zero-padded to two digits, like \"09\".\t\ns\tSecond within minute, like \"9\".\t\nZZZZZ\tTimezone offset with colon separating hours and minutes, like \"-08:00\".\t\nZZZZ\tTimezone offset with just sign and digit, like \"-0800\".\t\nZZZ\tLike ZZZZ but with a special case \"UTC\" for UTC.\t\nZ\tLike ZZZZZ but with a special case \"Z\" for UTC.\t\nAny non-letter characters, such as punctuation, are reproduced verbatim in the output. To include literal letters in the format string, enclose them in single quotes '. To include a literal quote, escape it by doubling the quotes.\n> formatdate(\"h'h'mm\", \"2018-01-02T23:12:01-08:00\") 23h12 > formatdate(\"H 'o''clock'\", \"2018-01-02T23:12:01-08:00\") 11 o'clock \nThis format specification syntax is intended to make it easy for a reader to guess which format will result even if they are not experts on the syntax. Therefore there are no predefined shorthands for common formats, but format strings for various RFC-specified formats are given below to be copied into your configuration as needed:\nRFC 822 and RFC RFC 2822: \"DD MMM YYYY hh:mm ZZZ\"\nRFC 850: \"EEEE, DD-MMM-YY hh:mm:ss ZZZ\"\nRFC 1123: \"EEE, DD MMM YYYY hh:mm:ss ZZZ\"\nRFC 3339: \"YYYY-MM-DD'T'hh:mm:ssZ\" (but this is also the input format, so such a conversion is redundant.)\nformat is a more general formatting function for arbitrary data.\ntimestamp returns the current date and time in a format suitable for input to formatdate."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/chomp",
  "text": "chomp - Functions - Configuration Language | Terraform\nchomp removes newline characters at the end of a string.\nThis can be useful if, for example, the string was read from a file that has a newline character at the end.\n> chomp(\"hello\\n\") hello > chomp(\"hello\\r\\n\") hello > chomp(\"hello\\n\\n\") hello \ntrimspace, which removes all types of whitespace from both the start and the end of a string."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/endswith",
  "text": "endswith - Functions - Configuration Language | Terraform\nendswith Function\n> endswith(\"hello world\", \"world\") true > endswith(\"hello world\", \"hello\") false "
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/formatlist",
  "text": "formatlist - Functions - Configuration Language | Terraform\nformatlist produces a list of strings by formatting a number of other values according to a specification string.\nformatlist(spec, values...) \nThe specification string uses the same syntax as format.\nThe given values can be a mixture of list and non-list arguments. Any given lists must be the same length, which decides the length of the resulting list.\nThe list arguments are iterated together in order by index, while the non-list arguments are used repeatedly for each iteration. The format string is evaluated once per element of the list arguments.\n> formatlist(\"Hello, %s!\", [\"Valentina\", \"Ander\", \"Olivia\", \"Sam\"]) [ \"Hello, Valentina!\", \"Hello, Ander!\", \"Hello, Olivia!\", \"Hello, Sam!\", ] > formatlist(\"%s, %s!\", \"Salutations\", [\"Valentina\", \"Ander\", \"Olivia\", \"Sam\"]) [ \"Salutations, Valentina!\", \"Salutations, Ander!\", \"Salutations, Olivia!\", \"Salutations, Sam!\", ] \nformat defines the specification syntax used by this function and produces a single string as its result."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/indent",
  "text": "indent - Functions - Configuration Language | Terraform\nindent Function\nindent adds a given number of spaces to the beginnings of all but the first line in a given multi-line string.\nindent(num_spaces, string) \nThis function is useful for inserting a multi-line string into an already-indented context in another string:\n> \" items: ${indent(2, \"[\\n foo,\\n bar,\\n]\\n\")}\" items: [ foo, bar, ] \nThe first line of the string is not indented so that, as above, it can be placed after an introduction sequence that has already begun the line."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/join",
  "text": "join - Functions - Configuration Language | Terraform\njoin produces a string by concatenating all of the elements of the specified list of strings with the specified separator.\n> join(\"-\", [\"foo\", \"bar\", \"baz\"]) \"foo-bar-baz\" > join(\", \", [\"foo\", \"bar\", \"baz\"]) foo, bar, baz > join(\", \", [\"foo\"]) foo"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/lower",
  "text": "lower - Functions - Configuration Language | Terraform\nlower Function\nlower converts all cased letters in the given string to lowercase.\n> lower(\"HELLO\") hello > lower(\"!\") ! \nupper converts letters in a string to uppercase."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/regex",
  "text": "regex - Functions - Configuration Language | Terraform\nregex applies a regular expression to a string and returns the matching substrings.\nThe return type of regex depends on the capture groups, if any, in the pattern:\nIf the pattern has no capture groups at all, the result is a single string covering the substring matched by the pattern as a whole.\nIf the pattern has one or more unnamed capture groups, the result is a list of the captured substrings in the same order as the definition of the capture groups.\nIf the pattern has one or more named capture groups, the result is a map of the captured substrings, using the capture group names as map keys.\nIt's not valid to mix both named and unnamed capture groups in the same pattern.\nIf the given pattern does not match at all, the regex raises an error. To test whether a given pattern matches a string, use regexall and test that the result has length greater than zero.\nThe pattern is a string containing a mixture of literal characters and special matching operators as described in the following table. Note that when giving a regular expression pattern as a literal quoted string in the Terraform language, the quoted string itself already uses backslash \\ as an escape character for the string, so any backslashes intended to be recognized as part of the pattern must be escaped as \\\\.\n.\tAny character except newline\t\n[xyz]\tAny character listed between the brackets (x, y, and z in this example)\t\n[a-z]\tAny character between a and z, inclusive\t\n[^xyz]\tThe opposite of [xyz]\t\n\\d\tASCII digits (0 through 9, inclusive)\t\n\\D\tAnything except ASCII digits\t\n\\s\tASCII spaces (space, tab, newline, carriage return, form feed)\t\n\\S\tAnything except ASCII spaces\t\n\\w\tThe same as [0-9A-Za-z_]\t\n\\W\tAnything except the characters matched by \\w\t\n[[:alnum:]]\tThe same as [0-9A-Za-z]\t\n[[:alpha:]]\tThe same as [A-Za-z]\t\n[[:ascii:]]\tAny ASCII character\t\n[[:blank:]]\tASCII tab or space\t\n[[:cntrl:]]\tASCII/Unicode control characters\t\n[[:digit:]]\tThe same as [0-9]\t\n[[:graph:]]\tAll \"graphical\" (printable) ASCII characters\t\n[[:lower:]]\tThe same as [a-z]\t\n[[:print:]]\tThe same as [[:graph:]]\t\n[[:punct:]]\tThe same as [!-/:-@[-`{-~]\t\n[[:space:]]\tThe same as [\\t\\n\\v\\f\\r ]\t\n[[:upper:]]\tThe same as [A-Z]\t\n[[:word:]]\tThe same as \\w\t\n[[:xdigit:]]\tThe same as [0-9A-Fa-f]\t\n\\pN\tUnicode character class by using single-letter class names (\"N\" in this example)\t\n\\p{Greek}\tUnicode character class by unicode name (\"Greek\" in this example)\t\n\\PN\tThe opposite of \\pN\t\n\\P{Greek}\tThe opposite of \\p{Greek}\t\nxy\tx followed immediately by y\t\nx&#124;y\teither x or y, preferring x\t\nx*\tzero or more x, preferring more\t\nx*?\tzero or more x, preferring fewer\t\nx+\tone or more x, preferring more\t\nx+?\tone or more x, preferring fewer\t\nx?\tzero or one x, preferring one\t\nx??\tzero or one x, preferring zero\t\nx{n,m}\tbetween n and m repetitions of x, preferring more\t\nx{n,m}?\tbetween n and m repetitions of x, preferring fewer\t\nx{n,}\tat least n repetitions of x, preferring more\t\nx{n,}?\tat least n repetitions of x, preferring fewer\t\nx{n}\texactly n repetitions of x\t\n(x)\tunnamed capture group for sub-pattern x\t\n(?P<name>x)\tnamed capture group, named name, for sub-pattern x\t\n(?:x)\tnon-capturing sub-pattern x\t\n\\*\tLiteral * for any punctuation character *\t\n\\Q...\\E\tLiteral ... for any text ... as long as it does not include literally \\E\t\nIn addition to the above matching operators that consume the characters they match, there are some additional operators that only match, but consume no characters. These are \"zero-width\" matching operators:\n^\tAt the beginning of the given string\t\n$\tAt the end of the given string\t\n\\A\tAt the beginning of the given string\t\n\\z\tAt the end of the given string\t\n\\b\tAt an ASCII word boundary (transition between \\w and either \\W, \\A or \\z, or vice-versa)\t\n\\B\tNot at an ASCII word boundary\t\nTerraform uses the RE2 regular expression language. This engine does not support all of the features found in some other regular expression engines; in particular, it does not support backreferences.\nSome of the matching behaviors described above can be modified by setting matching flags, activated using either the (?flags) operator (to activate within the current sub-pattern) or the (?flags:x) operator (to match x with the modified flags). Each flag is a single letter, and multiple flags can be set at once by listing multiple letters in the flags position. The available flags are listed in the table below:\nFlagMeaning\ni\tCase insensitive: a literal letter in the pattern matches both lowercase and uppercase versions of that letter\t\nm\tThe ^ and $ operators also match the beginning and end of lines within the string, marked by newline characters; behavior of \\A and \\z is unchanged\t\ns\tThe . operator also matches newline\t\nU\tThe meaning of presence or absense ? after a repetition operator is inverted. For example, x* is interpreted like x*? and vice-versa.\t\n> regex(\"[a-z]+\", \"53453453.345345aaabbbccc23454\") aaabbbccc > regex(\"(\\\\d\\\\d\\\\d\\\\d)-(\\\\d\\\\d)-(\\\\d\\\\d)\", \"2019-02-01\") [ \"2019\", \"02\", \"01\", ] > regex(\"^(?:(?P<scheme>[^:/?#]+):)?(?://(?P<authority>[^/?#]*))?\", \"https://terraform.io/docs/\") { \"authority\" = \"terraform.io\" \"scheme\" = \"https\" } > regex(\"[a-z]+\", \"53453453.34534523454\") Error: Error in function call Call to function \"regex\" failed: pattern did not match any part of the given string. \nregexall searches for potentially multiple matches of a given pattern in a string.\nreplace replaces a substring of a string with another string, optionally matching using the same regular expression syntax as regex."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/regexall",
  "text": "regexall - Functions - Configuration Language | Terraform\nregexall applies a regular expression to a string and returns a list of all matches.\nregexall(pattern, string) \nregexall is a variant of regex and uses the same pattern syntax. For any given input to regex, regexall returns a list of whatever type regex would've returned, with one element per match. That is:\nIf the pattern has no capture groups at all, the result is a list of strings.\nIf the pattern has one or more unnamed capture groups, the result is a list of lists.\nIf the pattern has one or more named capture groups, the result is a list of maps.\nregexall can also be used to test whether a particular string matches a given pattern, by testing whether the length of the resulting list of matches is greater than zero.\n> regexall(\"[a-z]+\", \"1234abcd5678efgh9\") [ \"abcd\", \"efgh\", ] > length(regexall(\"[a-z]+\", \"1234abcd5678efgh9\")) 2 > length(regexall(\"[a-z]+\", \"123456789\")) > 0 false \nregex searches for a single match of a given pattern, and returns an error if no match is found."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/replace",
  "text": "replace - Functions - Configuration Language | Terraform\nreplace Function\nreplace searches a given string for another given substring, and replaces each occurrence with a given replacement string.\nreplace(string, substring, replacement) \nIf substring is wrapped in forward slashes, it is treated as a regular expression, using the same pattern syntax as regex. If using a regular expression for the substring argument, the replacement string can incorporate captured strings from the input by using an $n sequence, where n is the index or name of a capture group.\n> replace(\"1 + 2 + 3\", \"+\", \"-\") 1 - 2 - 3 > replace(\"hello world\", \"/w.*d/\", \"everybody\") hello everybody \nregex searches a given string for a substring matching a given regular expression pattern."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/split",
  "text": "split - Functions - Configuration Language | Terraform\nsplit produces a list by dividing a given string at all occurrences of a given separator.\n> split(\",\", \"foo,bar,baz\") [ \"foo\", \"bar\", \"baz\", ] > split(\",\", \"foo\") [ \"foo\", ] > split(\",\", \"\") [ \"\", ]"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/startswith",
  "text": "startswith - Functions - Configuration Language | Terraform\nstartswith Function\nstartswith(string, prefix) \n> startswith(\"hello world\", \"hello\") true > startswith(\"hello world\", \"world\") false "
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/strcontains",
  "text": "strcontains - Functions - Configuration Language | Terraform\nstrcontains Function\nstrcontains function checks whether a substring is within another string.\nstrcontains(string, substr) \n> strcontains(\"hello world\", \"wor\") true > strcontains(\"hello world\", \"wod\") false"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/title",
  "text": "title - Functions - Configuration Language | Terraform\ntitle Function\ntitle converts the first letter of each word in the given string to uppercase.\n> title(\"hello world\") Hello World \nupper converts all letters in a string to uppercase.\nlower converts all letters in a string to lowercase."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/strrev",
  "text": "strrev - Functions - Configuration Language | Terraform\nstrrev reverses the characters in a string. Note that the characters are treated as Unicode characters (in technical terms, Unicode grapheme cluster boundaries are respected).\n> strrev(\"hello\") olleh > strrev(\"a \")  a \nreverse reverses a sequence."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/trim",
  "text": "trim - Functions - Configuration Language | Terraform\ntrim Function\ntrim removes the specified set of characters from the start and end of the given string.\ntrim(string, str_character_set) \nEvery occurrence of a character in the second argument is removed from the start and end of the string specified in the first argument.\n> trim(\"?!hello?!\", \"!?\") \"hello\" > trim(\"foobar\", \"far\") \"oob\" > trim(\" hello! world.! \", \"! \") \"hello! world.\" "
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/trimprefix",
  "text": "trimprefix - Functions - Configuration Language | Terraform\ntrimprefix Function\ntrimprefix removes the specified prefix from the start of the given string. If the string does not start with the prefix, the string is returned unchanged.\n> trimprefix(\"helloworld\", \"hello\") world > trimprefix(\"helloworld\", \"cat\") helloworld "
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/trimsuffix",
  "text": "trimsuffix - Functions - Configuration Language | Terraform\ntrimsuffix Function\ntrimsuffix removes the specified suffix from the end of the given string.\n> trimsuffix(\"helloworld\", \"world\") hello "
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/trimspace",
  "text": "trimspace - Functions - Configuration Language | Terraform\ntrimspace removes any space characters from the start and end of the given string.\nThis function follows the Unicode definition of \"space\", which includes regular spaces, tabs, newline characters, and various other space-like characters.\n> trimspace(\" hello\\n\\n\") hello \nchomp removes just line ending characters from the end of a string."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/formatdate",
  "text": "formatdate - Functions - Configuration Language | Terraform\nformatdate converts a timestamp into a different time format.\nformatdate(spec, timestamp) \nIn the Terraform language, timestamps are conventionally represented as strings using RFC 3339 \"Date and Time format\" syntax. formatdate requires the timestamp argument to be a string conforming to this syntax.\n> formatdate(\"DD MMM YYYY hh:mm ZZZ\", \"2018-01-02T23:12:01Z\") 02 Jan 2018 23:12 UTC > formatdate(\"EEEE, DD-MMM-YY hh:mm:ss ZZZ\", \"2018-01-02T23:12:01Z\") Tuesday, 02-Jan-18 23:12:01 UTC > formatdate(\"EEE, DD MMM YYYY hh:mm:ss ZZZ\", \"2018-01-02T23:12:01-08:00\") Tue, 02 Jan 2018 23:12:01 -0800 > formatdate(\"MMM DD, YYYY\", \"2018-01-02T23:12:01Z\") Jan 02, 2018 > formatdate(\"HH:mmaa\", \"2018-01-02T23:12:01Z\") 11:12pm \nThe format specification is a string that includes formatting sequences from the following table. This function is intended for producing common machine-oriented timestamp formats such as those defined in RFC822, RFC850, and RFC1123. It is not suitable for truly human-oriented date formatting because it is not locale-aware. In particular, it can produce month and day names only in English.\nThe specification may contain the following sequences:\nSequenceResult\nYYYY\tFour (or more) digit year, like \"2006\".\t\nYY\tThe year modulo 100, zero padded to at least two digits, like \"06\".\t\nMMMM\tEnglish month name unabbreviated, like \"January\".\t\nMMM\tEnglish month name abbreviated to three letters, like \"Jan\".\t\nMM\tMonth number zero-padded to two digits, like \"01\" for January.\t\nM\tMonth number with no padding, like \"1\" for January.\t\nDD\tDay of month number zero-padded to two digits, like \"02\".\t\nD\tDay of month number with no padding, like \"2\".\t\nEEEE\tEnglish day of week name unabbreviated, like \"Monday\".\t\nEEE\tEnglish day of week name abbreviated to three letters, like \"Mon\".\t\nhh\t24-hour number zero-padded to two digits, like \"02\".\t\nh\t24-hour number unpadded, like \"2\".\t\nHH\t12-hour number zero-padded to two digits, like \"02\".\t\nH\t12-hour number unpadded, like \"2\".\t\nAA\tHour AM/PM marker in uppercase, like \"AM\".\t\naa\tHour AM/PM marker in lowercase, like \"am\".\t\nmm\tMinute within hour zero-padded to two digits, like \"05\".\t\nm\tMinute within hour unpadded, like \"5\".\t\nss\tSecond within minute zero-padded to two digits, like \"09\".\t\ns\tSecond within minute, like \"9\".\t\nZZZZZ\tTimezone offset with colon separating hours and minutes, like \"-08:00\".\t\nZZZZ\tTimezone offset with just sign and digit, like \"-0800\".\t\nZZZ\tLike ZZZZ but with a special case \"UTC\" for UTC.\t\nZ\tLike ZZZZZ but with a special case \"Z\" for UTC.\t\nAny non-letter characters, such as punctuation, are reproduced verbatim in the output. To include literal letters in the format string, enclose them in single quotes '. To include a literal quote, escape it by doubling the quotes.\n> formatdate(\"h'h'mm\", \"2018-01-02T23:12:01-08:00\") 23h12 > formatdate(\"H 'o''clock'\", \"2018-01-02T23:12:01-08:00\") 11 o'clock \nThis format specification syntax is intended to make it easy for a reader to guess which format will result even if they are not experts on the syntax. Therefore there are no predefined shorthands for common formats, but format strings for various RFC-specified formats are given below to be copied into your configuration as needed:\nRFC 822 and RFC RFC 2822: \"DD MMM YYYY hh:mm ZZZ\"\nRFC 850: \"EEEE, DD-MMM-YY hh:mm:ss ZZZ\"\nRFC 1123: \"EEE, DD MMM YYYY hh:mm:ss ZZZ\"\nRFC 3339: \"YYYY-MM-DD'T'hh:mm:ssZ\" (but this is also the input format, so such a conversion is redundant.)\nformat is a more general formatting function for arbitrary data.\ntimestamp returns the current date and time in a format suitable for input to formatdate."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/chomp",
  "text": "chomp - Functions - Configuration Language | Terraform\nchomp removes newline characters at the end of a string.\nThis can be useful if, for example, the string was read from a file that has a newline character at the end.\n> chomp(\"hello\\n\") hello > chomp(\"hello\\r\\n\") hello > chomp(\"hello\\n\\n\") hello \ntrimspace, which removes all types of whitespace from both the start and the end of a string."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.6.x/functions/upper",
  "text": "upper - Functions - Configuration Language | Terraform\nupper Function\nupper converts all cased letters in the given string to uppercase.\n> upper(\"hello\") HELLO > upper(\"!\") ! \nlower converts letters in a string to lowercase."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/indent",
  "text": "indent Function\nindent adds a given number of spaces to the beginnings of all but the first line in a given multi-line string.\nThis function is useful for inserting a multi-line string into an already-indented context in another string:\n> \" items: ${indent(2, \"[\\n foo,\\n bar,\\n]\\n\")}\" items: [ foo, bar, ] \nThe first line of the string is not indented so that, as above, it can be placed after an introduction sequence that has already begun the line."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/endswith",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/formatlist",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/regex",
  "text": "x\\|y\teither x or y, preferring x\t"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/join",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/regexall",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/lower",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/replace",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/split",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/startswith",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/strrev",
  "text": "Skip to main content\nstrrev Function"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/title",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/trim",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/trimprefix",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/trimspace",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/formatdate",
  "text": "formatdate - Functions - Configuration Language | Terraform\nformatdate converts a timestamp into a different time format.\nformatdate(spec, timestamp) \nIn the Terraform language, timestamps are conventionally represented as strings using RFC 3339 \"Date and Time format\" syntax. formatdate requires the timestamp argument to be a string conforming to this syntax.\n> formatdate(\"DD MMM YYYY hh:mm ZZZ\", \"2018-01-02T23:12:01Z\") 02 Jan 2018 23:12 UTC > formatdate(\"EEEE, DD-MMM-YY hh:mm:ss ZZZ\", \"2018-01-02T23:12:01Z\") Tuesday, 02-Jan-18 23:12:01 UTC > formatdate(\"EEE, DD MMM YYYY hh:mm:ss ZZZ\", \"2018-01-02T23:12:01-08:00\") Tue, 02 Jan 2018 23:12:01 -0800 > formatdate(\"MMM DD, YYYY\", \"2018-01-02T23:12:01Z\") Jan 02, 2018 > formatdate(\"HH:mmaa\", \"2018-01-02T23:12:01Z\") 11:12pm \nThe format specification is a string that includes formatting sequences from the following table. This function is intended for producing common machine-oriented timestamp formats such as those defined in RFC822, RFC850, and RFC1123. It is not suitable for truly human-oriented date formatting because it is not locale-aware. In particular, it can produce month and day names only in English.\nThe specification may contain the following sequences:\nSequenceResult\nYYYY\tFour (or more) digit year, like \"2006\".\t\nYY\tThe year modulo 100, zero padded to at least two digits, like \"06\".\t\nMMMM\tEnglish month name unabbreviated, like \"January\".\t\nMMM\tEnglish month name abbreviated to three letters, like \"Jan\".\t\nMM\tMonth number zero-padded to two digits, like \"01\" for January.\t\nM\tMonth number with no padding, like \"1\" for January.\t\nDD\tDay of month number zero-padded to two digits, like \"02\".\t\nD\tDay of month number with no padding, like \"2\".\t\nEEEE\tEnglish day of week name unabbreviated, like \"Monday\".\t\nEEE\tEnglish day of week name abbreviated to three letters, like \"Mon\".\t\nhh\t24-hour number zero-padded to two digits, like \"02\".\t\nh\t24-hour number unpadded, like \"2\".\t\nHH\t12-hour number zero-padded to two digits, like \"02\".\t\nH\t12-hour number unpadded, like \"2\".\t\nAA\tHour AM/PM marker in uppercase, like \"AM\".\t\naa\tHour AM/PM marker in lowercase, like \"am\".\t\nmm\tMinute within hour zero-padded to two digits, like \"05\".\t\nm\tMinute within hour unpadded, like \"5\".\t\nss\tSecond within minute zero-padded to two digits, like \"09\".\t\ns\tSecond within minute, like \"9\".\t\nZZZZZ\tTimezone offset with colon separating hours and minutes, like \"-08:00\".\t\nZZZZ\tTimezone offset with just sign and digit, like \"-0800\".\t\nZZZ\tLike ZZZZ but with a special case \"UTC\" for UTC.\t\nZ\tLike ZZZZZ but with a special case \"Z\" for UTC.\t\nAny non-letter characters, such as punctuation, are reproduced verbatim in the output. To include literal letters in the format string, enclose them in single quotes '. To include a literal quote, escape it by doubling the quotes.\n> formatdate(\"h'h'mm\", \"2018-01-02T23:12:01-08:00\") 23h12 > formatdate(\"H 'o''clock'\", \"2018-01-02T23:12:01-08:00\") 11 o'clock \nThis format specification syntax is intended to make it easy for a reader to guess which format will result even if they are not experts on the syntax. Therefore there are no predefined shorthands for common formats, but format strings for various RFC-specified formats are given below to be copied into your configuration as needed:\nRFC 822 and RFC RFC 2822: \"DD MMM YYYY hh:mm ZZZ\"\nRFC 850: \"EEEE, DD-MMM-YY hh:mm:ss ZZZ\"\nRFC 1123: \"EEE, DD MMM YYYY hh:mm:ss ZZZ\"\nRFC 3339: \"YYYY-MM-DD'T'hh:mm:ssZ\" (but this is also the input format, so such a conversion is redundant.)\nformat is a more general formatting function for arbitrary data.\ntimestamp returns the current date and time in a format suitable for input to formatdate."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/trimsuffix",
  "text": "Skip to main content"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.4.x/functions/upper",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/endswith",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/chomp",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/formatlist",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/indent",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/join",
  "text": "join produces a string by concatenating together all elements of a given list of strings with the given delimiter.\n> join(\", \", [\"foo\", \"bar\", \"baz\"]) foo, bar, baz > join(\", \", [\"foo\"]) foo"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/lower",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/regex",
  "text": "x\\|y\teither x or y, preferring x\t"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/regexall",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/replace",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/split",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/startswith",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/strrev",
  "text": "Skip to main content\nstrrev Function"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/trimprefix",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/title",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/trim",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/trimsuffix",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/trimspace",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/upper",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.3.x/functions/formatdate",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/chomp",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/formatlist",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/indent",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/join",
  "text": "join produces a string by concatenating together all elements of a given list of strings with the given delimiter.\n> join(\", \", [\"foo\", \"bar\", \"baz\"]) foo, bar, baz > join(\", \", [\"foo\"]) foo"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/regex",
  "text": "x\\|y\teither x or y, preferring x\t"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/lower",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/regexall",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/replace",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/strrev",
  "text": "strrev Function"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/split",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/title",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/trim",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/trimprefix",
  "text": "We use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy."
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/trimsuffix",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/trimspace",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/upper",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.2.x/functions/formatdate",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/chomp",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/endswith",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/formatlist",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/indent",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/join",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/lower",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/regexall",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/regex",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/replace",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/split",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/startswith",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/strcontains",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/strrev",
  "text": "strrev Function"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/title",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/trim",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/trimprefix",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/trimsuffix",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/trimspace",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/upper",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.5.x/functions/formatdate",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.1.x/functions/join",
  "text": "join produces a string by concatenating together all elements of a given list of strings with the given delimiter.\n> join(\", \", [\"foo\", \"bar\", \"baz\"]) foo, bar, baz > join(\", \", [\"foo\"]) foo"
},
{
  "url": "https://developer.hashicorp.com/terraform/language/v1.1.x/functions/regex",
  "text": "x\\|y\teither x or y, preferring x\t"
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.12.x/kv/import",
  "text": "Commands: KV Import | Consul\nCommand: consul kv import\nThe kv import command is used to import KV pairs from the JSON representation generated by the kv export command.\nThe table below shows this command's required ACLs. Configuration of blocking queries and agent caching are not supported from commands, but may be from the corresponding HTTP endpoint.\nUsage: consul kv import [options] [DATA]\nCommand Options\n-prefix - Key prefix for imported data. The default value is empty meaning root. Added in Consul 1.10.\nEnterprise Options\n-partition=<string> - Specifies the partition to query. If not provided, the partition will be inferred from the request's ACL token, or will default to the default partition. Partitions are a Consul Enterprise feature added in v1.11.0.\n-namespace=<string> - Specifies the namespace to query. If not provided, the namespace will be inferred from the request's ACL token, or will default to the default namespace. Namespaces are a Consul Enterprise feature added in v1.7.0.\nAPI Options\n-ca-file=<value> - Path to a CA file to use for TLS when communicating with Consul. This can also be specified via the CONSUL_CACERT environment variable.\n-ca-path=<value> - Path to a directory of CA certificates to use for TLS when communicating with Consul. This can also be specified via the CONSUL_CAPATH environment variable.\n-client-cert=<value> - Path to a client cert file to use for TLS when verify_incoming is enabled. This can also be specified via the CONSUL_CLIENT_CERT environment variable.\n-client-key=<value> - Path to a client key file to use for TLS when verify_incoming is enabled. This can also be specified via the CONSUL_CLIENT_KEY environment variable.\n-http-addr=<addr> - Address of the Consul agent with the port. This can be an IP address or DNS address, but it must include the port. This can also be specified via the CONSUL_HTTP_ADDR environment variable. In Consul 0.8 and later, the default value is http://127.0.0.1:8500, and https can optionally be used instead. The scheme can also be set to HTTPS by setting the environment variable CONSUL_HTTP_SSL=true. This may be a unix domain socket using unix:///path/to/socket if the agent is configured to listen that way.\n-tls-server-name=<value> - The server name to use as the SNI host when connecting via TLS. This can also be specified via the CONSUL_TLS_SERVER_NAME environment variable.\n-token=<value> - ACL token to use in the request. This can also be specified via the CONSUL_HTTP_TOKEN environment variable. If unspecified, the query will default to the token of the Consul agent at the HTTP address.\n-token-file=<value> - File containing the ACL token to use in the request instead of one specified via the -token argument or CONSUL_HTTP_TOKEN environment variable. This can also be specified via the CONSUL_HTTP_TOKEN_FILE environment variable.\n-datacenter=<name> - Name of the datacenter to query. If unspecified, the query will default to the datacenter of the Consul agent at the HTTP address.\n-stale - Permit any Consul server (non-leader) to respond to this request. This allows for lower latency and higher throughput, but can result in stale data. This option has no effect on non-read operations. The default value is false.\nTo import from a file, prepend the filename with @:\n$ consul kv import @values.json # Output \nTo import from stdin, use - as the data parameter:\n$ cat values.json | consul kv import - # Output \nYou can also pass the JSON directly, however care must be taken with shell escaping:\n$ consul kv import \"$(cat values.json)\" # Output \nTo import under prefix, use -prefix option:\n$ cat values.json | consul kv import -prefix=sub/dir/ - # Output"
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.13.x/kv/import",
  "text": "Commands: KV Import | Consul\nCommand: consul kv import\nThe kv import command is used to import KV pairs from the JSON representation generated by the kv export command.\nThe table below shows this command's required ACLs. Configuration of blocking queries and agent caching are not supported from commands, but may be from the corresponding HTTP endpoint.\nUsage: consul kv import [options] [DATA]\nCommand Options\n-prefix - Key prefix for imported data. The default value is empty meaning root. Added in Consul 1.10.\nEnterprise Options\n-partition=<string> - Specifies the partition to query. If not provided, the partition will be inferred from the request's ACL token, or will default to the default partition. Partitions are a Consul Enterprise feature added in v1.11.0.\n-namespace=<string> - Specifies the namespace to query. If not provided, the namespace will be inferred from the request's ACL token, or will default to the default namespace. Namespaces are a Consul Enterprise feature added in v1.7.0.\nAPI Options\n-ca-file=<value> - Path to a CA file to use for TLS when communicating with Consul. This can also be specified via the CONSUL_CACERT environment variable.\n-ca-path=<value> - Path to a directory of CA certificates to use for TLS when communicating with Consul. This can also be specified via the CONSUL_CAPATH environment variable.\n-client-cert=<value> - Path to a client cert file to use for TLS when verify_incoming is enabled. This can also be specified via the CONSUL_CLIENT_CERT environment variable.\n-client-key=<value> - Path to a client key file to use for TLS when verify_incoming is enabled. This can also be specified via the CONSUL_CLIENT_KEY environment variable.\n-http-addr=<addr> - Address of the Consul agent with the port. This can be an IP address or DNS address, but it must include the port. This can also be specified via the CONSUL_HTTP_ADDR environment variable. In Consul 0.8 and later, the default value is http://127.0.0.1:8500, and https can optionally be used instead. The scheme can also be set to HTTPS by setting the environment variable CONSUL_HTTP_SSL=true. This may be a unix domain socket using unix:///path/to/socket if the agent is configured to listen that way.\n-tls-server-name=<value> - The server name to use as the SNI host when connecting via TLS. This can also be specified via the CONSUL_TLS_SERVER_NAME environment variable.\n-token=<value> - ACL token to use in the request. This can also be specified via the CONSUL_HTTP_TOKEN environment variable. If unspecified, the query will default to the token of the Consul agent at the HTTP address.\n-token-file=<value> - File containing the ACL token to use in the request instead of one specified via the -token argument or CONSUL_HTTP_TOKEN environment variable. This can also be specified via the CONSUL_HTTP_TOKEN_FILE environment variable.\n-datacenter=<name> - Name of the datacenter to query. If unspecified, the query will default to the datacenter of the Consul agent at the HTTP address.\n-stale - Permit any Consul server (non-leader) to respond to this request. This allows for lower latency and higher throughput, but can result in stale data. This option has no effect on non-read operations. The default value is false.\nTo import from a file, prepend the filename with @:\n$ consul kv import @values.json # Output \nTo import from stdin, use - as the data parameter:\n$ cat values.json | consul kv import - # Output \nYou can also pass the JSON directly, however care must be taken with shell escaping:\n$ consul kv import \"$(cat values.json)\" # Output \nTo import under prefix, use -prefix option:\n$ cat values.json | consul kv import -prefix=sub/dir/ - # Output"
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.11.x/kv/import",
  "text": "Commands: KV Import | Consul\nCommand: consul kv import\nThe kv import command is used to import KV pairs from the JSON representation generated by the kv export command.\nThe table below shows this command's required ACLs. Configuration of blocking queries and agent caching are not supported from commands, but may be from the corresponding HTTP endpoint.\nUsage: consul kv import [options] [DATA]\nAPI Options\n-ca-file=<value> - Path to a CA file to use for TLS when communicating with Consul. This can also be specified via the CONSUL_CACERT environment variable.\n-ca-path=<value> - Path to a directory of CA certificates to use for TLS when communicating with Consul. This can also be specified via the CONSUL_CAPATH environment variable.\n-client-cert=<value> - Path to a client cert file to use for TLS when verify_incoming is enabled. This can also be specified via the CONSUL_CLIENT_CERT environment variable.\n-client-key=<value> - Path to a client key file to use for TLS when verify_incoming is enabled. This can also be specified via the CONSUL_CLIENT_KEY environment variable.\n-http-addr=<addr> - Address of the Consul agent with the port. This can be an IP address or DNS address, but it must include the port. This can also be specified via the CONSUL_HTTP_ADDR environment variable. In Consul 0.8 and later, the default value is http://127.0.0.1:8500, and https can optionally be used instead. The scheme can also be set to HTTPS by setting the environment variable CONSUL_HTTP_SSL=true. This may be a unix domain socket using unix:///path/to/socket if the agent is configured to listen that way.\n-tls-server-name=<value> - The server name to use as the SNI host when connecting via TLS. This can also be specified via the CONSUL_TLS_SERVER_NAME environment variable.\n-token=<value> - ACL token to use in the request. This can also be specified via the CONSUL_HTTP_TOKEN environment variable. If unspecified, the query will default to the token of the Consul agent at the HTTP address.\n-token-file=<value> - File containing the ACL token to use in the request instead of one specified via the -token argument or CONSUL_HTTP_TOKEN environment variable. This can also be specified via the CONSUL_HTTP_TOKEN_FILE environment variable.\n-datacenter=<name> - Name of the datacenter to query. If unspecified, the query will default to the datacenter of the Consul agent at the HTTP address.\n-stale - Permit any Consul server (non-leader) to respond to this request. This allows for lower latency and higher throughput, but can result in stale data. This option has no effect on non-read operations. The default value is false.\nKV Import Options\n-prefix - Key prefix for imported data. The default value is empty meaning root. Added in Consul 1.10.\nEnterprise Options\n-namespace=<string> - Specifies the namespace to query. If not provided, the namespace will be inferred from the request's ACL token, or will default to the default namespace. Namespaces are a Consul Enterprise feature added in v1.7.0.\n-partition=<string> - Specifies the partition to query. If not provided, the partition will be inferred from the request's ACL token, or will default to the default partition. Partitions are a Consul Enterprise feature added in v1.11.0.\nTo import from a file, prepend the filename with @:\n$ consul kv import @values.json # Output \nTo import from stdin, use - as the data parameter:\n$ cat values.json | consul kv import - # Output \nYou can also pass the JSON directly, however care must be taken with shell escaping:\n$ consul kv import \"$(cat values.json)\" # Output \nTo import under prefix, use -prefix option:\n$ cat values.json | consul kv import -prefix=sub/dir/ - # Output"
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.10.x/kv/import",
  "text": "Commands: KV Import | Consul\nCommand: consul kv import\nThe kv import command is used to import KV pairs from the JSON representation generated by the kv export command.\nUsage: consul kv import [options] [DATA]\nAPI Options\n-ca-file=<value> - Path to a CA file to use for TLS when communicating with Consul. This can also be specified via the CONSUL_CACERT environment variable.\n-ca-path=<value> - Path to a directory of CA certificates to use for TLS when communicating with Consul. This can also be specified via the CONSUL_CAPATH environment variable.\n-client-cert=<value> - Path to a client cert file to use for TLS when verify_incoming is enabled. This can also be specified via the CONSUL_CLIENT_CERT environment variable.\n-client-key=<value> - Path to a client key file to use for TLS when verify_incoming is enabled. This can also be specified via the CONSUL_CLIENT_KEY environment variable.\n-http-addr=<addr> - Address of the Consul agent with the port. This can be an IP address or DNS address, but it must include the port. This can also be specified via the CONSUL_HTTP_ADDR environment variable. In Consul 0.8 and later, the default value is http://127.0.0.1:8500, and https can optionally be used instead. The scheme can also be set to HTTPS by setting the environment variable CONSUL_HTTP_SSL=true. This may be a unix domain socket using unix:///path/to/socket if the agent is configured to listen that way.\n-tls-server-name=<value> - The server name to use as the SNI host when connecting via TLS. This can also be specified via the CONSUL_TLS_SERVER_NAME environment variable.\n-token=<value> - ACL token to use in the request. This can also be specified via the CONSUL_HTTP_TOKEN environment variable. If unspecified, the query will default to the token of the Consul agent at the HTTP address.\n-token-file=<value> - File containing the ACL token to use in the request instead of one specified via the -token argument or CONSUL_HTTP_TOKEN environment variable. This can also be specified via the CONSUL_HTTP_TOKEN_FILE environment variable.\n-datacenter=<name> - Name of the datacenter to query. If unspecified, the query will default to the datacenter of the Consul agent at the HTTP address.\n-stale - Permit any Consul server (non-leader) to respond to this request. This allows for lower latency and higher throughput, but can result in stale data. This option has no effect on non-read operations. The default value is false.\nKV Import Options\n-prefix - Key prefix for imported data. The default value is empty meaning root.\nEnterprise Options\n-namespace=<string> - Specifies the namespace to query. If not provided, the namespace will be inferred from the request's ACL token, or will default to the default namespace. Namespaces are a Consul Enterprise feature added in v1.7.0.\nTo import from a file, prepend the filename with @:\n$ consul kv import @values.json # Output \nTo import from stdin, use - as the data parameter:\n$ cat values.json | consul kv import - # Output \nYou can also pass the JSON directly, however care must be taken with shell escaping:\n$ consul kv import \"$(cat values.json)\" # Output \nTo import under prefix, use -prefix option:\n$ cat values.json | consul kv import -prefix=sub/dir/ - # Output"
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.9.x/kv/import",
  "text": "$ consul kv import \"$(cat values.json)\" # Output"
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.8.x/kv/import",
  "text": "$ consul kv import \"$(cat values.json)\" # Output"
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.18.x/kv/export",
  "text": "Commands: KV Export | Consul\nCommand: consul kv export\nThe kv export command is used to retrieve KV pairs for the given prefix from Consul's KV store, and write a JSON representation to stdout. This can be used with the command \"consul kv import\" to move entire trees between Consul clusters.\nThe table below shows this command's required ACLs. Configuration of blocking queries and agent caching are not supported from commands, but may be from the corresponding HTTP endpoint.\nUsage: consul kv export [options] [PREFIX]\n-partition=<string> - Specifies the partition to query. If not provided, the partition will be inferred from the request's ACL token, or will default to the default partition. Partitions are a Consul Enterprise feature added in v1.11.0.\nTo export the tree at \"vault/\" in the key value store:\n$ consul kv export vault/ # JSON output"
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.17.x/kv/export",
  "text": "Commands: KV Export | Consul\nCommand: consul kv export\nThe kv export command is used to retrieve KV pairs for the given prefix from Consul's KV store, and write a JSON representation to stdout. This can be used with the command \"consul kv import\" to move entire trees between Consul clusters.\nUsage: consul kv export [options] [PREFIX]\nTo export the tree at \"vault/\" in the key value store:\n$ consul kv export vault/ # JSON output"
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.16.x/kv/export",
  "text": "Commands: KV Export | Consul\nCommand: consul kv export\nThe kv export command is used to retrieve KV pairs for the given prefix from Consul's KV store, and write a JSON representation to stdout. This can be used with the command \"consul kv import\" to move entire trees between Consul clusters.\nUsage: consul kv export [options] [PREFIX]\nTo export the tree at \"vault/\" in the key value store:\n$ consul kv export vault/ # JSON output"
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.15.x/kv/export",
  "text": "Commands: KV Export | Consul\nCommand: consul kv export\nThe kv export command is used to retrieve KV pairs for the given prefix from Consul's KV store, and write a JSON representation to stdout. This can be used with the command \"consul kv import\" to move entire trees between Consul clusters.\nUsage: consul kv export [options] [PREFIX]\nTo export the tree at \"vault/\" in the key value store:\n$ consul kv export vault/ # JSON output"
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.12.x/kv/export",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.14.x/kv/export",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.13.x/kv/export",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.11.x/kv/export",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.10.x/kv/export",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.9.x/kv/export",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.18.x/kv",
  "text": "Commands: KV | Consul | HashiCorp Developer\nCommand: consul kv\nThe kv command is used to interact with Consul's KV store via the command line. It exposes top-level commands for inserting, updating, reading, and deleting from the store. This command is available in Consul 0.7.1 and later.\nThe KV store is also accessible via the HTTP API.\nUsage: consul kv <subcommand>\nFor the exact documentation for your Consul version, run consul kv -h to view the complete list of subcommands.\nUsage: consul kv <subcommand> [options] [args] # ... Subcommands: delete Removes data from the KV store export Exports part of the KV tree in JSON format get Retrieves or lists data from the KV store import Imports part of the KV tree in JSON format put Sets or updates data in the KV store \nFor more information, examples, and usage about a subcommand, click on the name of the subcommand in the sidebar or one of the links below:\ndelete\nexport\nget\nimport\nput\nTo create or update the key named \"redis/config/connections\" to the value \"5\" in Consul's KV store:\n$ consul kv put redis/config/connections 5 Success! Data written to: redis/config/connections \nTo read a value back from Consul:\n$ consul kv get redis/config/connections 5 \nOr you can query for detailed information:\n$ consul kv get -detailed redis/config/connections CreateIndex 336 Flags 0 Key redis/config/connections LockIndex 0 ModifyIndex 336 Session - Value 5 \nFinally, deleting a key is just as easy:\n$ consul kv delete redis/config/connections Success! Data deleted at key: redis/config/connections \nFor more examples, ask for subcommand help or view the subcommand documentation by clicking on one of the links in the sidebar."
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.16.x/kv",
  "text": "Commands: KV | Consul | HashiCorp Developer\nCommand: consul kv\nThe kv command is used to interact with Consul's KV store via the command line. It exposes top-level commands for inserting, updating, reading, and deleting from the store. This command is available in Consul 0.7.1 and later.\nThe KV store is also accessible via the HTTP API.\nUsage: consul kv <subcommand>\nFor the exact documentation for your Consul version, run consul kv -h to view the complete list of subcommands.\nUsage: consul kv <subcommand> [options] [args] # ... Subcommands: delete Removes data from the KV store export Exports part of the KV tree in JSON format get Retrieves or lists data from the KV store import Imports part of the KV tree in JSON format put Sets or updates data in the KV store \nFor more information, examples, and usage about a subcommand, click on the name of the subcommand in the sidebar or one of the links below:\ndelete\nexport\nget\nimport\nput\nTo create or update the key named \"redis/config/connections\" to the value \"5\" in Consul's KV store:\n$ consul kv put redis/config/connections 5 Success! Data written to: redis/config/connections \nTo read a value back from Consul:\n$ consul kv get redis/config/connections 5 \nOr you can query for detailed information:\n$ consul kv get -detailed redis/config/connections CreateIndex 336 Flags 0 Key redis/config/connections LockIndex 0 ModifyIndex 336 Session - Value 5 \nFinally, deleting a key is just as easy:\n$ consul kv delete redis/config/connections Success! Data deleted at key: redis/config/connections \nFor more examples, ask for subcommand help or view the subcommand documentation by clicking on one of the links in the sidebar."
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.8.x/kv/export",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.17.x/kv",
  "text": "Commands: KV | Consul | HashiCorp Developer\nCommand: consul kv\nThe kv command is used to interact with Consul's KV store via the command line. It exposes top-level commands for inserting, updating, reading, and deleting from the store. This command is available in Consul 0.7.1 and later.\nThe KV store is also accessible via the HTTP API.\nUsage: consul kv <subcommand>\nFor the exact documentation for your Consul version, run consul kv -h to view the complete list of subcommands.\nUsage: consul kv <subcommand> [options] [args] # ... Subcommands: delete Removes data from the KV store export Exports part of the KV tree in JSON format get Retrieves or lists data from the KV store import Imports part of the KV tree in JSON format put Sets or updates data in the KV store \nFor more information, examples, and usage about a subcommand, click on the name of the subcommand in the sidebar or one of the links below:\ndelete\nexport\nget\nimport\nput\nTo create or update the key named \"redis/config/connections\" to the value \"5\" in Consul's KV store:\n$ consul kv put redis/config/connections 5 Success! Data written to: redis/config/connections \nTo read a value back from Consul:\n$ consul kv get redis/config/connections 5 \nOr you can query for detailed information:\n$ consul kv get -detailed redis/config/connections CreateIndex 336 Flags 0 Key redis/config/connections LockIndex 0 ModifyIndex 336 Session - Value 5 \nFinally, deleting a key is just as easy:\n$ consul kv delete redis/config/connections Success! Data deleted at key: redis/config/connections \nFor more examples, ask for subcommand help or view the subcommand documentation by clicking on one of the links in the sidebar."
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.15.x/kv",
  "text": "Commands: KV | Consul | HashiCorp Developer\nCommand: consul kv\nThe kv command is used to interact with Consul's KV store via the command line. It exposes top-level commands for inserting, updating, reading, and deleting from the store. This command is available in Consul 0.7.1 and later.\nThe KV store is also accessible via the HTTP API.\nUsage: consul kv <subcommand>\nFor the exact documentation for your Consul version, run consul kv -h to view the complete list of subcommands.\nUsage: consul kv <subcommand> [options] [args] # ... Subcommands: delete Removes data from the KV store export Exports part of the KV tree in JSON format get Retrieves or lists data from the KV store import Imports part of the KV tree in JSON format put Sets or updates data in the KV store \nFor more information, examples, and usage about a subcommand, click on the name of the subcommand in the sidebar or one of the links below:\ndelete\nexport\nget\nimport\nput\nTo create or update the key named \"redis/config/connections\" to the value \"5\" in Consul's KV store:\n$ consul kv put redis/config/connections 5 Success! Data written to: redis/config/connections \nTo read a value back from Consul:\n$ consul kv get redis/config/connections 5 \nOr you can query for detailed information:\n$ consul kv get -detailed redis/config/connections CreateIndex 336 Flags 0 Key redis/config/connections LockIndex 0 ModifyIndex 336 Session - Value 5 \nFinally, deleting a key is just as easy:\n$ consul kv delete redis/config/connections Success! Data deleted at key: redis/config/connections \nFor more examples, ask for subcommand help or view the subcommand documentation by clicking on one of the links in the sidebar."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.13.x/acl/acl-rules",
  "text": "Consul | HashiCorp Developer\nThis page does not exist for version v1.13.x.\nPlease select either the most recent version or a valid version that includes the page you are looking for."
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.13.x/kv",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.14.x/kv",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.12.x/kv",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.11.x/kv",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.10.x/kv",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.9.x/kv",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.8.x/kv",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/api-docs/v1.11.x/acl/acl",
  "text": "Consul | HashiCorp Developer\nThis page does not exist for version v1.11.x.\nPlease select either the most recent version or a valid version that includes the page you are looking for."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.11.x/acl/acl-rules",
  "text": "Consul | HashiCorp Developer\nThis page does not exist for version v1.11.x.\nPlease select either the most recent version or a valid version that includes the page you are looking for."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.12.x/acl/acl-rules",
  "text": "Consul | HashiCorp Developer\nThis page does not exist for version v1.12.x.\nPlease select either the most recent version or a valid version that includes the page you are looking for."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/internals/security",
  "text": "We couldn't find the page you're looking for.\nPlease check the url you entered for typos, go back to the page you came from, or go to one of the pages below.\nHashiCorp Cloud Platform\nTerraform\nPacker\nConsul\nVault\nBoundary\nNomad\nWaypoint\nVagrant\nHashiCorp Developer"
},
{
  "url": "https://developer.hashicorp.com/hcp/docs/consul/features",
  "text": "HashiCorp Developer\nWe couldn't find the page you're looking for.\nPlease check the url you entered for typos, go back to the page you came from, or go to one of the pages below.\nHashiCorp Cloud Platform\nTerraform\nPacker\nConsul\nVault\nBoundary\nNomad\nWaypoint\nVagrant\nHashiCorp Developer"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.11.x/ecs/get-started/install",
  "text": "This page does not exist for version v1.11.x."
},
{
  "url": "https://developer.hashicorp.com/consul/tutorials/cloud-get-started/consul-client-virtual-machines",
  "text": "We couldn't find the page you're looking for.\nPlease check the url you entered for typos, go back to the page you came from, or go to one of the pages below.\nHashiCorp Cloud Platform\nTerraform\nPacker\nConsul\nVault\nBoundary\nNomad\nWaypoint\nVagrant\nHashiCorp Developer"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.9.x/k8s/connect/ambassador",
  "text": "Ambassador Integration - Kubernetes | Consul\nAmbassador Integration with Consul Connect\nIn addition to enabling Kubernetes services to discover and securely connect to each other, Connect also can help route traffic into a Kubernetes cluster from outside, when paired with an ingress controller like DataWire's Ambassador.\nAmbassador is a popular Kubernetes-native service that acts as an ingress controller or API gateway. It supports an optional integration with Consul that allows it to route incoming traffic to the proxies for your Connect-enabled services.\nThis means you can have end-to-end encryption from the browser, to Ambassador, to your Kubernetes services.\nBefore you start, install Consul and enable Connect on the agents inside the cluster. Decide whether you will enable service sync or manually register your services with Consul.\nOnce you have tested and verified that everything is working, you can proceed with the Ambassador installation. Full instructions are available on the Ambassador site, but a summary of the steps is as follows:\nIf you are deploying to GKE, create a RoleBinding to grant you cluster admin rights:\nkubectl create clusterrolebinding my-cluster-admin-binding \\ --clusterrole=cluster-admin \\ --user=$(gcloud info --format=\"value(config.account)\") \nInstall Ambassador and a LoadBalancer service for it:\nkubectl apply -f https://www.getambassador.io/yaml/ambassador/ambassador-rbac.yaml kubectl apply -f https://www.getambassador.io/yaml/ambassador/ambassador-service.yaml \nInstall the Ambassador Consul Connector:\nkubectl apply -f https://www.getambassador.io/yaml/consul/ambassador-consul-connector.yaml \nAdd TLSContext and Mapping annotations to your existing services, directing HTTPS traffic to port 20000, which is opened by the Connect proxy. Here is an example of doing this for the static-server example used in the documentation for the Connect sidecar:\napiVersion: v1 kind: Service metadata: name: static-service annotations: getambassador.io/config: | --- apiVersion: ambassador/v1 kind: TLSContext name: ambassador-consul hosts: [] secret: ambassador-consul-connect --- apiVersion: ambassador/v1 kind: Mapping name: static-service_mapping prefix: /echo/ tls: ambassador-consul service: https://static-server:443 spec: type: NodePort selector: app: http-echo ports: - port: 443 name: https-echo targetPort: 20000 \nOnce Ambassador finishes deploying, you should have a new LoadBalancer service with a public-facing IP address. Connecting to the HTTP port on this address should display the output from the static service.\nkubectl describe service ambassador \nThe Ambassador service definition provided in their documentation currently does not serve pages over HTTPS. To enable HTTPS for full end-to-end encryption, follow these steps.\nFirst, upload your public SSL certificate and private key as a Kubernetes secret.\nkubectl create secret tls ambassador-certs --cert=fullchain.pem --key=privkey.pem \nDownload a copy of the ambassador-service.yaml file from Ambassador. Replace the metadata section with one that includes an Ambassador TLS configuration block, using the secret name you created in the previous step. Then add an entry for port 443 to the LoadBalancer spec. Here is a complete example:\napiVersion: v1 kind: Service metadata: name: ambassador annotations: getambassador.io/config: | --- apiVersion: ambassador/v1 kind: Module name: tls config: server: enabled: True secret: ambassador-certs spec: type: LoadBalancer externalTrafficPolicy: Local ports: - port: 80 targetPort: http protocol: TCP name: http - port: 443 targetPort: https protocol: TCP name: https selector: service: ambassador \nUpdate the service definition by applying it with kubectl:\nkubectl apply -f ambassador-service.yaml \nYou should now be able to test the SSL connection from your browser.\nWhen Ambassador is unable to establish an authenticated connection to the Connect proxy servers, browser connections will display this message:\nupstream connect error or disconnect/reset before headers \nThis error can have a number of different causes. Here are some things to check and troubleshooting steps you can take.\nCheck intentions between Ambassador and your upstream service\nIf you followed the above installation guide, Consul should have registered a service called \"ambassador\". Make sure you create an intention to allow it to connect to your own services.\nTo check whether Ambassador is allowed to connect, use the intention check subcommand.\n$ consul intention check ambassador http-echo Allowed \nConfirm upstream proxy sidecar is running\nFirst, find the name of the pod that contains your service.\n$ kubectl get pods -l app=http-echo,role=server NAME READY STATUS RESTARTS AGE http-echo-7fb79566d6-jmccp 2/2 Running 0 1d \nThen describe the pod to make sure that the sidecar is present and running.\n$ kubectl describe pod http-echo-7fb79566d6-jmccp [...] Containers: consul-connect-envoy-sidecar: [...] State: Running Ready: True \nStart up a downstream proxy and try connecting to it\nLog into one of your Consul server pods (or any pod that has a Consul binary in it).\n$ kubectl exec -ti consul-server-0 -- /bin/sh \nOnce inside the pod, try starting a test proxy. Use the name of your service in place of http-echo.\n# consul connect proxy -service=ambassador -upstream http-echo:1234 ==> Consul Connect proxy starting... Configuration mode: Flags Service: http-echo-client Upstream: http-echo => :1234 Public listener: Disabled \nIf the proxy starts successfully, try connecting to it. Verify the output is as you expect.\n# curl localhost:1234 \"hello world\" \nDon't forget to kill the test proxy when you're done.\n# kill %1 ==> Consul Connect proxy shutdown # exit \nCheck Ambassador Connect sidecar logs\nFind the name of the Connect Integration pod and make sure it is running.\n$ kubectl get pods -l app=ambassador-pro,component=consul-connect NAME READY STATUS RESTARTS AGE ambassador-pro-consul-connect-integration-f88fcb99f-hxk75 1/1 Running 0 1d \nDump the logs from the integration pod. If the service is running correctly, there won't be much in there.\n$ kubectl logs ambassador-pro-consul-connect-integration-f88fcb99f-hxk75 time=\"2019-03-13T19:42:12Z\" level=info msg=\"Starting Consul Connect Integration\" consul_host=10.142.0.21 consul_port=8500 version=0.2.3 2019/03/13 19:42:12 Watching CA leaf for ambassador time=\"2019-03-13T19:42:12Z\" level=debug msg=\"Computed kubectl command and arguments\" args=\"[kubectl apply -f -]\" time=\"2019-03-13T19:42:14Z\" level=info msg=\"Updating TLS certificate secret\" namespace= secret=ambassador-consul-connect \nCheck Ambassador logs\nMake sure the Ambassador pod itself is running.\n$ kubectl get pods -l service=ambassador NAME READY STATUS RESTARTS AGE ambassador-655875b5d9-vpc2v 2/2 Running 0 1d \nFinally, check the logs for the main Ambassador pod.\n$ kubectl logs ambassador-655875b5d9-vpc2v \nCheck Ambassador admin interface\nForward the admin port from the Ambassador pod to your local machine.\n$ kubectl port-forward pods/ambassador-655875b5d9-vpc2v 8877:8877 \nYou should then be able to open http://localhost:8877/ambassador/v0/diag/ in your browser and view Ambassador's routing table. The table lists each URL mapping that has been set up. Service names will appear in green if Ambassador believes they are healthy, and red otherwise.\nFrom this interface, you can also enable debug logging via the yellow \"Set Debug On\" button, which might give you a better idea of what's happening when requests fail.\nGetting support\nIf you have tried the above troubleshooting steps and are still stuck, DataWire provides support for Ambassador via the popular Slack chat app. You can request access and then join the #ambassador room to get help."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.9.x/acl",
  "text": "This page does not exist for version v1.9.x."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.9.x/platform/k8s/ambassador",
  "text": "This page does not exist for version v1.9.x."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.8.x/k8s/connect/ambassador",
  "text": "Ambassador Integration - Kubernetes | Consul\nAmbassador Integration with Consul Connect\nIn addition to enabling Kubernetes services to discover and securely connect to each other, Connect also can help route traffic into a Kubernetes cluster from outside, when paired with an ingress controller like DataWire's Ambassador.\nAmbassador is a popular Kubernetes-native service that acts as an ingress controller or API gateway. It supports an optional integration with Consul that allows it to route incoming traffic to the proxies for your Connect-enabled services.\nThis means you can have end-to-end encryption from the browser, to Ambassador, to your Kubernetes services.\nBefore you start, install Consul and enable Connect on the agents inside the cluster. Decide whether you will enable service sync or manually register your services with Consul.\nOnce you have tested and verified that everything is working, you can proceed with the Ambassador installation. Full instructions are available on the Ambassador site, but a summary of the steps is as follows:\nIf you are deploying to GKE, create a RoleBinding to grant you cluster admin rights:\nkubectl create clusterrolebinding my-cluster-admin-binding \\ --clusterrole=cluster-admin \\ --user=$(gcloud info --format=\"value(config.account)\") \nInstall Ambassador and a LoadBalancer service for it:\nkubectl apply -f https://www.getambassador.io/yaml/ambassador/ambassador-rbac.yaml kubectl apply -f https://www.getambassador.io/yaml/ambassador/ambassador-service.yaml \nInstall the Ambassador Consul Connector:\nkubectl apply -f https://www.getambassador.io/yaml/consul/ambassador-consul-connector.yaml \nAdd TLSContext and Mapping annotations to your existing services, directing HTTPS traffic to port 20000, which is opened by the Connect proxy. Here is an example of doing this for the static-server example used in the documentation for the Connect sidecar:\napiVersion: v1 kind: Service metadata: name: static-service annotations: getambassador.io/config: | --- apiVersion: ambassador/v1 kind: TLSContext name: ambassador-consul hosts: [] secret: ambassador-consul-connect --- apiVersion: ambassador/v1 kind: Mapping name: static-service_mapping prefix: /echo/ tls: ambassador-consul service: https://static-server:443 spec: type: NodePort selector: app: http-echo ports: - port: 443 name: https-echo targetPort: 20000 \nOnce Ambassador finishes deploying, you should have a new LoadBalancer service with a public-facing IP address. Connecting to the HTTP port on this address should display the output from the static service.\nkubectl describe service ambassador \nThe Ambassador service definition provided in their documentation currently does not serve pages over HTTPS. To enable HTTPS for full end-to-end encryption, follow these steps.\nFirst, upload your public SSL certificate and private key as a Kubernetes secret.\nkubectl create secret tls ambassador-certs --cert=fullchain.pem --key=privkey.pem \nDownload a copy of the ambassador-service.yaml file from Ambassador. Replace the metadata section with one that includes an Ambassador TLS configuration block, using the secret name you created in the previous step. Then add an entry for port 443 to the LoadBalancer spec. Here is a complete example:\napiVersion: v1 kind: Service metadata: name: ambassador annotations: getambassador.io/config: | --- apiVersion: ambassador/v1 kind: Module name: tls config: server: enabled: True secret: ambassador-certs spec: type: LoadBalancer externalTrafficPolicy: Local ports: - port: 80 targetPort: http protocol: TCP name: http - port: 443 targetPort: https protocol: TCP name: https selector: service: ambassador \nUpdate the service definition by applying it with kubectl:\nkubectl apply -f ambassador-service.yaml \nYou should now be able to test the SSL connection from your browser.\nWhen Ambassador is unable to establish an authenticated connection to the Connect proxy servers, browser connections will display this message:\nupstream connect error or disconnect/reset before headers \nThis error can have a number of different causes. Here are some things to check and troubleshooting steps you can take.\nCheck intentions between Ambassador and your upstream service\nIf you followed the above installation guide, Consul should have registered a service called \"ambassador\". Make sure you create an intention to allow it to connect to your own services.\nTo check whether Ambassador is allowed to connect, use the intention check subcommand.\n$ consul intention check ambassador http-echo Allowed \nConfirm upstream proxy sidecar is running\nFirst, find the name of the pod that contains your service.\n$ kubectl get pods -l app=http-echo,role=server NAME READY STATUS RESTARTS AGE http-echo-7fb79566d6-jmccp 2/2 Running 0 1d \nThen describe the pod to make sure that the sidecar is present and running.\n$ kubectl describe pod http-echo-7fb79566d6-jmccp [...] Containers: consul-connect-envoy-sidecar: [...] State: Running Ready: True \nStart up a downstream proxy and try connecting to it\nLog into one of your Consul server pods (or any pod that has a Consul binary in it).\n$ kubectl exec -ti consul-server-0 -- /bin/sh \nOnce inside the pod, try starting a test proxy. Use the name of your service in place of http-echo.\n# consul connect proxy -service=ambassador -upstream http-echo:1234 ==> Consul Connect proxy starting... Configuration mode: Flags Service: http-echo-client Upstream: http-echo => :1234 Public listener: Disabled \nIf the proxy starts successfully, try connecting to it. Verify the output is as you expect.\n# curl localhost:1234 \"hello world\" \nDon't forget to kill the test proxy when you're done.\n# kill %1 ==> Consul Connect proxy shutdown # exit \nCheck Ambassador Connect sidecar logs\nFind the name of the Connect Integration pod and make sure it is running.\n$ kubectl get pods -l app=ambassador-pro,component=consul-connect NAME READY STATUS RESTARTS AGE ambassador-pro-consul-connect-integration-f88fcb99f-hxk75 1/1 Running 0 1d \nDump the logs from the integration pod. If the service is running correctly, there won't be much in there.\n$ kubectl logs ambassador-pro-consul-connect-integration-f88fcb99f-hxk75 time=\"2019-03-13T19:42:12Z\" level=info msg=\"Starting Consul Connect Integration\" consul_host=10.142.0.21 consul_port=8500 version=0.2.3 2019/03/13 19:42:12 Watching CA leaf for ambassador time=\"2019-03-13T19:42:12Z\" level=debug msg=\"Computed kubectl command and arguments\" args=\"[kubectl apply -f -]\" time=\"2019-03-13T19:42:14Z\" level=info msg=\"Updating TLS certificate secret\" namespace= secret=ambassador-consul-connect \nCheck Ambassador logs\nMake sure the Ambassador pod itself is running.\n$ kubectl get pods -l service=ambassador NAME READY STATUS RESTARTS AGE ambassador-655875b5d9-vpc2v 2/2 Running 0 1d \nFinally, check the logs for the main Ambassador pod.\n$ kubectl logs ambassador-655875b5d9-vpc2v \nCheck Ambassador admin interface\nForward the admin port from the Ambassador pod to your local machine.\n$ kubectl port-forward pods/ambassador-655875b5d9-vpc2v 8877:8877 \nYou should then be able to open http://localhost:8877/ambassador/v0/diag/ in your browser and view Ambassador's routing table. The table lists each URL mapping that has been set up. Service names will appear in green if Ambassador believes they are healthy, and red otherwise.\nFrom this interface, you can also enable debug logging via the yellow \"Set Debug On\" button, which might give you a better idea of what's happening when requests fail.\nGetting support\nIf you have tried the above troubleshooting steps and are still stuck, DataWire provides support for Ambassador via the popular Slack chat app. You can request access and then join the #ambassador room to get help."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.8.x/k8s/ambassador",
  "text": "This page does not exist for version v1.8.x."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.8.x/acl",
  "text": "This page does not exist for version v1.8.x."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/nia/usage/run",
  "text": "Run Consul-Terraform-Sync | Consul | HashiCorp Developer\nThis topic describes the basic procedure for running Consul-Terraform-Sync (CTS). Verify that you have met the basic requirements before attempting to run CTS.\nMove the consul-terraform-sync binary to a location available on your PATH.\n$ mv ~/Downloads/consul-terraform-sync /usr/local/bin/consul-terraform-sync \nCreate the config.hcl file and configure the options for your use case. Refer to the configuration reference for details about all CTS configurations.\nRun Consul-Terraform-Sync (CTS).\n$ consul-terraform-sync start -config-file <config.hcl> \nCheck status of tasks. Replace port number if configured in Step 2. Refer to Consul-Terraform-Sync API for additional information.\n$ curl localhost:8558/status/tasks \nYou can configure CTS for high availability, which is an enterprise capability that ensures that all changes to Consul that occur during a failover transition are processed and that CTS continues to operate as expected.\nYou can start CTS in inspect mode to review and test your configuration before applying any changes. Inspect mode allows you to verify that the changes work as expected before running them in an unsupervised daemon mode.\nFor hands-on instructions on using inspect mode, refer to the Consul-Terraform-Sync Run Modes and Status Inspection tutorial."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.12.x/ecs/get-started/install",
  "text": "This page does not exist for version v1.12.x."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.13.x/nia/usage/requirements",
  "text": "Requirements | Consul | HashiCorp Developer\nThe following components are required to run Consul-Terraform-Sync (CTS):\nA Terraform provider\nA Terraform module\nA Consul cluster running outside of the consul-terraform-sync daemon\nYou can add support for your network infrastructure through Terraform providers so that you can apply Terraform modules to implement network integrations.\nThe following guidance is for running CTS using the Terraform driver. The Terraform Cloud driverEnterprise has additional prerequisites.\nBelow are several steps towards a minimum Consul setup required for running CTS.\nInstall Consul\nCTS is a daemon that runs alongside Consul, similar to other Consul ecosystem tools like Consul Template. CTS is not included with the Consul binary and needs to be installed separately.\nTo install a local Consul agent, refer to the Getting Started: Install Consul Tutorial.\nFor information on compatible Consul versions, refer to the Consul compatibility matrix.\nRun an agent\nThe Consul agent must be running in order to dynamically update network devices. Refer to the Consul agent documentation for information about configuring and starting a Consul agent. For hands-on instructions about running Consul agents, refer to the Getting Started: Run the Consul Agent Tutorial.\nWhen running a Consul agent with CTS in production, consider that CTS uses blocking queries to monitor task dependencies, such as changes to registered services. This results in multiple long-running TCP connections between CTS and the agent to poll changes for each dependency. Consul may quickly reach the agent connection limits if CTS is monitoring a high number of services.\nTo avoid reaching the limit prematurely, we recommend using HTTP/2 (requires HTTPS) to communicate between CTS and the Consul agent. When using HTTP/2, CTS establishes a single connection and reuses it for all communication. Refer to the Consul Configuration section for details. \nAlternatively, you can configure the limits.http_max_conns_per_client option to set a maximimum number of connections to meet your needs. \nRegister services\nCTS monitors the Consul catalog for service changes that lead to downstream changes to your network devices. Without services, your CTS daemon is operational but idle. You can register services with your Consul agent by either loading a service definition or by sending an HTTP API request.\nThe following HTTP API request example registers a service named web with your Consul agent: \n$ echo '{ \"ID\": \"web\", \"Name\": \"web\", \"Address\": \"10.10.10.10\", \"Port\": 8000 }' > payload.json $ curl --request PUT --data @payload.json http://localhost:8500/v1/agent/service/register \nThe example represents a non-existent web service running at 10.10.10.10:8000 that is now available for CTS to consume. \nYou can configure CTS to monitor the web service, execute a task, and update network device(s) by configuring web in the condition \"services\" task block. If the web service has any non-default values, it can also be configured in condition \"services\".\nFor more details on registering a service using the HTTP API endpoint, refer to the register service API docs.\nFor hands-on instructions on registering a service by loading a service definition, refer to the Getting Started: Register a Service with Consul Service Discovery Tutorial.\nRun a cluster\nFor production environments, we recommend operating a Consul cluster rather than a single agent. Refer to Getting Started: Create a Local Consul Datacenter for instructions on starting multiple Consul agents and joining them into a cluster.\nCTS integrations for the Terraform driver use Terraform providers as plugins to interface with specific network infrastructure platforms. The Terraform driver for CTS inherits the expansive collection of Terraform providers to integrate with. You can also specify a provider source in the required_providers configuration to use providers written by the community (requires Terraform 0.13 or later).\nFinding Terraform providers\nTo find providers for the infrastructure platforms you use, browse the providers section of the Terraform Registry.\nHow to create a provider\nIf a Terraform provider does not exist for your environment, you can create a new Terraform provider and publish it to the registry so that you can use it within a network integration task or create a compatible Terraform module. Refer to the following Terraform tutorial and documentation for additional information on creating and publishing providers:\nSetup and Implement Read \nPublishing Providers. \nThe Terraform module for a task in CTS is the core component of the integration. It declares which resources to use and how your infrastructure is dynamically updated. The module, along with how it is configured within a task, determines the conditions under which your infrastructure is updated.\nWorking with a Terraform provider, you can write an integration task for CTS by creating a Terraform module that is compatible with the Terraform driver. You can also use a module built by partners.\nRefer to Configuration for information about configuring CTS and how to use Terraform providers and modules for tasks.\nPartner Terraform Modules\nThe modules listed below are available to use and are compatible with CTS.\nA10 Networks\nDynamic Load Balancing with Group Member Updates: Terraform Registry / GitHub\nAvi Networks\nScale Up and Scale Down Pool and Pool Members (Servers): GitHub\nAWS Application Load Balancer (ALB)\nCreate Listener Rule and Target Group for an AWS ALB, Forward Traffic to Consul Ingress Gateway: Terraform Registry / GitHub\nCheckpoint\nDynamic Firewalling with Address Object Updates: Terraform Registry / GitHub\nCisco ACI\nPolicy Based Redirection: Terraform Registry / GitHub\nCreate and Update Cisco ACI Endpoint Security Groups: Terraform Registry / GitHub\nCitrix ADC\nCreate, Update, and Delete Service Groups in Citrix ADC: Terraform Registry / GitHub\nF5\nDynamic Load Balancing with Pool Member Updates: Terraform Registry / GitHub\nNS1\nCreate, Delete, and Update DNS Records and Zones: Terraform Registry / GitHub\nPalo Alto Networks\nDynamic Address Group (DAG) Tags: Terraform Registry / GitHub\nAddress Group and Dynamic Address Group (DAG) Tags: Terraform Registry / GitHub"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.11.x/nia/usage/requirements",
  "text": "Requirements | Consul | HashiCorp Developer\nThe following components are required to run Consul-Terraform-Sync (CTS):\nA Terraform provider\nA Terraform module\nA Consul cluster running outside of the consul-terraform-sync daemon\nYou can add support for your network infrastructure through Terraform providers so that you can apply Terraform modules to implement network integrations.\nThe following guidance is for running CTS using the Terraform driver. The Terraform Cloud driverEnterprise has additional prerequisites.\nBelow are several steps towards a minimum Consul setup required for running CTS.\nInstall Consul\nCTS is a daemon that runs alongside Consul, similar to other Consul ecosystem tools like Consul Template. CTS is not included with the Consul binary and needs to be installed separately.\nTo install a local Consul agent, refer to the Getting Started: Install Consul Tutorial.\nFor information on compatible Consul versions, refer to the Consul compatibility matrix.\nRun an agent\nThe Consul agent must be running in order to dynamically update network devices. Refer to the Consul agent documentation for information about configuring and starting a Consul agent. For hands-on instructions about running Consul agents, refer to the Getting Started: Run the Consul Agent Tutorial.\nWhen running a Consul agent with CTS in production, consider that CTS uses blocking queries to monitor task dependencies, such as changes to registered services. This results in multiple long-running TCP connections between CTS and the agent to poll changes for each dependency. Consul may quickly reach the agent connection limits if CTS is monitoring a high number of services.\nTo avoid reaching the limit prematurely, we recommend using HTTP/2 (requires HTTPS) to communicate between CTS and the Consul agent. When using HTTP/2, CTS establishes a single connection and reuses it for all communication. Refer to the Consul Configuration section for details. \nAlternatively, you can configure the limits.http_max_conns_per_client option to set a maximimum number of connections to meet your needs. \nRegister services\nCTS monitors the Consul catalog for service changes that lead to downstream changes to your network devices. Without services, your CTS daemon is operational but idle. You can register services with your Consul agent by either loading a service definition or by sending an HTTP API request.\nThe following HTTP API request example registers a service named web with your Consul agent: \n$ echo '{ \"ID\": \"web\", \"Name\": \"web\", \"Address\": \"10.10.10.10\", \"Port\": 8000 }' > payload.json $ curl --request PUT --data @payload.json http://localhost:8500/v1/agent/service/register \nThe example represents a non-existent web service running at 10.10.10.10:8000 that is now available for CTS to consume. \nYou can configure CTS to monitor the web service, execute a task, and update network device(s) by configuring web in the condition \"services\" task block. If the web service has any non-default values, it can also be configured in condition \"services\".\nFor more details on registering a service using the HTTP API endpoint, refer to the register service API docs.\nFor hands-on instructions on registering a service by loading a service definition, refer to the Getting Started: Register a Service with Consul Service Discovery Tutorial.\nRun a cluster\nFor production environments, we recommend operating a Consul cluster rather than a single agent. Refer to Getting Started: Create a Local Consul Datacenter for instructions on starting multiple Consul agents and joining them into a cluster.\nCTS integrations for the Terraform driver use Terraform providers as plugins to interface with specific network infrastructure platforms. The Terraform driver for CTS inherits the expansive collection of Terraform providers to integrate with. You can also specify a provider source in the required_providers configuration to use providers written by the community (requires Terraform 0.13 or later).\nFinding Terraform providers\nTo find providers for the infrastructure platforms you use, browse the providers section of the Terraform Registry.\nHow to create a provider\nIf a Terraform provider does not exist for your environment, you can create a new Terraform provider and publish it to the registry so that you can use it within a network integration task or create a compatible Terraform module. Refer to the following Terraform tutorial and documentation for additional information on creating and publishing providers:\nSetup and Implement Read \nPublishing Providers. \nThe Terraform module for a task in CTS is the core component of the integration. It declares which resources to use and how your infrastructure is dynamically updated. The module, along with how it is configured within a task, determines the conditions under which your infrastructure is updated.\nWorking with a Terraform provider, you can write an integration task for CTS by creating a Terraform module that is compatible with the Terraform driver. You can also use a module built by partners.\nRefer to Configuration for information about configuring CTS and how to use Terraform providers and modules for tasks.\nPartner Terraform Modules\nThe modules listed below are available to use and are compatible with CTS.\nA10 Networks\nDynamic Load Balancing with Group Member Updates: Terraform Registry / GitHub\nAvi Networks\nScale Up and Scale Down Pool and Pool Members (Servers): GitHub\nAWS Application Load Balancer (ALB)\nCreate Listener Rule and Target Group for an AWS ALB, Forward Traffic to Consul Ingress Gateway: Terraform Registry / GitHub\nCheckpoint\nDynamic Firewalling with Address Object Updates: Terraform Registry / GitHub\nCisco ACI\nPolicy Based Redirection: Terraform Registry / GitHub\nCreate and Update Cisco ACI Endpoint Security Groups: Terraform Registry / GitHub\nCitrix ADC\nCreate, Update, and Delete Service Groups in Citrix ADC: Terraform Registry / GitHub\nF5\nDynamic Load Balancing with Pool Member Updates: Terraform Registry / GitHub\nNS1\nCreate, Delete, and Update DNS Records and Zones: Terraform Registry / GitHub\nPalo Alto Networks\nDynamic Address Group (DAG) Tags: Terraform Registry / GitHub\nAddress Group and Dynamic Address Group (DAG) Tags: Terraform Registry / GitHub"
},
{
  "url": "https://developer.hashicorp.com/terraform/docs/registry/modules/publish",
  "text": "We couldn't find the page you're looking for.\nPlease check the url you entered for typos, go back to the page you came from, or go to one of the pages below.\nHashiCorp Cloud Platform\nTerraform\nPacker\nConsul\nVault\nBoundary\nNomad\nWaypoint\nVagrant"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.10.x/connect/l7-traffic/discovery-chain",
  "text": "Discovery Chain | Consul | HashiCorp Developer\n1.6.0+: This feature is available in Consul versions 1.6.0 and newer.\nThis topic is part of a low-level API primarily targeted at developers building external Connect proxy integrations.\nThe service discovery process can be modeled as a \"discovery chain\" which passes through three distinct stages: routing, splitting, and resolution. Each of these stages is controlled by a set of configuration entries. By configuring different phases of the discovery chain a user can control how proxy upstreams are ultimately resolved to specific instances for load balancing.\nNote: The discovery chain is currently only used to discover Connect proxy upstreams.\nThe configuration entries used in the discovery chain are designed to be simple to read and modify for narrowly tailored changes, but at discovery-time the various configuration entries interact in more complex ways. For example:\nIf a service-resolver is created with a service redirect defined, then all references made to the original service in any other configuration entry is replaced with the redirect destination.\nIf a service-resolver is created with a default subset defined then all references made to the original service in any other configuration entry that did not specify a subset will be replaced with the default.\nIf a service-splitter is created with a service split, and the target service has its own service-splitter then the overall effect is flattened and only a single aggregate traffic split is ultimately configured in the proxy.\nservice-resolver redirect loops must be rejected as invalid.\nservice-router and service-splitter configuration entries require an L7 compatible protocol be set for the service via either a service-defaults or proxy-defaults config entry. Violations must be rejected as invalid.\nIf an upstream configuration datacenter parameter is defined then any configuration entry that does not explicitly refer to a desired datacenter should use that value from the upstream.\nTo correctly interpret a collection of configuration entries as a valid discovery chain, we first compile them into a form more directly usable by the layers responsible for configuring Connect sidecar proxies.\nYou can interact with the compiler directly using the discovery-chain API.\nCompilation Parameters\nService Name - The service being discovered by name.\nDatacenter - The datacenter to use as the basis of compilation.\nOverrides - Discovery-time tweaks to apply when compiling. These should be derived from either the proxy or upstream configurations if either are set.\nCompilation Results\nThe response is a single wrapped CompiledDiscoveryChain field:\n{ \"Chain\": {...<CompiledDiscoveryChain>...} } \nCompiledDiscoveryChain\nThe chain encodes a digraph of nodes and targets. Nodes are the compiled representation of various discovery chain stages and targets are instructions on how to use the health API to retrieve relevant service instance lists.\nYou should traverse the nodes starting with StartNode. The nodes can be resolved by name using the Nodes field. Targets can be resolved by name using the Targets field.\nServiceName (string) - The requested service.\nNamespace (string) - The requested namespace.\nDatacenter (string) - The requested datacenter.\nCustomizationHash (string: <optional>) - A unique hash of any overrides that affected the compilation of the discovery chain.\nIf set, this value should be used to prefix/suffix any generated load balancer data plane objects to avoid sharing customized and non-customized versions.\nProtocol (string) - The overall protocol shared by everything in the chain.\nStartNode (string) - The first key into the Nodes map that should be followed when traversing the discovery chain.\nNodes (map<string|DiscoveryGraphNode>) - All nodes available for traversal in the chain keyed by a unique name. You can walk this by starting with StartNode.\nThe names should be treated as opaque values and are only guaranteed to be consistent within a single compilation.\nTargets (map<string|DiscoveryTarget>) - A list of all targets used in this chain.\nThe names should be treated as opaque values and are only guaranteed to be consistent within a single compilation.\nDiscoveryGraphNode\nA single node in the compiled discovery chain.\nType (string) - The type of the node. Valid values are: router, splitter, and resolver.\nName (string) - The unique name of the node.\nRoutes (array<DiscoveryRoute>) - Only set for Type:router. List of routes to render.\nDefinition (ServiceRoute) - Relevant portion of underlying service-router route.\nNextNode (string) - The name of the next node in the chain in Nodes.\nSplits (array<DiscoverySplit>) - Only set for Type:splitter. List of traffic splits.\nWeight (float32) - Copy of underlying service-splitter weight field.\nNextNode (string) - The name of the next node in the chain in Nodes.\nResolver (DiscoveryResolver: <optional>) - Only set for Type:resolver. How to resolve the service instances.\nDefault (bool) - Set to true if no service-resolver config entry is defined for this node and the default was synthesized.\nConnectTimeout (duration) - Copy of the underlying service-resolver ConnectTimeout field. If one is not defined the default of 5s is returned.\nTarget (string) - The name of the target to use found in Targets.\nFailover (DiscoveryFailover: <optional>) - Compiled form of the underlying service-resolver Failover definition to use for this request.\nTargets (array<string>) - List of targets found in Targets to failover to in order of preference.\nLoadBalancer (LoadBalancer: <optional>) - Copy of the underlying service-resolver LoadBalancer field.\nIf a service-splitter splits between services with differing LoadBalancer configuration the first hash-based load balancing policy is copied.\nDiscoveryTarget\nID (string) - The unique name of this target.\nService (string) - The service to query when resolving a list of service instances.\nServiceSubset (string: <optional>) - The subset of the service to resolve.\nNamespace (string) - The namespace to use when resolving a list of service instances.\nDatacenter (string) - The datacenter to use when resolving a list of service instances.\nSubset (ServiceResolverSubset) - Copy of the underlying service-resolver Subsets definition for this target.\nFilter (string: \"\") - The filter expression to be used for selecting instances of the requested service. If empty all healthy instances are returned.\nOnlyPassing (bool: false) - Specifies the behavior of the resolver's health check interpretation. If this is set to false, instances with checks in the passing as well as the warning states will be considered healthy. If this is set to true, only instances with checks in the passing state will be considered healthy.\nMeshGateway (MeshGatewayConfig) - The mesh gateway configuration to use when connecting to this target's service instances.\nMode (string: \"\") - One of none, local, or remote.\nExternal (bool: false) - True if this target is outside of this consul cluster.\nConnectTimeout (duration) - Copy of the underlying service-resolver ConnectTimeout field. If one is not defined the default of 5s is returned.\nSNI (string) - This value should be used as the SNI value when connecting to this set of endpoints over TLS.\nName (string) - The unique name for this target for use when generating load balancer objects. This has a structure similar to SNI, but will not be affected by SNI customizations such as ExternalSNI."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.18.x/security/acl/tokens/create/create-a-service-token",
  "text": "Create tokens for service registration | Consul\nThis topic describes how to create a token that you can use to register a service and discover services in the Consul catalog. If you are using Consul service mesh, a sidecar proxy can use the token to discover and route traffic to other services.\nServices must present a token linked to policies that grant the appropriate set of permissions in order to be discoverable or to interact with other services in a mesh.\nTemplated policies versus custom policies\nYou can create tokens linked to custom policies or to templated policies. Templated policies are constructs in Consul that enable you to quickly grant permissions for common use cases, rather than creating similar policies.\nWe recommend using the builtin/service templated policy to grant permissions for service discovery and service mesh use cases rather than creating a custom policy. This is because the builtin/service templated policy automatically grant the service and its sidecar proxy service:write, service:read, and node:read.\nYour organization may have requirements or processes for deploying services in a way that is inconsistent with the builtin/service templated policy. In these cases, you can create custom policies and link them to tokens.\nCore ACL functionality is available in all versions of Consul.\nThe service token must be linked to policies that grant the following permissions:\nservice:write: Enables the service to update the catalog. If service mesh is enabled, the service's sidecar proxy can also update the catalog. Note that this permission implicitly grants intention:read permission to sidecar proxies so that they can read and enforce intentions. Refer to Intention Management Permissions for details.\nservice:read: Enables the service to learn about other services in the network. If service mesh is enabled, the service's sidecar proxy can also learn about other services in the network.\nnode:read: Enables the sidecar proxy to discover and route traffic to other services in the catalog if service mesh is enabled.\nAuthentication\nYou must provide an ACL token linked to a policy with acl:write permissions to create and modify ACL tokens and policies using the CLI or API.\nYou can provide the token manually using the -token option on the command line, but we recommend setting the CONSUL_HTTP_TOKEN environment variable to simplify your workflow:\n$ export CONSUL_HTTP_TOKEN=<acl-token-secret-id> \nThe Consul CLI automatically reads the CONSUL_HTTP_TOKEN environment variable so that you do not have to pass the token to every Consul CLI command.\nTo authenticate calls to the Consul HTTP API, you must provide the token in the X-Consul-Token header for each call:\n$ curl --header \"X-Consul-Token: $CONSUL_HTTP_TOKEN\" ... \nTo learn about alternative ways to authenticate, refer to the following documentation:\nCLI Authentication\nAPI Authentication\nRefer to Templated policies for information about creating templated policies that you can link to tokens.\nYou can manually create tokens using the Consul command line or API endpoint. You can also enable Consul to dynamically create tokens from trusted external systems using an auth method.\nRun the consul acl token create command and specify a templated policy to link to create a token. Refer to Consul ACL Token Create for details about the consul acl token create command.\nThe following example creates an ACL token linked to the builtin/service templated policy for a service named svc1.\n$ consul acl token create \\ -description \"Service token for svc1\" \\ -templated-policy \"builtin/service\" \\ -var \"name:api\" \nRefer to Templated policies for information about the use of templated policies in tokens and roles.\nYou can manually create tokens using the Consul command line or API endpoint. You can also enable Consul to dynamically create tokens from trusted external systems using an auth method.\nRun the consul acl token create command and specify the policy or templated policy to link to create a token. Refer to Consul ACL Token Create for details about the consul acl token create command.\nYou can specify an admin partition, namespace, or both when creating tokens in Consul Enterprise. The token can only include permissions in the specified scope, if any. The following example creates an ACL token that the service can use to register in the ns1 namespace of partition ptn1:\n$ consul acl token create -partition \"ptn1\" -namespace \"ns1\" \\ -description \"Service token for svc1\" \\ -templated-policy \"builtin/service\" -var \"name:svc1\" \nWhen you are unable to link tokens to a templated policy, you can define policies, register them with Consul, and link the policies to tokens that enable services to register into the Consul catalog.\nDefine a policy\nYou can send policy definitions as command line or API arguments or define them in an external HCL or JSON file. Refer to ACL Rules for details about all of the rules you can use in your policies.\nThe following example policy is defined in a file. The policy grants the svc1 service write permissions so that it can register into the catalog. For service mesh, the policy grants the svc1-sidecar-proxy service write permissions so that the sidecar proxy can register into the catalog. It grants service and node read permissions to discover and route to other services.\nservice \"svc1\" { policy = \"write\" } service \"svc1-sidecar-proxy\" { policy = \"write\" } service_prefix \"\" { policy = \"read\" } node_prefix \"\" { policy = \"read\" } \nRegister the policy with Consul\nAfter defining the policies, you can register them with Consul using the command line or API endpoint.\nRun the consul acl policy create command and specify the policy rules to create a policy. The following example registers a policy defined in svc1-register.hcl:\n$ consul acl policy create \\ -name \"svc1-register\" -rules @svc1-register.hcl \\ -description \"Allow svc1 to register into the catalog\" \nRefer to Consul ACL Policy Create for details about the consul acl token create command.\nLink the policy to a token\nAfter registering the policies into Consul, you can create and link tokens using the Consul command line or API endpoint. You can also enable Consul to dynamically create tokens from trusted external systems using an auth method.\nRun the consul acl token create command and specify the policy name or ID to create a token linked to the policy. Refer to Consul ACL Token Create for details about the consul acl token create command.\nThe following commands create the ACL token linked to the policy svc1-register.\n$ consul acl token create \\ -description \"Service token for svc1\" \\ -policy-name \"svc1-register\" \nWhen you are unable to link tokens to a templated policy, you can define policies, register them with Consul, and link the policies to tokens that enable services to register into the Consul catalog.\nDefine a policy\nYou can send policy definitions as command line or API arguments or define them in an external HCL or JSON file. Refer to ACL Rules for details about all of the rules you can use in your policies.\nYou can specify an admin partition and namespace when creating policies in Consul Enterprise. The policy is only valid in the specified scopes.\nThe following example policy is defined in a file. The policy allows the svc1 service to register in the ns1 namespace of partition ptn1. For service mesh, the policy grants the svc1-sidecar-proxy service write permissions so that the sidecar proxy can register into the catalog. It grants service and node read permissions to discover and route to other services.\npartition \"ptn1\" { namespace \"ns1\" { service \"svc1\" { policy = \"write\" } service \"svc1-sidecar-proxy\" { policy = \"write\" } service_prefix \"\" { policy = \"read\" } node_prefix \"\" { policy = \"read\" } } } \nRegister the policy with Consul\nAfter defining the policies, you can register them with Consul using the command line or API endpoint.\nRun the consul acl policy create command and specify the policy rules to create a policy. The following example registers a policy defined in svc1-register.hcl:\n$ consul acl policy create -partition \"ptn1\" -namespace \"ns1\" \\ -name \"svc1-register\" -rules @svc1-register.hcl \\ -description \"Custom policy for service svc1\" \nRefer to Consul ACL Policy Create for details about the consul acl token create command.\nLink the policy to a token\nAfter registering the policies into Consul, you can create and link tokens using the Consul command line or API endpoint. You can also enable Consul to dynamically create tokens from trusted external systems using an auth method.\nRun the consul acl token create command and specify the policy name or ID to create a token linked to the policy. Refer to Consul ACL Token Create for details about the consul acl token create command.\nYou can specify an admin partition and namespace when creating tokens in Consul Enterprise. The token is only valid in the specified scopes. The following example creates an ACL token that the service can use to register in the ns1 namespace of partition ptn1:\nThe following commands create the ACL token linked to the policy svc1-register.\n$ consul acl token create -partition \"ptn1\" -namespace \"ns1\" \\ -description \"Service token for svc1\" \\ -policy-name \"svc1-register\""
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.9.x/connect/l7-traffic/discovery-chain",
  "text": "Discovery Chain | Consul | HashiCorp Developer\n1.6.0+: This feature is available in Consul versions 1.6.0 and newer.\nThis topic is part of a low-level API primarily targeted at developers building external Connect proxy integrations.\nThe service discovery process can be modeled as a \"discovery chain\" which passes through three distinct stages: routing, splitting, and resolution. Each of these stages is controlled by a set of configuration entries. By configuring different phases of the discovery chain a user can control how proxy upstreams are ultimately resolved to specific instances for load balancing.\nNote: The discovery chain is currently only used to discover Connect proxy upstreams.\nThe configuration entries used in the discovery chain are designed to be simple to read and modify for narrowly tailored changes, but at discovery-time the various configuration entries interact in more complex ways. For example:\nIf a service-resolver is created with a service redirect defined, then all references made to the original service in any other configuration entry is replaced with the redirect destination.\nIf a service-resolver is created with a default subset defined then all references made to the original service in any other configuration entry that did not specify a subset will be replaced with the default.\nIf a service-splitter is created with a service split, and the target service has its own service-splitter then the overall effect is flattened and only a single aggregate traffic split is ultimately configured in the proxy.\nservice-resolver redirect loops must be rejected as invalid.\nservice-router and service-splitter configuration entries require an L7 compatible protocol be set for the service via either a service-defaults or proxy-defaults config entry. Violations must be rejected as invalid.\nIf an upstream configuration datacenter parameter is defined then any configuration entry that does not explicitly refer to a desired datacenter should use that value from the upstream.\nTo correctly interpret a collection of configuration entries as a valid discovery chain, we first compile them into a form more directly usable by the layers responsible for configuring Connect sidecar proxies.\nYou can interact with the compiler directly using the discovery-chain API.\nCompilation Parameters\nService Name - The service being discovered by name.\nDatacenter - The datacenter to use as the basis of compilation.\nOverrides - Discovery-time tweaks to apply when compiling. These should be derived from either the proxy or upstream configurations if either are set.\nCompilation Results\nThe response is a single wrapped CompiledDiscoveryChain field:\n{ \"Chain\": {...<CompiledDiscoveryChain>...} } \nCompiledDiscoveryChain\nThe chain encodes a digraph of nodes and targets. Nodes are the compiled representation of various discovery chain stages and targets are instructions on how to use the health API to retrieve relevant service instance lists.\nYou should traverse the nodes starting with StartNode. The nodes can be resolved by name using the Nodes field. Targets can be resolved by name using the Targets field.\nServiceName (string) - The requested service.\nNamespace (string) - The requested namespace.\nDatacenter (string) - The requested datacenter.\nCustomizationHash (string: <optional>) - A unique hash of any overrides that affected the compilation of the discovery chain.\nIf set, this value should be used to prefix/suffix any generated load balancer data plane objects to avoid sharing customized and non-customized versions.\nProtocol (string) - The overall protocol shared by everything in the chain.\nStartNode (string) - The first key into the Nodes map that should be followed when traversing the discovery chain.\nNodes (map<string|DiscoveryGraphNode>) - All nodes available for traversal in the chain keyed by a unique name. You can walk this by starting with StartNode.\nThe names should be treated as opaque values and are only guaranteed to be consistent within a single compilation.\nTargets (map<string|DiscoveryTarget>) - A list of all targets used in this chain.\nThe names should be treated as opaque values and are only guaranteed to be consistent within a single compilation.\nDiscoveryGraphNode\nA single node in the compiled discovery chain.\nType (string) - The type of the node. Valid values are: router, splitter, and resolver.\nName (string) - The unique name of the node.\nRoutes (array<DiscoveryRoute>) - Only set for Type:router. List of routes to render.\nDefinition (ServiceRoute) - Relevant portion of underlying service-router route.\nNextNode (string) - The name of the next node in the chain in Nodes.\nSplits (array<DiscoverySplit>) - Only set for Type:splitter. List of traffic splits.\nWeight (float32) - Copy of underlying service-splitter weight field.\nNextNode (string) - The name of the next node in the chain in Nodes.\nResolver (DiscoveryResolver: <optional>) - Only set for Type:resolver. How to resolve the service instances.\nDefault (bool) - Set to true if no service-resolver config entry is defined for this node and the default was synthesized.\nConnectTimeout (duration) - Copy of the underlying service-resolver ConnectTimeout field. If one is not defined the default of 5s is returned.\nTarget (string) - The name of the target to use found in Targets.\nFailover (DiscoveryFailover: <optional>) - Compiled form of the underlying service-resolver Failover definition to use for this request.\nTargets (array<string>) - List of targets found in Targets to failover to in order of preference.\nLoadBalancer (LoadBalancer: <optional>) - Copy of the underlying service-resolver LoadBalancer field.\nIf a service-splitter splits between services with differing LoadBalancer configuration the first hash-based load balancing policy is copied.\nDiscoveryTarget\nID (string) - The unique name of this target.\nService (string) - The service to query when resolving a list of service instances.\nServiceSubset (string: <optional>) - The subset of the service to resolve.\nNamespace (string) - The namespace to use when resolving a list of service instances.\nDatacenter (string) - The datacenter to use when resolving a list of service instances.\nSubset (ServiceResolverSubset) - Copy of the underlying service-resolver Subsets definition for this target.\nFilter (string: \"\") - The filter expression to be used for selecting instances of the requested service. If empty all healthy instances are returned.\nOnlyPassing (bool: false) - Specifies the behavior of the resolver's health check interpretation. If this is set to false, instances with checks in the passing as well as the warning states will be considered healthy. If this is set to true, only instances with checks in the passing state will be considered healthy.\nMeshGateway (MeshGatewayConfig) - The mesh gateway configuration to use when connecting to this target's service instances.\nMode (string: \"\") - One of none, local, or remote.\nExternal (bool: false) - True if this target is outside of this consul cluster.\nSNI (string) - This value should be used as the SNI value when connecting to this set of endpoints over TLS.\nName (string) - The unique name for this target for use when generating load balancer objects. This has a structure similar to SNI, but will not be affected by SNI customizations such as ExternalSNI."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.12.x/nia/usage/requirements",
  "text": "Requirements | Consul | HashiCorp Developer\nThe following components are required to run Consul-Terraform-Sync (CTS):\nA Terraform provider\nA Terraform module\nA Consul cluster running outside of the consul-terraform-sync daemon\nYou can add support for your network infrastructure through Terraform providers so that you can apply Terraform modules to implement network integrations.\nThe following guidance is for running CTS using the Terraform driver. The Terraform Cloud driverEnterprise has additional prerequisites.\nBelow are several steps towards a minimum Consul setup required for running CTS.\nInstall Consul\nCTS is a daemon that runs alongside Consul, similar to other Consul ecosystem tools like Consul Template. CTS is not included with the Consul binary and needs to be installed separately.\nTo install a local Consul agent, refer to the Getting Started: Install Consul Tutorial.\nFor information on compatible Consul versions, refer to the Consul compatibility matrix.\nRun an agent\nThe Consul agent must be running in order to dynamically update network devices. Refer to the Consul agent documentation for information about configuring and starting a Consul agent. For hands-on instructions about running Consul agents, refer to the Getting Started: Run the Consul Agent Tutorial.\nWhen running a Consul agent with CTS in production, consider that CTS uses blocking queries to monitor task dependencies, such as changes to registered services. This results in multiple long-running TCP connections between CTS and the agent to poll changes for each dependency. Consul may quickly reach the agent connection limits if CTS is monitoring a high number of services.\nTo avoid reaching the limit prematurely, we recommend using HTTP/2 (requires HTTPS) to communicate between CTS and the Consul agent. When using HTTP/2, CTS establishes a single connection and reuses it for all communication. Refer to the Consul Configuration section for details. \nAlternatively, you can configure the limits.http_max_conns_per_client option to set a maximimum number of connections to meet your needs. \nRegister services\nCTS monitors the Consul catalog for service changes that lead to downstream changes to your network devices. Without services, your CTS daemon is operational but idle. You can register services with your Consul agent by either loading a service definition or by sending an HTTP API request.\nThe following HTTP API request example registers a service named web with your Consul agent: \n$ echo '{ \"ID\": \"web\", \"Name\": \"web\", \"Address\": \"10.10.10.10\", \"Port\": 8000 }' > payload.json $ curl --request PUT --data @payload.json http://localhost:8500/v1/agent/service/register \nThe example represents a non-existent web service running at 10.10.10.10:8000 that is now available for CTS to consume. \nYou can configure CTS to monitor the web service, execute a task, and update network device(s) by configuring web in the condition \"services\" task block. If the web service has any non-default values, it can also be configured in condition \"services\".\nFor more details on registering a service using the HTTP API endpoint, refer to the register service API docs.\nFor hands-on instructions on registering a service by loading a service definition, refer to the Getting Started: Register a Service with Consul Service Discovery Tutorial.\nRun a cluster\nFor production environments, we recommend operating a Consul cluster rather than a single agent. Refer to Getting Started: Create a Local Consul Datacenter for instructions on starting multiple Consul agents and joining them into a cluster.\nCTS integrations for the Terraform driver use Terraform providers as plugins to interface with specific network infrastructure platforms. The Terraform driver for CTS inherits the expansive collection of Terraform providers to integrate with. You can also specify a provider source in the required_providers configuration to use providers written by the community (requires Terraform 0.13 or later).\nFinding Terraform providers\nTo find providers for the infrastructure platforms you use, browse the providers section of the Terraform Registry.\nHow to create a provider\nIf a Terraform provider does not exist for your environment, you can create a new Terraform provider and publish it to the registry so that you can use it within a network integration task or create a compatible Terraform module. Refer to the following Terraform tutorial and documentation for additional information on creating and publishing providers:\nSetup and Implement Read \nPublishing Providers. \nThe Terraform module for a task in CTS is the core component of the integration. It declares which resources to use and how your infrastructure is dynamically updated. The module, along with how it is configured within a task, determines the conditions under which your infrastructure is updated.\nWorking with a Terraform provider, you can write an integration task for CTS by creating a Terraform module that is compatible with the Terraform driver. You can also use a module built by partners.\nRefer to Configuration for information about configuring CTS and how to use Terraform providers and modules for tasks.\nPartner Terraform Modules\nThe modules listed below are available to use and are compatible with CTS.\nA10 Networks\nDynamic Load Balancing with Group Member Updates: Terraform Registry / GitHub\nAvi Networks\nScale Up and Scale Down Pool and Pool Members (Servers): GitHub\nAWS Application Load Balancer (ALB)\nCreate Listener Rule and Target Group for an AWS ALB, Forward Traffic to Consul Ingress Gateway: Terraform Registry / GitHub\nCheckpoint\nDynamic Firewalling with Address Object Updates: Terraform Registry / GitHub\nCisco ACI\nPolicy Based Redirection: Terraform Registry / GitHub\nCreate and Update Cisco ACI Endpoint Security Groups: Terraform Registry / GitHub\nCitrix ADC\nCreate, Update, and Delete Service Groups in Citrix ADC: Terraform Registry / GitHub\nF5\nDynamic Load Balancing with Pool Member Updates: Terraform Registry / GitHub\nNS1\nCreate, Delete, and Update DNS Records and Zones: Terraform Registry / GitHub\nPalo Alto Networks\nDynamic Address Group (DAG) Tags: Terraform Registry / GitHub\nAddress Group and Dynamic Address Group (DAG) Tags: Terraform Registry / GitHub"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.17.x/security/acl/tokens/create/create-a-service-token",
  "text": "Create tokens for service registration | Consul\nThis topic describes how to create a token that you can use to register a service and discover services in the Consul catalog. If you are using Consul service mesh, a sidecar proxy can use the token to discover and route traffic to other services.\nServices must present a token linked to policies that grant the appropriate set of permissions in order to be discoverable or to interact with other services in a mesh.\nTemplated policies versus custom policies\nYou can create tokens linked to custom policies or to templated policies. Templated policies are constructs in Consul that enable you to quickly grant permissions for common use cases, rather than creating similar policies.\nWe recommend using the builtin/service templated policy to grant permissions for service discovery and service mesh use cases rather than creating a custom policy. This is because the builtin/service templated policy automatically grant the service and its sidecar proxy service:write, service:read, and node:read.\nYour organization may have requirements or processes for deploying services in a way that is inconsistent with the builtin/service templated policy. In these cases, you can create custom policies and link them to tokens.\nCore ACL functionality is available in all versions of Consul.\nThe service token must be linked to policies that grant the following permissions:\nservice:write: Enables the service to update the catalog. If service mesh is enabled, the service's sidecar proxy can also update the catalog. Note that this permission implicitly grants intention:read permission to sidecar proxies so that they can read and enforce intentions. Refer to Intention Management Permissions for details.\nservice:read: Enables the service to learn about other services in the network. If service mesh is enabled, the service's sidecar proxy can also learn about other services in the network.\nnode:read: Enables the sidecar proxy to discover and route traffic to other services in the catalog if service mesh is enabled.\nAuthentication\nYou must provide an ACL token linked to a policy with acl:write permissions to create and modify ACL tokens and policies using the CLI or API.\nYou can provide the token manually using the -token option on the command line, but we recommend setting the CONSUL_HTTP_TOKEN environment variable to simplify your workflow:\n$ export CONSUL_HTTP_TOKEN=<acl-token-secret-id> \nThe Consul CLI automatically reads the CONSUL_HTTP_TOKEN environment variable so that you do not have to pass the token to every Consul CLI command.\nTo authenticate calls to the Consul HTTP API, you must provide the token in the X-Consul-Token header for each call:\n$ curl --header \"X-Consul-Token: $CONSUL_HTTP_TOKEN\" ... \nTo learn about alternative ways to authenticate, refer to the following documentation:\nCLI Authentication\nAPI Authentication\nRefer to Templated policies for information about creating templated policies that you can link to tokens.\nYou can manually create tokens using the Consul command line or API endpoint. You can also enable Consul to dynamically create tokens from trusted external systems using an auth method.\nRun the consul acl token create command and specify a templated policy to link to create a token. Refer to Consul ACL Token Create for details about the consul acl token create command.\nThe following example creates an ACL token linked to the builtin/service templated policy for a service named svc1.\n$ consul acl token create \\ -description \"Service token for svc1\" \\ -templated-policy \"builtin/service\" \\ -var \"name:api\" \nRefer to Templated policies for information about the use of templated policies in tokens and roles.\nYou can manually create tokens using the Consul command line or API endpoint. You can also enable Consul to dynamically create tokens from trusted external systems using an auth method.\nRun the consul acl token create command and specify the policy or templated policy to link to create a token. Refer to Consul ACL Token Create for details about the consul acl token create command.\nYou can specify an admin partition, namespace, or both when creating tokens in Consul Enterprise. The token can only include permissions in the specified scope, if any. The following example creates an ACL token that the service can use to register in the ns1 namespace of partition ptn1:\n$ consul acl token create -partition \"ptn1\" -namespace \"ns1\" \\ -description \"Service token for svc1\" \\ -templated-policy \"builtin/service\" -var \"name:svc1\" \nWhen you are unable to link tokens to a templated policy, you can define policies, register them with Consul, and link the policies to tokens that enable services to register into the Consul catalog.\nDefine a policy\nYou can send policy definitions as command line or API arguments or define them in an external HCL or JSON file. Refer to ACL Rules for details about all of the rules you can use in your policies.\nThe following example policy is defined in a file. The policy grants the svc1 service write permissions so that it can register into the catalog. For service mesh, the policy grants the svc1-sidecar-proxy service write permissions so that the sidecar proxy can register into the catalog. It grants service and node read permissions to discover and route to other services.\nservice \"svc1\" { policy = \"write\" } service \"svc1-sidecar-proxy\" { policy = \"write\" } service_prefix \"\" { policy = \"read\" } node_prefix \"\" { policy = \"read\" } \nRegister the policy with Consul\nAfter defining the policies, you can register them with Consul using the command line or API endpoint.\nRun the consul acl policy create command and specify the policy rules to create a policy. The following example registers a policy defined in svc1-register.hcl:\n$ consul acl policy create \\ -name \"svc1-register\" -rules @svc1-register.hcl \\ -description \"Allow svc1 to register into the catalog\" \nRefer to Consul ACL Policy Create for details about the consul acl token create command.\nLink the policy to a token\nAfter registering the policies into Consul, you can create and link tokens using the Consul command line or API endpoint. You can also enable Consul to dynamically create tokens from trusted external systems using an auth method.\nRun the consul acl token create command and specify the policy name or ID to create a token linked to the policy. Refer to Consul ACL Token Create for details about the consul acl token create command.\nThe following commands create the ACL token linked to the policy svc1-register.\n$ consul acl token create \\ -description \"Service token for svc1\" \\ -policy-name \"svc1-register\" \nWhen you are unable to link tokens to a templated policy, you can define policies, register them with Consul, and link the policies to tokens that enable services to register into the Consul catalog.\nDefine a policy\nYou can send policy definitions as command line or API arguments or define them in an external HCL or JSON file. Refer to ACL Rules for details about all of the rules you can use in your policies.\nYou can specify an admin partition and namespace when creating policies in Consul Enterprise. The policy is only valid in the specified scopes.\nThe following example policy is defined in a file. The policy allows the svc1 service to register in the ns1 namespace of partition ptn1. For service mesh, the policy grants the svc1-sidecar-proxy service write permissions so that the sidecar proxy can register into the catalog. It grants service and node read permissions to discover and route to other services.\npartition \"ptn1\" { namespace \"ns1\" { service \"svc1\" { policy = \"write\" } service \"svc1-sidecar-proxy\" { policy = \"write\" } service_prefix \"\" { policy = \"read\" } node_prefix \"\" { policy = \"read\" } } } \nRegister the policy with Consul\nAfter defining the policies, you can register them with Consul using the command line or API endpoint.\nRun the consul acl policy create command and specify the policy rules to create a policy. The following example registers a policy defined in svc1-register.hcl:\n$ consul acl policy create -partition \"ptn1\" -namespace \"ns1\" \\ -name \"svc1-register\" -rules @svc1-register.hcl \\ -description \"Custom policy for service svc1\" \nRefer to Consul ACL Policy Create for details about the consul acl token create command.\nLink the policy to a token\nAfter registering the policies into Consul, you can create and link tokens using the Consul command line or API endpoint. You can also enable Consul to dynamically create tokens from trusted external systems using an auth method.\nRun the consul acl token create command and specify the policy name or ID to create a token linked to the policy. Refer to Consul ACL Token Create for details about the consul acl token create command.\nYou can specify an admin partition and namespace when creating tokens in Consul Enterprise. The token is only valid in the specified scopes. The following example creates an ACL token that the service can use to register in the ns1 namespace of partition ptn1:\nThe following commands create the ACL token linked to the policy svc1-register.\n$ consul acl token create -partition \"ptn1\" -namespace \"ns1\" \\ -description \"Service token for svc1\" \\ -policy-name \"svc1-register\""
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.16.x/security/acl/tokens/create/create-a-service-token",
  "text": "Create tokens for service registration | Consul\nThis topic describes how to create a token that you can use to register a service and discover services in the Consul catalog. If you are using Consul service mesh, a sidecar proxy can use the token to discover and route traffic to other services.\nServices must present a token linked to policies that grant the appropriate set of permissions in order to be discoverable or to interact with other services in a mesh.\nService identities versus custom policies\nYou can create tokens linked to custom policies or to service identities. Service identities are constructs in Consul that enable you to quickly grant permissions for a group of services, rather than creating similar policies for each service.\nWe recommend using a service identity to grant permissions for service discovery and service mesh use cases rather than creating a custom policy. This is because service identities automatically grant the service and its sidecar proxy service:write, service:read, and node:read.\nYour organization may have requirements or processes for deploying services in a way that is inconsistent with service and node identities. In these cases, you can create custom policies and link them to tokens.\nCore ACL functionality is available in all versions of Consul.\nThe service token must be linked to policies that grant the following permissions:\nservice:write: Enables the service to update the catalog. If service mesh is enabled, the service's sidecar proxy can also update the catalog. Note that this permission implicitly grants intention:read permission to sidecar proxies so that they can read and enforce intentions. Refer to Intention Management Permissions for details.\nservice:read: Enables the service to learn about other services in the network. If service mesh is enabled, the service's sidecar proxy can also learn about other services in the network.\nnode:read: Enables the sidecar proxy to discover and route traffic to other services in the catalog if service mesh is enabled.\nAuthentication\nYou must provide an ACL token linked to a policy with acl:write permissions to create and modify ACL tokens and policies using the CLI or API.\nYou can provide the token manually using the -token option on the command line, but we recommend setting the CONSUL_HTTP_TOKEN environment variable to simplify your workflow:\n$ export CONSUL_HTTP_TOKEN=<acl-token-secret-id> \nThe Consul CLI automatically reads the CONSUL_HTTP_TOKEN environment variable so that you do not have to pass the token to every Consul CLI command.\nTo authenticate calls to the Consul HTTP API, you must provide the token in the X-Consul-Token header for each call:\n$ curl --header \"X-Consul-Token: $CONSUL_HTTP_TOKEN\" ... \nTo learn about alternative ways to authenticate, refer to the following documentation:\nCLI Authentication\nAPI Authentication\nRefer to Service identities for information about creating service identities that you can link to tokens.\nRun the consul acl token create command and specify the policy or service identity to link to create a token. Refer to Consul ACL Token Create for details about the consul acl token create command.\nThe following example creates an ACL token linked to a service identity for a service named svc1.\n$ consul acl token create \\ -description \"Service token for svc1\" \\ -service-identity \"svc1\" \nRefer to Service identities for information about creating service identities that you can link to tokens.\nRun the consul acl token create command and specify the policy or service identity to link to create a token. Refer to Consul ACL Token Create for details about the consul acl token create command.\nYou can specify an admin partition, namespace, or both when creating tokens in Consul Enterprise. The token can only include permissions in the specified scope, if any. The following example creates an ACL token that the service can use to register in the ns1 namespace of partition ptn1:\n$ consul acl token create -partition \"ptn1\" -namespace \"ns1\" \\ -description \"Service token for svc1\" \\ -service-identity \"svc1\" \nWhen you are unable to link tokens to a service identity, you can define policies, register them with Consul, and link the policies to tokens that enable services to register into the Consul catalog.\nThe following example policy is defined in a file. The policy grants the svc1 service write permissions so that it can register into the catalog. For service mesh, the policy grants the svc1-sidecar-proxy service write permissions so that the sidecar proxy can register into the catalog. It grants service and node read permissions to discover and route to other services.\nservice \"svc1\" { policy = \"write\" } service \"svc1-sidecar-proxy\" { policy = \"write\" } service_prefix \"\" { policy = \"read\" } node_prefix \"\" { policy = \"read\" } \n$ consul acl policy create \\ -name \"svc1-register\" -rules @svc1-register.hcl \\ -description \"Allow svc1 to register into the catalog\" \n$ consul acl token create \\ -description \"Service token for svc1\" \\ -policy-name \"svc1-register\" \nWhen you are unable to link tokens to a service identity, you can define policies, register them with Consul, and link the policies to tokens that enable services to register into the Consul catalog.\nYou can specify an admin partition and namespace when creating policies in Consul Enterprise. The policy is only valid in the specified scopes.\nThe following example policy is defined in a file. The policy allows the svc1 service to register in the ns1 namespace of partition ptn1. For service mesh, the policy grants the svc1-sidecar-proxy service write permissions so that the sidecar proxy can register into the catalog. It grants service and node read permissions to discover and route to other services.\npartition \"ptn1\" { namespace \"ns1\" { service \"svc1\" { policy = \"write\" } service \"svc1-sidecar-proxy\" { policy = \"write\" } service_prefix \"\" { policy = \"read\" } node_prefix \"\" { policy = \"read\" } } } \n$ consul acl policy create -partition \"ptn1\" -namespace \"ns1\" \\ -name \"svc1-register\" -rules @svc1-register.hcl \\ -description \"Custom policy for service svc1\" \nYou can specify an admin partition and namespace when creating tokens in Consul Enterprise. The token is only valid in the specified scopes. The following example creates an ACL token that the service can use to register in the ns1 namespace of partition ptn1:\n$ consul acl token create -partition \"ptn1\" -namespace \"ns1\" \\ -description \"Service token for svc1\" \\ -policy-name \"svc1-register\""
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.15.x/security/acl/tokens/create/create-a-service-token",
  "text": "Create tokens for service registration | Consul\nThis topic describes how to create a token that you can use to register a service and discover services in the Consul catalog. If you are using Consul service mesh, a sidecar proxy can use the token to discover and route traffic to other services.\nServices must present a token linked to policies that grant the appropriate set of permissions in order to be discoverable or to interact with other services in a mesh.\nService identities versus custom policies\nYou can create tokens linked to custom policies or to service identities. Service identities are constructs in Consul that enable you to quickly grant permissions for a group of services, rather than creating similar policies for each service.\nWe recommend using a service identity to grant permissions for service discovery and service mesh use cases rather than creating a custom policy. This is because service identities automatically grant the service and its sidecar proxy service:write, service:read, and node:read.\nYour organization may have requirements or processes for deploying services in a way that is inconsistent with service and node identities. In these cases, you can create custom policies and link them to tokens.\nCore ACL functionality is available in all versions of Consul.\nThe service token must be linked to policies that grant the following permissions:\nservice:write: Enables the service to update the catalog. If service mesh is enabled, the service's sidecar proxy can also update the catalog. Note that this permission implicitly grants intention:read permission to sidecar proxies so that they can read and enforce intentions. Refer to Intention Management Permissions for details.\nservice:read: Enables the service to learn about other services in the network. If service mesh is enabled, the service's sidecar proxy can also learn about other services in the network.\nnode:read: Enables the sidecar proxy to discover and route traffic to other services in the catalog if service mesh is enabled.\nAuthentication\nYou must provide an ACL token linked to a policy with acl:write permissions to create and modify ACL tokens and policies using the CLI or API.\nYou can provide the token manually using the -token option on the command line, but we recommend setting the CONSUL_HTTP_TOKEN environment variable to simplify your workflow:\n$ export CONSUL_HTTP_TOKEN=<acl-token-secret-id> \nThe Consul CLI automatically reads the CONSUL_HTTP_TOKEN environment variable so that you do not have to pass the token to every Consul CLI command.\nTo authenticate calls to the Consul HTTP API, you must provide the token in the X-Consul-Token header for each call:\n$ curl --header \"X-Consul-Token: $CONSUL_HTTP_TOKEN\" ... \nTo learn about alternative ways to authenticate, refer to the following documentation:\nCLI Authentication\nAPI Authentication\nRefer to Service identities for information about creating service identities that you can link to tokens.\nRun the consul acl token create command and specify the policy or service identity to link to create a token. Refer to Consul ACL Token Create for details about the consul acl token create command.\nThe following example creates an ACL token linked to a service identity for a service named svc1.\n$ consul acl token create \\ -description \"Service token for svc1\" \\ -service-identity \"svc1\" \nRefer to Service identities for information about creating service identities that you can link to tokens.\nRun the consul acl token create command and specify the policy or service identity to link to create a token. Refer to Consul ACL Token Create for details about the consul acl token create command.\nYou can specify an admin partition, namespace, or both when creating tokens in Consul Enterprise. The token can only include permissions in the specified scope, if any. The following example creates an ACL token that the service can use to register in the ns1 namespace of partition ptn1:\n$ consul acl token create -partition \"ptn1\" -namespace \"ns1\" \\ -description \"Service token for svc1\" \\ -service-identity \"svc1\" \nWhen you are unable to link tokens to a service identity, you can define policies, register them with Consul, and link the policies to tokens that enable services to register into the Consul catalog.\nThe following example policy is defined in a file. The policy grants the svc1 service write permissions so that it can register into the catalog. For service mesh, the policy grants the svc1-sidecar-proxy service write permissions so that the sidecar proxy can register into the catalog. It grants service and node read permissions to discover and route to other services.\nservice \"svc1\" { policy = \"write\" } service \"svc1-sidecar-proxy\" { policy = \"write\" } service_prefix \"\" { policy = \"read\" } node_prefix \"\" { policy = \"read\" } \n$ consul acl policy create \\ -name \"svc1-register\" -rules @svc1-register.hcl \\ -description \"Allow svc1 to register into the catalog\" \n$ consul acl token create \\ -description \"Service token for svc1\" \\ -policy-name \"svc1-register\" \nWhen you are unable to link tokens to a service identity, you can define policies, register them with Consul, and link the policies to tokens that enable services to register into the Consul catalog.\nYou can specify an admin partition and namespace when creating policies in Consul Enterprise. The policy is only valid in the specified scopes.\nThe following example policy is defined in a file. The policy allows the svc1 service to register in the ns1 namespace of partition ptn1. For service mesh, the policy grants the svc1-sidecar-proxy service write permissions so that the sidecar proxy can register into the catalog. It grants service and node read permissions to discover and route to other services.\npartition \"ptn1\" { namespace \"ns1\" { service \"svc1\" { policy = \"write\" } service \"svc1-sidecar-proxy\" { policy = \"write\" } service_prefix \"\" { policy = \"read\" } node_prefix \"\" { policy = \"read\" } } } \n$ consul acl policy create -partition \"ptn1\" -namespace \"ns1\" \\ -name \"svc1-register\" -rules @svc1-register.hcl \\ -description \"Custom policy for service svc1\" \nYou can specify an admin partition and namespace when creating tokens in Consul Enterprise. The token is only valid in the specified scopes. The following example creates an ACL token that the service can use to register in the ns1 namespace of partition ptn1:\n$ consul acl token create -partition \"ptn1\" -namespace \"ns1\" \\ -description \"Service token for svc1\" \\ -policy-name \"svc1-register\""
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.14.x/security/acl/tokens/create/create-a-service-token",
  "text": "Service identities versus custom policies\nYou can create tokens linked to custom policies or to service identities. Service identities are constructs in Consul that enable you to quickly grant permissions for a group of services, rather than creating similar policies for each service.\nWe recommend using a service identity to grant permissions for service discovery and service mesh use cases rather than creating a custom policy. This is because service identities automatically grant the service and its sidecar proxy service:write, service:read, and node:read.\nYour organization may have requirements or processes for deploying services in a way that is inconsistent with service and node identities. In these cases, you can create custom policies and link them to tokens.\nThe following example creates an ACL token linked to a service identity for a service named svc1.\n$ consul acl token create \\ -description \"Service token for svc1\" \\ -service-identity \"svc1\" \n$ consul acl token create -partition \"ptn1\" -namespace \"ns1\" \\ -description \"Service token for svc1\" \\ -service-identity \"svc1\" "
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.17.x/connect/cluster-peering/usage/peering-traffic-management",
  "text": "Cluster Peering L7 Traffic Management | Consul\nManage L7 traffic with cluster peering\nThis usage topic describes how to configure and apply the service-resolver configuration entry to set up redirects and failovers between services that have an existing cluster peering connection.\nFor Kubernetes-specific guidance for managing L7 traffic with cluster peering, refer to Manage L7 traffic with cluster peering on Kubernetes.\nWhen you use cluster peering to connect datacenters through their admin partitions, you can use dynamic traffic management to configure your service mesh so that services automatically forward traffic to services hosted on peer clusters.\nHowever, the service-splitter and service-router configuration entry kinds do not natively support directly targeting a service instance hosted on a peer. Before you can split or route traffic to a service on a peer, you must define the service hosted on the peer as an upstream service by configuring a failover in the service-resolver configuration entry. Then, you can set up a redirect in a second service resolver to interact with the peer service by name.\nFor more information about formatting, updating, and managing configuration entries in Consul, refer to How to use configuration entries.\nTo configure L7 traffic management behavior in deployments with cluster peering connections, complete the following steps in order:\nDefine the peer cluster as a failover target in the service resolver configuration.\nThe following examples update the service-resolver configuration entry in cluster-01 so that Consul redirects traffic intended for the frontend service to a backup instance in peer cluster-02 when it detects multiple connection failures.\nKind = \"service-resolver\" Name = \"frontend\" ConnectTimeout = \"15s\" Failover = { \"*\" = { Targets = [ {Peer = \"cluster-02\"} ] } } \nDefine the desired behavior in service-splitter or service-router configuration entries.\nThe following example splits traffic evenly between frontend services hosted on peers by defining the desired behavior locally:\nKind = \"service-splitter\" Name = \"frontend\" Splits = [ { Weight = 50 ## defaults to service with same name as configuration entry (\"frontend\") }, { Weight = 50 Service = \"frontend-peer\" }, ] \nCreate a local service-resolver configuration entry named frontend-peer and define a redirect targeting the peer and its service:\nKind = \"service-resolver\" Name = \"frontend-peer\" Redirect { Service = frontend Peer = \"cluster-02\" }"
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.17.x/connect/expose",
  "text": "Commands: Connect Expose | Consul\nCommand: consul connect expose\nThe connect expose subcommand is used to expose a mesh-enabled service through an Ingress gateway by modifying the gateway's configuration and adding an intention to allow traffic from the gateway to the service. See the Ingress gateway documentation for more information about Ingress gateways.\nUsage: consul connect expose [options] Exposes a mesh-enabled service through the given ingress gateway, using the given protocol and port. \nCommand Options\n-ingress-gateway - (Required) The name of the ingress gateway service to use. A partition and namespace can optionally be specified as a prefix via the '[[partition/]namespace]/service' format. If a partition is not specified 'default' is assumed. If a partition is specified a namespace must be specified. Partitions and namespaces are Enterprise features.\n-port - (Required) The listener port to use for the service on the Ingress gateway.\n-service - (Required) The name of destination service to expose. A namespace can optionally be specified as a prefix via the '[[partition/]namespace]/service' format. If a partition is not specified 'default' is assumed. If a partition is specified a namespace must be specified. Partitions and namespaces are Enterprise features.\n-protocol - The protocol for the service. Defaults to 'tcp'.\n-host - Additional DNS hostname to use for routing to this service. Can be specified multiple times.\nThe example below shows using the expose command to make the foo service available through the Ingress gateway service ingress. The protocol argument is optional and defaults to tcp if not provided.\n$ consul connect expose -service=foo -ingress-gateway=ingress -port 8888 -protocol=tcp Successfully updated config entry for ingress service \"ingress\" Successfully set up intention for \"ingress\" -> \"foo\" \nRunning the command again when the config entry/intention are already set up will result in a no-op:\n$ consul connect expose -service=foo -ingress-gateway=ingress -port 8888 -protocol=tcp Service \"foo\" already exposed through listener with port 8888 Intention already exists for \"ingress\" -> \"foo\""
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.17.x/connect",
  "text": "Commands: Connect | Consul | HashiCorp Developer\nCommand: consul connect\nThe connect command is used to interact with the connect subsystem that provides Consul's service mesh capabilities. It exposes commands for running service mesh proxies and for viewing/updating the service mesh Certificate Authority (CA) configuration.\nUsage: consul connect <subcommand>\nFor the exact documentation for your Consul version, run consul connect -h to view the complete list of subcommands.\nUsage: consul connect <subcommand> [options] [args] This command has subcommands for interacting with Consul service mesh. Here are some simple examples, and more detailed examples are available in the subcommands or the documentation. Run the production service mesh proxy $ consul connect envoy For more examples, ask for subcommand help or view the documentation. Subcommands: ca Interact with the Consul service mesh Certificate Authority (CA) envoy Runs or configures Envoy as a service mesh proxy expose Expose a mesh-enabled service through an Ingress gateway proxy Runs a non-production, built-in service mesh sidecar proxy redirect-traffic Applies iptables rules for traffic redirection \nFor more information, examples, and usage about a subcommand, click on the name of the subcommand in the sidebar."
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.17.x/connect/redirect-traffic",
  "text": "Commands: Connect Redirect Traffic | Consul\nCommand: consul connect redirect-traffic\nThe connect redirect-traffic command is used to apply traffic redirection rules to enforce all traffic to go through the Envoy proxy when using Consul service mesh in the Transparent Proxy mode.\nThis command requires iptables command line utility to be installed, and as a result, this command can currently only run on linux. The user running the command needs to have NET_ADMIN capability.\nBy default, this command will apply rules to intercept and redirect all inbound and outbound TCP traffic to the Envoy's inbound and outbound ports accordingly.\nWhen proxy-id is specified, additional exclusion rules will be applied based on proxy's configuration stored in the local Consul agent. This includes redirecting to the proxy's inbound and outbound ports specified in the service registration.\nUsage: consul connect redirect-traffic [options]\nCommand Options\n-node-name - The node name where the proxy service is registered. It requires proxy-id to be specified. This is needed if running in an environment without client agents.\n-consul-dns-ip - The IP address of the Consul DNS resolver. If provided, DNS queries will be redirected to the provided IP address for name resolution.\n-consul-dns-port - The port of the Consul DNS resolver. If provided, DNS queries will be redirected to the provided IP address for name resolution.\n-proxy-id - The proxy service ID. This service ID must already be registered with the local agent.\n-proxy-inbound-port - The inbound port that the proxy is listening on.\n-proxy-outbound-port - The outbound port that the proxy is listening on. When not provided, 15001 is used by default.\n-proxy-uid - The user ID of the proxy to exclude from traffic redirection.\n-exclude-inbound-port - Inbound port to exclude from traffic redirection. May be provided multiple times.\n-exclude-outbound-cidr - Outbound CIDR to exclude from traffic redirection. May be provided multiple times.\n-exclude-outbound-port - Outbound port to exclude from traffic redirection. May be provided multiple times.\n-exclude-uid - Additional user ID to exclude from traffic redirection. May be provided multiple times.\n-netns - The Linux network namespace where traffic redirection rules should apply. This must be a path to the network namespace, e.g. /var/run/netns/foo.\nBasic Rules\nThe default traffic redirection rules can be applied with:\n$ consul connect redirect-traffic \\ -proxy-uid 1234 \\ -proxy-inbound-port 20000 \nUsing Registered Proxy Configuration\nTo automatically apply rules based on proxy's service registration, use the following command:\n$ consul connect redirect-traffic -proxy-uid 1234 -proxy-id web \nThis command assumes that the proxy service is registered with the local agent and that the local agent is reachable."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.9.x/connect/l7-traffic",
  "text": "Connect - L7 Traffic Management | Consul\n1.6.0+: This feature is available in Consul versions 1.6.0 and newer.\nLayer 7 traffic management allows operators to divide L7 traffic between different subsets of service instances when using Connect.\nThere are many ways you may wish to carve up a single datacenter's pool of services beyond simply returning all healthy instances for load balancing. Canary testing, A/B tests, blue/green deploys, and soft multi-tenancy (prod/qa/staging sharing compute resources) all require some mechanism of carving out portions of the Consul catalog smaller than the level of a single service and configuring when that subset should receive traffic.\nNote: This feature is not compatible with the built-in proxy, native proxies, and some Envoy proxy escape hatches.\nConnect proxy upstreams are discovered using a series of stages: routing, splitting, and resolution. These stages represent different ways of managing L7 traffic.\nEach stage of this discovery process can be dynamically reconfigured via various configuration entries. When a configuration entry is missing, that stage will fall back on reasonable default behavior.\nRouting\nA service-router config entry kind is the first configurable stage.\nA router config entry allows for a user to intercept traffic using L7 criteria such as path prefixes or http headers, and change behavior such as by sending traffic to a different service or service subset.\nThese config entries may only reference service-splitter or service-resolver entries.\nExamples can be found in the service-router documentation.\nSplitting\nA service-splitter config entry kind is the next stage after routing.\nA splitter config entry allows for a user to choose to split incoming requests across different subsets of a single service (like during staged canary rollouts), or perhaps across different services (like during a v2 rewrite or other type of codebase migration).\nThese config entries may only reference service-splitter or service-resolver entries.\nIf one splitter references another splitter the overall effects are flattened into one effective splitter config entry which reflects the multiplicative union. For instance:\nsplitter[A]: A_v1=50%, A_v2=50% splitter[B]: A=50%, B=50% --------------------- splitter[effective_B]: A_v1=25%, A_v2=25%, B=50% \nExamples can be found in the service-splitter documentation.\nResolution\nA service-resolver config entry kind is the last stage.\nA resolver config entry allows for a user to define which instances of a service should satisfy discovery requests for the provided name.\nExamples of things you can do with resolver config entries:\nControl where to send traffic if all instances of api in the current datacenter are unhealthy.\nConfigure service subsets based on Service.Meta.version values.\nSend all traffic for web that does not specify a service subset to the version1 subset.\nSend all traffic for api to new-api.\nSend all traffic for api in all datacenters to instances of api in dc2.\nCreate a \"virtual service\" api-dc2 that sends traffic to instances of api in dc2. This can be referenced in upstreams or in other config entries.\nIf no resolver config is defined for a service it is assumed 100% of traffic flows to the healthy instances of a service with the same name in the current datacenter/namespace and discovery terminates.\nThis should feel similar in spirit to various uses of Prepared Queries, but is not intended to be a drop-in replacement currently.\nThese config entries may only reference other service-resolver entries.\nExamples can be found in the service-resolver documentation.\nNote: service-resolver config entries kinds function at L4 (unlike service-router and service-splitter kinds). These can be created for services of any protocol such as tcp."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.17.x/proxies/deploy-sidecar-services",
  "text": "This page does not exist for version v1.17.x."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.10.x/connect/l7-traffic",
  "text": "Connect - L7 Traffic Management | Consul\n1.6.0+: This feature is available in Consul versions 1.6.0 and newer.\nLayer 7 traffic management allows operators to divide L7 traffic between different subsets of service instances when using Connect.\nThere are many ways you may wish to carve up a single datacenter's pool of services beyond simply returning all healthy instances for load balancing. Canary testing, A/B tests, blue/green deploys, and soft multi-tenancy (prod/qa/staging sharing compute resources) all require some mechanism of carving out portions of the Consul catalog smaller than the level of a single service and configuring when that subset should receive traffic.\nNote: This feature is not compatible with the built-in proxy, native proxies, and some Envoy proxy escape hatches.\nConnect proxy upstreams are discovered using a series of stages: routing, splitting, and resolution. These stages represent different ways of managing L7 traffic.\nEach stage of this discovery process can be dynamically reconfigured via various configuration entries. When a configuration entry is missing, that stage will fall back on reasonable default behavior.\nRouting\nA service-router config entry kind is the first configurable stage.\nA router config entry allows for a user to intercept traffic using L7 criteria such as path prefixes or http headers, and change behavior such as by sending traffic to a different service or service subset.\nThese config entries may only reference service-splitter or service-resolver entries.\nExamples can be found in the service-router documentation.\nSplitting\nA service-splitter config entry kind is the next stage after routing.\nA splitter config entry allows for a user to choose to split incoming requests across different subsets of a single service (like during staged canary rollouts), or perhaps across different services (like during a v2 rewrite or other type of codebase migration).\nThese config entries may only reference service-splitter or service-resolver entries.\nIf one splitter references another splitter the overall effects are flattened into one effective splitter config entry which reflects the multiplicative union. For instance:\nsplitter[A]: A_v1=50%, A_v2=50% splitter[B]: A=50%, B=50% --------------------- splitter[effective_B]: A_v1=25%, A_v2=25%, B=50% \nExamples can be found in the service-splitter documentation.\nResolution\nA service-resolver config entry kind is the last stage.\nA resolver config entry allows for a user to define which instances of a service should satisfy discovery requests for the provided name.\nExamples of things you can do with resolver config entries:\nControl where to send traffic if all instances of api in the current datacenter are unhealthy.\nConfigure service subsets based on Service.Meta.version values.\nSend all traffic for web that does not specify a service subset to the version1 subset.\nSend all traffic for api to new-api.\nSend all traffic for api in all datacenters to instances of api in dc2.\nCreate a \"virtual service\" api-dc2 that sends traffic to instances of api in dc2. This can be referenced in upstreams or in other config entries.\nIf no resolver config is defined for a service it is assumed 100% of traffic flows to the healthy instances of a service with the same name in the current datacenter/namespace and discovery terminates.\nThis should feel similar in spirit to various uses of Prepared Queries, but is not intended to be a drop-in replacement currently.\nThese config entries may only reference other service-resolver entries.\nExamples can be found in the service-resolver documentation.\nNote: service-resolver config entries kinds can function at L4 (unlike service-router and service-splitter kinds). These can be created for services of any protocol such as tcp."
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.15.x/connect",
  "text": "Commands: Connect | Consul | HashiCorp Developer\nCommand: consul connect\nThe connect command is used to interact with the connect subsystem that provides Consul's service mesh capabilities. It exposes commands for running service mesh proxies and for viewing/updating the service mesh Certificate Authority (CA) configuration.\nUsage: consul connect <subcommand>\nFor the exact documentation for your Consul version, run consul connect -h to view the complete list of subcommands.\nUsage: consul connect <subcommand> [options] [args] This command has subcommands for interacting with Consul service mesh. Here are some simple examples, and more detailed examples are available in the subcommands or the documentation. Run the production service mesh proxy $ consul connect envoy For more examples, ask for subcommand help or view the documentation. Subcommands: ca Interact with the Consul service mesh Certificate Authority (CA) envoy Runs or configures Envoy as a service mesh proxy expose Expose a mesh-enabled service through an Ingress gateway proxy Runs a non-production, built-in service mesh sidecar proxy redirect-traffic Applies iptables rules for traffic redirection \nFor more information, examples, and usage about a subcommand, click on the name of the subcommand in the sidebar."
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.15.x/connect/redirect-traffic",
  "text": "Commands: Connect Redirect Traffic | Consul\nCommand: consul connect redirect-traffic\nThe connect redirect-traffic command is used to apply traffic redirection rules to enforce all traffic to go through the Envoy proxy when using Consul service mesh in the Transparent Proxy mode.\nThis command requires iptables command line utility to be installed, and as a result, this command can currently only run on linux. The user running the command needs to have NET_ADMIN capability.\nBy default, this command will apply rules to intercept and redirect all inbound and outbound TCP traffic to the Envoy's inbound and outbound ports accordingly.\nWhen proxy-id is specified, additional exclusion rules will be applied based on proxy's configuration stored in the local Consul agent. This includes redirecting to the proxy's inbound and outbound ports specified in the service registration.\nUsage: consul connect redirect-traffic [options]\n-node-name - The node name where the proxy service is registered. It requires proxy-id to be specified. This is needed if running in an environment without client agents.\n-consul-dns-ip - The IP address of the Consul DNS resolver. If provided, DNS queries will be redirected to the provided IP address for name resolution.\n-consul-dns-port - The port of the Consul DNS resolver. If provided, DNS queries will be redirected to the provided IP address for name resolution.\n-proxy-id - The proxy service ID. This service ID must already be registered with the local agent.\n-proxy-inbound-port - The inbound port that the proxy is listening on.\n-proxy-outbound-port - The outbound port that the proxy is listening on. When not provided, 15001 is used by default.\n-proxy-uid - The user ID of the proxy to exclude from traffic redirection.\n-exclude-inbound-port - Inbound port to exclude from traffic redirection. May be provided multiple times.\n-exclude-outbound-cidr - Outbound CIDR to exclude from traffic redirection. May be provided multiple times.\n-exclude-outbound-port - Outbound port to exclude from traffic redirection. May be provided multiple times.\n-exclude-uid - Additional user ID to exclude from traffic redirection. May be provided multiple times.\n-netns - The Linux network namespace where traffic redirection rules should apply. This must be a path to the network namespace, e.g. /var/run/netns/foo.\nBasic Rules\nThe default traffic redirection rules can be applied with:\n$ consul connect redirect-traffic \\ -proxy-uid 1234 \\ -proxy-inbound-port 20000 \nUsing Registered Proxy Configuration\nTo automatically apply rules based on proxy's service registration, use the following command:\n$ consul connect redirect-traffic -proxy-uid 1234 -proxy-id web \nThis command assumes that the proxy service is registered with the local agent and that the local agent is reachable."
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.15.x/connect/expose",
  "text": "Commands: Connect Expose | Consul\nCommand: consul connect expose\nThe connect expose subcommand is used to expose a mesh-enabled service through an Ingress gateway by modifying the gateway's configuration and adding an intention to allow traffic from the gateway to the service. See the Ingress gateway documentation for more information about Ingress gateways.\nUsage: consul connect expose [options] Exposes a mesh-enabled service through the given ingress gateway, using the given protocol and port. \n-ingress-gateway - (Required) The name of the ingress gateway service to use. A partition and namespace can optionally be specified as a prefix via the '[[partition/]namespace]/service' format. If a partition is not specified 'default' is assumed. If a partition is specified a namespace must be specified. Partitions and namespaces are Enterprise features.\n-port - (Required) The listener port to use for the service on the Ingress gateway.\n-service - (Required) The name of destination service to expose. A namespace can optionally be specified as a prefix via the '[[partition/]namespace]/service' format. If a partition is not specified 'default' is assumed. If a partition is specified a namespace must be specified. Partitions and namespaces are Enterprise features.\n-protocol - The protocol for the service. Defaults to 'tcp'.\n-host - Additional DNS hostname to use for routing to this service. Can be specified multiple times.\nThe example below shows using the expose command to make the foo service available through the Ingress gateway service ingress. The protocol argument is optional and defaults to tcp if not provided.\n$ consul connect expose -service=foo -ingress-gateway=ingress -port 8888 -protocol=tcp Successfully updated config entry for ingress service \"ingress\" Successfully set up intention for \"ingress\" -> \"foo\" \nRunning the command again when the config entry/intention are already set up will result in a no-op:\n$ consul connect expose -service=foo -ingress-gateway=ingress -port 8888 -protocol=tcp Service \"foo\" already exposed through listener with port 8888 Intention already exists for \"ingress\" -> \"foo\""
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.16.x/connect/cluster-peering/usage/peering-traffic-management",
  "text": "Cluster Peering L7 Traffic Management | Consul\nManage L7 traffic with cluster peering\nThis usage topic describes how to configure and apply the service-resolver configuration entry to set up redirects and failovers between services that have an existing cluster peering connection.\nFor Kubernetes-specific guidance for managing L7 traffic with cluster peering, refer to Manage L7 traffic with cluster peering on Kubernetes.\nWhen you use cluster peering to connect datacenters through their admin partitions, you can use dynamic traffic management to configure your service mesh so that services automatically forward traffic to services hosted on peer clusters.\nHowever, the service-splitter and service-router configuration entry kinds do not natively support directly targeting a service instance hosted on a peer. Before you can split or route traffic to a service on a peer, you must define the service hosted on the peer as an upstream service by configuring a failover in the service-resolver configuration entry. Then, you can set up a redirect in a second service resolver to interact with the peer service by name.\nFor more information about formatting, updating, and managing configuration entries in Consul, refer to How to use configuration entries.\nTo configure L7 traffic management behavior in deployments with cluster peering connections, complete the following steps in order:\nDefine the peer cluster as a failover target in the service resolver configuration.\nThe following examples update the service-resolver configuration entry in cluster-01 so that Consul redirects traffic intended for the frontend service to a backup instance in peer cluster-02 when it detects multiple connection failures.\nKind = \"service-resolver\" Name = \"frontend\" ConnectTimeout = \"15s\" Failover = { \"*\" = { Targets = [ {Peer = \"cluster-02\"} ] } } \nDefine the desired behavior in service-splitter or service-router configuration entries.\nThe following example splits traffic evenly between frontend services hosted on peers by defining the desired behavior locally:\nKind = \"service-splitter\" Name = \"frontend\" Splits = [ { Weight = 50 ## defaults to service with same name as configuration entry (\"frontend\") }, { Weight = 50 Service = \"frontend-peer\" }, ] \nCreate a local service-resolver configuration entry named frontend-peer and define a redirect targeting the peer and its service:\nKind = \"service-resolver\" Name = \"frontend-peer\" Redirect { Service = frontend Peer = \"cluster-02\" }"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.16.x/proxies/deploy-sidecar-services",
  "text": "This page does not exist for version v1.16.x."
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.16.x/connect/expose",
  "text": "Commands: Connect Expose | Consul\nCommand: consul connect expose\nThe connect expose subcommand is used to expose a mesh-enabled service through an Ingress gateway by modifying the gateway's configuration and adding an intention to allow traffic from the gateway to the service. See the Ingress gateway documentation for more information about Ingress gateways.\nUsage: consul connect expose [options] Exposes a mesh-enabled service through the given ingress gateway, using the given protocol and port. \n-ingress-gateway - (Required) The name of the ingress gateway service to use. A partition and namespace can optionally be specified as a prefix via the '[[partition/]namespace]/service' format. If a partition is not specified 'default' is assumed. If a partition is specified a namespace must be specified. Partitions and namespaces are Enterprise features.\n-port - (Required) The listener port to use for the service on the Ingress gateway.\n-service - (Required) The name of destination service to expose. A namespace can optionally be specified as a prefix via the '[[partition/]namespace]/service' format. If a partition is not specified 'default' is assumed. If a partition is specified a namespace must be specified. Partitions and namespaces are Enterprise features.\n-protocol - The protocol for the service. Defaults to 'tcp'.\n-host - Additional DNS hostname to use for routing to this service. Can be specified multiple times.\nThe example below shows using the expose command to make the foo service available through the Ingress gateway service ingress. The protocol argument is optional and defaults to tcp if not provided.\n$ consul connect expose -service=foo -ingress-gateway=ingress -port 8888 -protocol=tcp Successfully updated config entry for ingress service \"ingress\" Successfully set up intention for \"ingress\" -> \"foo\" \nRunning the command again when the config entry/intention are already set up will result in a no-op:\n$ consul connect expose -service=foo -ingress-gateway=ingress -port 8888 -protocol=tcp Service \"foo\" already exposed through listener with port 8888 Intention already exists for \"ingress\" -> \"foo\""
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.16.x/connect",
  "text": "Commands: Connect | Consul | HashiCorp Developer\nCommand: consul connect\nThe connect command is used to interact with the connect subsystem that provides Consul's service mesh capabilities. It exposes commands for running service mesh proxies and for viewing/updating the service mesh Certificate Authority (CA) configuration.\nUsage: consul connect <subcommand>\nFor the exact documentation for your Consul version, run consul connect -h to view the complete list of subcommands.\nUsage: consul connect <subcommand> [options] [args] This command has subcommands for interacting with Consul service mesh. Here are some simple examples, and more detailed examples are available in the subcommands or the documentation. Run the production service mesh proxy $ consul connect envoy For more examples, ask for subcommand help or view the documentation. Subcommands: ca Interact with the Consul service mesh Certificate Authority (CA) envoy Runs or configures Envoy as a service mesh proxy expose Expose a mesh-enabled service through an Ingress gateway proxy Runs a non-production, built-in service mesh sidecar proxy redirect-traffic Applies iptables rules for traffic redirection \nFor more information, examples, and usage about a subcommand, click on the name of the subcommand in the sidebar."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/agent/config-entries/service-resolver",
  "text": "Consul | HashiCorp Developer\nWe couldn't find the page you're looking for.\nPlease check the url you entered for typos, go back to the page you came from, or go to one of the pages below.\nHashiCorp Cloud Platform\nTerraform\nPacker\nConsul\nVault\nBoundary\nNomad\nWaypoint\nVagrant\nHashiCorp Developer"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/agent/config-entries/service-defaults",
  "text": "Consul | HashiCorp Developer\nWe couldn't find the page you're looking for.\nPlease check the url you entered for typos, go back to the page you came from, or go to one of the pages below.\nHashiCorp Cloud Platform\nTerraform\nPacker\nConsul\nVault\nBoundary\nNomad\nWaypoint\nVagrant\nHashiCorp Developer"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.17.x/enterprise/license/utilization-reporting",
  "text": "Automated license utilization reporting | Consul\nThis topic describes how to enable automated license utilization reporting in Consul Enterprise. This feature automatically sends license utilization data to HashiCorp so that you do not have to manually collect and report it. It also enables you to review your license usage with the monitoring solution you already use, such as Splunk and Datadog, as you optimize and manage your deployments.\nYou can use automated license utilization report to understand how much additional networking infrastructure you can deploy under your current contract. This feature helps you protect against overutilization and budget for predicted consumption.\nAutomated reporting shares the minimum data required to validate license utilization as defined in our contracts. This data mostly consists of computed metrics, and it will never contain Personal Identifiable Information (PII) or other sensitive information. Automated reporting shares the data with HashiCorp using a secure unidirectional HTTPS API and makes an auditable record in the product logs each time it submits a report. This process is GDPR-compliant.\nAutomated license utilization reporting does not support air-gapped installations, which are systems with no network interfaces.\nThe following versions of Consul Enterprise support automated license utilization reporting:\nConsul Enterprise v1.16.0 and newer.\nPatch releases of Consul Enterprise v1.15.4 and newer.\nPatch releases of Consul Enterprise v1.14.8 and newer.\nPatch releases of Consul Enterprise v1.13.9 and newer.\nDownload a supported release from the Consul Versions page.\nBefore you enable automated reporting, make sure that outbound network traffic is configured correctly and upgrade your enterprise product to a version that supports it. If your installation is air-gapped or network settings are not in place, automated reporting will not work.\nTo enable automated reporting, complete the following steps:\nAllow outbound HTTPS traffic on port 443\nCheck product logs\nAllow outbound HTTPS traffic on port 443\nMake sure that your network allows HTTPS egress on port 443 from https://reporting.hashicorp.services by adding the following IP addresses to your allow-list:\n100.20.70.12\n35.166.5.222\n23.95.85.111\n44.215.244.1\nCheck product logs\nAutomatic license utilization reporting starts sending data within roughly 24 hours. Check the product logs for records that the data sent successfully.\n[DEBUG] beginning snapshot export [DEBUG] creating payload [DEBUG] marshalling payload to json [DEBUG] generating authentication headers [DEBUG] creating request [DEBUG] sending request [DEBUG] performing request: method=POST url=https://census.license.hashicorp.services [DEBUG] recording audit record [INFO] reporting: Report sent: auditRecord={\"payload\":{\"payload_version\":\"1\",\"license_id\":\"d2cdd857-4202-5a45-70a6-e4b531050c34\",\"product\":\"consul\",\"product_version\":\"1.16.0-dev+ent\",\"export_timestamp\":\"2023-05-26T20:09:13.753921087Z\",\"snapshots\":[{\"snapshot_version\":1,\"snapshot_id\":\"0001J724F90F4XWQDSAA76ZQWA\",\"process_id\":\"01H1CTJPC1S8H7Q45MKTJ689ZW\",\"timestamp\":\"2023-05-26T20:09:13.753513962Z\",\"schema_version\":\"1.0.0\",\"service\":\"consul\",\"metrics\":{\"consul.billable.nodes\":{\"key\":\"consul.billable.nodes\",\"kind\":\"counter\",\"mode\":\"write\",\"value\":2},\"consul.billable.service_instances\":{\"key\":\"consul.billable.service_instances\",\"kind\":\"counter\",\"mode\":\"write\",\"value\":2}}}],\"metadata\":{}}} [DEBUG] completed recording audit record [DEBUG] export finished successfully\" \nIf your installation is air-gapped or your network does not allow the correct egress, the logs show an error.\n[DEBUG] reporting: beginning snapshot export [DEBUG] reporting: creating payload [DEBUG] reporting: marshalling payload to json [DEBUG] reporting: generating authentication headers [DEBUG] reporting: creating request [DEBUG] reporting: sending request [DEBUG] reporting: performing request: method=POST url=https://census.license.hashicorp.services [DEBUG] reporting: error status code received: statusCode=403 \nIn this case, reconfigure your network to allow egress and check the logs again in roughly 24 hours to confirm that automated reporting works correctly.\nIf your installation is air-gapped or you want to manually collect and report on the same license utilization metrics, you can opt out of automated reporting.\nManually reporting these metrics can be time consuming. Opting out of automated reporting does not mean that you also opt out from sending license utilization metrics. Customers who opt out of automated reporting are still required to manually collect and send license utilization metrics to HashiCorp.\nIf you are considering opting out because you are worried about the data, we strongly recommend that you review the example payloads before opting out. If you have concerns with any of the automatically reported data, raise these concerns with your account manager.\nThere are two methods for opting out of automated reporting:\nHCL configuration (recommended)\nEnvironment variable (requires restart)\nWe recommend opting out in your product's configuration file because it does not require a system restart. Add the following block to your configuration.hcl or configuration.json file.\nreporting { license { enabled = false } } \nWhen opting out using an environment variable, the system provides a startup message confirming that you have disabled automated reporting. Set the following environment variable to disable automated reporting:\n$ export OPTOUT_LICENSE_REPORTING=true \nAfter you set the environment variable, restart your system to complete the process for opting out.\nCheck your product logs roughly 24 hours after opting out to make sure that the system is not trying to send reports. Keep in mind that if your configuration file and environment variable differ, the environment variable setting takes precedence.\nHashiCorp collects the following utilization data as JSON payloads: exporter_version - The version of the licensing exporter\n{ \"payload\": { \"payload_version\": \"1\", \"license_id\": \"d2cdd857-4202-5a45-70a6-e4b531050c34\", \"product\": \"consul\", \"product_version\": \"1.16.0-dev+ent\", \"export_timestamp\": \"2023-05-26T20:09:13.753921087Z\", \"snapshots\": [ { \"snapshot_version\": 1, \"snapshot_id\": \"0001J724F90F4XWQDSAA76ZQWA\", \"process_id\": \"01H1CTJPC1S8H7Q45MKTJ689ZW\", \"timestamp\": \"2023-05-26T20:09:13.753513962Z\", \"schema_version\": \"1.0.0\", \"service\": \"consul\", \"metrics\": { \"consul.billable.nodes\": { \"key\": \"consul.billable.nodes\", \"kind\": \"counter\", \"mode\": \"write\", \"value\": 2 }, \"consul.billable.service_instances\": { \"key\": \"consul.billable.service_instances\", \"kind\": \"counter\", \"mode\": \"write\", \"value\": 2 } } } ], \"metadata\": {} } }"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.17.x/enterprise/license/faq",
  "text": "Enterprise License FAQ | Consul\nFrequently Asked Questions (FAQ)\nThis FAQ is for the license changes introduced in Consul Enterprise version 1.10.0. Consul Enterprise automatically loads Consul licenses when the server agent starts.\nStarting with Consul Enterprise 1.10.0, the license enablement process is different.\nHashiCorp Enterprise servers will no longer start without a license. Servers require licenses to be provided from either an environment variable or file. If the license is missing, invalid, or expired, the server will immediately exit. This check is part of the server boot-up process.\nIn previous versions of HashiCorp enterprise products, one server could distribute a license to other servers via the Raft protocol. This will no longer work since each server must be able to find a valid license during the startup process.\nAll customers on Consul Enterprise 1.8/1.9 must first upgrade their client and server agents to the latest patch release. During the upgrade the license file must also be configured on client agents in an environment variable or file path, otherwise the Consul agents will fail to retrieve the license with a valid agent token. The upgrade process varies if ACLs are enabled or disabled in the cluster. Refer to the instructions on upgrading to 1.10.x for details.\nPlease visit the Enterprise License Tutorial.\nThe list below is a great starting point for learning more about the license changes introduced in Consul Enterprise 1.10.0+ent.\nConsul Enterprise Upgrade Documentation\nConsul License Documentation\nLicense configuration values documentation\nInstall a HashiCorp Enterprise License Tutorial\nThe license changes introduced in 1.10.0 only affect Consul Enterprise. This impacts customers that have an enterprise binary (trial or non-trial licenses) downloaded from https://releases.hashicorp.com. The license changes do not impact customers with the baked-in licensed binaries. In a later release of Consul Enterprise, baked-in binaries will be deprecated.\nStarting with Consul Enterprise 1.10.0, a valid license is required on-disk (auto-loading) or as an environment variable for Consul Enterprise to successfully boot-up. The in-storage license feature will not be supported starting with Consul Enterprise 1.10.0+ent. All Consul Enterprise clusters using 1.10.0+ent must ensure that there is a valid license on-disk (auto-loaded) or as an environment variable.\nConsul Enterprise 1.10.0+ server agents require a valid license to start. The license can be provided on disk (auto-loaded) or as an environment variable. There is no longer a 6-hour trial period in which Consul Enterprise server agents can operate without a license.\nYesConsul will continue normal operation and can be restarted after license expiration as defined in expiration_time. As license expiration is approached or passed, Consul will issue warnings in the system logs.\nConsul will only cease operation after license termination, which occurs 10 years after license expiration and is defined in termination_time.\nStarting with Consul 1.14, and patch releases 1.13.3 and 1.12.6, Consul will support non-terminating licenses: Please contact your support representative for more details on non-terminating licenses. An expired license will not allow Consul versions released after the expiration date to run. It will not be possible to upgrade to a new version of Consul released after license expiration.\nThere are upgrade requirements that affect Consul Enterprise clients. Please review the upgrade requirements documentation.\nSame behavior as Consul clients. See answer for Does this affect client agents? \nConsul server agents will detect the absence of the license and immediately exit.\nConsul client agents will attempt to retrieve the license from servers if certain conditions are met:\nACLs are enabled.\nAn ACL token is provided to the client agent.\nThe client agents configuration contains retry_join addresses.\nThe retry join addresses are addresses of the Consul servers.\nConsul snapshot agents will attempt to retrieve the license from servers if certain conditions are met: ACLs are enabled, a ACL token is provided to the client agent, the client agents configuration contains retry_join addresses, the retry join addresses are addresses of the Consul servers.\nContact your HashiCorp Support contact for a development license.\nTrial install will cease operation 24 hours after 30-day license expiration: Trial licenses are not meant to be used in production. This is unlike non-trial licenses which provide a 10 year grace period as described in the answer for Q: Is there a grace period when licenses expire?.\nContact your organization's HashiCorp account team for information on how to renew your organization's enterprise license.\nContact your organization's HashiCorp account team for information on how to renew your organization's enterprise license.\nThe license files are not locked to a specific cluster or cluster node. The above changes apply to all nodes in a cluster.\nThis will not impact HCP Consul.\nConsul Enterprise binaries starting with 1.10.0+ent, will be subject to EULA check. Release 1.10.0+ent introduces the EULA check for trial licenses (non-trial licenses already go through EULA check during contractual agreement).\nThe agreement to a EULA happens only once (when the user gets their license), Consul Enterprise will check for the presence of a valid license every time a node restarts.\nWhen a customer upgrades existing clusters to a 1.10.0+ent release, they need to have a valid license to successfully upgrade. This valid license must be auto-loaded.\nWhen a customer deploys new clusters to a 1.10.0+ent release, they need to have a valid license to successfully upgrade. This valid license must be on-disk (auto-loaded).\nNew Consul cluster deployments using 1.10.0+ent will need to have a valid license on servers to successfully deploy. This valid license must be on-disk (auto-loaded) or as an environment variable. Please see the upgrade requirements.\nVM\nRun consul license get -signed to extract the license from their running cluster. Store the license in a secure location on disk.\nSet up the necessary configuration so that when Consul Enterprise reboots it will have access to the required license. This could be via the client agent configuration file or an environment variable.\nVisit the Enterprise License Tutorial for detailed steps on how to install the license key.\nFollow the Consul upgrade documentation.\nKubernetes\nNOTE: If you are not upgrading Consul or your Helm chart version then no action is required.\nIn order to use Consul Enterprise 1.10.0 or greater on Kubernetes you must use version 0.32.0 or greater of the Helm chart.\nYou should already have a Consul Enterprise license set as a Kubernetes secret. If you do not, refer to how to obtain a copy of your license. Once you have the license then create a Kubernetes secret containing the license as described in Kubernetes - Consul Enterprise.\nFollow the Kubernetes Upgrade Docs to upgrade. No changes to your values.yaml file are needed to enable enterprise autoloading since this support is built in to consul-helm 0.32.0 and greater.\nWarning: If you are upgrading the Helm chart but not upgrading the Consul version, you must set server.enterpriseLicense.enableLicenseAutoload: false. See Kubernetes - Consul Enterprise for more details.\nVM\nAcquire a valid Consul Enterprise license. If you are an existing HashiCorp enterprise customer you may contact your organization's customer success manager (CSM) for information on how to get your organization's enterprise license.\nStore the license in a secure location on disk.\nSet up the necessary configuration so that when Consul Enterprise reboots it will have the required license. This could be via the client agent configuration file or an environment variable. Visit the Enterprise License Tutorial for detailed steps on how to install the license key.\nFollow the Consul upgrade documentation.\nKubernetes\nAcquire a valid Consul Enterprise license. If you are an existing HashiCorp enterprise customer you may contact your organization's customer success manager (CSM) for information on how to get your organization's enterprise license.\nSet up the necessary configuration so that when Consul Enterprise reboots it will have the required license. This could be via the client agent configuration file or an environment variable. Visit the Enterprise License Tutorial for detailed steps on how to install the license key.\nProceed with the helm upgrade instructions\nWhen downgrading to a version of Consul before 1.10.0+ent, customers will need to follow the previous process for applying an enterprise licenses to Consul Enterprise.\nVerify that you meet the upgrade requirements.\nAssume a scenario where there are three Consul server nodes:\nNode A: v1.9.5\nNode B: v1.10.0 [Leader]\nNode C: v1.9.5\nThe command consul license put is issued from Node A. This will result in an error due to how Consul routes calls to the server node, Node B in this example. Because Node A is a follower when the call consul license put is issued, the call will be redirected to Node B (leader). The consul license put operation will fail due to being removed from 1.10.0. This is a scenario that could occur if a customer downgrades from 1.10.0+ and the Consul leadership has not transferred back over to a node not running 1.10.0+. This also has the potential of occurring when upgrading if scheduled license updates or autoscaling groups recoveries are in place.\nIf you are using a Consul Enterprise version prior to 1.10.0 and decide to upgrade the Helm chart to version 0.32.0 or newer, but not the Consul version. You will need to add the following configuration.\nserver: enterpriseLicense: enableLicenseAutoload: false"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.18.x/enterprise/license/utilization-reporting",
  "text": "Automated license utilization reporting | Consul\nThis topic describes how to enable automated license utilization reporting in Consul Enterprise. This feature automatically sends license utilization data to HashiCorp so that you do not have to manually collect and report it. It also enables you to review your license usage with the monitoring solution you already use, such as Splunk and Datadog, as you optimize and manage your deployments.\nYou can use automated license utilization report to understand how much additional networking infrastructure you can deploy under your current contract. This feature helps you protect against overutilization and budget for predicted consumption.\nAutomated reporting shares the minimum data required to validate license utilization as defined in our contracts. This data mostly consists of computed metrics, and it will never contain Personal Identifiable Information (PII) or other sensitive information. Automated reporting shares the data with HashiCorp using a secure unidirectional HTTPS API and makes an auditable record in the product logs each time it submits a report. This process is GDPR-compliant.\nAutomated license utilization reporting does not support air-gapped installations, which are systems with no network interfaces.\nThe following versions of Consul Enterprise support automated license utilization reporting:\nConsul Enterprise v1.16.0 and newer.\nPatch releases of Consul Enterprise v1.15.4 and newer.\nPatch releases of Consul Enterprise v1.14.8 and newer.\nPatch releases of Consul Enterprise v1.13.9 and newer.\nDownload a supported release from the Consul Versions page.\nBefore you enable automated reporting, make sure that outbound network traffic is configured correctly and upgrade your enterprise product to a version that supports it. If your installation is air-gapped or network settings are not in place, automated reporting will not work.\nTo enable automated reporting, complete the following steps:\nAllow outbound HTTPS traffic on port 443\nCheck product logs\nAllow outbound HTTPS traffic on port 443\nMake sure that your network allows HTTPS egress on port 443 from https://reporting.hashicorp.services by adding the following IP addresses to your allow-list:\n100.20.70.12\n35.166.5.222\n23.95.85.111\n44.215.244.1\nCheck product logs\nAutomatic license utilization reporting starts sending data within roughly 24 hours. Check the product logs for records that the data sent successfully.\n[DEBUG] beginning snapshot export [DEBUG] creating payload [DEBUG] marshalling payload to json [DEBUG] generating authentication headers [DEBUG] creating request [DEBUG] sending request [DEBUG] performing request: method=POST url=https://census.license.hashicorp.services [DEBUG] recording audit record [INFO] reporting: Report sent: auditRecord={\"payload\":{\"payload_version\":\"1\",\"license_id\":\"d2cdd857-4202-5a45-70a6-e4b531050c34\",\"product\":\"consul\",\"product_version\":\"1.16.0-dev+ent\",\"export_timestamp\":\"2023-05-26T20:09:13.753921087Z\",\"snapshots\":[{\"snapshot_version\":1,\"snapshot_id\":\"0001J724F90F4XWQDSAA76ZQWA\",\"process_id\":\"01H1CTJPC1S8H7Q45MKTJ689ZW\",\"timestamp\":\"2023-05-26T20:09:13.753513962Z\",\"schema_version\":\"1.0.0\",\"service\":\"consul\",\"metrics\":{\"consul.billable.nodes\":{\"key\":\"consul.billable.nodes\",\"kind\":\"counter\",\"mode\":\"write\",\"value\":2},\"consul.billable.service_instances\":{\"key\":\"consul.billable.service_instances\",\"kind\":\"counter\",\"mode\":\"write\",\"value\":2}}}],\"metadata\":{}}} [DEBUG] completed recording audit record [DEBUG] export finished successfully\" \nIf your installation is air-gapped or your network does not allow the correct egress, the logs show an error.\n[DEBUG] reporting: beginning snapshot export [DEBUG] reporting: creating payload [DEBUG] reporting: marshalling payload to json [DEBUG] reporting: generating authentication headers [DEBUG] reporting: creating request [DEBUG] reporting: sending request [DEBUG] reporting: performing request: method=POST url=https://census.license.hashicorp.services [DEBUG] reporting: error status code received: statusCode=403 \nIn this case, reconfigure your network to allow egress and check the logs again in roughly 24 hours to confirm that automated reporting works correctly.\nIf your installation is air-gapped or you want to manually collect and report on the same license utilization metrics, you can opt out of automated reporting.\nManually reporting these metrics can be time consuming. Opting out of automated reporting does not mean that you also opt out from sending license utilization metrics. Customers who opt out of automated reporting are still required to manually collect and send license utilization metrics to HashiCorp.\nIf you are considering opting out because you are worried about the data, we strongly recommend that you review the example payloads before opting out. If you have concerns with any of the automatically reported data, raise these concerns with your account manager.\nThere are two methods for opting out of automated reporting:\nHCL configuration (recommended)\nEnvironment variable (requires restart)\nWe recommend opting out in your product's configuration file because it does not require a system restart. Add the following block to your configuration.hcl or configuration.json file.\nreporting { license { enabled = false } } \nWhen opting out using an environment variable, the system provides a startup message confirming that you have disabled automated reporting. Set the following environment variable to disable automated reporting:\n$ export OPTOUT_LICENSE_REPORTING=true \nAfter you set the environment variable, restart your system to complete the process for opting out.\nCheck your product logs roughly 24 hours after opting out to make sure that the system is not trying to send reports. Keep in mind that if your configuration file and environment variable differ, the environment variable setting takes precedence.\nHashiCorp collects the following utilization data as JSON payloads: exporter_version - The version of the licensing exporter\n{ \"payload\": { \"payload_version\": \"1\", \"license_id\": \"d2cdd857-4202-5a45-70a6-e4b531050c34\", \"product\": \"consul\", \"product_version\": \"1.16.0-dev+ent\", \"export_timestamp\": \"2023-05-26T20:09:13.753921087Z\", \"snapshots\": [ { \"snapshot_version\": 1, \"snapshot_id\": \"0001J724F90F4XWQDSAA76ZQWA\", \"process_id\": \"01H1CTJPC1S8H7Q45MKTJ689ZW\", \"timestamp\": \"2023-05-26T20:09:13.753513962Z\", \"schema_version\": \"1.0.0\", \"service\": \"consul\", \"metrics\": { \"consul.billable.nodes\": { \"key\": \"consul.billable.nodes\", \"kind\": \"counter\", \"mode\": \"write\", \"value\": 2 }, \"consul.billable.service_instances\": { \"key\": \"consul.billable.service_instances\", \"kind\": \"counter\", \"mode\": \"write\", \"value\": 2 } } } ], \"metadata\": {} } }"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.18.x/enterprise/license/faq",
  "text": "Enterprise License FAQ | Consul\nFrequently Asked Questions (FAQ)\nThis FAQ is for the license changes introduced in Consul Enterprise version 1.10.0. Consul Enterprise automatically loads Consul licenses when the server agent starts.\nStarting with Consul Enterprise 1.10.0, the license enablement process is different.\nHashiCorp Enterprise servers will no longer start without a license. Servers require licenses to be provided from either an environment variable or file. If the license is missing, invalid, or expired, the server will immediately exit. This check is part of the server boot-up process.\nIn previous versions of HashiCorp enterprise products, one server could distribute a license to other servers via the Raft protocol. This will no longer work since each server must be able to find a valid license during the startup process.\nAll customers on Consul Enterprise 1.8/1.9 must first upgrade their client and server agents to the latest patch release. During the upgrade the license file must also be configured on client agents in an environment variable or file path, otherwise the Consul agents will fail to retrieve the license with a valid agent token. The upgrade process varies if ACLs are enabled or disabled in the cluster. Refer to the instructions on upgrading to 1.10.x for details.\nPlease visit the Enterprise License Tutorial.\nThe list below is a great starting point for learning more about the license changes introduced in Consul Enterprise 1.10.0+ent.\nConsul Enterprise Upgrade Documentation\nConsul License Documentation\nLicense configuration values documentation\nInstall a HashiCorp Enterprise License Tutorial\nThe license changes introduced in 1.10.0 only affect Consul Enterprise. This impacts customers that have an enterprise binary (trial or non-trial licenses) downloaded from https://releases.hashicorp.com. The license changes do not impact customers with the baked-in licensed binaries. In a later release of Consul Enterprise, baked-in binaries will be deprecated.\nStarting with Consul Enterprise 1.10.0, a valid license is required on-disk (auto-loading) or as an environment variable for Consul Enterprise to successfully boot-up. The in-storage license feature will not be supported starting with Consul Enterprise 1.10.0+ent. All Consul Enterprise clusters using 1.10.0+ent must ensure that there is a valid license on-disk (auto-loaded) or as an environment variable.\nConsul Enterprise 1.10.0+ server agents require a valid license to start. The license can be provided on disk (auto-loaded) or as an environment variable. There is no longer a 6-hour trial period in which Consul Enterprise server agents can operate without a license.\nYesConsul will continue normal operation and can be restarted after license expiration as defined in expiration_time. As license expiration is approached or passed, Consul will issue warnings in the system logs.\nConsul will only cease operation after license termination, which occurs 10 years after license expiration and is defined in termination_time.\nStarting with Consul 1.14, and patch releases 1.13.3 and 1.12.6, Consul will support non-terminating licenses: Please contact your support representative for more details on non-terminating licenses. An expired license will not allow Consul versions released after the expiration date to run. It will not be possible to upgrade to a new version of Consul released after license expiration.\nThere are upgrade requirements that affect Consul Enterprise clients. Please review the upgrade requirements documentation.\nSame behavior as Consul clients. See answer for Does this affect client agents? \nConsul server agents will detect the absence of the license and immediately exit.\nConsul client agents will attempt to retrieve the license from servers if certain conditions are met:\nACLs are enabled.\nAn ACL token is provided to the client agent.\nThe client agents configuration contains retry_join addresses.\nThe retry join addresses are addresses of the Consul servers.\nConsul snapshot agents will attempt to retrieve the license from servers if certain conditions are met: ACLs are enabled, a ACL token is provided to the client agent, the client agents configuration contains retry_join addresses, the retry join addresses are addresses of the Consul servers.\nContact your HashiCorp Support contact for a development license.\nTrial install will cease operation 24 hours after 30-day license expiration: Trial licenses are not meant to be used in production. This is unlike non-trial licenses which provide a 10 year grace period as described in the answer for Q: Is there a grace period when licenses expire?.\nContact your organization's HashiCorp account team for information on how to renew your organization's enterprise license.\nContact your organization's HashiCorp account team for information on how to renew your organization's enterprise license.\nThe license files are not locked to a specific cluster or cluster node. The above changes apply to all nodes in a cluster.\nThis will not impact HCP Consul Dedicated.\nConsul Enterprise binaries starting with 1.10.0+ent, will be subject to EULA check. Release 1.10.0+ent introduces the EULA check for trial licenses (non-trial licenses already go through EULA check during contractual agreement).\nThe agreement to a EULA happens only once (when the user gets their license), Consul Enterprise will check for the presence of a valid license every time a node restarts.\nWhen a customer upgrades existing clusters to a 1.10.0+ent release, they need to have a valid license to successfully upgrade. This valid license must be auto-loaded.\nWhen a customer deploys new clusters to a 1.10.0+ent release, they need to have a valid license to successfully upgrade. This valid license must be on-disk (auto-loaded).\nNew Consul cluster deployments using 1.10.0+ent will need to have a valid license on servers to successfully deploy. This valid license must be on-disk (auto-loaded) or as an environment variable. Please see the upgrade requirements.\nVM\nRun consul license get -signed to extract the license from their running cluster. Store the license in a secure location on disk.\nSet up the necessary configuration so that when Consul Enterprise reboots it will have access to the required license. This could be via the client agent configuration file or an environment variable.\nVisit the Enterprise License Tutorial for detailed steps on how to install the license key.\nFollow the Consul upgrade documentation.\nKubernetes\nNOTE: If you are not upgrading Consul or your Helm chart version then no action is required.\nIn order to use Consul Enterprise 1.10.0 or greater on Kubernetes you must use version 0.32.0 or greater of the Helm chart.\nYou should already have a Consul Enterprise license set as a Kubernetes secret. If you do not, refer to how to obtain a copy of your license. Once you have the license then create a Kubernetes secret containing the license as described in Kubernetes - Consul Enterprise.\nFollow the Kubernetes Upgrade Docs to upgrade. No changes to your values.yaml file are needed to enable enterprise autoloading since this support is built in to consul-helm 0.32.0 and greater.\nWarning: If you are upgrading the Helm chart but not upgrading the Consul version, you must set server.enterpriseLicense.enableLicenseAutoload: false. See Kubernetes - Consul Enterprise for more details.\nVM\nAcquire a valid Consul Enterprise license. If you are an existing HashiCorp enterprise customer you may contact your organization's customer success manager (CSM) for information on how to get your organization's enterprise license.\nStore the license in a secure location on disk.\nSet up the necessary configuration so that when Consul Enterprise reboots it will have the required license. This could be via the client agent configuration file or an environment variable. Visit the Enterprise License Tutorial for detailed steps on how to install the license key.\nFollow the Consul upgrade documentation.\nKubernetes\nAcquire a valid Consul Enterprise license. If you are an existing HashiCorp enterprise customer you may contact your organization's customer success manager (CSM) for information on how to get your organization's enterprise license.\nSet up the necessary configuration so that when Consul Enterprise reboots it will have the required license. This could be via the client agent configuration file or an environment variable. Visit the Enterprise License Tutorial for detailed steps on how to install the license key.\nProceed with the helm upgrade instructions\nWhen downgrading to a version of Consul before 1.10.0+ent, customers will need to follow the previous process for applying an enterprise licenses to Consul Enterprise.\nVerify that you meet the upgrade requirements.\nAssume a scenario where there are three Consul server nodes:\nNode A: v1.9.5\nNode B: v1.10.0 [Leader]\nNode C: v1.9.5\nThe command consul license put is issued from Node A. This will result in an error due to how Consul routes calls to the server node, Node B in this example. Because Node A is a follower when the call consul license put is issued, the call will be redirected to Node B (leader). The consul license put operation will fail due to being removed from 1.10.0. This is a scenario that could occur if a customer downgrades from 1.10.0+ and the Consul leadership has not transferred back over to a node not running 1.10.0+. This also has the potential of occurring when upgrading if scheduled license updates or autoscaling groups recoveries are in place.\nIf you are using a Consul Enterprise version prior to 1.10.0 and decide to upgrade the Helm chart to version 0.32.0 or newer, but not the Consul version. You will need to add the following configuration.\nserver: enterpriseLicense: enableLicenseAutoload: false"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.15.x/enterprise/license/utilization-reporting",
  "text": "Automated license utilization reporting | Consul\nThis topic describes how to enable automated license utilization reporting in Consul Enterprise. This feature automatically sends license utilization data to HashiCorp so that you do not have to manually collect and report it. It also enables you to review your license usage with the monitoring solution you already use, such as Splunk and Datadog, as you optimize and manage your deployments.\nYou can use automated license utilization report to understand how much additional networking infrastructure you can deploy under your current contract. This feature helps you protect against overutilization and budget for predicted consumption.\nAutomated reporting shares the minimum data required to validate license utilization as defined in our contracts. This data mostly consists of computed metrics, and it will never contain Personal Identifiable Information (PII) or other sensitive information. Automated reporting shares the data with HashiCorp using a secure unidirectional HTTPS API and makes an auditable record in the product logs each time it submits a report. This process is GDPR-compliant.\nAutomated license utilization reporting does not support air-gapped installations, which are systems with no network interfaces.\nThe following versions of Consul Enterprise support automated license utilization reporting:\nPatch releases of Consul Enterprise v1.15.4 and newer.\nPatch releases of Consul Enterprise v1.14.8 and newer.\nPatch releases of Consul Enterprise v1.13.9 and newer.\nDownload a supported release from the Consul Versions page.\nBefore you enable automated reporting, make sure that outbound network traffic is configured correctly and upgrade your enterprise product to a version that supports it. If your installation is air-gapped or network settings are not in place, automated reporting will not work. \nTo enable automated reporting, complete the following steps:\nMake sure that your network allows HTTPS egress on port 443 from https://reporting.hashicorp.services by adding the following IP adddresses to your allow-list:\n100.20.70.12\n35.166.5.222\n23.95.85.111\n44.215.244.1\nAutomatic license utilization reporting starts sending data within roughly 24 hours. Check the product logs for records that the data sent successfully.\n[DEBUG] beginning snapshot export [DEBUG] creating payload [DEBUG] marshalling payload to json [DEBUG] generating authentication headers [DEBUG] creating request [DEBUG] sending request [DEBUG] performing request: method=POST url=https://census.license.hashicorp.services [DEBUG] recording audit record [INFO] reporting: Report sent: auditRecord={\"payload\":{\"payload_version\":\"1\",\"license_id\":\"d2cdd857-4202-5a45-70a6-e4b531050c34\",\"product\":\"consul\",\"product_version\":\"1.16.0-dev+ent\",\"export_timestamp\":\"2023-05-26T20:09:13.753921087Z\",\"snapshots\":[{\"snapshot_version\":1,\"snapshot_id\":\"0001J724F90F4XWQDSAA76ZQWA\",\"process_id\":\"01H1CTJPC1S8H7Q45MKTJ689ZW\",\"timestamp\":\"2023-05-26T20:09:13.753513962Z\",\"schema_version\":\"1.0.0\",\"service\":\"consul\",\"metrics\":{\"consul.billable.nodes\":{\"key\":\"consul.billable.nodes\",\"kind\":\"counter\",\"mode\":\"write\",\"value\":2},\"consul.billable.service_instances\":{\"key\":\"consul.billable.service_instances\",\"kind\":\"counter\",\"mode\":\"write\",\"value\":2}}}],\"metadata\":{}}} [DEBUG] completed recording audit record [DEBUG] export finished successfully\" \nIf your installation is air-gapped or your network does not allow the correct egress, the logs show an error. \n[DEBUG] reporting: beginning snapshot export [DEBUG] reporting: creating payload [DEBUG] reporting: marshalling payload to json [DEBUG] reporting: generating authentication headers [DEBUG] reporting: creating request [DEBUG] reporting: sending request [DEBUG] reporting: performing request: method=POST url=https://census.license.hashicorp.services [DEBUG] reporting: error status code received: statusCode=403 \nIn this case, reconfigure your network to allow egress and check the logs again in roughly 24 hours to confirm that automated reporting works correctly. \nIf your installation is air-gapped or you want to manually collect and report on the same license utilization metrics, you can opt out of automated reporting. \nManually reporting these metrics can be time consuming. Opting out of automated reporting does not mean that you also opt out from sending license utilization metrics. Customers who opt out of automated reporting are still required to manually collect and send license utilization metrics to HashiCorp. \nIf you are considering opting out because you are worried about the data, we strongly recommend that you review the example payloads before opting out. If you have concerns with any of the automatically reported data, raise these concerns with your account manager. \nThere are two methods for opting out of automated reporting: \nHCL configuration (recommended)\nEnvironment variable (requires restart)\nWe recommend opting out in your product's configuration file because it does not require a system restart. Add the following block to your configuration.hcl or configuration.json file.\nreporting { license { enabled = false } } \nWhen opting out using an environment variable, the system provides a startup message confirming that you have disabled automated reporting. Set the following environment variable to disable automated reporting:\n$ export OPTOUT_LICENSE_REPORTING=true \nAfter you set the environment variable, restart your system to complete the process for opting out.\nCheck your product logs roughly 24 hours after opting out to make sure that the system is not trying to send reports. Keep in mind that if your configuration file and environment variable differ, the environment variable setting takes precedence.\nHashiCorp collects the following utilization data as JSON payloads: exporter_version - The version of the licensing exporter\n{ \"payload\": { \"payload_version\": \"1\", \"license_id\": \"d2cdd857-4202-5a45-70a6-e4b531050c34\", \"product\": \"consul\", \"product_version\": \"1.16.0-dev+ent\", \"export_timestamp\": \"2023-05-26T20:09:13.753921087Z\", \"snapshots\": [ { \"snapshot_version\": 1, \"snapshot_id\": \"0001J724F90F4XWQDSAA76ZQWA\", \"process_id\": \"01H1CTJPC1S8H7Q45MKTJ689ZW\", \"timestamp\": \"2023-05-26T20:09:13.753513962Z\", \"schema_version\": \"1.0.0\", \"service\": \"consul\", \"metrics\": { \"consul.billable.nodes\": { \"key\": \"consul.billable.nodes\", \"kind\": \"counter\", \"mode\": \"write\", \"value\": 2 }, \"consul.billable.service_instances\": { \"key\": \"consul.billable.service_instances\", \"kind\": \"counter\", \"mode\": \"write\", \"value\": 2 } } } ], \"metadata\": {} } }"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.15.x/enterprise/license/faq",
  "text": "Enterprise License FAQ | Consul\nFrequently Asked Questions (FAQ)\nThis FAQ is for the license changes introduced in Consul Enterprise version 1.10.0. Consul Enterprise automatically loads Consul licenses when the server agent starts.\nStarting with Consul Enterprise 1.10.0, the license enablement process is different.\nHashiCorp Enterprise servers will no longer start without a license. Servers require licenses to be provided from either an environment variable or file. If the license is missing, invalid, or expired, the server will immediately exit. This check is part of the server boot-up process.\nIn previous versions of HashiCorp enterprise products, one server could distribute a license to other servers via the Raft protocol. This will no longer work since each server must be able to find a valid license during the startup process.\nAll customers on Consul Enterprise 1.8/1.9 must first upgrade their client and server agents to the latest patch release. During the upgrade the license file must also be configured on client agents in an environment variable or file path, otherwise the Consul agents will fail to retrieve the license with a valid agent token. The upgrade process varies if ACLs are enabled or disabled in the cluster. Refer to the instructions on upgrading to 1.10.x for details.\nPlease visit the Enterprise License Tutorial.\nThe list below is a great starting point for learning more about the license changes introduced in Consul Enterprise 1.10.0+ent.\nConsul Enterprise Upgrade Documentation\nConsul License Documentation\nLicense configuration values documentation\nInstall a HashiCorp Enterprise License Tutorial\nThe license changes introduced in 1.10.0 only affect Consul Enterprise. This impacts customers that have an enterprise binary (trial or non-trial licenses) downloaded from https://releases.hashicorp.com. The license changes do not impact customers with the baked-in licensed binaries. In a later release of Consul Enterprise, baked-in binaries will be deprecated.\nStarting with Consul Enterprise 1.10.0, a valid license is required on-disk (auto-loading) or as an environment variable for Consul Enterprise to successfully boot-up. The in-storage license feature will not be supported starting with Consul Enterprise 1.10.0+ent. All Consul Enterprise clusters using 1.10.0+ent must ensure that there is a valid license on-disk (auto-loaded) or as an environment variable.\nConsul Enterprise 1.10.0+ server agents require a valid license to start. The license can be provided on disk (auto-loaded) or as an environment variable. There is no longer a 6-hour trial period in which Consul Enterprise server agents can operate without a license.\nYesConsul will continue normal operation and can be restarted after license expiration as defined in expiration_time. As license expiration is approached or passed, Consul will issue warnings in the system logs.\nConsul will only cease operation after license termination, which occurs 10 years after license expiration and is defined in termination_time.\nStarting with Consul 1.14, and patch releases 1.13.3 and 1.12.6, Consul will support non-terminating licenses: Please contact your support representative for more details on non-terminating licenses. An expired license will not allow Consul versions released after the expiration date to run. It will not be possible to upgrade to a new version of Consul released after license expiration.\nThere are upgrade requirements that affect Consul Enterprise clients. Please review the upgrade requirements documentation.\nSame behavior as Consul clients. See answer for Does this affect client agents? \nConsul server agents will detect the absence of the license and immediately exit.\nConsul client agents will attempt to retrieve the license from servers if certain conditions are met:\nACLs are enabled.\nAn ACL token is provided to the client agent.\nThe client agents configuration contains retry_join addresses.\nThe retry join addresses are addresses of the Consul servers.\nConsul snapshot agents will attempt to retrieve the license from servers if certain conditions are met: ACLs are enabled, a ACL token is provided to the client agent, the client agents configuration contains retry_join addresses, the retry join addresses are addresses of the Consul servers.\nContact your HashiCorp Support contact for a development license.\nTrial install will cease operation 24 hours after 30-day license expiration: Trial licenses are not meant to be used in production. This is unlike non-trial licenses which provide a 10 year grace period as described in the answer for Q: Is there a grace period when licenses expire?.\nThe license files are not locked to a specific cluster or cluster node. The above changes apply to all nodes in a cluster.\nThis will not impact HCP Consul.\nConsul Enterprise binaries starting with 1.10.0+ent, will be subject to EULA check. Release 1.10.0+ent introduces the EULA check for trial licenses (non-trial licenses already go through EULA check during contractual agreement).\nThe agreement to a EULA happens only once (when the user gets their license), Consul Enterprise will check for the presence of a valid license every time a node restarts.\nWhen a customer upgrades existing clusters to a 1.10.0+ent release, they need to have a valid license to successfully upgrade. This valid license must be auto-loaded.\nWhen a customer deploys new clusters to a 1.10.0+ent release, they need to have a valid license to successfully upgrade. This valid license must be on-disk (auto-loaded).\nNew Consul cluster deployments using 1.10.0+ent will need to have a valid license on servers to successfully deploy. This valid license must be on-disk (auto-loaded) or as an environment variable. Please see the upgrade requirements.\nRun consul license get -signed to extract the license from their running cluster. Store the license in a secure location on disk.\nSet up the necessary configuration so that when Consul Enterprise reboots it will have access to the required license. This could be via the client agent configuration file or an environment variable.\nVisit the Enterprise License Tutorial for detailed steps on how to install the license key.\nNOTE: If you are not upgrading Consul or your Helm chart version then no action is required.\nIn order to use Consul Enterprise 1.10.0 or greater on Kubernetes you must use version 0.32.0 or greater of the Helm chart.\nYou should already have a Consul Enterprise license set as a Kubernetes secret. If you do not, refer to how to obtain a copy of your license. Once you have the license then create a Kubernetes secret containing the license as described in Kubernetes - Consul Enterprise.\nFollow the Kubernetes Upgrade Docs to upgrade. No changes to your values.yaml file are needed to enable enterprise autoloading since this support is built in to consul-helm 0.32.0 and greater.\nWarning: If you are upgrading the Helm chart but not upgrading the Consul version, you must set server.enterpriseLicense.enableLicenseAutoload: false. See Kubernetes - Consul Enterprise for more details.\nStore the license in a secure location on disk.\nProceed with the helm upgrade instructions\nWhen downgrading to a version of Consul before 1.10.0+ent, customers will need to follow the previous process for applying an enterprise licenses to Consul Enterprise.\nVerify that you meet the upgrade requirements.\nAssume a scenario where there are three Consul server nodes:\nNode A: v1.9.5\nNode B: v1.10.0 [Leader]\nNode C: v1.9.5\nThe command consul license put is issued from Node A. This will result in an error due to how Consul routes calls to the server node, Node B in this example. Because Node A is a follower when the call consul license put is issued, the call will be redirected to Node B (leader). The consul license put operation will fail due to being removed from 1.10.0. This is a scenario that could occur if a customer downgrades from 1.10.0+ and the Consul leadership has not transferred back over to a node not running 1.10.0+. This also has the potential of occurring when upgrading if scheduled license updates or autoscaling groups recoveries are in place.\nIf you are using a Consul Enterprise version prior to 1.10.0 and decide to upgrade the Helm chart to version 0.32.0 or newer, but not the Consul version. You will need to add the following configuration.\nserver: enterpriseLicense: enableLicenseAutoload: false"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.14.x/enterprise/license/utilization-reporting",
  "text": "Automated license utilization reporting | Consul\nThis topic describes how to enable automated license utilization reporting in Consul Enterprise. This feature automatically sends license utilization data to HashiCorp so that you do not have to manually collect and report it. It also enables you to review your license usage with the monitoring solution you already use, such as Splunk and Datadog, as you optimize and manage your deployments.\nYou can use automated license utilization report to understand how much additional networking infrastructure you can deploy under your current contract. This feature helps you protect against overutilization and budget for predicted consumption.\nAutomated reporting shares the minimum data required to validate license utilization as defined in our contracts. This data mostly consists of computed metrics, and it will never contain Personal Identifiable Information (PII) or other sensitive information. Automated reporting shares the data with HashiCorp using a secure unidirectional HTTPS API and makes an auditable record in the product logs each time it submits a report. This process is GDPR-compliant.\nAutomated license utilization reporting does not support air-gapped installations, which are systems with no network interfaces.\nThe following versions of Consul Enterprise support automated license utilization reporting:\nPatch releases of Consul Enterprise 1.14.8 and newer.\nPatch releases of Consul Enterprise 1.13.9 and newer.\nDownload a supported release from the Consul Versions page.\nBefore you enable automated reporting, make sure that outbound network traffic is configured correctly and upgrade your enterprise product to a version that supports it. If your installation is air-gapped or network settings are not in place, automated reporting will not work. \nTo enable automated reporting, complete the following steps:\nMake sure that your network allows HTTPS egress on port 443 from https://reporting.hashicorp.services by adding the following IP adddresses to your allow-list:\n100.20.70.12\n35.166.5.222\n23.95.85.111\n44.215.244.1\nAutomatic license utilization reporting starts sending data within roughly 24 hours. Check the product logs for records that the data sent successfully.\n[DEBUG] beginning snapshot export [DEBUG] creating payload [DEBUG] marshalling payload to json [DEBUG] generating authentication headers [DEBUG] creating request [DEBUG] sending request [DEBUG] performing request: method=POST url=https://census.license.hashicorp.services [DEBUG] recording audit record [INFO] reporting: Report sent: auditRecord={\"payload\":{\"payload_version\":\"1\",\"license_id\":\"d2cdd857-4202-5a45-70a6-e4b531050c34\",\"product\":\"consul\",\"product_version\":\"1.16.0-dev+ent\",\"export_timestamp\":\"2023-05-26T20:09:13.753921087Z\",\"snapshots\":[{\"snapshot_version\":1,\"snapshot_id\":\"0001J724F90F4XWQDSAA76ZQWA\",\"process_id\":\"01H1CTJPC1S8H7Q45MKTJ689ZW\",\"timestamp\":\"2023-05-26T20:09:13.753513962Z\",\"schema_version\":\"1.0.0\",\"service\":\"consul\",\"metrics\":{\"consul.billable.nodes\":{\"key\":\"consul.billable.nodes\",\"kind\":\"counter\",\"mode\":\"write\",\"value\":2},\"consul.billable.service_instances\":{\"key\":\"consul.billable.service_instances\",\"kind\":\"counter\",\"mode\":\"write\",\"value\":2}}}],\"metadata\":{}}} [DEBUG] completed recording audit record [DEBUG] export finished successfully\" \nIf your installation is air-gapped or your network does not allow the correct egress, the logs show an error. \n[DEBUG] reporting: beginning snapshot export [DEBUG] reporting: creating payload [DEBUG] reporting: marshalling payload to json [DEBUG] reporting: generating authentication headers [DEBUG] reporting: creating request [DEBUG] reporting: sending request [DEBUG] reporting: performing request: method=POST url=https://census.license.hashicorp.services [DEBUG] reporting: error status code received: statusCode=403 \nIn this case, reconfigure your network to allow egress and check the logs again in roughly 24 hours to confirm that automated reporting works correctly.\nIf your installation is air-gapped or you want to manually collect and report on the same license utilization metrics, you can opt out of automated reporting. \nManually reporting these metrics can be time consuming. Opting out of automated reporting does not mean that you also opt out from sending license utilization metrics. Customers who opt out of automated reporting are still required to manually collect and send license utilization metrics to HashiCorp. \nIf you are considering opting out because you are worried about the data, we strongly recommend that you review the example payloads before opting out. If you have concerns with any of the automatically reported data, raise these concerns with your account manager. \nThere are two methods for opting out of automated reporting: \nHCL configuration (recommended)\nEnvironment variable (requires restart)\nWe recommend opting out in your product's configuration file because it does not require a system restart. Add the following block to your configuration.hcl or configuration.json file.\nreporting { license { enabled = false } } \nWhen opting out using an environment variable, the system provides a startup message confirming that you have disabled automated reporting. Set the following environment variable to disable automated reporting:\n$ export OPTOUT_LICENSE_REPORTING=true \nAfter you set the environment variable, restart your system to complete the process for opting out.\nCheck your product logs roughly 24 hours after opting out to make sure that the system is not trying to send reports. Keep in mind that if your configuration file and environment variable differ, the environment variable setting takes precedence.\nHashiCorp collects the following utilization data as JSON payloads: exporter_version - The version of the licensing exporter\n{ \"payload\": { \"payload_version\": \"1\", \"license_id\": \"d2cdd857-4202-5a45-70a6-e4b531050c34\", \"product\": \"consul\", \"product_version\": \"1.16.0-dev+ent\", \"export_timestamp\": \"2023-05-26T20:09:13.753921087Z\", \"snapshots\": [ { \"snapshot_version\": 1, \"snapshot_id\": \"0001J724F90F4XWQDSAA76ZQWA\", \"process_id\": \"01H1CTJPC1S8H7Q45MKTJ689ZW\", \"timestamp\": \"2023-05-26T20:09:13.753513962Z\", \"schema_version\": \"1.0.0\", \"service\": \"consul\", \"metrics\": { \"consul.billable.nodes\": { \"key\": \"consul.billable.nodes\", \"kind\": \"counter\", \"mode\": \"write\", \"value\": 2 }, \"consul.billable.service_instances\": { \"key\": \"consul.billable.service_instances\", \"kind\": \"counter\", \"mode\": \"write\", \"value\": 2 } } } ], \"metadata\": {} } }"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.13.x/enterprise/license/faq",
  "text": "Enterprise License FAQ | Consul\nFrequently Asked Questions (FAQ)\nThis FAQ is for the license changes introduced in Consul Enterprise version 1.10.0. Consul Enterprise automatically loads Consul licenses when the server agent starts.\nStarting with Consul Enterprise 1.10.0, the license enablement process is different.\nHashiCorp Enterprise servers will no longer start without a license. Servers require licenses to be provided from either an environment variable or file. If the license is missing, invalid, or expired, the server will immediately exit. This check is part of the server boot-up process.\nIn previous versions of HashiCorp enterprise products, one server could distribute a license to other servers via the Raft protocol. This will no longer work since each server must be able to find a valid license during the startup process.\nAll customers on Consul Enterprise 1.8/1.9 must first upgrade their client and server agents to the latest patch release. During the upgrade the license file must also be configured on client agents in an environment variable or file path, otherwise the Consul agents will fail to retrieve the license with a valid agent token. The upgrade process varies if ACLs are enabled or disabled in the cluster. Refer to the instructions on upgrading to 1.10.x for details.\nPlease visit the Enterprise License Tutorial.\nThe list below is a great starting point for learning more about the license changes introduced in Consul Enterprise 1.10.0+ent.\nConsul Enterprise Upgrade Documentation\nConsul License Documentation\nLicense configuration values documentation\nInstall a HashiCorp Enterprise License Tutorial\nThe license changes introduced in 1.10.0 only affect Consul Enterprise. This impacts customers that have an enterprise binary (trial or non-trial licenses) downloaded from https://releases.hashicorp.com. The license changes do not impact customers with the baked-in licensed binaries. In a later release of Consul Enterprise, baked-in binaries will be deprecated.\nStarting with Consul Enterprise 1.10.0, a valid license is required on-disk (auto-loading) or as an environment variable for Consul Enterprise to successfully boot-up. The in-storage license feature will not be supported starting with Consul Enterprise 1.10.0+ent. All Consul Enterprise clusters using 1.10.0+ent must ensure that there is a valid license on-disk (auto-loaded) or as an environment variable.\nConsul Enterprise 1.10.0+ server agents require a valid license to start. The license can be provided on disk (auto-loaded) or as an environment variable. There is no longer a 6-hour trial period in which Consul Enterprise server agents can operate without a license.\nYesConsul will continue normal operation and can be restarted after license expiration as defined in expiration_time. As license expiration is approached or passed, Consul will issue warnings in the system logs.\nConsul will only cease operation after license termination, which occurs 10 years after license expiration and is defined in termination_time.\nStarting with Consul 1.14, and patch releases 1.13.3 and 1.12.6, Consul will support non-terminating licenses: Please contact your support representative for more details on non-terminating licenses. An expired license will not allow Consul versions released after the expiration date to run. It will not be possible to upgrade to a new version of Consul released after license expiration.\nThere are upgrade requirements that affect Consul Enterprise clients. Please review the upgrade requirements documentation.\nSame behavior as Consul clients. See answer for Does this affect client agents? \nConsul server agents will detect the absence of the license and immediately exit.\nConsul client agents will attempt to retrieve the license from servers if certain conditions are met:\nACLs are enabled.\nAn ACL token is provided to the client agent.\nThe client agents configuration contains start_join/retry_join addresses.\nThe start/retry join addresses are addresses of the Consul servers.\nConsul snapshot agents will attempt to retrieve the license from servers if certain conditions are met: ACLs are enabled, a ACL token is provided to the client agent, the client agents configuration contains start_join/retry_join addresses, the start/retry join addresses are addresses of the Consul servers.\nVisit consul.io/trial for a free 30-day trial license.\nTrial install will cease operation 24 hours after 30-day license expiration: Trial licenses are not meant to be used in production. This is unlike non-trial licenses which provide a 10 year grace period as described in the answer for Q: Is there a grace period when licenses expire?.\nContact your organization's HashiCorp account team or email support-softwaredelivery@hashicorp.com for information on how to renew your organization's enterprise license.\nContact your organization's HashiCorp account team or email support-softwaredelivery@hashicorp.com for information on how to renew your organization's enterprise license.\nThe license files are not locked to a specific cluster or cluster node. The above changes apply to all nodes in a cluster.\nThis will not impact HCP Consul.\nConsul Enterprise binaries starting with 1.10.0+ent, will be subject to EULA check. Release 1.10.0+ent introduces the EULA check for trial licenses (non-trial licenses already go through EULA check during contractual agreement).\nThe agreement to a EULA happens only once (when the user gets their license), Consul Enterprise will check for the presence of a valid license every time a node restarts.\nWhen a customer upgrades existing clusters to a 1.10.0+ent release, they need to have a valid license to successfully upgrade. This valid license must be auto-loaded.\nWhen a customer deploys new clusters to a 1.10.0+ent release, they need to have a valid license to successfully upgrade. This valid license must be on-disk (auto-loaded).\nNew Consul cluster deployments using 1.10.0+ent will need to have a valid license on servers to successfully deploy. This valid license must be on-disk (auto-loaded) or as an environment variable. Please see the upgrade requirements.\nRun consul license get -signed to extract the license from their running cluster. Store the license in a secure location on disk.\nSet up the necessary configuration so that when Consul Enterprise reboots it will have access to the required license. This could be via the client agent configuration file or an environment variable.\nVisit the Enterprise License Tutorial for detailed steps on how to install the license key.\nNOTE: If you are not upgrading Consul or your Helm chart version then no action is required.\nIn order to use Consul Enterprise 1.10.0 or greater on Kubernetes you must use version 0.32.0 or greater of the Helm chart.\nYou should already have a Consul Enterprise license set as a Kubernetes secret. If you do not, refer to how to obtain a copy of your license. Once you have the license then create a Kubernetes secret containing the license as described in Kubernetes - Consul Enterprise.\nFollow the Kubernetes Upgrade Docs to upgrade. No changes to your values.yaml file are needed to enable enterprise autoloading since this support is built in to consul-helm 0.32.0 and greater.\nWarning: If you are upgrading the Helm chart but not upgrading the Consul version, you must set server.enterpriseLicense.enableLicenseAutoload: false. See Kubernetes - Consul Enterprise for more details.\nAcquire a valid Consul Enterprise license. If you are an existing HashiCorp enterprise customer you may contact your organization's customer success manager (CSM) or email support-softwaredelivery@hashicorp.com for information on how to get your organization's enterprise license.\nStore the license in a secure location on disk.\nAcquire a valid Consul Enterprise license. If you are an existing HashiCorp enterprise customer you may contact your organization's customer success manager (CSM) or email support-softwaredelivery@hashicorp.com for information on how to get your organization's enterprise license.\nProceed with the helm upgrade instructions\nWhen downgrading to a version of Consul before 1.10.0+ent, customers will need to follow the previous process for applying an enterprise licenses to Consul Enterprise.\nVerify that you meet the upgrade requirements.\nAssume a scenario where there are three Consul server nodes:\nNode A: v1.9.5\nNode B: v1.10.0 [Leader]\nNode C: v1.9.5\nThe command consul license put is issued from Node A. This will result in an error due to how Consul routes calls to the server node, Node B in this example. Because Node A is a follower when the call consul license put is issued, the call will be redirected to Node B (leader). The consul license put operation will fail due to being removed from 1.10.0. This is a scenario that could occur if a customer downgrades from 1.10.0+ and the Consul leadership has not transferred back over to a node not running 1.10.0+. This also has the potential of occurring when upgrading if scheduled license updates or autoscaling groups recoveries are in place.\nIf you are using a Consul Enterprise version prior to 1.10.0 and decide to upgrade the Helm chart to version 0.32.0 or newer, but not the Consul version. You will need to add the following configuration.\nserver: enterpriseLicense: enableLicenseAutoload: false"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.13.x/enterprise/license/utilization-reporting",
  "text": "Patch releases of Consul Enterprise v1.13.9 and newer.\nBefore you enable automated reporting, make sure that outbound network traffic is configured correctly and upgrade your enterprise product to a version that supports it. If your installation is air-gapped or network settings are not in place, automated reporting will not work.\nMake sure that your network allows HTTPS egress on port 443 from https://reporting.hashicorp.services by adding the following IP adddresses to your allow-list:\nIf your installation is air-gapped or your network does not allow the correct egress, the logs show an error. \nIn this case, reconfigure your network to allow egress and check the logs again in roughly 24 hours to confirm that automated reporting works correctly.\nIf your installation is air-gapped or you want to manually collect and report on the same license utilization metrics, you can opt out of automated reporting. \nManually reporting these metrics can be time consuming. Opting out of automated reporting does not mean that you also opt out from sending license utilization metrics. Customers who opt out of automated reporting are still required to manually collect and send license utilization metrics to HashiCorp. \nIf you are considering opting out because you are worried about the data, we strongly recommend that you review the example payloads before opting out. If you have concerns with any of the automatically reported data, raise these concerns with your account manager. \nThere are two methods for opting out of automated reporting: "
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.14.x/enterprise/license/faq",
  "text": "The client agents configuration contains start_join/retry_join addresses.\nThe start/retry join addresses are addresses of the Consul servers.\nConsul snapshot agents will attempt to retrieve the license from servers if certain conditions are met: ACLs are enabled, a ACL token is provided to the client agent, the client agents configuration contains start_join/retry_join addresses, the start/retry join addresses are addresses of the Consul servers.\nContact your HashiCorp Support contact for a development license.\nThis will not impact HCP Consul."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/faq",
  "text": "Consul | HashiCorp Developer\nWe couldn't find the page you're looking for.\nPlease check the url you entered for typos, go back to the page you came from, or go to one of the pages below.\nHashiCorp Cloud Platform\nTerraform\nPacker\nConsul\nVault\nBoundary\nNomad\nWaypoint\nVagrant\nHashiCorp Developer"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/agent/cloud-auto-join",
  "text": "Consul | HashiCorp Developer\nWe couldn't find the page you're looking for.\nPlease check the url you entered for typos, go back to the page you came from, or go to one of the pages below.\nHashiCorp Cloud Platform\nTerraform\nPacker\nConsul\nVault\nBoundary\nNomad\nWaypoint\nVagrant\nHashiCorp Developer"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.10.x/compatibility",
  "text": "This page does not exist for version v1.10.x.\nPlease select either the most recent version or a valid version that includes the page you are looking for."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.10.x/upgrading/compatibility",
  "text": "Consul Protocol Compatibility Promise | Consul\nWe expect Consul to run in large clusters of long-running agents. Because safely upgrading agents in this sort of environment relies heavily on backwards compatibility, we have a strong commitment to keeping different Consul versions protocol-compatible with each other.\nWe promise that every subsequent release of Consul will remain backwards compatible with at least one prior version. Concretely: version 0.5 can speak to 0.4 (and vice versa) but may not be able to speak to 0.1.\nBackwards compatibility is automatic unless otherwise noted. Consul agents by default will speak the latest protocol but can understand earlier ones.\nNote: If speaking an earlier protocol, new features may not be available.\nThe ability for an agent to speak an earlier protocol is to ensure that any agent can be upgraded without cluster disruption. Consul agents can be updated one at a time, one version at a time.\nFor more details on the specifics of upgrading, see the upgrading page.\nConsul VersionProtocol Compatibility\n0.1 - 0.3\t1\t\n0.4\t1, 2\t\n0.5\t1, 2. 0.5.X servers cannot be mixed with older servers.\t\n0.6\t1, 2, 3\t\n>= 0.7\t2, 3. Will automatically use protocol > 2 when speaking to compatible agents\t\nNote: Raft Protocol is versioned separately, but maintains compatibility with at least one prior version. See here for details."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.10.x/platform/k8s/upgrading",
  "text": "This page does not exist for version v1.10.x.\nPlease select either the most recent version or a valid version that includes the page you are looking for."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/upgrade-specific",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/docs/agent/services",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/docs/acl/auth-methods/jwt",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.10.x/operator/area",
  "text": "Commands: Operator Area | Consul\nCommand: consul operator area\nConsul Enterprise supports network areas, which are operator-defined relationships between servers in two different Consul datacenters. The operator area command is used to interact with Consul's network area subsystem.\nUnlike Consul's WAN feature, network areas use just the server RPC port for communication, and relationships can be made between independent pairs of datacenters, so not all servers need to be fully connected. This allows for complex topologies among Consul datacenters like hub/spoke and more general trees.\nSee the Network Areas Guide for more details.\nUsage: consul operator area <subcommand> [options] The operator area command is used to interact with Consul's network area subsystem. Network areas are used to link together Consul servers in different Consul datacenters. With network areas, Consul datacenters can be linked together in ways other than a fully-connected mesh, as is required for Consul's WAN. Subcommands: create Create a new network area delete Remove a network area join Join Consul servers into an existing network area list List network areas members Display Consul server members present in network areas update Update the configuration of a network area \nIf ACLs are enabled, the client will need to supply an ACL Token with operator read or write privileges to use these commands.\nThis command creates a new network area.\nUsage: consul operator area create [options]\nAPI Options\n-ca-file=<value> - Path to a CA file to use for TLS when communicating with Consul. This can also be specified via the CONSUL_CACERT environment variable.\n-ca-path=<value> - Path to a directory of CA certificates to use for TLS when communicating with Consul. This can also be specified via the CONSUL_CAPATH environment variable.\n-client-cert=<value> - Path to a client cert file to use for TLS when verify_incoming is enabled. This can also be specified via the CONSUL_CLIENT_CERT environment variable.\n-client-key=<value> - Path to a client key file to use for TLS when verify_incoming is enabled. This can also be specified via the CONSUL_CLIENT_KEY environment variable.\n-http-addr=<addr> - Address of the Consul agent with the port. This can be an IP address or DNS address, but it must include the port. This can also be specified via the CONSUL_HTTP_ADDR environment variable. In Consul 0.8 and later, the default value is http://127.0.0.1:8500, and https can optionally be used instead. The scheme can also be set to HTTPS by setting the environment variable CONSUL_HTTP_SSL=true. This may be a unix domain socket using unix:///path/to/socket if the agent is configured to listen that way.\n-tls-server-name=<value> - The server name to use as the SNI host when connecting via TLS. This can also be specified via the CONSUL_TLS_SERVER_NAME environment variable.\n-token=<value> - ACL token to use in the request. This can also be specified via the CONSUL_HTTP_TOKEN environment variable. If unspecified, the query will default to the token of the Consul agent at the HTTP address.\n-token-file=<value> - File containing the ACL token to use in the request instead of one specified via the -token argument or CONSUL_HTTP_TOKEN environment variable. This can also be specified via the CONSUL_HTTP_TOKEN_FILE environment variable.\n-datacenter=<name> - Name of the datacenter to query. If unspecified, the query will default to the datacenter of the Consul agent at the HTTP address.\n-stale - Permit any Consul server (non-leader) to respond to this request. This allows for lower latency and higher throughput, but can result in stale data. This option has no effect on non-read operations. The default value is false.\nCommand Options\n-peer-datacenter=<value> - Declares the peer Consul datacenter that will make up the other side of this network area. Network areas always involve a pair of datacenters: the datacenter where the area was created, and the peer datacenter. This is required.\n-retry-join=<value> Specifies the address of a Consul server to join to, such as an IP or hostname with an optional port number. This is optional and can be specified multiple times.\n-use-tls=<value> Specifies whether gossip over this area should be encrypted with TLS if possible. Must be either true or false.\nThe output looks like this, displaying the ID of the newly-created network area:\nCreated area \"d2872ec5-68ea-b862-b75d-0bee99aca100\" with peer datacenter \"other\"! \nThe return code will indicate success or failure.\nThis command deletes an existing network area.\nUsage: consul operator area delete [options]\nAPI Options\n-ca-file=<value> - Path to a CA file to use for TLS when communicating with Consul. This can also be specified via the CONSUL_CACERT environment variable.\n-ca-path=<value> - Path to a directory of CA certificates to use for TLS when communicating with Consul. This can also be specified via the CONSUL_CAPATH environment variable.\n-client-cert=<value> - Path to a client cert file to use for TLS when verify_incoming is enabled. This can also be specified via the CONSUL_CLIENT_CERT environment variable.\n-client-key=<value> - Path to a client key file to use for TLS when verify_incoming is enabled. This can also be specified via the CONSUL_CLIENT_KEY environment variable.\n-http-addr=<addr> - Address of the Consul agent with the port. This can be an IP address or DNS address, but it must include the port. This can also be specified via the CONSUL_HTTP_ADDR environment variable. In Consul 0.8 and later, the default value is http://127.0.0.1:8500, and https can optionally be used instead. The scheme can also be set to HTTPS by setting the environment variable CONSUL_HTTP_SSL=true. This may be a unix domain socket using unix:///path/to/socket if the agent is configured to listen that way.\n-tls-server-name=<value> - The server name to use as the SNI host when connecting via TLS. This can also be specified via the CONSUL_TLS_SERVER_NAME environment variable.\n-token=<value> - ACL token to use in the request. This can also be specified via the CONSUL_HTTP_TOKEN environment variable. If unspecified, the query will default to the token of the Consul agent at the HTTP address.\n-token-file=<value> - File containing the ACL token to use in the request instead of one specified via the -token argument or CONSUL_HTTP_TOKEN environment variable. This can also be specified via the CONSUL_HTTP_TOKEN_FILE environment variable.\n-datacenter=<name> - Name of the datacenter to query. If unspecified, the query will default to the datacenter of the Consul agent at the HTTP address.\n-stale - Permit any Consul server (non-leader) to respond to this request. This allows for lower latency and higher throughput, but can result in stale data. This option has no effect on non-read operations. The default value is false.\nCommand Options\n-id=<value> - Looks up the area to operate on by its ID. This can be given instead of a peer datacenter.\n-peer-datacenter=<value> - Looks up the area to operate on by its peer datacenter. This can be given instead of an ID.\nThe output looks like this:\nDeleted area \"154941b0-80e2-9d69-c560-ab2c02807332\"! \nThe return code will indicate success or failure.\nThis command joins Consul servers into an existing network area by address, such as an IP or hostname with an optional port. Multiple addresses may be given.\nUsage: consul operator area join [options] ADDRESSES\nAPI Options\n-ca-file=<value> - Path to a CA file to use for TLS when communicating with Consul. This can also be specified via the CONSUL_CACERT environment variable.\n-ca-path=<value> - Path to a directory of CA certificates to use for TLS when communicating with Consul. This can also be specified via the CONSUL_CAPATH environment variable.\n-client-cert=<value> - Path to a client cert file to use for TLS when verify_incoming is enabled. This can also be specified via the CONSUL_CLIENT_CERT environment variable.\n-client-key=<value> - Path to a client key file to use for TLS when verify_incoming is enabled. This can also be specified via the CONSUL_CLIENT_KEY environment variable.\n-http-addr=<addr> - Address of the Consul agent with the port. This can be an IP address or DNS address, but it must include the port. This can also be specified via the CONSUL_HTTP_ADDR environment variable. In Consul 0.8 and later, the default value is http://127.0.0.1:8500, and https can optionally be used instead. The scheme can also be set to HTTPS by setting the environment variable CONSUL_HTTP_SSL=true. This may be a unix domain socket using unix:///path/to/socket if the agent is configured to listen that way.\n-tls-server-name=<value> - The server name to use as the SNI host when connecting via TLS. This can also be specified via the CONSUL_TLS_SERVER_NAME environment variable.\n-token=<value> - ACL token to use in the request. This can also be specified via the CONSUL_HTTP_TOKEN environment variable. If unspecified, the query will default to the token of the Consul agent at the HTTP address.\n-token-file=<value> - File containing the ACL token to use in the request instead of one specified via the -token argument or CONSUL_HTTP_TOKEN environment variable. This can also be specified via the CONSUL_HTTP_TOKEN_FILE environment variable.\n-datacenter=<name> - Name of the datacenter to query. If unspecified, the query will default to the datacenter of the Consul agent at the HTTP address.\n-stale - Permit any Consul server (non-leader) to respond to this request. This allows for lower latency and higher throughput, but can result in stale data. This option has no effect on non-read operations. The default value is false.\nCommand Options\n-id=<value> - Looks up the area to operate on by its ID. This can be given instead of a peer datacenter.\n-peer-datacenter=<value> - Looks up the area to operate on by its peer datacenter. This can be given instead of an ID.\nThe output looks like this:\nAddress Joined Error 10.1.2.3 false failed to connect to \"10.1.2.3:8300\": dial tcp 10.1.2.3:8300: i/o timeout 10.1.2.4 true (none) 10.1.2.5 true (none) \nThe Error field will have a human-readable error message if Consul was unable to join the given address.\nThe return code will indicate success or failure.\nThis command lists all network areas.\nUsage: consul operator area list [options]\nAPI Options\n-ca-file=<value> - Path to a CA file to use for TLS when communicating with Consul. This can also be specified via the CONSUL_CACERT environment variable.\n-ca-path=<value> - Path to a directory of CA certificates to use for TLS when communicating with Consul. This can also be specified via the CONSUL_CAPATH environment variable.\n-client-cert=<value> - Path to a client cert file to use for TLS when verify_incoming is enabled. This can also be specified via the CONSUL_CLIENT_CERT environment variable.\n-client-key=<value> - Path to a client key file to use for TLS when verify_incoming is enabled. This can also be specified via the CONSUL_CLIENT_KEY environment variable.\n-http-addr=<addr> - Address of the Consul agent with the port. This can be an IP address or DNS address, but it must include the port. This can also be specified via the CONSUL_HTTP_ADDR environment variable. In Consul 0.8 and later, the default value is http://127.0.0.1:8500, and https can optionally be used instead. The scheme can also be set to HTTPS by setting the environment variable CONSUL_HTTP_SSL=true. This may be a unix domain socket using unix:///path/to/socket if the agent is configured to listen that way.\n-tls-server-name=<value> - The server name to use as the SNI host when connecting via TLS. This can also be specified via the CONSUL_TLS_SERVER_NAME environment variable.\n-token=<value> - ACL token to use in the request. This can also be specified via the CONSUL_HTTP_TOKEN environment variable. If unspecified, the query will default to the token of the Consul agent at the HTTP address.\n-token-file=<value> - File containing the ACL token to use in the request instead of one specified via the -token argument or CONSUL_HTTP_TOKEN environment variable. This can also be specified via the CONSUL_HTTP_TOKEN_FILE environment variable.\n-datacenter=<name> - Name of the datacenter to query. If unspecified, the query will default to the datacenter of the Consul agent at the HTTP address.\n-stale - Permit any Consul server (non-leader) to respond to this request. This allows for lower latency and higher throughput, but can result in stale data. This option has no effect on non-read operations. The default value is false.\nThe output looks like this:\nArea PeerDC RetryJoin 6a52a0af-62e2-dad4-da60-e66acc37096c dc2 10.1.2.3,10.1.2.4,10.1.2.5 96e33424-f5ce-9fcd-ecab-27974e36678f other (none) \nArea is the ID of the network area.\nPeerDC is the peer datacenter for the area.\nRetryJoin is the list of servers to join, defined when the area was created.\nThe return code will indicate success or failure.\nThis command displays Consul server nodes present in a network area, or all areas if no area is specified.\nUsage: consul operator area members [options]\nCommand Options\n-id=<value> - Looks up the area to operate on by its ID. This can be given instead of a peer datacenter.\n-peer-datacenter=<value> - Looks up the area to operate on by its peer datacenter. This can be given instead of an ID.\nThe output looks like this:\nArea Node Address Status Build Protocol DC RTT 6a52a0af-62e2-dad4-da60-e66acc37096c node-1.dc1 127.0.0.1:8300 alive 0.8.0 2 dc1 0s 6a52a0af-62e2-dad4-da60-e66acc37096c node-2.dc1 127.0.0.2:8300 alive 0.8.0 2 dc1 594.191s 96e33424-f5ce-9fcd-ecab-27974e36678f node-1.dc1 127.0.0.1:8300 alive 0.8.0 2 dc1 0s 96e33424-f5ce-9fcd-ecab-27974e36678f node-2.dc1 127.0.0.2:8300 alive 0.8.0 2 dc1 634.109s \nArea is the ID of the network area.\nNode is the name of the node.\nAddress is the IP and server RPC port for the node.\nStatus is the current health status of the node, as determined by the network area distributed failure detector. This will be \"alive\", \"leaving\", \"left\", or \"failed\". A \"failed\" status means that other servers are not able to probe this server over its server RPC interface.\nBuild has the Consul version running on the node.\nProtocol is the protocol version being spoken by the node.\nDC is the node's Consul datacenter.\nRTT is an estimated network round trip time from the server answering the query to the given server, in a human-readable format. This is computed using network coordinates.\nThis command updates the configuration of network area.\nUsage: consul operator area update [options]\n-id=<value> - Looks up the area to operate on by its ID. This can be given instead of a peer datacenter.\n-peer-datacenter=<value> - Declares the peer Consul datacenter that will make up the other side of this network area. Network areas always involve a pair of datacenters: the datacenter where the area was created, and the peer datacenter. This is required.\n-use-tls=<value> Specifies whether gossip over this area should be encrypted with TLS if possible. Must be either true or false.\nUpdated area \"d2872ec5-68ea-b862-b75d-0bee99aca100\" "
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.10.x/k8s/connect/observability/metrics",
  "text": "Metrics | Consul | HashiCorp Developer\nConsul on Kubernetes integrates with Prometheus and Grafana to provide metrics for Consul Service Mesh. The metrics available are:\nConnect Service metrics\nSidecar proxy metrics\nConsul agent metrics\nIngress, Terminating and Mesh Gateway metrics\nSpecific sidecar proxy metrics can also be seen in the Consul UI Topology Visualization view. This section documents how to enable each of these.\nNote: Metrics will be supported in Consul-helm >= 0.31.0 and consul-k8s >= 0.25.0. However, enabling the metrics merging feature with Helm value (defaultEnableMerging) or annotation (consul.hashicorp.com/enable-metrics-merging) can only be used with Consul 1.10.0 and above. The other metrics configuration can still be used before Consul 1.10.0.\nPrometheus annotations are used to instruct Prometheus to scrape metrics from Pods. Prometheus annotations only support scraping from one endpoint on a Pod, so Consul on Kubernetes supports metrics merging whereby service metrics and sidecar proxy metrics are merged into one endpoint. If there are no service metrics, it also supports just scraping the sidecar proxy metrics.\nThe diagram below illustrates how the metrics integration works when merging is enabled:\nConnect service metrics can be configured with the Helm values nested under connectInject.metrics.\nMetrics and metrics merging can be enabled by default for all connect-injected Pods with the following Helm values:\nconnectInject: metrics: defaultEnabled: true # by default, this inherits from the value global.metrics.enabled defaultEnableMerging: true \nThey can also be overridden on a per-Pod basis using the annotations consul.hashicorp.com/enable-metrics and consul.hashicorp.com/enable-metrics-merging.\nIn most cases, the default settings will be sufficient. If you are encountering issues with colliding ports or service metrics not being merged, you may need to change the defaults.\nThe Prometheus annotations configure the endpoint to scrape the metrics from. As shown in the diagram, the annotations point to a listener on 0.0.0.0:20200 on the Envoy sidecar. This listener and the corresponding Prometheus annotations can be configured with the following Helm values (or overridden on a per-Pod basis with Consul annotations consul.hashicorp.com/prometheus-scrape-port and consul.hashicorp.com/prometheus-scrape-path):\nconnectInject: metrics: defaultPrometheusScrapePort: 20200 defaultPrometheusScrapePath: \"/metrics\" \nThose Helm values will result in the following Prometheus annotations being automatically added to the Pod for scraping:\nmetadata: annotations: prometheus.io/scrape: \"true\" prometheus.io/path: \"/metrics\" prometheus.io/port: \"20200\" \nWhen metrics alone are enabled, the listener in the diagram on 0.0.0.0:20200 would point directly at the sidecar metrics endpoint, rather than the merged metrics endpoint. The Prometheus scraping annotations would stay the same.\nWhen metrics and metrics merging are both enabled, metrics are combined from the service and the sidecar proxy, and exposed via a local server on the Consul sidecar for scraping. This endpoint is called the merged metrics endpoint and defaults to 127.0.0.1:20100/stats/prometheus. The listener will target the merged metrics endpoint in the above case. It can be configured with the following Helm values (or overridden on a per-Pod basis with consul.hashicorp.com/merged-metrics-port):\nconnectInject: metrics: defaultMergedMetricsPort: 20100 \nThe endpoint to scrape service metrics from can be configured only on a per-Pod basis via the Pod annotations consul.hashicorp.com/service-metrics-port and consul.hashicorp.com/service-metrics-path. If these are not configured, the service metrics port will default to the port used to register the service with Consul (consul.hashicorp.com/connect-service-port), which in turn defaults to the first port on the first container of the Pod. The service metrics path will default to /metrics.\nMetrics from the Consul server and client Pods can be scraped via Prometheus by setting the field global.metrics.enableAgentMetrics to true. Additionally, one can configure the metrics retention time on the agents by configuring the field global.metrics.agentMetricsRetentionTime which expects a duration and defaults to \"1m\". This value must be greater than \"0m\" for the Consul servers and clients to emit metrics at all. As the Prometheus deployment currently does not support scraping TLS endpoints, agent metrics are currently unsupported when TLS is enabled.\nglobal: metrics: enabled: true enableAgentMetrics: true agentMetricsRetentionTime: \"1m\" \nMetrics from the Consul gateways, namely the Ingress Gateways, Terminating Gateways and the Mesh Gateways can be scraped via Prometheus by setting the field global.metrics.enableGatewayMetrics to true. The gateways emit standard Envoy proxy metrics. To ensure that the metrics are not exposed to the public internet (as Mesh and Ingress gateways can have public IPs), their metrics endpoints are exposed on the Pod IP of the respective gateway instance, rather than on all interfaces on 0.0.0.0.\nglobal: metrics: enabled: true enableGatewayMetrics: true \nConsuls built-in UI has a topology visualization for services part of the Consul Service Mesh. The topology visualization has the ability to fetch basic metrics from a metrics provider for each service and display those metrics as part of the topology visualization.\nThe diagram below illustrates how the UI displays service metrics for a sample application:\nThe topology view is configured under ui.metrics. This will enable the Consul UI to query the provider specified by ui.metrics.provider at the URL of the Prometheus server ui.metrics.baseURL to display sidecar proxy metrics for the service. The UI will display some specific sidecar proxy Prometheus metrics when ui.metrics.enabled is true and ui.enabled is true. The value of ui.metrics.enabled defaults to \"-\" which means it will inherit from the value of global.metrics.enabled.\nui: enabled: true metrics: enabled: true # by default, this inherits from the value global.metrics.enabled provider: \"prometheus\" baseURL: http://prometheus-server \nThe Helm chart contains demo manifests for deploying Prometheus. It can be installed with Helm via prometheus.enabled. This manifest is based on the community manifest for Prometheus. The Prometheus deployment is designed to allow quick bootstrapping for trial and demo use cases, and is not recommended for production use-cases.\nPrometheus will be installed in the same namespace as Consul, and will be installed and uninstalled along with the Consul installation.\nGrafana can optionally be utilized with Prometheus to display metrics. The installation and configuration of Grafana must be managed separately from the Consul Helm chart. The Layer 7 Observability with Prometheus, Grafana, and Kubernetes) tutorial provides an installation walkthrough using Helm.\nprometheus: enabled: true"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.9.x/upgrading/compatibility",
  "text": "Consul Protocol Compatibility Promise | Consul\nWe expect Consul to run in large clusters of long-running agents. Because safely upgrading agents in this sort of environment relies heavily on backwards compatibility, we have a strong commitment to keeping different Consul versions protocol-compatible with each other.\nWe promise that every subsequent release of Consul will remain backwards compatible with at least one prior version. Concretely: version 0.5 can speak to 0.4 (and vice versa) but may not be able to speak to 0.1.\nBackwards compatibility is automatic unless otherwise noted. Consul agents by default will speak the latest protocol but can understand earlier ones.\nNote: If speaking an earlier protocol, new features may not be available.\nThe ability for an agent to speak an earlier protocol is to ensure that any agent can be upgraded without cluster disruption. Consul agents can be updated one at a time, one version at a time.\nFor more details on the specifics of upgrading, see the upgrading page.\nConsul VersionProtocol Compatibility\n0.1 - 0.3\t1\t\n0.4\t1, 2\t\n0.5\t1, 2. 0.5.X servers cannot be mixed with older servers.\t\n0.6\t1, 2, 3\t\n>= 0.7\t2, 3. Will automatically use protocol > 2 when speaking to compatible agents\t\nNote: Raft Protocol is versioned separately, but maintains compatibility with at least one prior version. See here for details."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/agent/watches",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.9.x/compatibility",
  "text": "This page does not exist for version v1.9.x.\nPlease select either the most recent version or a valid version that includes the page you are looking for."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.9.x/platform/k8s/upgrading",
  "text": "This page does not exist for version v1.9.x.\nPlease select either the most recent version or a valid version that includes the page you are looking for."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.9.x/discovery/checks",
  "text": "Monitor Services - Check Definitions | Consul\nOne of the primary roles of the agent is management of system-level and application-level health checks. A health check is considered to be application-level if it is associated with a service. If not associated with a service, the check monitors the health of the entire node. Review the health checks tutorial to get a more complete example on how to leverage health check capabilities in Consul.\nA check is defined in a configuration file or added at runtime over the HTTP interface. Checks created via the HTTP interface persist with that node.\nThere are several different kinds of checks:\nScript + Interval - These checks depend on invoking an external application that performs the health check, exits with an appropriate exit code, and potentially generates some output. A script is paired with an invocation interval (e.g. every 30 seconds). This is similar to the Nagios plugin system. The output of a script check is limited to 4KB. Output larger than this will be truncated. By default, Script checks will be configured with a timeout equal to 30 seconds. It is possible to configure a custom Script check timeout value by specifying the timeout field in the check definition. When the timeout is reached on Windows, Consul will wait for any child processes spawned by the script to finish. For any other system, Consul will attempt to force-kill the script and any child processes it has spawned once the timeout has passed. In Consul 0.9.0 and later, script checks are not enabled by default. To use them you can either use :\nenable_local_script_checks: enable script checks defined in local config files. Script checks defined via the HTTP API will not be allowed.\nenable_script_checks: enable script checks regardless of how they are defined.\nSecurity Warning: Enabling script checks in some configurations may introduce a remote execution vulnerability which is known to be targeted by malware. We strongly recommend enable_local_script_checks instead. See this blog post for more details.\nHTTP + Interval - These checks make an HTTP GET request to the specified URL, waiting the specified interval amount of time between requests (eg. 30 seconds). The status of the service depends on the HTTP response code: any 2xx code is considered passing, a 429 Too ManyRequests is a warning, and anything else is a failure. This type of check should be preferred over a script that uses curl or another external process to check a simple HTTP operation. By default, HTTP checks are GET requests unless the method field specifies a different method. Additional header fields can be set through the header field which is a map of lists of strings, e.g. {\"x-foo\": [\"bar\", \"baz\"]}. By default, HTTP checks will be configured with a request timeout equal to 10 seconds.\nIt is possible to configure a custom HTTP check timeout value by specifying the timeout field in the check definition. The output of the check is limited to roughly 4KB. Responses larger than this will be truncated. HTTP checks also support TLS. By default, a valid TLS certificate is expected. Certificate verification can be turned off by setting the tls_skip_verify field to true in the check definition.\nConsul follows HTTP redirects by default. Set the disable_redirects field to true to disable redirects.\nTCP + Interval - These checks make a TCP connection attempt to the specified IP/hostname and port, waiting interval amount of time between attempts (e.g. 30 seconds). If no hostname is specified, it defaults to \"localhost\". The status of the service depends on whether the connection attempt is successful (ie - the port is currently accepting connections). If the connection is accepted, the status is success, otherwise the status is critical. In the case of a hostname that resolves to both IPv4 and IPv6 addresses, an attempt will be made to both addresses, and the first successful connection attempt will result in a successful check. This type of check should be preferred over a script that uses netcat or another external process to check a simple socket operation. By default, TCP checks will be configured with a request timeout of 10 seconds. It is possible to configure a custom TCP check timeout value by specifying the timeout field in the check definition.\nTime to Live (TTL) - These checks retain their last known state for a given TTL. The state of the check must be updated periodically over the HTTP interface. If an external system fails to update the status within a given TTL, the check is set to the failed state. This mechanism, conceptually similar to a dead man's switch, relies on the application to directly report its health. For example, a healthy app can periodically PUT a status update to the HTTP endpoint; if the app fails, the TTL will expire and the health check enters a critical state. The endpoints used to update health information for a given check are: pass, warn, fail, and update. TTL checks also persist their last known status to disk. This allows the Consul agent to restore the last known status of the check across restarts. Persisted check status is valid through the end of the TTL from the time of the last check.\nDocker + Interval - These checks depend on invoking an external application which is packaged within a Docker Container. The application is triggered within the running container via the Docker Exec API. We expect that the Consul agent user has access to either the Docker HTTP API or the unix socket. Consul uses $DOCKER_HOST to determine the Docker API endpoint. The application is expected to run, perform a health check of the service running inside the container, and exit with an appropriate exit code. The check should be paired with an invocation interval. The shell on which the check has to be performed is configurable which makes it possible to run containers which have different shells on the same host. Check output for Docker is limited to 4KB. Any output larger than this will be truncated. In Consul 0.9.0 and later, the agent must be configured with enable_script_checks set to true in order to enable Docker health checks.\ngRPC + Interval - These checks are intended for applications that support the standard gRPC health checking protocol. The state of the check will be updated by probing the configured endpoint, waiting interval amount of time between probes (eg. 30 seconds). By default, gRPC checks will be configured with a default timeout of 10 seconds. It is possible to configure a custom timeout value by specifying the timeout field in the check definition. gRPC checks will default to not using TLS, but TLS can be enabled by setting grpc_use_tls in the check definition. If TLS is enabled, then by default, a valid TLS certificate is expected. Certificate verification can be turned off by setting the tls_skip_verify field to true in the check definition. To check on a specific service instead of the whole gRPC server, add the service identifier after the gRPC check's endpoint in the following format /:service_identifier.\nAlias - These checks alias the health state of another registered node or service. The state of the check will be updated asynchronously, but is nearly instant. For aliased services on the same agent, the local state is monitored and no additional network resources are consumed. For other services and nodes, the check maintains a blocking query over the agent's connection with a current server and allows stale requests. If there are any errors in watching the aliased node or service, the check state will be critical. For the blocking query, the check will use the ACL token set on the service or check definition or otherwise will fall back to the default ACL token set with the agent (acl_token).\nA script check:\n{ \"check\": { \"id\": \"mem-util\", \"name\": \"Memory utilization\", \"args\": [\"/usr/local/bin/check_mem.py\", \"-limit\", \"256MB\"], \"interval\": \"10s\", \"timeout\": \"1s\" } } \nA HTTP check:\ncheck = { id = \"api\" name = \"HTTP API on port 5000\" http = \"https://localhost:5000/health\" tls_server_name = \"\" tls_skip_verify = false method = \"POST\" header = { Content-Type = [\"application/json\"] } body = \"{\\\"method\\\":\\\"health\\\"}\" disable_redirects = true interval = \"10s\" timeout = \"1s\" } \n{ \"check\": { \"id\": \"api\", \"name\": \"HTTP API on port 5000\", \"http\": \"https://localhost:5000/health\", \"tls_skip_verify\": false, \"method\": \"POST\", \"header\": {\"Content-Type\": [\"application/json\"]}, \"body\": \"{\\\"method\\\":\\\"health\\\"}\", \"interval\": \"10s\", \"timeout\": \"1s\" } } \nA TCP check:\n{ \"check\": { \"id\": \"ssh\", \"name\": \"SSH TCP on port 22\", \"tcp\": \"localhost:22\", \"interval\": \"10s\", \"timeout\": \"1s\" } } \nA TTL check:\n{ \"check\": { \"id\": \"web-app\", \"name\": \"Web App Status\", \"notes\": \"Web app does a curl internally every 10 seconds\", \"ttl\": \"30s\" } } \nA Docker check:\n{ \"check\": { \"id\": \"mem-util\", \"name\": \"Memory utilization\", \"docker_container_id\": \"f972c95ebf0e\", \"shell\": \"/bin/bash\", \"args\": [\"/usr/local/bin/check_mem.py\"], \"interval\": \"10s\" } } \nA gRPC check for the whole application:\n{ \"check\": { \"id\": \"mem-util\", \"name\": \"Service health status\", \"grpc\": \"127.0.0.1:12345\", \"grpc_use_tls\": true, \"interval\": \"10s\" } } \nA gRPC check for the specific my_service service:\n{ \"check\": { \"id\": \"mem-util\", \"name\": \"Service health status\", \"grpc\": \"127.0.0.1:12345/my_service\", \"grpc_use_tls\": true, \"interval\": \"10s\" } } \nAn alias check for a local service:\n{ \"check\": { \"id\": \"web-alias\", \"alias_service\": \"web\" } } \nConfiguration info: The alias check configuration expects the alias to be registered on the same agent as the one you are aliasing. If the service is not registered with the same agent, \"alias_node\": \"<node_id>\" must also be specified. When using alias_node, if no service is specified, the check will alias the health of the node. If a service is specified, the check will alias the specified service on this particular node.\nEach type of definition must include a name and may optionally provide an id and notes field. The id must be unique per agent otherwise only the last defined check with that id will be registered. If the id is not set and the check is embedded within a service definition a unique check id is generated. Otherwise, id will be set to name. If names might conflict, unique IDs should be provided.\nThe notes field is opaque to Consul but can be used to provide a human-readable description of the current state of the check. Similarly, an external process updating a TTL check via the HTTP interface can set the notes value.\nChecks may also contain a token field to provide an ACL token. This token is used for any interaction with the catalog for the check, including anti-entropy syncs and deregistration. For Alias checks, this token is used if a remote blocking query is necessary to watch the state of the aliased node or service.\nScript, TCP, HTTP, Docker, and gRPC checks must include an interval field. This field is parsed by Go's time package, and has the following formatting specification:\nA duration string is a possibly signed sequence of decimal numbers, each with optional fraction and a unit suffix, such as \"300ms\", \"-1.5h\" or \"2h45m\". Valid time units are \"ns\", \"us\" (or \"s\"), \"ms\", \"s\", \"m\", \"h\".\nIn Consul 0.7 and later, checks that are associated with a service may also contain an optional deregister_critical_service_after field, which is a timeout in the same Go time format as interval and ttl. If a check is in the critical state for more than this configured value, then its associated service (and all of its associated checks) will automatically be deregistered. The minimum timeout is 1 minute, and the process that reaps critical services runs every 30 seconds, so it may take slightly longer than the configured timeout to trigger the deregistration. This should generally be configured with a timeout that's much, much longer than any expected recoverable outage for the given service.\nTo configure a check, either provide it as a -config-file option to the agent or place it inside the -config-dir of the agent. The file must end in a \".json\" or \".hcl\" extension to be loaded by Consul. Check definitions can also be updated by sending a SIGHUP to the agent. Alternatively, the check can be registered dynamically using the HTTP API.\nA check script is generally free to do anything to determine the status of the check. The only limitations placed are that the exit codes must obey this convention:\nExit code 0 - Check is passing\nExit code 1 - Check is warning\nAny other code - Check is failing\nThis is the only convention that Consul depends on. Any output of the script will be captured and stored in the output field.\nIn Consul 0.9.0 and later, the agent must be configured with enable_script_checks set to true in order to enable script checks.\nBy default, when checks are registered against a Consul agent, the state is set immediately to \"critical\". This is useful to prevent services from being registered as \"passing\" and entering the service pool before they are confirmed to be healthy. In certain cases, it may be desirable to specify the initial state of a health check. This can be done by specifying the status field in a health check definition, like so:\n{ \"check\": { \"id\": \"mem\", \"args\": [\"/bin/check_mem\", \"-limit\", \"256MB\"], \"interval\": \"10s\", \"status\": \"passing\" } } \nThe above service definition would cause the new \"mem\" check to be registered with its initial state set to \"passing\".\nHealth checks may optionally be bound to a specific service. This ensures that the status of the health check will only affect the health status of the given service instead of the entire node. Service-bound health checks may be provided by adding a service_id field to a check configuration:\n{ \"check\": { \"id\": \"web-app\", \"name\": \"Web App Status\", \"service_id\": \"web-app\", \"ttl\": \"30s\" } } \nIn the above configuration, if the web-app health check begins failing, it will only affect the availability of the web-app service. All other services provided by the node will remain unchanged.\nThe enable_agent_tls_for_checks agent configuration option can be utilized to have HTTP or gRPC health checks to use the agent's credentials when configured for TLS.\nMultiple check definitions can be defined using the checks (plural) key in your configuration file.\n{ \"checks\": [ { \"id\": \"chk1\", \"name\": \"mem\", \"args\": [\"/bin/check_mem\", \"-limit\", \"256MB\"], \"interval\": \"5s\" }, { \"id\": \"chk2\", \"name\": \"/health\", \"http\": \"http://localhost:5000/health\", \"interval\": \"15s\" }, { \"id\": \"chk3\", \"name\": \"cpu\", \"args\": [\"/bin/check_cpu\"], \"interval\": \"10s\" }, ... ] } \nIn Consul 1.7.0 and later, a check may be configured to become passing/critical only after a specified number of consecutive checks return passing/critical. The status will not transition states until the configured threshold is reached.\nThis feature is available for HTTP, TCP, gRPC, Docker & Monitor checks. By default, both passing and critical thresholds will be set to 0 so the check status will always reflect the last check result.\n{ \"checks\": [ { \"name\": \"HTTP TCP on port 80\", \"tcp\": \"localhost:80\", \"interval\": \"10s\", \"timeout\": \"1s\", \"success_before_passing\": 3, \"failures_before_critical\": 3 } ] }"
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.9.x/operator/area",
  "text": "Commands: Operator Area | Consul\nCommand: consul operator area\nConsul Enterprise supports network areas, which are operator-defined relationships between servers in two different Consul datacenters. The operator area command is used to interact with Consul's network area subsystem.\nUnlike Consul's WAN feature, network areas use just the server RPC port for communication, and relationships can be made between independent pairs of datacenters, so not all servers need to be fully connected. This allows for complex topologies among Consul datacenters like hub/spoke and more general trees.\nSee the Network Areas Guide for more details.\nUsage: consul operator area <subcommand> [options] The operator area command is used to interact with Consul's network area subsystem. Network areas are used to link together Consul servers in different Consul datacenters. With network areas, Consul datacenters can be linked together in ways other than a fully-connected mesh, as is required for Consul's WAN. Subcommands: create Create a new network area delete Remove a network area join Join Consul servers into an existing network area list List network areas members Display Consul server members present in network areas update Update the configuration of a network area \nIf ACLs are enabled, the client will need to supply an ACL Token with operator read or write privileges to use these commands.\nThis command creates a new network area.\nUsage: consul operator area create [options]\n-peer-datacenter=<value> - Declares the peer Consul datacenter that will make up the other side of this network area. Network areas always involve a pair of datacenters: the datacenter where the area was created, and the peer datacenter. This is required.\n-retry-join=<value> Specifies the address of a Consul server to join to, such as an IP or hostname with an optional port number. This is optional and can be specified multiple times.\n-use-tls=<value> Specifies whether gossip over this area should be encrypted with TLS if possible. Must be either true or false.\nThe output looks like this, displaying the ID of the newly-created network area:\nCreated area \"d2872ec5-68ea-b862-b75d-0bee99aca100\" with peer datacenter \"other\"! \nThis command deletes an existing network area.\nUsage: consul operator area delete [options]\n-peer-datacenter=<value> - Looks up the area to operate on by its peer datacenter. This can be given instead of an ID.\nDeleted area \"154941b0-80e2-9d69-c560-ab2c02807332\"! \nThis command joins Consul servers into an existing network area by address, such as an IP or hostname with an optional port. Multiple addresses may be given.\nUsage: consul operator area join [options] ADDRESSES\nAddress Joined Error 10.1.2.3 false failed to connect to \"10.1.2.3:8300\": dial tcp 10.1.2.3:8300: i/o timeout 10.1.2.4 true (none) 10.1.2.5 true (none) \nThe Error field will have a human-readable error message if Consul was unable to join the given address.\nThis command lists all network areas.\nUsage: consul operator area list [options]\nArea PeerDC RetryJoin 6a52a0af-62e2-dad4-da60-e66acc37096c dc2 10.1.2.3,10.1.2.4,10.1.2.5 96e33424-f5ce-9fcd-ecab-27974e36678f other (none) \nArea is the ID of the network area.\nPeerDC is the peer datacenter for the area.\nRetryJoin is the list of servers to join, defined when the area was created.\nThis command displays Consul server nodes present in a network area, or all areas if no area is specified.\nUsage: consul operator area members [options]\nArea Node Address Status Build Protocol DC RTT 6a52a0af-62e2-dad4-da60-e66acc37096c node-1.dc1 127.0.0.1:8300 alive 0.8.0 2 dc1 0s 6a52a0af-62e2-dad4-da60-e66acc37096c node-2.dc1 127.0.0.2:8300 alive 0.8.0 2 dc1 594.191s 96e33424-f5ce-9fcd-ecab-27974e36678f node-1.dc1 127.0.0.1:8300 alive 0.8.0 2 dc1 0s 96e33424-f5ce-9fcd-ecab-27974e36678f node-2.dc1 127.0.0.2:8300 alive 0.8.0 2 dc1 634.109s \nArea is the ID of the network area.\nNode is the name of the node.\nAddress is the IP and server RPC port for the node.\nStatus is the current health status of the node, as determined by the network area distributed failure detector. This will be \"alive\", \"leaving\", \"left\", or \"failed\". A \"failed\" status means that other servers are not able to probe this server over its server RPC interface.\nBuild has the Consul version running on the node.\nProtocol is the protocol version being spoken by the node.\nDC is the node's Consul datacenter.\nRTT is an estimated network round trip time from the server answering the query to the given server, in a human-readable format. This is computed using network coordinates.\nThis command updates the configuration of network area.\nUsage: consul operator area update [options]\n-peer-datacenter=<value> - Declares the peer Consul datacenter that will make up the other side of this network area. Network areas always involve a pair of datacenters: the datacenter where the area was created, and the peer datacenter. This is required.\n-use-tls=<value> Specifies whether gossip over this area should be encrypted with TLS if possible. Must be either true or false.\nUpdated area \"d2872ec5-68ea-b862-b75d-0bee99aca100\" "
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.9.x/connect/registration",
  "text": "Connect - Proxy Registration | Consul\nTo make Connect aware of proxies you will need to register them in a service definition, just like you would register any other service with Consul. This section outlines your options for registering Connect proxies, either using independent registrations, or in nested sidecar registrations.\nTo register proxies with independent proxy service registrations, you can define them in either in config files or via the API just like any other service. Learn more about all of the options you can define when registering your proxy service in the proxy registration documentation.\nTo reduce the amount of boilerplate needed for a sidecar proxy, application service definitions may define an inline sidecar service block. This is an opinionated shorthand for a separate full proxy registration as described above. For a description of how to configure the sidecar proxy as well as the opinionated defaults, see the sidecar service registrations documentation."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.9.x/k8s/connect/observability/metrics",
  "text": "Metrics | Consul | HashiCorp Developer\nConsul on Kubernetes integrates with Prometheus and Grafana to provide metrics for Consul Service Mesh. The metrics available are:\nConnect Service metrics\nSidecar proxy metrics\nConsul agent metrics\nIngress, Terminating and Mesh Gateway metrics\nSpecific sidecar proxy metrics can also be seen in the Consul UI Topology Visualization view. This section documents how to enable each of these.\nNote: Metrics will be supported in Consul-helm >= 0.31.0 and consul-k8s >= 0.25.0. However, enabling the metrics merging feature with Helm value (defaultEnableMerging) or annotation (consul.hashicorp.com/enable-metrics-merging) can only be used with Consul 1.10.0-alpha1 and above. The other metrics configuration can still be used before Consul 1.10.0-alpha1.\nPrometheus annotations are used to instruct Prometheus to scrape metrics from Pods. Prometheus annotations only support scraping from one endpoint on a Pod, so Consul on Kubernetes supports metrics merging whereby service metrics and sidecar proxy metrics are merged into one endpoint. If there are no service metrics, it also supports just scraping the sidecar proxy metrics.\nThe diagram below illustrates how the metrics integration works when merging is enabled:\nConnect service metrics can be configured with the Helm values nested under connectInject.metrics.\nMetrics and metrics merging can be enabled by default for all connect-injected Pods with the following Helm values:\nconnectInject: metrics: defaultEnabled: true # by default, this inherits from the value global.metrics.enabled defaultEnableMerging: true \nThey can also be overridden on a per-Pod basis using the annotations consul.hashicorp.com/enable-metrics and consul.hashicorp.com/enable-metrics-merging.\nIn most cases, the default settings will be sufficient. If you are encountering issues with colliding ports or service metrics not being merged, you may need to change the defaults.\nThe Prometheus annotations configure the endpoint to scrape the metrics from. As shown in the diagram, the annotations point to a listener on 0.0.0.0:20200 on the Envoy sidecar. This listener and the corresponding Prometheus annotations can be configured with the following Helm values (or overridden on a per-Pod basis with Consul annotations consul.hashicorp.com/prometheus-scrape-port and consul.hashicorp.com/prometheus-scrape-path):\nconnectInject: metrics: defaultPrometheusScrapePort: 20200 defaultPrometheusScrapePath: \"/metrics\" \nThose Helm values will result in the following Prometheus annotations being automatically added to the Pod for scraping:\nmetadata: annotations: prometheus.io/scrape: \"true\" prometheus.io/path: \"/metrics\" prometheus.io/port: \"20200\" \nWhen metrics alone are enabled, the listener in the diagram on 0.0.0.0:20200 would point directly at the sidecar metrics endpoint, rather than the merged metrics endpoint. The Prometheus scraping annotations would stay the same.\nWhen metrics and metrics merging are both enabled, metrics are combined from the service and the sidecar proxy, and exposed via a local server on the Consul sidecar for scraping. This endpoint is called the merged metrics endpoint and defaults to 127.0.0.1:20100/stats/prometheus. The listener will target the merged metrics endpoint in the above case. It can be configured with the following Helm values (or overridden on a per-Pod basis with consul.hashicorp.com/merged-metrics-port):\nconnectInject: metrics: defaultMergedMetricsPort: 20100 \nThe endpoint to scrape service metrics from can be configured only on a per-Pod basis via the Pod annotations consul.hashicorp.com/service-metrics-port and consul.hashicorp.com/service-metrics-path. If these are not configured, the service metrics port will default to the port used to register the service with Consul (consul.hashicorp.com/connect-service-port), which in turn defaults to the first port on the first container of the Pod. The service metrics path will default to /metrics.\nMetrics from the Consul server and client Pods can be scraped via Prometheus by setting the field global.metrics.enableAgentMetrics to true. Additionally, one can configure the metrics retention time on the agents by configuring the field global.metrics.agentMetricsRetentionTime which expects a duration and defaults to \"1m\". This value must be greater than \"0m\" for the Consul servers and clients to emit metrics at all. As the Prometheus deployment currently does not support scraping TLS endpoints, agent metrics are currently unsupported when TLS is enabled.\nglobal: metrics: enabled: true enableAgentMetrics: true agentMetricsRetentionTime: \"1m\" \nMetrics from the Consul gateways, namely the Ingress Gateways, Terminating Gateways and the Mesh Gateways can be scraped via Prometheus by setting the field global.metrics.enableGatewayMetrics to true. The gateways emit standard Envoy proxy metrics. To ensure that the metrics are not exposed to the public internet (as Mesh and Ingress gateways can have public IPs), their metrics endpoints are exposed on the Pod IP of the respective gateway instance, rather than on all interfaces on 0.0.0.0.\nglobal: metrics: enabled: true enableGatewayMetrics: true \nConsuls built-in UI has a topology visualization for services part of the Consul Service Mesh. The topology visualization has the ability to fetch basic metrics from a metrics provider for each service and display those metrics as part of the topology visualization.\nThe diagram below illustrates how the UI displays service metrics for a sample application:\nThe topology view is configured under ui.metrics. This will enable the Consul UI to query the provider specified by ui.metrics.provider at the URL of the Prometheus server ui.metrics.baseURL to display sidecar proxy metrics for the service. The UI will display some specific sidecar proxy Prometheus metrics when ui.metrics.enabled is true and ui.enabled is true. The value of ui.metrics.enabled defaults to \"-\" which means it will inherit from the value of global.metrics.enabled.\nui: enabled: true metrics: enabled: true # by default, this inherits from the value global.metrics.enabled provider: \"prometheus\" baseURL: http://prometheus-server \nThe Helm chart contains demo manifests for Prometheus and Grafana. They can be installed with Helm via prometheus.enabled and grafana.enabled. These manifests are based on the community manifests for Prometheus and Grafana. These are designed to allow quick bootstrapping for trial and demo use cases and not for production use-cases.\nPrometheus and Grafana will be installed in the same namespace that Consul will be installed in and will be installed and uninstalled along with the Consul installation.\nprometheus: enabled: true grafana: enabled: true"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.16.x/enterprise/license/utilization-reporting",
  "text": "Consul Enterprise v1.16.0 and newer.\nPatch releases of Consul Enterprise v1.15.4 and newer.\nPatch releases of Consul Enterprise v1.14.8 and newer.\nBefore you enable automated reporting, make sure that outbound network traffic is configured correctly and upgrade your enterprise product to a version that supports it. If your installation is air-gapped or network settings are not in place, automated reporting will not work.\nMake sure that your network allows HTTPS egress on port 443 from https://reporting.hashicorp.services by adding the following IP addresses to your allow-list:\nIf your installation is air-gapped or your network does not allow the correct egress, the logs show an error.\nIf your installation is air-gapped or you want to manually collect and report on the same license utilization metrics, you can opt out of automated reporting.\nManually reporting these metrics can be time consuming. Opting out of automated reporting does not mean that you also opt out from sending license utilization metrics. Customers who opt out of automated reporting are still required to manually collect and send license utilization metrics to HashiCorp.\nIf you are considering opting out because you are worried about the data, we strongly recommend that you review the example payloads before opting out. If you have concerns with any of the automatically reported data, raise these concerns with your account manager.\nThere are two methods for opting out of automated reporting:"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.16.x/enterprise/license/faq",
  "text": "The client agents configuration contains retry_join addresses.\nThe retry join addresses are addresses of the Consul servers.\nConsul snapshot agents will attempt to retrieve the license from servers if certain conditions are met: ACLs are enabled, a ACL token is provided to the client agent, the client agents configuration contains retry_join addresses, the retry join addresses are addresses of the Consul servers."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.17.x/k8s/connect/cluster-peering/usage/manage-peering",
  "text": "Manage Cluster Peering Connections on Kubernetes | Consul\nThis usage topic describes how to manage cluster peering connections on Kubernetes deployments.\nAfter you establish a cluster peering connection, you can get a list of all active peering connections, read a specific peering connection's information, and delete peering connections.\nFor general guidance for managing cluster peering connections, refer to Manage L7 traffic with cluster peering.\nTo reset the cluster peering connection, you need to generate a new peering token from the cluster where you created the PeeringAcceptor CRD. The only way to create or set a new peering token is to manually adjust the value of the annotation consul.hashicorp.com/peering-version. Creating a new token causes the previous token to expire.\nIn the PeeringAcceptor CRD, add the annotation consul.hashicorp.com/peering-version. If the annotation already exists, update its value to a higher version.\nacceptor.yml\napiVersion: consul.hashicorp.com/v1alpha1 kind: PeeringAcceptor metadata: name: cluster-02 annotations: consul.hashicorp.com/peering-version: \"1\" ## The peering version you want to set, must be in quotes spec: peer: secret: name: \"peering-token\" key: \"data\" backend: \"kubernetes\" \nAfter updating PeeringAcceptor, repeat all of the steps to establish a new peering connection.\nIn Consul on Kubernetes deployments, you can list all active peering connections in a cluster using the Consul CLI.\nIf necessary, configure your CLI to interact with the Consul cluster.\nRun the consul peering list CLI command.\n$ consul peering list Name State Imported Svcs Exported Svcs Meta cluster-02 ACTIVE 0 2 env=production cluster-03 PENDING 0 0 \nIn Consul on Kubernetes deployments, you can get information about individual peering connections between clusters using the Consul CLI.\nIf necessary, configure your CLI to interact with the Consul cluster.\nRun the consul peering read CLI command.\n$ consul peering read -name cluster-02 Name: cluster-02 ID: 3b001063-8079-b1a6-764c-738af5a39a97 State: ACTIVE Meta: env=production Peer ID: e83a315c-027e-bcb1-7c0c-a46650904a05 Peer Server Name: server.dc1.consul Peer CA Pems: 0 Peer Server Addresses: 10.0.0.1:8300 Imported Services: 0 Exported Services: 2 Create Index: 89 Modify Index: 89 \nTo end a peering connection in Kubernetes deployments, delete both the PeeringAcceptor and PeeringDialer resources.\nDelete the PeeringDialer resource from the second cluster.\n$ kubectl --context $CLUSTER2_CONTEXT delete --filename dialer.yaml \nDelete the PeeringAcceptor resource from the first cluster.\n$ kubectl --context $CLUSTER1_CONTEXT delete --filename acceptor.yaml \nTo confirm that you deleted your peering connection in cluster-01, query the /health HTTP endpoint:\nExec into the server pod for the first cluster.\n$ kubectl exec -it consul-server-0 --context $CLUSTER1_CONTEXT -- /bin/sh \nIf you've enabled ACLs, export an ACL token to access the /health HTP endpoint for services. The bootstrap token may be used if an ACL token is not already provisioned.\n$ export CONSUL_HTTP_TOKEN=<INSERT BOOTSTRAP ACL TOKEN> \nQuery the /health HTTP endpoint. Peered services with deleted connections should no longe appear.\n$ curl \"localhost:8500/v1/health/connect/backend?peer=cluster-02\""
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.17.x/k8s/connect/cluster-peering/usage/l7-traffic",
  "text": "Manage L7 Traffic With Cluster Peering on Kubernetes | Consul\nThis usage topic describes how to configure the service-resolver custom resource definition (CRD) to set up and manage L7 traffic between services that have an existing cluster peering connection in Consul on Kubernetes deployments.\nFor general guidance for managing L7 traffic with cluster peering, refer to Manage L7 traffic with cluster peering.\nWhen you use cluster peering to connect datacenters through their admin partitions, you can use dynamic traffic management to configure your service mesh so that services automatically forward traffic to services hosted on peer clusters.\nHowever, the service-splitter and service-router CRDs do not natively support directly targeting a service instance hosted on a peer. Before you can split or route traffic to a service on a peer, you must define the service hosted on the peer as an upstream service by configuring a failover in a service-resolver CRD. Then, you can set up a redirect in a second service resolver to interact with the peer service by name.\nFor more information about formatting, updating, and managing configuration entries in Consul, refer to How to use configuration entries.\nTo configure L7 traffic management behavior in deployments with cluster peering connections, complete the following steps in order:\nDefine the peer cluster as a failover target in the service resolver configuration. \nThe following example updates the service-resolver CRD in cluster-01 so that Consul redirects traffic intended for the frontend service to a backup instance in peer cluster-02 when it detects multiple connection failures.\napiVersion: consul.hashicorp.com/v1alpha1 kind: ServiceResolver metadata: name: frontend spec: connectTimeout: 15s failover: '*': targets: - peer: 'cluster-02' service: 'frontend' namespace: 'default' \nDefine the desired behavior in service-splitter or service-router CRD.\nThe following example splits traffic evenly between frontend and frontend-peer:\napiVersion: consul.hashicorp.com/v1alpha1 kind: ServiceSplitter metadata: name: frontend spec: splits: - weight: 50 ## defaults to service with same name as configuration entry (\"frontend\") - weight: 50 service: frontend-peer \nCreate a second service-resolver configuration entry on the local cluster that resolves the name of the peer service you used when splitting or routing the traffic.\nThe following example uses the name frontend-peer to define a redirect targeting the frontend service on the peer cluster-02:\napiVersion: consul.hashicorp.com/v1alpha1 kind: ServiceResolver metadata: name: frontend-peer spec: redirect: peer: 'cluster-02' service: 'frontend'"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.17.x/k8s/connect/cluster-peering/usage/create-sameness-groups",
  "text": "Create sameness groups | Consul\nCreate sameness groups on Kubernetes\nThis topic describes how to create a sameness group, which designates a set of admin partitions as functionally identical in a Consul deployment running Kubernetes. Adding an admin partition to a sameness group enables Consul to recognize services registered to remote partitions with cluster peering connections as instances of the same service when they share a name and Consul namespace.\nFor information about configuring a failover strategy using sameness groups, refer to Failover with sameness groups.\nSameness groups are a user-defined set of partitions with identical configurations, includingcustom resource definitions (CRDs) for service and proxy defaults. Partitions on separate clusters should have an established cluster peering connection in order to recognize each other.\nTo create and use sameness groups in your network, complete the following steps:\nCreate sameness group custom resource definitions (CRDs) for each member of the group. For each partition that you want to include in the sameness group, you must write and apply a sameness group CRD that defines the groups members from that partitions perspective. Refer to the sameness group configuration entry reference for details on configuration hierarchy, default values, and specifications.\nExport services to members of the sameness group. You must write and apply an exported services CRD that makes the partitions services available to other members of the group. Refer to exported services configuration entry reference for additional specification information.\nCreate service intentions for each member of the sameness group. For each partition that you want to include in the sameness group, you must write and apply service intentions CRDs to authorize traffic to your services from all members of the group. Refer to the service intentions configuration entry reference for additional specification information.\nAll datacenters where you want to create sameness groups must run Consul v1.16 or later. Refer to upgrade instructions for more information about how to upgrade your deployment.\nA Consul Enterprise license is required.\nBefore you begin\nBefore creating a sameness group, take the following actions to prepare your network:\nCheck Consul namespaces and service naming conventions\nSameness groups are defined at the partition level. Consul assumes all partitions in the group have identical configurations, including identical service names and identical Consul namespaces. This behavior occurs even when two partitions in the group contain functionally different services that share a common name and namespace. For example, if distinct services both named api were registered to different members of a sameness group, it could lead to errors because requests may be sent to the incorrect service.\nTo prevent errors, check the names of the services deployed to your network and the namespaces they are deployed in. Pay particular attention to the default namespace to confirm that services have unique names. If different services share a name, you should either change one of the services names or deploy one of the services to a different namespace.\nDeploy mesh gateways for each partition\nMesh gateways are required for cluster peering connections and recommended to secure cross-partition traffic in a single datacenter. Therefore, we recommend securing your network, and especially your production environment, by deploying mesh gateways to each datacenter. Refer to mesh gateways specifications for more information about configuring mesh gateways.\nEstablish cluster peering relationships between remote partitions\nYou must establish connections with cluster peers before you can create a sameness group that includes them. A cluster peering connection exists between two admin partitions in different datacenters, and each connection between two partitions must be established separately with each peer. Refer to establish cluster peering connections for step-by-step instructions.\nYou can establish and manage cluster peering relationships between all of your self-managed clusters using HCP Consul Central. For more information, refer to cluster peering global view in the HCP documentation.\nTo establish cluster peering connections and define a group as part of the same workflow, follow instructions up to Export services between clusters. You can use the same exported services and service intention configuration entries to establish the cluster peering connection and create the sameness group.\nTo create a sameness group, you must write and apply a set of three CRDs for each partition that is a member of the group:\nSameness group CRDs: Define the sameness group from each partitions perspective.\nExported services CRDs: Make services available to other partitions in the group.\nService intentions CRDs: Authorize traffic between services across partitions.\nDefine the sameness group from each partitions perspective\nTo define a sameness group for a partition, create a sameness group CRD that describes the partitions and cluster peers that are part of the group. Typically, this order follows this pattern:\nThe local partition\nOther partitions in the same datacenter\nPartitions with established cluster peering relationships\nIf you want all services to failover to other instances in the sameness group by default, set spec.defaultForFailover=true and list the group members in the order you want to use in a failover scenario. Refer to failover with sameness groups for more information.\nBe aware that the sameness group CRDs are different for each partition. The following example demonstrates how to format three different CRDs for three partitions that are part of the sameness group product-group when Partition 1 and Partition 2 are in DC1, and the third partition is Partition 1 in DC2:\nproduct-group.yaml\napiVersion: consul.hashicorp.com/v1alpha1 kind: SamenessGroup metadata: name: product-group spec: defaultForFailover: true members: - partition: partition-1 - partition: partition-2 - peer: dc2-partition-1 \nAfter you create the CRD, apply it to the Consul server with the following kubectl CLI command:\n$ kubectl apply -f product-group.yaml \nThen, repeat the process to create and apply a CRD for every partition that is a member of the sameness group.\nExport services to other partitions in the sameness group\nTo make services available to other members of the sameness group, you must write and apply an exported services CRD for each partition in the group. This CRD exports the local partition's services to the rest of the group members. In each CRD, set the sameness group as the consumer for the exported services. You can export multiple services in a single exported services configuration entry.\nBecause you are configuring the consumer to reference the sameness group instead of listing out each partition and cluster peer, you do not need to edit this configuration again when you add a partition or peer to the group.\nThe following example demonstrates how to format three different ExportedServices CRDs to make a service named api deployed to the store namespace of each partition available to all other group members:\napiVersion: consul.hashicorp.com/v1alpha1 Kind: ExportedServices metadata: name: partition-1 spec: services: - name: api namespace: store consumers: - samenessGroup: product-group \nFor more information about exporting services, including examples of CRDs that export multiple services at the same time, refer to the exported services configuration entry reference.\nAfter you create each exported services configuration entry, apply it to the Consul server with the following CLI command:\n$ kubectl apply -f product-group-export.yaml \nExport services for cluster peers and sameness groups as part of the same workflow\nCreating a cluster peering connection between two partitions and then adding the partitions to a sameness group requires that you write and apply two separate exported services CRDs. One CRD exports services to the peer, and a second CRD exports services to other members of the group.\nIf your goal for peering clusters is to create a sameness group, you can write and apply a single exported services configuration entry by configuring the services[].consumers block with the samenessGroup field instead of the peer field. Be aware that this scenario requires you to write the SamenessGroup CRD to Kubernetes before you apply the ExportedServices CRD that references the sameness group.\nExporting the service to other members of the sameness group makes the services visible to remote partitions, but you must also create service intentions so that local services are authorized to send and receive traffic from a member of the sameness group.\nFor each partition that is a member of the group, write and apply a service intentions CRD that defines intentions for the services that are part of the group. In the sources block of the configuration entry, include the service name, its namespace, the sameness group and grant allow permissions.\nBecause you are using the sameness group in the sources block rather than listing out each partition and cluster peer, you do not have to make further edits to the service intentions configuration entries when members are added to or removed from the group.\nThe following example demonstrates how to format three different ServiceIntentions CRDs to make a service named api available to all instances of payments deployed in all members of the sameness group including the local partition. In this example, api is deployed to the store namespace in all three partitions.\napiVersion: consul.hashicorp.com/v1alpha1 kind: ServiceIntentions metadata: name: api-intentions spec: sources: - name: api action: allow namespace: store samenessGroup: product-group \nRefer to create and manage intentions for more information about how to create and apply service intentions in Consul.\nAfter you create each service intentions configuration entry, apply it to the Consul server with the following CLI command:\n$ kubectl apply -f api-intentions.yaml \nCreate service intentions for cluster peers and sameness groups as part of the same workflow\nCreating a cluster peering connection between two partitions and then adding the partitions to a sameness group requires that you write and apply two separate service intentions CRDs. One CRD authorizes services for the peer, and a second CRD authorizes services for other members of the group.\nIf your goal for peering clusters is to create a sameness group, you can write and apply a single service intentions CRD by configuring the sources block with the samenessGroup field instead of the peer field. Be aware that this scenario requires you to write the SamenessGroup CRD to Kubernetes before you apply the ServiceIntentions CRD that references the sameness group.\nWhen defaultForFailover=true in a sameness group CRD, additional upstream configuration is not required.\nAfter creating a sameness group, you can also set up failover between services in a sameness group. Refer to Failover with sameness groups for more information."
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.17.x/peering",
  "text": "Commands: Peering | Consul | HashiCorp Developer\nCommand: consul peering\nUse the peering command to create and manage peering connections between Consul clusters, including token generation and consumption. Refer to Create and Manage Peerings Connections for an overview of the CLI workflow for cluster peering.\nUsage: consul peering <subcommand> [options] # ... Subcommands: delete Close and delete a peering connection establish Consume a peering token and establish a connection with the accepting cluster exported-services Lists the services exported to the peer generate-token Generate a peering token for use by a dialing cluster list List the local cluster's peering connections read Read detailed information on a peering connection \nFor more information, examples, and usage about a subcommand, click on the name of the subcommand in the sidebar or one of the links below:\ndelete\nestablish\ngenerate-token\nlist\nread\nexported-services"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.18.x/k8s/connect/cluster-peering/usage/create-sameness-groups",
  "text": "Create sameness groups | Consul\nCreate sameness groups on Kubernetes\nThis topic describes how to create a sameness group, which designates a set of admin partitions as functionally identical in a Consul deployment running Kubernetes. Adding an admin partition to a sameness group enables Consul to recognize services registered to remote partitions with cluster peering connections as instances of the same service when they share a name and Consul namespace.\nFor information about configuring a failover strategy using sameness groups, refer to Failover with sameness groups.\nSameness groups are a user-defined set of partitions with identical configurations, includingcustom resource definitions (CRDs) for service and proxy defaults. Partitions on separate clusters should have an established cluster peering connection in order to recognize each other.\nTo create and use sameness groups in your network, complete the following steps:\nCreate sameness group custom resource definitions (CRDs) for each member of the group. For each partition that you want to include in the sameness group, you must write and apply a sameness group CRD that defines the groups members from that partitions perspective. Refer to the sameness group configuration entry reference for details on configuration hierarchy, default values, and specifications.\nExport services to members of the sameness group. You must write and apply an exported services CRD that makes the partitions services available to other members of the group. Refer to exported services configuration entry reference for additional specification information.\nCreate service intentions for each member of the sameness group. For each partition that you want to include in the sameness group, you must write and apply service intentions CRDs to authorize traffic to your services from all members of the group. Refer to the service intentions configuration entry reference for additional specification information.\nAll datacenters where you want to create sameness groups must run Consul v1.16 or later. Refer to upgrade instructions for more information about how to upgrade your deployment.\nA Consul Enterprise license is required.\nBefore you begin\nBefore creating a sameness group, take the following actions to prepare your network:\nCheck Consul namespaces and service naming conventions\nSameness groups are defined at the partition level. Consul assumes all partitions in the group have identical configurations, including identical service names and identical Consul namespaces. This behavior occurs even when two partitions in the group contain functionally different services that share a common name and namespace. For example, if distinct services both named api were registered to different members of a sameness group, it could lead to errors because requests may be sent to the incorrect service.\nTo prevent errors, check the names of the services deployed to your network and the namespaces they are deployed in. Pay particular attention to the default namespace to confirm that services have unique names. If different services share a name, you should either change one of the services names or deploy one of the services to a different namespace.\nDeploy mesh gateways for each partition\nMesh gateways are required for cluster peering connections and recommended to secure cross-partition traffic in a single datacenter. Therefore, we recommend securing your network, and especially your production environment, by deploying mesh gateways to each datacenter. Refer to mesh gateways specifications for more information about configuring mesh gateways.\nEstablish cluster peering relationships between remote partitions\nYou must establish connections with cluster peers before you can create a sameness group that includes them. A cluster peering connection exists between two admin partitions in different datacenters, and each connection between two partitions must be established separately with each peer. Refer to establish cluster peering connections for step-by-step instructions.\nYou can establish and manage cluster peering relationships between all of your self-managed clusters using HCP Consul Central. For more information, refer to cluster peering global view in the HCP documentation.\nTo establish cluster peering connections and define a group as part of the same workflow, follow instructions up to Export services between clusters. You can use the same exported services and service intention configuration entries to establish the cluster peering connection and create the sameness group.\nTo create a sameness group, you must write and apply a set of three CRDs for each partition that is a member of the group:\nSameness group CRDs: Define the sameness group from each partitions perspective.\nExported services CRDs: Make services available to other partitions in the group.\nService intentions CRDs: Authorize traffic between services across partitions.\nDefine the sameness group from each partitions perspective\nTo define a sameness group for a partition, create a sameness group CRD that describes the partitions and cluster peers that are part of the group. Typically, this order follows this pattern:\nThe local partition\nOther partitions in the same datacenter\nPartitions with established cluster peering relationships\nIf you want all services to failover to other instances in the sameness group by default, set spec.defaultForFailover=true and list the group members in the order you want to use in a failover scenario. Refer to failover with sameness groups for more information.\nBe aware that the sameness group CRDs are different for each partition. The following example demonstrates how to format three different CRDs for three partitions that are part of the sameness group product-group when Partition 1 and Partition 2 are in DC1, and the third partition is Partition 1 in DC2:\nproduct-group.yaml\napiVersion: consul.hashicorp.com/v1alpha1 kind: SamenessGroup metadata: name: product-group spec: defaultForFailover: true members: - partition: partition-1 - partition: partition-2 - peer: dc2-partition-1 \nAfter you create the CRD, apply it to the Consul server with the following kubectl CLI command:\n$ kubectl apply -f product-group.yaml \nThen, repeat the process to create and apply a CRD for every partition that is a member of the sameness group.\nExport services to other partitions in the sameness group\nTo make services available to other members of the sameness group, you must write and apply an exported services CRD for each partition in the group. This CRD exports the local partition's services to the rest of the group members. In each CRD, set the sameness group as the consumer for the exported services. You can export multiple services in a single exported services configuration entry.\nBecause you are configuring the consumer to reference the sameness group instead of listing out each partition and cluster peer, you do not need to edit this configuration again when you add a partition or peer to the group.\nThe following example demonstrates how to format three different ExportedServices CRDs to make a service named api deployed to the store namespace of each partition available to all other group members:\napiVersion: consul.hashicorp.com/v1alpha1 Kind: ExportedServices metadata: name: partition-1 spec: services: - name: api namespace: store consumers: - samenessGroup: product-group \nFor more information about exporting services, including examples of CRDs that export multiple services at the same time, refer to the exported services configuration entry reference.\nAfter you create each exported services configuration entry, apply it to the Consul server with the following CLI command:\n$ kubectl apply -f product-group-export.yaml \nExport services for cluster peers and sameness groups as part of the same workflow\nCreating a cluster peering connection between two partitions and then adding the partitions to a sameness group requires that you write and apply two separate exported services CRDs. One CRD exports services to the peer, and a second CRD exports services to other members of the group.\nIf your goal for peering clusters is to create a sameness group, you can write and apply a single exported services configuration entry by configuring the services[].consumers block with the samenessGroup field instead of the peer field. Be aware that this scenario requires you to write the SamenessGroup CRD to Kubernetes before you apply the ExportedServices CRD that references the sameness group.\nExporting the service to other members of the sameness group makes the services visible to remote partitions, but you must also create service intentions so that local services are authorized to send and receive traffic from a member of the sameness group.\nFor each partition that is a member of the group, write and apply a service intentions CRD that defines intentions for the services that are part of the group. In the sources block of the configuration entry, include the service name, its namespace, the sameness group and grant allow permissions.\nBecause you are using the sameness group in the sources block rather than listing out each partition and cluster peer, you do not have to make further edits to the service intentions configuration entries when members are added to or removed from the group.\nThe following example demonstrates how to format three different ServiceIntentions CRDs to make a service named api available to all instances of payments deployed in all members of the sameness group including the local partition. In this example, api is deployed to the store namespace in all three partitions.\napiVersion: consul.hashicorp.com/v1alpha1 kind: ServiceIntentions metadata: name: api-intentions spec: sources: - name: api action: allow namespace: store samenessGroup: product-group \nRefer to create and manage intentions for more information about how to create and apply service intentions in Consul.\nAfter you create each service intentions configuration entry, apply it to the Consul server with the following CLI command:\n$ kubectl apply -f api-intentions.yaml \nCreate service intentions for cluster peers and sameness groups as part of the same workflow\nCreating a cluster peering connection between two partitions and then adding the partitions to a sameness group requires that you write and apply two separate service intentions CRDs. One CRD authorizes services for the peer, and a second CRD authorizes services for other members of the group.\nIf your goal for peering clusters is to create a sameness group, you can write and apply a single service intentions CRD by configuring the sources block with the samenessGroup field instead of the peer field. Be aware that this scenario requires you to write the SamenessGroup CRD to Kubernetes before you apply the ServiceIntentions CRD that references the sameness group.\nWhen defaultForFailover=true in a sameness group CRD, additional upstream configuration is not required.\nAfter creating a sameness group, you can also set up failover between services in a sameness group. Refer to Failover with sameness groups for more information."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.18.x/connect/cluster-peering/usage/peering-traffic-management",
  "text": "Cluster Peering L7 Traffic Management | Consul\nManage L7 traffic with cluster peering\nThis usage topic describes how to configure and apply the service-resolver configuration entry to set up redirects and failovers between services that have an existing cluster peering connection.\nFor Kubernetes-specific guidance for managing L7 traffic with cluster peering, refer to Manage L7 traffic with cluster peering on Kubernetes.\nWhen you use cluster peering to connect datacenters through their admin partitions, you can use dynamic traffic management to configure your service mesh so that services automatically forward traffic to services hosted on peer clusters.\nHowever, the service-splitter and service-router configuration entry kinds do not natively support directly targeting a service instance hosted on a peer. Before you can split or route traffic to a service on a peer, you must define the service hosted on the peer as an upstream service by configuring a failover in the service-resolver configuration entry. Then, you can set up a redirect in a second service resolver to interact with the peer service by name.\nFor more information about formatting, updating, and managing configuration entries in Consul, refer to How to use configuration entries.\nTo configure L7 traffic management behavior in deployments with cluster peering connections, complete the following steps in order:\nDefine the peer cluster as a failover target in the service resolver configuration.\nThe following examples update the service-resolver configuration entry in cluster-01 so that Consul redirects traffic intended for the frontend service to a backup instance in peer cluster-02 when it detects multiple connection failures.\nKind = \"service-resolver\" Name = \"frontend\" ConnectTimeout = \"15s\" Failover = { \"*\" = { Targets = [ {Peer = \"cluster-02\"} ] } } \nDefine the desired behavior in service-splitter or service-router configuration entries.\nThe following example splits traffic evenly between frontend services hosted on peers by defining the desired behavior locally:\nKind = \"service-splitter\" Name = \"frontend\" Splits = [ { Weight = 50 ## defaults to service with same name as configuration entry (\"frontend\") }, { Weight = 50 Service = \"frontend-peer\" }, ] \nCreate a local service-resolver configuration entry named frontend-peer and define a redirect targeting the peer and its service:\nKind = \"service-resolver\" Name = \"frontend-peer\" Redirect { Service = frontend Peer = \"cluster-02\" }"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.18.x/k8s/connect/cluster-peering/usage/manage-peering",
  "text": "Manage Cluster Peering Connections on Kubernetes | Consul\nThis usage topic describes how to manage cluster peering connections on Kubernetes deployments.\nAfter you establish a cluster peering connection, you can get a list of all active peering connections, read a specific peering connection's information, and delete peering connections.\nFor general guidance for managing cluster peering connections, refer to Manage L7 traffic with cluster peering.\nTo reset the cluster peering connection, you need to generate a new peering token from the cluster where you created the PeeringAcceptor CRD. The only way to create or set a new peering token is to manually adjust the value of the annotation consul.hashicorp.com/peering-version. Creating a new token causes the previous token to expire.\nIn the PeeringAcceptor CRD, add the annotation consul.hashicorp.com/peering-version. If the annotation already exists, update its value to a higher version.\nacceptor.yml\napiVersion: consul.hashicorp.com/v1alpha1 kind: PeeringAcceptor metadata: name: cluster-02 annotations: consul.hashicorp.com/peering-version: \"1\" ## The peering version you want to set, must be in quotes spec: peer: secret: name: \"peering-token\" key: \"data\" backend: \"kubernetes\" \nAfter updating PeeringAcceptor, repeat all of the steps to establish a new peering connection.\nIn Consul on Kubernetes deployments, you can list all active peering connections in a cluster using the Consul CLI.\nIf necessary, configure your CLI to interact with the Consul cluster.\nRun the consul peering list CLI command.\n$ consul peering list Name State Imported Svcs Exported Svcs Meta cluster-02 ACTIVE 0 2 env=production cluster-03 PENDING 0 0 \nIn Consul on Kubernetes deployments, you can get information about individual peering connections between clusters using the Consul CLI.\nIf necessary, configure your CLI to interact with the Consul cluster.\nRun the consul peering read CLI command.\n$ consul peering read -name cluster-02 Name: cluster-02 ID: 3b001063-8079-b1a6-764c-738af5a39a97 State: ACTIVE Meta: env=production Peer ID: e83a315c-027e-bcb1-7c0c-a46650904a05 Peer Server Name: server.dc1.consul Peer CA Pems: 0 Peer Server Addresses: 10.0.0.1:8300 Imported Services: 0 Exported Services: 2 Create Index: 89 Modify Index: 89 \nTo end a peering connection in Kubernetes deployments, delete both the PeeringAcceptor and PeeringDialer resources.\nDelete the PeeringDialer resource from the second cluster.\n$ kubectl --context $CLUSTER2_CONTEXT delete --filename dialer.yaml \nDelete the PeeringAcceptor resource from the first cluster.\n$ kubectl --context $CLUSTER1_CONTEXT delete --filename acceptor.yaml \nTo confirm that you deleted your peering connection in cluster-01, query the /health HTTP endpoint:\nExec into the server pod for the first cluster.\n$ kubectl exec -it consul-server-0 --context $CLUSTER1_CONTEXT -- /bin/sh \nIf you've enabled ACLs, export an ACL token to access the /health HTP endpoint for services. The bootstrap token may be used if an ACL token is not already provisioned.\n$ export CONSUL_HTTP_TOKEN=<INSERT BOOTSTRAP ACL TOKEN> \nQuery the /health HTTP endpoint. Peered services with deleted connections should no longe appear.\n$ curl \"localhost:8500/v1/health/connect/backend?peer=cluster-02\""
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.18.x/k8s/connect/cluster-peering/usage/l7-traffic",
  "text": "Manage L7 Traffic With Cluster Peering on Kubernetes | Consul\nThis usage topic describes how to configure the service-resolver custom resource definition (CRD) to set up and manage L7 traffic between services that have an existing cluster peering connection in Consul on Kubernetes deployments.\nFor general guidance for managing L7 traffic with cluster peering, refer to Manage L7 traffic with cluster peering.\nWhen you use cluster peering to connect datacenters through their admin partitions, you can use dynamic traffic management to configure your service mesh so that services automatically forward traffic to services hosted on peer clusters.\nHowever, the service-splitter and service-router CRDs do not natively support directly targeting a service instance hosted on a peer. Before you can split or route traffic to a service on a peer, you must define the service hosted on the peer as an upstream service by configuring a failover in a service-resolver CRD. Then, you can set up a redirect in a second service resolver to interact with the peer service by name.\nFor more information about formatting, updating, and managing configuration entries in Consul, refer to How to use configuration entries.\nTo configure L7 traffic management behavior in deployments with cluster peering connections, complete the following steps in order:\nDefine the peer cluster as a failover target in the service resolver configuration. \nThe following example updates the service-resolver CRD in cluster-01 so that Consul redirects traffic intended for the frontend service to a backup instance in peer cluster-02 when it detects multiple connection failures.\napiVersion: consul.hashicorp.com/v1alpha1 kind: ServiceResolver metadata: name: frontend spec: connectTimeout: 15s failover: '*': targets: - peer: 'cluster-02' service: 'frontend' namespace: 'default' \nDefine the desired behavior in service-splitter or service-router CRD.\nThe following example splits traffic evenly between frontend and frontend-peer:\napiVersion: consul.hashicorp.com/v1alpha1 kind: ServiceSplitter metadata: name: frontend spec: splits: - weight: 50 ## defaults to service with same name as configuration entry (\"frontend\") - weight: 50 service: frontend-peer \nCreate a second service-resolver configuration entry on the local cluster that resolves the name of the peer service you used when splitting or routing the traffic.\nThe following example uses the name frontend-peer to define a redirect targeting the frontend service on the peer cluster-02:\napiVersion: consul.hashicorp.com/v1alpha1 kind: ServiceResolver metadata: name: frontend-peer spec: redirect: peer: 'cluster-02' service: 'frontend'"
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.18.x/peering",
  "text": "Commands: Peering | Consul | HashiCorp Developer\nCommand: consul peering\nUse the peering command to create and manage peering connections between Consul clusters, including token generation and consumption. Refer to Create and Manage Peerings Connections for an overview of the CLI workflow for cluster peering.\nUsage: consul peering <subcommand> [options] # ... Subcommands: delete Close and delete a peering connection establish Consume a peering token and establish a connection with the accepting cluster exported-services Lists the services exported to the peer generate-token Generate a peering token for use by a dialing cluster list List the local cluster's peering connections read Read detailed information on a peering connection \nFor more information, examples, and usage about a subcommand, click on the name of the subcommand in the sidebar or one of the links below:\ndelete\nestablish\ngenerate-token\nlist\nread\nexported-services"
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.14.x/peering",
  "text": "Commands: Peering | Consul | HashiCorp Developer\nCommand: consul peering\nUse the peering command to create and manage peering connections between Consul clusters, including token generation and consumption. Refer to Create and Manage Peerings Connections for an overview of the CLI workflow for cluster peering.\nUsage: consul peering <subcommand> [options] # ... Subcommands: delete Close and delete a peering connection establish Consume a peering token and establish a connection with the accepting cluster generate-token Generate a peering token for use by a dialing cluster list List the local cluster's peering connections read Read detailed information on a peering connection \nFor more information, examples, and usage about a subcommand, click on the name of the subcommand in the sidebar or one of the links below:\ndelete\nestablish\ngenerate-token\nlist\nread"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.14.x/connect/cluster-peering/usage/peering-traffic-management",
  "text": "Cluster Peering L7 Traffic Management | Consul\nManage L7 traffic with cluster peering\nThis usage topic describes how to configure and apply the service-resolver configuration entry to set up redirects and failovers between services that have an existing cluster peering connection.\nFor Kubernetes-specific guidance for managing L7 traffic with cluster peering, refer to Manage L7 traffic with cluster peering on Kubernetes.\nWhen you use cluster peering to connect datacenters through their admin partitions, you can use dynamic traffic management to configure your service mesh so that services automatically forward traffic to services hosted on peer clusters.\nHowever, the service-splitter and service-router configuration entry kinds do not natively support directly targeting a service instance hosted on a peer. Before you can split or route traffic to a service on a peer, you must define the service hosted on the peer as an upstream service by configuring a failover in the service-resolver configuration entry. Then, you can set up a redirect in a second service resolver to interact with the peer service by name.\nFor more information about formatting, updating, and managing configuration entries in Consul, refer to How to use configuration entries.\nTo configure L7 traffic management behavior in deployments with cluster peering connections, complete the following steps in order:\nDefine the peer cluster as a failover target in the service resolver configuration.\nThe following examples update the service-resolver configuration entry in cluster-01 so that Consul redirects traffic intended for the frontend service to a backup instance in peer cluster-02 when it detects multiple connection failures.\nKind = \"service-resolver\" Name = \"frontend\" ConnectTimeout = \"15s\" Failover = { \"*\" = { Targets = [ {Peer = \"cluster-02\"} ] } } \nDefine the desired behavior in service-splitter or service-router configuration entries.\nThe following example splits traffic evenly between frontend services hosted on peers by defining the desired behavior locally:\nKind = \"service-splitter\" Name = \"frontend\" Splits = [ { Weight = 50 ## defaults to service with same name as configuration entry (\"frontend\") }, { Weight = 50 Service = \"frontend-peer\" }, ] \nCreate a local service-resolver configuration entry named frontend-peer and define a redirect targeting the peer and its service:\nKind = \"service-resolver\" Name = \"frontend-peer\" Redirect { Service = frontend Peer = \"cluster-02\" }"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.14.x/k8s/connect/cluster-peering/usage/manage-peering",
  "text": "Manage Cluster Peering Connections on Kubernetes | Consul\nThis usage topic describes how to manage cluster peering connections on Kubernetes deployments.\nAfter you establish a cluster peering connection, you can get a list of all active peering connections, read a specific peering connection's information, and delete peering connections.\nFor general guidance for managing cluster peering connections, refer to Manage L7 traffic with cluster peering.\nTo reset the cluster peering connection, you need to generate a new peering token from the cluster where you created the PeeringAcceptor CRD. The only way to create or set a new peering token is to manually adjust the value of the annotation consul.hashicorp.com/peering-version. Creating a new token causes the previous token to expire.\nIn the PeeringAcceptor CRD, add the annotation consul.hashicorp.com/peering-version. If the annotation already exists, update its value to a higher version.\nacceptor.yml\napiVersion: consul.hashicorp.com/v1alpha1 kind: PeeringAcceptor metadata: name: cluster-02 annotations: consul.hashicorp.com/peering-version: \"1\" ## The peering version you want to set, must be in quotes spec: peer: secret: name: \"peering-token\" key: \"data\" backend: \"kubernetes\" \nAfter updating PeeringAcceptor, repeat all of the steps to establish a new peering connection.\nIn Consul on Kubernetes deployments, you can list all active peering connections in a cluster using the Consul CLI.\nRun the consul peering list CLI command.\n$ consul peering list Name State Imported Svcs Exported Svcs Meta cluster-02 ACTIVE 0 2 env=production cluster-03 PENDING 0 0 \nIn Consul on Kubernetes deployments, you can get information about individual peering connections between clusters using the Consul CLI.\nRun the consul peering read CLI command.\n$ consul peering read -name cluster-02 Name: cluster-02 ID: 3b001063-8079-b1a6-764c-738af5a39a97 State: ACTIVE Meta: env=production Peer ID: e83a315c-027e-bcb1-7c0c-a46650904a05 Peer Server Name: server.dc1.consul Peer CA Pems: 0 Peer Server Addresses: 10.0.0.1:8300 Imported Services: 0 Exported Services: 2 Create Index: 89 Modify Index: 89 \nTo end a peering connection in Kubernetes deployments, delete both the PeeringAcceptor and PeeringDialer resources.\nDelete the PeeringDialer resource from the second cluster.\n$ kubectl --context $CLUSTER2_CONTEXT delete --filename dialer.yaml \nDelete the PeeringAcceptor resource from the first cluster.\n$ kubectl --context $CLUSTER1_CONTEXT delete --filename acceptor.yaml \nTo confirm that you deleted your peering connection in cluster-01, query the the /health HTTP endpoint:\nExec into the server pod for the first cluster.\n$ kubectl exec -it consul-server-0 --context $CLUSTER1_CONTEXT -- /bin/sh \nIf you've enabled ACLs, export an ACL token to access the /health HTP endpoint for services. The bootstrap token may be used if an ACL token is not already provisioned.\n$ export CONSUL_HTTP_TOKEN=<INSERT BOOTSTRAP ACL TOKEN> \nQuery the the /health HTTP endpoint. Peered services with deleted connections should no longe appear.\n$ curl \"localhost:8500/v1/health/connect/backend?peer=cluster-02\""
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.15.x/peering",
  "text": "Commands: Peering | Consul | HashiCorp Developer\nCommand: consul peering\nUse the peering command to create and manage peering connections between Consul clusters, including token generation and consumption. Refer to Create and Manage Peerings Connections for an overview of the CLI workflow for cluster peering.\nUsage: consul peering <subcommand> [options] # ... Subcommands: delete Close and delete a peering connection establish Consume a peering token and establish a connection with the accepting cluster generate-token Generate a peering token for use by a dialing cluster list List the local cluster's peering connections read Read detailed information on a peering connection \nFor more information, examples, and usage about a subcommand, click on the name of the subcommand in the sidebar or one of the links below:\ndelete\nestablish\ngenerate-token\nlist\nread"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.14.x/k8s/connect/cluster-peering/usage/l7-traffic",
  "text": "Manage L7 Traffic With Cluster Peering on Kubernetes | Consul\nThis usage topic describes how to configure the service-resolver custom resource definition (CRD) to set up and manage L7 traffic between services that have an existing cluster peering connection in Consul on Kubernetes deployments.\nFor general guidance for managing L7 traffic with cluster peering, refer to Manage L7 traffic with cluster peering.\nHowever, the service-splitter and service-router CRDs do not natively support directly targeting a service instance hosted on a peer. Before you can split or route traffic to a service on a peer, you must define the service hosted on the peer as an upstream service by configuring a failover in a service-resolver CRD. Then, you can set up a redirect in a second service resolver to interact with the peer service by name.\nDefine the peer cluster as a failover target in the service resolver configuration. \nThe following example updates the service-resolver CRD in cluster-01 so that Consul redirects traffic intended for the frontend service to a backup instance in peer cluster-02 when it detects multiple connection failures.\napiVersion: consul.hashicorp.com/v1alpha1 kind: ServiceResolver metadata: name: frontend spec: connectTimeout: 15s failover: '*': targets: - peer: 'cluster-02' service: 'frontend' namespace: 'default' \nDefine the desired behavior in service-splitter or service-router CRD.\nThe following example splits traffic evenly between frontend and frontend-peer:\napiVersion: consul.hashicorp.com/v1alpha1 kind: ServiceSplitter metadata: name: frontend spec: splits: - weight: 50 ## defaults to service with same name as configuration entry (\"frontend\") - weight: 50 service: frontend-peer \nCreate a second service-resolver configuration entry on the local cluster that resolves the name of the peer service you used when splitting or routing the traffic.\nThe following example uses the name frontend-peer to define a redirect targeting the frontend service on the peer cluster-02:\napiVersion: consul.hashicorp.com/v1alpha1 kind: ServiceResolver metadata: name: frontend-peer spec: redirect: peer: 'cluster-02' service: 'frontend'"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.15.x/connect/cluster-peering/usage/peering-traffic-management",
  "text": "Cluster Peering L7 Traffic Management | Consul\nManage L7 traffic with cluster peering\nThis usage topic describes how to configure and apply the service-resolver configuration entry to set up redirects and failovers between services that have an existing cluster peering connection.\nFor Kubernetes-specific guidance for managing L7 traffic with cluster peering, refer to Manage L7 traffic with cluster peering on Kubernetes.\nHowever, the service-splitter and service-router configuration entry kinds do not natively support directly targeting a service instance hosted on a peer. Before you can split or route traffic to a service on a peer, you must define the service hosted on the peer as an upstream service by configuring a failover in the service-resolver configuration entry. Then, you can set up a redirect in a second service resolver to interact with the peer service by name.\nDefine the peer cluster as a failover target in the service resolver configuration.\nThe following examples update the service-resolver configuration entry in cluster-01 so that Consul redirects traffic intended for the frontend service to a backup instance in peer cluster-02 when it detects multiple connection failures.\nKind = \"service-resolver\" Name = \"frontend\" ConnectTimeout = \"15s\" Failover = { \"*\" = { Targets = [ {Peer = \"cluster-02\"} ] } } \nDefine the desired behavior in service-splitter or service-router configuration entries.\nThe following example splits traffic evenly between frontend services hosted on peers by defining the desired behavior locally:\nKind = \"service-splitter\" Name = \"frontend\" Splits = [ { Weight = 50 ## defaults to service with same name as configuration entry (\"frontend\") }, { Weight = 50 Service = \"frontend-peer\" }, ] \nCreate a local service-resolver configuration entry named frontend-peer and define a redirect targeting the peer and its service:\nKind = \"service-resolver\" Name = \"frontend-peer\" Redirect { Service = frontend Peer = \"cluster-02\" }"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.15.x/k8s/connect/cluster-peering/usage/manage-peering",
  "text": "Manage Cluster Peering Connections on Kubernetes | Consul\nThis usage topic describes how to manage cluster peering connections on Kubernetes deployments.\nAfter you establish a cluster peering connection, you can get a list of all active peering connections, read a specific peering connection's information, and delete peering connections.\nFor general guidance for managing cluster peering connections, refer to Manage L7 traffic with cluster peering.\nTo reset the cluster peering connection, you need to generate a new peering token from the cluster where you created the PeeringAcceptor CRD. The only way to create or set a new peering token is to manually adjust the value of the annotation consul.hashicorp.com/peering-version. Creating a new token causes the previous token to expire.\nIn the PeeringAcceptor CRD, add the annotation consul.hashicorp.com/peering-version. If the annotation already exists, update its value to a higher version.\nacceptor.yml\napiVersion: consul.hashicorp.com/v1alpha1 kind: PeeringAcceptor metadata: name: cluster-02 annotations: consul.hashicorp.com/peering-version: \"1\" ## The peering version you want to set, must be in quotes spec: peer: secret: name: \"peering-token\" key: \"data\" backend: \"kubernetes\" \nAfter updating PeeringAcceptor, repeat all of the steps to establish a new peering connection.\nIn Consul on Kubernetes deployments, you can list all active peering connections in a cluster using the Consul CLI.\nRun the consul peering list CLI command.\n$ consul peering list Name State Imported Svcs Exported Svcs Meta cluster-02 ACTIVE 0 2 env=production cluster-03 PENDING 0 0 \nIn Consul on Kubernetes deployments, you can get information about individual peering connections between clusters using the Consul CLI.\nRun the consul peering read CLI command.\n$ consul peering read -name cluster-02 Name: cluster-02 ID: 3b001063-8079-b1a6-764c-738af5a39a97 State: ACTIVE Meta: env=production Peer ID: e83a315c-027e-bcb1-7c0c-a46650904a05 Peer Server Name: server.dc1.consul Peer CA Pems: 0 Peer Server Addresses: 10.0.0.1:8300 Imported Services: 0 Exported Services: 2 Create Index: 89 Modify Index: 89 \nTo end a peering connection in Kubernetes deployments, delete both the PeeringAcceptor and PeeringDialer resources.\nDelete the PeeringDialer resource from the second cluster.\n$ kubectl --context $CLUSTER2_CONTEXT delete --filename dialer.yaml \nDelete the PeeringAcceptor resource from the first cluster.\n$ kubectl --context $CLUSTER1_CONTEXT delete --filename acceptor.yaml \nTo confirm that you deleted your peering connection in cluster-01, query the the /health HTTP endpoint:\nExec into the server pod for the first cluster.\n$ kubectl exec -it consul-server-0 --context $CLUSTER1_CONTEXT -- /bin/sh \nIf you've enabled ACLs, export an ACL token to access the /health HTP endpoint for services. The bootstrap token may be used if an ACL token is not already provisioned.\n$ export CONSUL_HTTP_TOKEN=<INSERT BOOTSTRAP ACL TOKEN> \nQuery the the /health HTTP endpoint. Peered services with deleted connections should no longe appear.\n$ curl \"localhost:8500/v1/health/connect/backend?peer=cluster-02\""
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.10.x/architecture/coordinates",
  "text": "Network Coordinates | Consul | HashiCorp Developer\nConsul uses a network tomography system to compute network coordinates for nodes in the cluster. These coordinates allow the network round trip time to be estimated between any two nodes using a very simple calculation. This allows for many useful applications, such as finding the service node nearest a requesting node, or failing over to services in the next closest datacenter.\nAll of this is provided through the use of the Serf library. Serf's network tomography is based on \"Vivaldi: A Decentralized Network Coordinate System\", with some enhancements based on other research. There are more details about Serf's network coordinates here.\nNetwork coordinates manifest in several ways inside Consul:\nThe consul rtt command can be used to query for the network round trip time between any two nodes.\nThe Catalog endpoints and Health endpoints can sort the results of queries based on the network round trip time from a given node using a \"?near=\" parameter.\nPrepared queries can automatically fail over services to other Consul datacenters based on network round trip times. See the Geo Failover for some examples.\nThe Coordinate endpoint exposes raw network coordinates for use in other applications.\nConsul uses Serf to manage two different gossip pools, one for the LAN with members of a given datacenter, and one for the WAN which is made up of just the Consul servers in all datacenters. It's important to note that network coordinates are not compatible between these two pools. LAN coordinates only make sense in calculations with other LAN coordinates, and WAN coordinates only make sense with other WAN coordinates.\nComputing the estimated network round trip time between any two nodes is simple once you have their coordinates. Here's a sample coordinate, as returned from the Coordinate endpoint.\n\"Coord\": { \"Adjustment\": 0.1, \"Error\": 1.5, \"Height\": 0.02, \"Vec\": [0.34,0.68,0.003,0.01,0.05,0.1,0.34,0.06] } \nAll values are floating point numbers in units of seconds, except for the error term which isn't used for distance calculations.\nHere's a complete example in Go showing how to compute the distance between two coordinates:\nimport ( \"math\" \"time\" \"github.com/hashicorp/serf/coordinate\" ) func dist(a *coordinate.Coordinate, b *coordinate.Coordinate) time.Duration { // Coordinates will always have the same dimensionality, so this is // just a sanity check. if len(a.Vec) != len(b.Vec) { panic(\"dimensions aren't compatible\") } // Calculate the Euclidean distance plus the heights. sumsq := 0.0 for i := 0; i < len(a.Vec); i++ { diff := a.Vec[i] - b.Vec[i] sumsq += diff * diff } rtt := math.Sqrt(sumsq) + a.Height + b.Height // Apply the adjustment components, guarding against negatives. adjusted := rtt + a.Adjustment + b.Adjustment if adjusted > 0.0 { rtt = adjusted } // Go's times are natively nanoseconds, so we convert from seconds. const secondsToNanoseconds = 1.0e9 return time.Duration(rtt * secondsToNanoseconds) }"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.10.x/commands",
  "text": "This page does not exist for version v1.10.x."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.10.x/connect/registration/sidecar-service",
  "text": "Connect - Sidecar Service Registration | Consul\nConnect proxies are typically deployed as \"sidecars\" that run on the same node as the single service instance that they handle traffic for. They might be on the same VM or running as a separate container in the same network namespace.\nTo simplify the configuration experience when deploying a sidecar for a service instance, Consul 1.3 introduced a new field in the Connect block of the service definition.\nTo deploy a service and sidecar proxy locally, complete the Getting Started guide.\nThe connect.sidecar_service field is a complete nested service definition on which almost any regular service definition field can be set. The exceptions are noted below. If used, the service definition is treated identically to another top-level service definition. The value of the nested definition is that all fields are optional with some opinionated defaults applied that make setting up a sidecar proxy much simpler.\nTo register a service instance with a sidecar, all that's needed is:\n{ \"service\": { \"name\": \"web\", \"port\": 8080, \"connect\": { \"sidecar_service\": {} } } } \nThis will register the web service as normal, but will also register another proxy service with defaults values used.\nThe above expands out to be equivalent to the following explicit service definitions:\n{ \"service\": { \"name\": \"web\", \"port\": 8080, } } { \"name\": \"web-sidecar-proxy\", \"port\": 20000, \"kind\": \"connect-proxy\", \"checks\": [ { \"Name\": \"Connect Sidecar Listening\", \"TCP\": \"127.0.0.1:20000\", \"Interval\": \"10s\" }, { \"name\": \"Connect Sidecar Aliasing web\", \"alias_service\": \"web\" } ], \"proxy\": { \"destination_service_name\": \"web\", \"destination_service_id\": \"web\", \"local_service_address\": \"127.0.0.1\", \"local_service_port\": 8080, } } \nDetails on how the defaults are determined are documented below.\nNote: Sidecar service registrations are only a shorthand for registering multiple services. Consul will not start up or manage the actual proxy processes for you.\nThe following example shows a service definition where some fields are overridden to customize the proxy configuration.\n{ \"name\": \"web\", \"port\": 8080, \"connect\": { \"sidecar_service\": { \"proxy\": { \"upstreams\": [ { \"destination_name\": \"db\", \"local_bind_port\": 9191 } ], \"config\": { \"handshake_timeout_ms\": 1000 } } } } } \nThis example customizes the proxy upstreams and some built-in proxy configuration.\nThe following fields are set by default on a sidecar service registration. With the exceptions noted any field may be overridden explicitly in the connect.sidecar_service definition to customize the proxy registration. The \"parent\" service refers to the service definition that embeds the sidecar proxy.\nid - ID defaults to being <parent-service-id>-sidecar-proxy. This can't be overridden as it is used to manage the lifecycle of the registration.\nname - Defaults to being <parent-service-name>-sidecar-proxy.\ntags - Defaults to the tags of the parent service.\nmeta - Defaults to the service metadata of the parent service.\nport - Defaults to being auto-assigned from a configurable range specified by sidecar_min_port and sidecar_max_port.\nkind - Defaults to connect-proxy. This can't be overridden currently.\ncheck, checks - By default we add a TCP check on the local address and port for the proxy, and a service alias check for the parent service. If either check or checks fields are set, only the provided checks are registered.\nproxy.destination_service_name - Defaults to the parent service name.\nproxy.destination_service_id - Defaults to the parent service ID.\nproxy.local_service_address - Defaults to 127.0.0.1.\nproxy.local_service_port - Defaults to the parent service port.\nAlmost all fields in a service definition may be set on the connect.sidecar_service except for the following:\nid - Sidecar services get an ID assigned and it is an error to override this. This ensures the agent can correctly deregister the sidecar service later when the parent service is removed.\nkind - Kind defaults to connect-proxy and there is currently no way to unset this to make the registration be for a regular non-connect-proxy service.\nconnect.sidecar_service - Service definitions can't be nested recursively.\nconnect.proxy - (Deprecated) Managed proxies can't be defined on a sidecar.\nconnect.native - Currently the kind is fixed to connect-proxy and it's an error to register a connect-proxy that is also Connect-native.\nSidecar service registration is mostly a configuration syntax helper to avoid adding lots of boiler plate for basic sidecar options, however the agent does have some specific behavior around their lifecycle that makes them easier to work with.\nThe agent fixes the ID of the sidecar service to be based on the parent service's ID. This enables the following behavior.\nA service instance can only ever have one sidecar service registered.\nWhen re-registering via API or reloading from configuration file:\nIf something changes in the nested sidecar service definition, the change will update the current sidecar registration instead of creating a new one.\nIf a service registration removes the nested sidecar_service then the previously registered sidecar for that service will be deregistered automatically.\nWhen reloading the configuration files, if a service definition changes its ID, then a new service instance and a new sidecar instance will be registered. The old ones will be removed since they are no longer found in the config files."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.11.x/troubleshoot/faq",
  "text": "Common Error Messages | Troubleshoot | Consul\nFrequently Asked Questions\nQ: Can I upgrade directly to a specific Helm chart version or should I upgrade one patch release at a time?\nIt is safe to upgrade directly to a specific version. Be sure to read the release notes for all versions you're upgrading through and look for any breaking changes.\nQ: Can I upgrade in place or should I spin up a new Kubernetes cluster?\nIt is always safer to spin up a new Kubernetes cluster but that is not an option for most teams. Consul supports upgrading in place.\nNon-production environments should be upgraded first. If upgrading a Consul version, Consul data should be backed up.\nQ: How can I run tcpdump on Consul servers?\nFirst, add the following to your values.yaml file so you can kubectl exec into the Consul server containers as root:\nserver: securityContext: runAsNonRoot: false runAsGroup: 0 runAsUser: 0 fsGroup: 0 \nRun a helm upgrade (see Upgrade Consul on Kubernetes for full upgrade instructions).\nNow, kubectl exec into a server pod:\n$ kubectl exec -it consul-server-0 -- sh \nInstall tcpdump:\n$ apk add --no-cache tcpdump $ which tcpdump /usr/bin/tcpdump \nQ: What is Checkpoint? / Does Consul call home?\nConsul makes use of a HashiCorp service called Checkpoint which is used to check for updates and critical security bulletins. Only anonymous information, which cannot be used to identify the user or host, is sent to Checkpoint. An anonymous ID is sent which helps de-duplicate warning messages. This anonymous ID can be disabled. In fact, using the Checkpoint service is optional and can be disabled.\nSee disable_anonymous_signature and disable_update_check.\nQ: Does Consul rely on UDP Broadcast or Multicast?\nConsul uses the Serf gossip protocol which relies on TCP and UDP unicast. Broadcast and Multicast are rarely available in a multi-tenant or cloud network environment. For that reason, Consul and Serf were both designed to avoid any dependence on those capabilities.\nQ: Is Consul eventually or strongly consistent?\nConsul has two important subsystems, the service catalog and the gossip protocol. The service catalog stores all the nodes, service instances, health check data, ACLs, and KV information. It is strongly consistent, and replicated using the consensus protocol.\nThe gossip protocol is used to track which nodes are part of the cluster and to detect a node or agent failure. This information is eventually consistent by nature. When the servers detects a change in membership, or receive a health update, they update the service catalog appropriately.\nBecause of this split, the answer to the question is subtle. Almost all client APIs interact with the service catalog and are strongly consistent. Updates to the catalog may come via the gossip protocol which is eventually consistent meaning the current state of the catalog can lag behind until the state is reconciled.\nQ: Are failed or left nodes ever removed?\nTo prevent an accumulation of dead nodes (nodes in either failed or left states), Consul will automatically remove dead nodes out of the catalog. This process is called reaping. This is currently done on a configurable interval of 72 hours. Reaping is similar to leaving, causing all associated services to be deregistered. Changing the reap interval for aesthetic reasons to trim the number of failed or left nodes is not advised (nodes in the failed or left state do not cause any additional burden on Consul).\nQ: Does Consul support delta updates for watchers or blocking queries?\nConsul does not currently support sending a delta or a change only response to a watcher or a blocking query. The API simply allows for an edge-trigger return with the full result. A client should keep the results of their last read and compute the delta client side.\nBy design, Consul offloads this to clients instead of attempting to support the delta calculation. This avoids expensive state maintenance on the servers as well as race conditions between data updates and watch registrations.\nQ: What network ports does Consul use?\nThe Ports Used section of the Configuration documentation lists all ports that Consul uses.\nQ: Does Consul require certain user process resource limits?\nThere should be only a small number of open file descriptors required for a Consul client agent. The gossip layers perform transient connections with other nodes, each connection to the client agent (such as for a blocking query) will open a connection, and there will typically be connections to one of the Consul servers. A small number of file descriptors are also required for watch handlers, health checks, log files, and so on.\nFor a Consul server agent, you should plan on the above requirements and an additional incoming connection from each of the nodes in the cluster. This should not be the common case, but in the worst case if there is a problem with the other servers you would expect the other client agents to all connect to a single server and so preparation for this possibility is helpful.\nThe default ulimits are usually sufficient for Consul, but you should closely scrutinize your own environment's specific needs and identify the root cause of any excessive resource utilization before arbitrarily increasing the limits.\nQ: What is the per-key value size limitation for Consul's key/value store?\nThe default recommended limit on a key's value size is 512KB. This is strictly enforced and an HTTP 413 status will be returned to any client that attempts to store more than that limit in a value. The limit can be increased by using the kv_max_value_size configuration option.\nIt should be noted that the Consul key/value store is not designed to be used as a general purpose database. See Server Performance for more details.\nQ: What data is replicated between Consul datacenters?\nIn general, data is not replicated between different Consul datacenters. When a request is made for a resource in another datacenter, the local Consul servers forward an RPC request to the remote Consul servers for that resource and return the results. If the remote datacenter is not available, then those resources will also not be available from that datacenter. That will not affect the requests to the local datacenter. There are some special situations where a limited subset of data can be replicated, such as with Consul's built-in ACL replication capability, or external tools like consul-replicate.\nQ: Can Consul natively handle protecting against other processes accessing Consul's memory state?\nConsul does not provide built-in memory access protections, and doesn't interact with the host system to change or manipulate viewing and doesn't interact with the host system to change or manipulate application security.\nWe recommend taking any precautions or remediation steps that you would normally do for individual processes, based on your operating system.\nPlease see our Security Model for more information.\nQ: Are the Consul Docker Images OCI Compliant?\nThe official Consul Docker image uses Docker image schema V2, which is OCI Compliant. To check the docker images on Docker Hub, use the command docker manifest inspect consul to inspect the manifest payload. The docker manifest inspect may require you to enable experimental features to use.\nQ: What browsers are supported by the Consul UI?\nConsul currently supports all 'evergreen' browsers, as they are generally on up-to-date versions. This means we support:\nChrome\nFirefox\nSafari\nMicrosoft Edge\nWe do not support Internet Explorer 11 (IE 11). Consul follows a similar alignment with Microsoft's own stance on IE 11, found on their support website."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.12.x/enterprise/license/overview",
  "text": "Enterprise Licenses | Consul | HashiCorp Developer\nConsul Enterprise License\nAll Consul Enterprise agents must be licensed when they are started. Where that license comes from will depend on which binary is in use, whether the agent is a server, client or snapshot agent, and whether ACLs have been enabled for the cluster.\nConsul Enterprise 1.10.0 removed temporary licensing. Prior to 1.10.0, Consul Enterprise agents could start without a license and then have a license applied to them later on via the CLI or API. That functionality has been removed and replaced with the ability to load licenses from the agent's configuration or environment. Also, prior to 1.10.0, server agents would automatically propagate the license between themselves. This no longer occurs and the license must be present on each server agent when it is started.\nApplying a License\nFor Consul Enterprise 1.10.0 or greater, a license must be available at the time the agent starts. For server agents this means that they must either have the license_path configuration set or have a license configured in the servers environment with the CONSUL_LICENSE or CONSUL_LICENSE_PATH environment variables. Both the configuration item and the CONSUL_LICENSE_PATH environment variable point to a file containing the license whereas the CONSUL_LICENSE environment variable should contain the license as the value. If multiple variables are set, the following order of precedence applies:\nCONSUL_LICENSE environment variable\nCONSUL_LICENSE_PATH environment variable\nlicense_path configuration item.\nClient agents and snapshot agents may also be licensed in the very same manner. However, to avoid the need to configure the license on many client agents and snapshot agents, those agents have the capability to retrieve the license automatically under the conditions described below.\nUpdating the license for an agent depends on the method you used to apply the license.\nIf you used the CONSUL_LICENSE environment variable: After updating the environment variable, restart the affected agents. \nIf you used the CONSUL_LICENSE_PATH environment variable: Update the license file first. Then, restart the affected agents. \nIf you used the license_path configuration item: Update the license file first. Then, run consul reload for the affected agents.\nClient Agent License Retrieval\nWhen a client agent starts without a license in its configuration or environment, it will try to retrieve the license from the servers via RPCs. That RPC always requires a valid non-anonymous ACL token to authorize the request but the token doesn't need any particular permissions. As the license is required before the client actually joins the cluster, where to make those RPC requests to is inferred from the start_join or retry_join configurations. If those are both unset or no agent token is set then the client agent will immediately shut itself down.\nIf all preliminary checks pass the client agent will attempt to reach out to any server on its RPC port to request the license. These requests will be retried for up to 5 minutes and if it is unable to retrieve a license within that time frame it will shut itself down.\nIf ACLs are disabled then the license must be provided to the client agent through one of the three methods listed below. Failure in providing the client agent with a licence will prevent the client agent from joining the cluster.\nCONSUL_LICENSE environment variable\nCONSUL_LICENSE_PATH environment variable\nlicense_path configuration item.\nSnapshot Agent License Retrieval\nThe snapshot agent has similar functionality to the client agent for automatically retrieving the license. However, instead of requiring a server agent to talk to, the snapshot agent can request the license from the server or client agent it would use for all other operations. It still requires an ACL token to authorize the request. Also like client agents, the snapshot agent will shut itself down after being unable to retrieve the license for 5 minutes."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.15.x/k8s/connect/cluster-peering/usage/l7-traffic",
  "text": "Manage L7 Traffic With Cluster Peering on Kubernetes | Consul\nThis usage topic describes how to configure the service-resolver custom resource definition (CRD) to set up and manage L7 traffic between services that have an existing cluster peering connection in Consul on Kubernetes deployments.\nFor general guidance for managing L7 traffic with cluster peering, refer to Manage L7 traffic with cluster peering.\nHowever, the service-splitter and service-router CRDs do not natively support directly targeting a service instance hosted on a peer. Before you can split or route traffic to a service on a peer, you must define the service hosted on the peer as an upstream service by configuring a failover in a service-resolver CRD. Then, you can set up a redirect in a second service resolver to interact with the peer service by name.\nDefine the peer cluster as a failover target in the service resolver configuration. \nThe following example updates the service-resolver CRD in cluster-01 so that Consul redirects traffic intended for the frontend service to a backup instance in peer cluster-02 when it detects multiple connection failures.\napiVersion: consul.hashicorp.com/v1alpha1 kind: ServiceResolver metadata: name: frontend spec: connectTimeout: 15s failover: '*': targets: - peer: 'cluster-02' service: 'frontend' namespace: 'default' \nDefine the desired behavior in service-splitter or service-router CRD.\nThe following example splits traffic evenly between frontend and frontend-peer:\napiVersion: consul.hashicorp.com/v1alpha1 kind: ServiceSplitter metadata: name: frontend spec: splits: - weight: 50 ## defaults to service with same name as configuration entry (\"frontend\") - weight: 50 service: frontend-peer \nCreate a second service-resolver configuration entry on the local cluster that resolves the name of the peer service you used when splitting or routing the traffic.\nThe following example uses the name frontend-peer to define a redirect targeting the frontend service on the peer cluster-02:\napiVersion: consul.hashicorp.com/v1alpha1 kind: ServiceResolver metadata: name: frontend-peer spec: redirect: peer: 'cluster-02' service: 'frontend'"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.10.x/enterprise/network-segments",
  "text": "Consul Enterprise Network Segments | Consul\nEnterprise\nThis feature requires Consul Enterprise version 0.9.3+ with the Global Visibility, Routing, and Scale module.\nConsul requires full connectivity between all agents (servers and clients) in a datacenter within a given LAN gossip pool. By default, all Consul agents will be a part of one shared Serf LAN gossip pool known as the <default> network segment, thus requiring full mesh connectivity within the datacenter.\nIn some environments, full connectivity between all agents is not possibleknown as a \"segmented network\". This is usually the result of business policies enforced through network rules or firewalls. To use Consul in a segmented network, you must break up the LAN gossip pool along network communication boundaries into separate \"network segments\". Network segments are isolated LAN gossip pools that only require full connectivity between agent members on the same segment.\nTo get started with network segments you can review the tutorial on HashiCorp Learn for Network Segments.\nInfo: Network segments enable you to operate a Consul datacenter without full mesh (LAN) connectivity between agents. To federate multiple Consul datacenters without full mesh (WAN) connectivity between all server agents in all datacenters, use Network Areas (Enterprise).\nTo help set context for this feature, it is useful to understand the various Consul networking models and their capabilities.\nCluster: A set of Consul servers forming a Raft quorum along with a collection of Consul clients, all set to the same datacenter, and joined together to form what we will call a \"local cluster\". Consul clients discover the Consul servers in their local cluster through the gossip mechanism and make RPC requests to them. LAN Gossip (OSS) is an open intra-cluster networking model, and Network Segments (Enterprise) creates multiple segments within one cluster.\nFederated Cluster: A cluster of clusters with a Consul server group per cluster each set per \"datacenter\". These Consul servers are federated together over the WAN. Consul clients make use of resources in federated clusters by forwarding RPCs through the Consul servers in their local cluster, but they never interact with remote Consul servers directly. There are currently two inter-cluster network models which can be viewed on HashiCorp Learn: WAN gossip (OSS) and Network Areas (Enterprise).\nLAN Gossip Pool: A set of Consul agents that have full mesh connectivity among themselves, and use Serf to maintain a shared view of the members of the pool for different purposes, like finding a Consul server in a local cluster, or finding servers in a remote cluster. A segmented LAN Gossip Pool limits a group of agents to only connect with the agents in its segment.\nServer agents are members of all segments. The datacenter includes a <default> segment, as well as additional segments defined in the segments server agent configuration option. Each additional segment is defined by:\na non-empty name\na unique port\noptionally, a custom bind and advertise address for the additional segment's Serf LAN listener on the server\nNote: Prior to Consul 1.7.3, a Consul server agent configured with too many network segments may not be able to start due to limitations in Serf.\nExample Server Configuration\nThe following server agent configuration will create two user-defined network segments: alpha and beta.\nExample network segments configuration for server agents\nsegments = [ { name = \"alpha\" bind = \"{{GetPrivateIP}}\" advertise = \"{{GetPrivateIP}}\" port = 8303 }, { name = \"beta\" bind = \"{{GetPrivateIP}}\" advertise = \"{{GetPrivateIP}}\" port = 8304 } ] \nThe server agent configuration options relevant to network segments are:\nports.serf_lan: The Serf LAN port on this server for the <default> network segment's gossip pool.\nsegments: A list of user-defined network segments on this server, including their names and Serf LAN ports.\nEach client agent can only be a member of one segment at a time. This will be the <default> segment unless otherwise specified in the agent's segment agent configuration option.\nJoin a Client to a Segment\nFor a client agent to join the Consul datacenter, it must connect to another agent (client or server) within its configured segment.\nClients A and B specify the same segment S. Client B is already joined to the segment S LAN gossip pool. Client A wants to join via Client B. In order to do so, Client A must connect to Client B's configured Serf LAN port.\nThere are several methods to specify the port used by the join operation, listed in order of precedence:\nSpecify an explicit port in the join address. This can be done at the CLI when starting the agent (e.g., consul agent -retry-join \"client-b-address:8303\"), or in the agent's configuration using the retry-join option. This method is not compatible with cloud auto-join.\nSpecify an alternate Serf LAN port for the agent. This can be done at the CLI when starting the agent (e.g., consul agent -retry-join \"client-b-address\" -serf-lan-port 8303), or in the agent's configuration using the serf_lan option. When a Serf LAN port is not explicitly specified in the join address, the agent will attempt to join the target host at the Serf LAN port specified in CLI or agent configuration.\nUse the default Serf LAN port (8301). The agent will attempt to join the target host on port 8301.\nAgents within a segment can use different port numbers for their Serf LAN port. For example, on the <default> segment, Server S can use port 8301, Client A can use 8303, and Client B can use 8304. However, if an agent wishes to join a segment via an agent that uses a different port number, the target agent's Serf LAN port must be specified in the join address (method 1 above).\nExample Client Configuration\nThe following client agent configuration will cause the agent to:\nOpen a Serf LAN listener port on 8303.\nAttempt to join the cluster via servers on port 8303 (since an alternate port is not specified in the retry_join addresses).\nExample network segment configuration for client agents\nnode_name = \"consul-client1\" retry_join = [\"consul-server1\", \"consul-server2\", \"consul-server3\"] segment = \"alpha\" ports = { serf_lan = 8303 } \nThe client agent configuration options relevant to network segments are:\nsegment: The name of the network segment this client agent belongs to.\nports.serf_lan: Serf LAN port for the above segment on this client. This is not required to match the configured Serf LAN port for other agents on this segment.\nretry_join or start_join: A list of agent addresses to join when starting. Ensure the correct Serf LAN port for this segment is used when joining the LAN gossip pool using one of the available configuration methods."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.11.x/agent/config/config-files",
  "text": "This page does not exist for version v1.11.x."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.11.x/enterprise/license/overview",
  "text": "Enterprise Licenses | Consul | HashiCorp Developer\nConsul Enterprise License\nAll Consul Enterprise agents must be licensed when they are started. Where that license comes from will depend on which binary is in use, whether the agent, is a server, client or snapshot agent and whether ACLs have been enabled for the cluster.\nConsul Enterprise 1.10.0 removed temporary licensing. Prior to 1.10.0, Consul Enterprise agents could start without a license and then have a license applied to them later on via the CLI or API. That functionality has been removed and replaced with the ability to load licenses from the agent's configuration or environment. Also, prior to 1.10.0, server agents would automatically propagate the license between themselves. This no longer occurs and the license must be present on each server agent when it is started.\nApplying a License\nFor Consul Enterprise 1.10.0 or greater, a license must be available at the time the agent starts. For server agents this means that they must either have the license_path configuration set or have a license configured in the servers environment with the CONSUL_LICENSE or CONSUL_LICENSE_PATH environment variables. Both the configuration item and the CONSUL_LICENSE_PATH environment variable point to a file containing the license whereas the CONSUL_LICENSE environment variable should contain the license as the value. If multiple variables are set, the following order of precedence applies:\nCONSUL_LICENSE environment variable\nCONSUL_LICENSE_PATH environment variable\nlicense_path configuration item.\nClient agents and snapshot agents may also be licensed in the very same manner. However, to avoid the need to configure the license on many client agents and snapshot agents, those agents have the capability to retrieve the license automatically under the conditions described below.\nClient Agent License Retrieval\nWhen a client agent starts without a license in its configuration or environment, it will try to retrieve the license from the servers via RPCs. That RPC always requires a valid non-anonymous ACL token to authorize the request but the token doesn't need any particular permissions. As the license is required before the client actually joins the cluster, where to make those RPC requests to is inferred from the start_join or retry_join configurations. If those are both unset or no agent token is set then the client agent will immediately shut itself down.\nIf all preliminary checks pass the client agent will attempt to reach out to any server on its RPC port to request the license. These requests will be retried for up to 5 minutes and if it is unable to retrieve a license within that time frame it will shut itself down.\nIf ACLs are disabled then the license must be provided to the client agent through one of the three methods listed below. Failure in providing the client agent with a licence will prevent the client agent from joining the cluster.\nCONSUL_LICENSE environment variable\nCONSUL_LICENSE_PATH environment variable\nlicense_path configuration item.\nSnapshot Agent License Retrieval\nThe snapshot agent has similar functionality to the client agent for automatically retrieving the license. However, instead of requiring a server agent to talk to, the snapshot agent can request the license from the server or client agent it would use for all other operations. It still requires an ACL token to authorize the request. Also like client agents, the snapshot agent will shut itself down after being unable to retrieve the license for 5 minutes."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.10.x/agent/config-entries",
  "text": "Configuration Entries | Consul | HashiCorp Developer\nConfiguration entries can be created to provide cluster-wide defaults for various aspects of Consul.\nOutside of Kubernetes, configuration entries can be specified in HCL or JSON using either snake_case or CamelCase for key names. On Kubernetes, configuration entries can be managed by custom resources in YAML.\nOutside of Kubernetes, every configuration entry specified in HCL or JSON has at least two fields: Kind and Name. Those two fields are used to uniquely identify a configuration entry. Configuration entries specified as HCL or JSON objects use either snake_case or CamelCase for key names.\nExample:\nKind = \"<supported kind>\" Name = \"<name of entry>\" \nOn Kubernetes, Kind is set as the custom resource kind and Name is set as metadata.name:\napiVersion: consul.hashicorp.com/v1alpha1 kind: <supported kind> metadata: name: <name of entry> \nSee Service Mesh - Config Entries for the list of supported config entries.\nSee Kubernetes Custom Resource Definitions.\nConfiguration entries outside of Kubernetes should be managed with the Consul CLI or API. Additionally, as a convenience for initial cluster bootstrapping, configuration entries can be specified in all of the Consul servers's configuration files\nManaging Configuration Entries with the CLI\nCreating or Updating a Configuration Entry\nThe consul config write command is used to create and update configuration entries. This command will load either a JSON or HCL file holding the configuration entry definition and then will push this configuration to Consul.\nExample HCL Configuration File:\nproxy-defaults.hcl\nKind = \"proxy-defaults\" Name = \"global\" Config { local_connect_timeout_ms = 1000 handshake_timeout_ms = 10000 } \nThen to apply this configuration, run:\n$ consul config write proxy-defaults.hcl \nIf you need to make changes to a configuration entry, simple edit that file and then rerun the command. This command will not output anything unless there is an error in applying the configuration entry. The write command also supports a -cas option to enable performing a compare-and-swap operation to prevent overwriting other unknown modifications.\nReading a Configuration Entry\nThe consul config read command is used to read the current value of a configuration entry. The configuration entry will be displayed in JSON form which is how its transmitted between the CLI client and Consul's HTTP API.\nExample:\n$ consul config read -kind service-defaults -name web { \"Kind\": \"service-defaults\", \"Name\": \"web\", \"Protocol\": \"http\" } \nListing Configuration Entries\nThe consul config list command is used to list out all the configuration entries for a given kind.\nExample:\n$ consul config list -kind service-defaults web api db \nDeleting Configuration Entries\nThe consul config delete command is used to delete an entry by specifying both its kind and name.\nExample:\n$ consul config delete -kind service-defaults -name web \nThis command will not output anything when the deletion is successful.\nConfiguration Entry Management with Namespaces Enterprise\nConfiguration entry operations support passing a namespace in order to isolate the entry to affect only operations within that namespace. This was added in Consul 1.7.0.\n$ consul config write service-defaults.hcl -namespace foo \n$ consul config list -kind service-defaults -namespace foo web api \nBootstrapping From A Configuration File\nConfiguration entries can be bootstrapped by adding them inline to each Consul server's configuration file. When a server gains leadership, it will attempt to initialize the configuration entries. If a configuration entry does not already exist outside of the servers configuration, then it will create it. If a configuration entry does exist, that matches both kind and name, then the server will do nothing."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.10.x/agent/checks",
  "text": "This page does not exist for version v1.10.x."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.10.x/agent/telemetry",
  "text": "Telemetry | Consul | HashiCorp Developer\nThe Consul agent collects various runtime metrics about the performance of different libraries and subsystems. These metrics are aggregated on a ten second (10s) interval and are retained for one minute. An interval is the period of time between instances of data being collected and aggregated.\nWhen telemetry is being streamed to an external metrics store, the interval is defined to be that store's flush interval.\nTo view this data, you must send a signal to the Consul process: on Unix, this is USR1 while on Windows it is BREAK. Once Consul receives the signal, it will dump the current telemetry information to the agent's stderr.\nThis telemetry information can be used for debugging or otherwise getting a better view of what Consul is doing. Review the Monitoring and Metrics tutorial to learn how collect and interpret Consul data.\nAdditionally, if the telemetry configuration options are provided, the telemetry information will be streamed to a statsite or statsd server where it can be aggregated and flushed to Graphite or any other metrics store. For a configuration example for Telegraf, review the Monitoring with Telegraf tutorial.\nThis information can also be viewed with the metrics endpoint in JSON format or using Prometheus format.\nBelow is sample output of a telemetry dump:\n[2014-01-29 10:56:50 -0800 PST][G] 'consul-agent.runtime.num_goroutines': 19.000 [2014-01-29 10:56:50 -0800 PST][G] 'consul-agent.runtime.alloc_bytes': 755960.000 [2014-01-29 10:56:50 -0800 PST][G] 'consul-agent.runtime.malloc_count': 7550.000 [2014-01-29 10:56:50 -0800 PST][G] 'consul-agent.runtime.free_count': 4387.000 [2014-01-29 10:56:50 -0800 PST][G] 'consul-agent.runtime.heap_objects': 3163.000 [2014-01-29 10:56:50 -0800 PST][G] 'consul-agent.runtime.total_gc_pause_ns': 1151002.000 [2014-01-29 10:56:50 -0800 PST][G] 'consul-agent.runtime.total_gc_runs': 4.000 [2014-01-29 10:56:50 -0800 PST][C] 'consul-agent.agent.ipc.accept': Count: 5 Sum: 5.000 [2014-01-29 10:56:50 -0800 PST][C] 'consul-agent.agent.ipc.command': Count: 10 Sum: 10.000 [2014-01-29 10:56:50 -0800 PST][C] 'consul-agent.serf.events': Count: 5 Sum: 5.000 [2014-01-29 10:56:50 -0800 PST][C] 'consul-agent.serf.events.foo': Count: 4 Sum: 4.000 [2014-01-29 10:56:50 -0800 PST][C] 'consul-agent.serf.events.baz': Count: 1 Sum: 1.000 [2014-01-29 10:56:50 -0800 PST][S] 'consul-agent.memberlist.gossip': Count: 50 Min: 0.007 Mean: 0.020 Max: 0.041 Stddev: 0.007 Sum: 0.989 [2014-01-29 10:56:50 -0800 PST][S] 'consul-agent.serf.queue.Intent': Count: 10 Sum: 0.000 [2014-01-29 10:56:50 -0800 PST][S] 'consul-agent.serf.queue.Event': Count: 10 Min: 0.000 Mean: 2.500 Max: 5.000 Stddev: 2.121 Sum: 25.000 \nThese are some metrics emitted that can help you understand the health of your cluster at a glance. A Grafana dashboard is also available, which is maintained by the Consul team and displays these metrics for easy visualization. For a full list of metrics emitted by Consul, see Metrics Reference\nTransaction timing\nMetric NameDescriptionUnitType\nconsul.kvs.apply\tMeasures the time it takes to complete an update to the KV store.\tms\ttimer\t\nconsul.txn.apply\tMeasures the time spent applying a transaction operation.\tms\ttimer\t\nconsul.raft.apply\tCounts the number of Raft transactions applied during the interval. This metric is only reported on the leader.\traft transactions / interval\tcounter\t\nconsul.raft.commitTime\tMeasures the time it takes to commit a new entry to the Raft log on the leader.\tms\ttimer\t\nWhy they're important: Taken together, these metrics indicate how long it takes to complete write operations in various parts of the Consul cluster. Generally these should all be fairly consistent and no more than a few milliseconds. Sudden changes in any of the timing values could be due to unexpected load on the Consul servers, or due to problems on the servers themselves.\nWhat to look for: Deviations (in any of these metrics) of more than 50% from baseline over the previous hour.\nLeadership changes\nMetric NameDescriptionUnitType\nconsul.raft.leader.lastContact\tMeasures the time since the leader was last able to contact the follower nodes when checking its leader lease.\tms\ttimer\t\nconsul.raft.state.candidate\tIncrements whenever a Consul server starts an election.\telections\tcounter\t\nconsul.raft.state.leader\tIncrements whenever a Consul server becomes a leader.\tleaders\tcounter\t\nWhy they're important: Normally, your Consul cluster should have a stable leader. If there are frequent elections or leadership changes, it would likely indicate network issues between the Consul servers, or that the Consul servers themselves are unable to keep up with the load.\nWhat to look for: For a healthy cluster, you're looking for a lastContact lower than 200ms, leader > 0 and candidate == 0. Deviations from this might indicate flapping leadership.\nAutopilot\nMetric NameDescriptionUnitType\nconsul.autopilot.healthy\tTracks the overall health of the local server cluster. If all servers are considered healthy by Autopilot, this will be set to 1. If any are unhealthy, this will be 0. All non-leader servers will report NaN.\thealth state\tgauge\t\nWhy it's important: Autopilot can expose the overall health of your cluster with a simple boolean.\nWhat to look for: Alert if healthy is 0. Some other indicators of an unhealthy cluster would be:\nconsul.raft.commitTime - This can help reflect the speed of state store changes being performed by the agent. If this number is rising, the server may be experiencing an issue due to degraded resources on the host.\nLeadership change metrics - Check for deviation from the recommended values. This can indicate failed leadership elections or flapping nodes.\nMemory usage\nMetric NameDescriptionUnitType\nconsul.runtime.alloc_bytes\tMeasures the number of bytes allocated by the Consul process.\tbytes\tgauge\t\nconsul.runtime.sys_bytes\tMeasures the total number of bytes of memory obtained from the OS.\tbytes\tgauge\t\nWhy they're important: Consul keeps all of its data in memory. If Consul consumes all available memory, it will crash.\nWhat to look for: If consul.runtime.sys_bytes exceeds 90% of total available system memory.\nNOTE: This metric is calculated using Go's runtime package MemStats. This will have a different output than using information gathered from top. For more information, see GH-4734.\nGarbage collection\nconsul.runtime.total_gc_pause_ns\tNumber of nanoseconds consumed by stop-the-world garbage collection (GC) pauses since Consul started.\tns\tgauge\t\nWhy it's important: GC pause is a \"stop-the-world\" event, meaning that all runtime threads are blocked until GC completes. Normally these pauses last only a few nanoseconds. But if memory usage is high, the Go runtime may GC so frequently that it starts to slow down Consul.\nWhat to look for: Warning if total_gc_pause_ns exceeds 2 seconds/minute, critical if it exceeds 5 seconds/minute.\nNOTE: total_gc_pause_ns is a cumulative counter, so in order to calculate rates (such as GC/minute), you will need to apply a function such as InfluxDB's non_negative_difference().\nNetwork activity - RPC Count\nconsul.client.rpc\tIncrements whenever a Consul agent in client mode makes an RPC request to a Consul server\trequests\tcounter\t\nconsul.client.rpc.exceeded\tIncrements whenever a Consul agent in client mode makes an RPC request to a Consul server gets rate limited by that agent's limits configuration.\trequests\tcounter\t\nconsul.client.rpc.failed\tIncrements whenever a Consul agent in client mode makes an RPC request to a Consul server and fails.\trequests\tcounter\t\nWhy they're important: These measurements indicate the current load created from a Consul agent, including when the load becomes high enough to be rate limited. A high RPC count, especially from consul.client.rpcexceeded meaning that the requests are being rate-limited, could imply a misconfigured Consul agent.\nWhat to look for: Sudden large changes to the consul.client.rpc metrics (greater than 50% deviation from baseline). consul.client.rpc.exceeded or consul.client.rpc.failed count > 0, as it implies that an agent is being rate-limited or fails to make an RPC request to a Consul server\nRaft Replication Capacity Issues\nconsul.raft.fsm.lastRestoreDuration\tMeasures the time taken to restore the FSM from a snapshot on an agent restart or from the leader calling installSnapshot. This is a gauge that holds it's value since most servers only restore during restarts which are typically infrequent.\tms\tgauge\t\nconsul.raft.leader.oldestLogAge\tThe number of milliseconds since the oldest log in the leader's log store was written. This can be important for replication health where write rate is high and the snapshot is large as followers may be unable to recover from a restart if restoring takes longer than the minimum value for the current leader. Compare this with consul.raft.fsm.lastRestoreDuration and consul.raft.rpc.installSnapshot to monitor. In normal usage this gauge value will grow linearly over time until a snapshot completes on the leader and the log is truncated.\tms\tgauge\t\nconsul.raft.rpc.installSnapshot\tMeasures the time taken to process the installSnapshot RPC call. This metric should only be seen on agents which are currently in the follower state.\tms\ttimer\t\nWhy they're important: These metrics allow operators to monitor the health and capacity of raft replication on servers. When Consul is handling large amounts of data and high write throughput it is possible for the cluster to get into the following state:\nWrite throughput is high (say 500 commits per second or more) and constant\nThe leader is writing out a large snapshot every minute or so\nThe snapshot is large enough that it takes considerable time to restore from disk on a restart or from the leader if a follower gets behind\nDisk IO available allows the leader to write a snapshot faster than it can be restored from disk on a follower\nUnder these conditions, a follower after a restart may be unable to catch up on replication and become a voter again since it takes longer to restore from disk or the leader than the leader takes to write a new snapshot and truncate its logs. Servers retain raft_trailing_logs (default 10240) log entries even if their snapshot was more recent. On a leader processing 500 commits/second, that is only about 20 seconds worth of logs. Assuming the leader is able to write out a snapshot and truncate the logs in less than 20 seconds, there will only be 20 seconds worth of \"recent\" logs available on the leader right after the leader has taken a snapshot and never more than about 80 seconds worth assuming it is taking a snapshot and truncating logs every 60 seconds.\nIn this state, followers must be able to restore a snapshot into memory and resume replication in under 80 seconds otherwise they will never be able to rejoin the cluster until write rates reduce. If they take more than 20 seconds then there will be a chance that they are unlucky with timing when they restart and have to download a snapshot again from the servers one or more times. If they take 50 seconds or more then they will likely fail to catch up more often than they succeed and will remain non-voters for some time until they happen to complete the restore just before the leader truncates its logs.\nIn the worst case, the follower will be left continually downloading snapshots from the leader which are always too old to use by the time they are restored. This can put additional strain on the leader transferring large snapshots repeatedly as well as reduce the fault tolerance and serving capacity of the cluster.\nSince Consul 1.5.3 raft_trailing_logs has been configurable. Increasing it allows the leader to retain more logs and give followers more time to restore and catch up. The tradeoff is potentially slower appends which eventually might affect write throughput and latency negatively so setting it arbitrarily high is not recommended. Before Consul 1.10.0 it required a rolling restart to change this configuration on the leader though and since no followers could restart without loosing health this could mean loosing cluster availability and needing to recover the cluster from a loss of quorum.\nSince Consul 1.10.0 raft_trailing_logs is now reloadable with consul reload or SIGHUP allowing operators to increase this without the leader restarting or loosing leadership allowing the cluster to be recovered gracefully.\nMonitoring these metrics can help avoid or diagnose this state.\nWhat to look for:\nconsul.raft.leader.oldestLogAge should look like a saw-tooth wave increasing linearly with time until the leader takes a snapshot and then jumping down as the oldest logs are truncated. The lowest point on that line should remain comfortably higher (i.e. 2x or more) than the time it takes to restore a snapshot.\nThere are two ways a snapshot can be restored on a follower: from disk on startup or from the leader during an installSnapshot RPC. The leader only sends an installSnapshot RPC if the follower is new and has no state, or if it's state is too old for it to catch up with the leaders logs.\nconsul.raft.fsm.lastRestoreDuration shows the time it took to restore from either source the last time it happened. Most of the time this is when the server was started. It's a gauge that will always show the last restore duration (in Consul 1.10.0 and later) however long ago that was.\nconsul.raft.rpc.installSnapshot is the timing information from the leader's perspective when it installs a new snapshot on a follower. It includes the time spent transferring the data as well as the follower restoring it. Since these events are typically infrequent, you may need to graph the last value observed, for example using max_over_time with a large range in Prometheus. While the restore part will also be reflected in lastRestoreDuration, it can be useful to observe this too since the logs need to be able to cover this entire operation including the snapshot delivery to ensure followers can always catch up safely.\nGraphing consul.raft.leader.oldestLogAge on the same axes as the other two metrics here can help see at a glance if restore times are creeping dangerously close to the limit of what the leader is retaining at the current write rate.\nNote that if servers don't restart often, then the snapshot could have grown significantly since the last restore happened so last restore times might not reflect what would happen if an agent restarts now.\nLicense Expiration Enterprise\nconsul.system.licenseExpiration\tNumber of hours until the Consul Enterprise license will expire.\thours\tgauge\t\nWhy they're important:\nThis measurement indicates how many hours are left before the Consul Enterprise license expires. When the license expires some Consul Enterprise features will cease to work. An example of this is that after expiration, it is no longer possible to create or modify resources in non-default namespaces or to manage namespace definitions themselves even though reads of namespaced resources will still work.\nWhat to look for:\nThis metric should be monitored to ensure that the license doesn't expire to prevent degradation of functionality.\nThis is a full list of metrics emitted by Consul.\nMetricDescriptionUnitType\nconsul.acl.blocked.{check,service}.deregistration\tIncrements whenever a deregistration fails for an entity (check or service) is blocked by an ACL.\trequests\tcounter\t\nconsul.acl.blocked.{check,node,service}.registration\tIncrements whenever a registration fails for an entity (check, node or service) is blocked by an ACL.\trequests\tcounter\t\nconsul.api.http\tMigrated from consul.http.. this samples how long it takes to service the given HTTP request for the given verb and path. Includes labels for path and method. path does not include details like service or key names, for these an underscore will be present as a placeholder (eg. path=v1.kv._)\tms\ttimer\t\nconsul.client.rpc\tIncrements whenever a Consul agent in client mode makes an RPC request to a Consul server. This gives a measure of how much a given agent is loading the Consul servers. Currently, this is only generated by agents in client mode, not Consul servers.\trequests\tcounter\t\nconsul.client.rpc.exceeded\tIncrements whenever a Consul agent in client mode makes an RPC request to a Consul server gets rate limited by that agent's limits configuration. This gives an indication that there's an abusive application making too many requests on the agent, or that the rate limit needs to be increased. Currently, this only applies to agents in client mode, not Consul servers.\trejected requests\tcounter\t\nconsul.client.rpc.failed\tIncrements whenever a Consul agent in client mode makes an RPC request to a Consul server and fails.\trequests\tcounter\t\nconsul.client.api.catalog_register.\tIncrements whenever a Consul agent receives a catalog register request.\trequests\tcounter\t\nconsul.client.api.success.catalog_register.\tIncrements whenever a Consul agent successfully responds to a catalog register request.\trequests\tcounter\t\nconsul.client.rpc.error.catalog_register.\tIncrements whenever a Consul agent receives an RPC error for a catalog register request.\terrors\tcounter\t\nconsul.client.api.catalog_deregister.\tIncrements whenever a Consul agent receives a catalog deregister request.\trequests\tcounter\t\nconsul.client.api.success.catalog_deregister.\tIncrements whenever a Consul agent successfully responds to a catalog deregister request.\trequests\tcounter\t\nconsul.client.rpc.error.catalog_deregister.\tIncrements whenever a Consul agent receives an RPC error for a catalog deregister request.\terrors\tcounter\t\nconsul.client.api.catalog_datacenters.\tIncrements whenever a Consul agent receives a request to list datacenters in the catalog.\trequests\tcounter\t\nconsul.client.api.success.catalog_datacenters.\tIncrements whenever a Consul agent successfully responds to a request to list datacenters.\trequests\tcounter\t\nconsul.client.rpc.error.catalog_datacenters.\tIncrements whenever a Consul agent receives an RPC error for a request to list datacenters.\terrors\tcounter\t\nconsul.client.api.catalog_nodes.\tIncrements whenever a Consul agent receives a request to list nodes from the catalog.\trequests\tcounter\t\nconsul.client.api.success.catalog_nodes.\tIncrements whenever a Consul agent successfully responds to a request to list nodes.\trequests\tcounter\t\nconsul.client.rpc.error.catalog_nodes.\tIncrements whenever a Consul agent receives an RPC error for a request to list nodes.\terrors\tcounter\t\nconsul.client.api.catalog_services.\tIncrements whenever a Consul agent receives a request to list services from the catalog.\trequests\tcounter\t\nconsul.client.api.success.catalog_services.\tIncrements whenever a Consul agent successfully responds to a request to list services.\trequests\tcounter\t\nconsul.client.rpc.error.catalog_services.\tIncrements whenever a Consul agent receives an RPC error for a request to list services.\terrors\tcounter\t\nconsul.client.api.catalog_service_nodes.\tIncrements whenever a Consul agent receives a request to list nodes offering a service.\trequests\tcounter\t\nconsul.client.api.success.catalog_service_nodes.\tIncrements whenever a Consul agent successfully responds to a request to list nodes offering a service.\trequests\tcounter\t\nconsul.client.api.error.catalog_service_nodes.\tIncrements whenever a Consul agent receives an RPC error for request to list nodes offering a service.\trequests\tcounter\t\nconsul.client.rpc.error.catalog_service_nodes.\tIncrements whenever a Consul agent receives an RPC error for a request to list nodes offering a service. \terrors\tcounter\t\nconsul.client.api.catalog_node_services.\tIncrements whenever a Consul agent receives a request to list services registered in a node. \trequests\tcounter\t\nconsul.client.api.success.catalog_node_services.\tIncrements whenever a Consul agent successfully responds to a request to list services in a node. \trequests\tcounter\t\nconsul.client.rpc.error.catalog_node_services.\tIncrements whenever a Consul agent receives an RPC error for a request to list services in a node. \terrors\tcounter\t\nconsul.client.api.catalog_node_service_list\tIncrements whenever a Consul agent receives a request to list a node's registered services.\trequests\tcounter\t\nconsul.client.rpc.error.catalog_node_service_list\tIncrements whenever a Consul agent receives an RPC error for request to list a node's registered services.\terrors\tcounter\t\nconsul.client.api.success.catalog_node_service_list\tIncrements whenever a Consul agent successfully responds to a request to list a node's registered services.\trequests\tcounter\t\nconsul.client.api.catalog_gateway_services.\tIncrements whenever a Consul agent receives a request to list services associated with a gateway.\trequests\tcounter\t\nconsul.client.api.success.catalog_gateway_services.\tIncrements whenever a Consul agent successfully responds to a request to list services associated with a gateway.\trequests\tcounter\t\nconsul.client.rpc.error.catalog_gateway_services.\tIncrements whenever a Consul agent receives an RPC error for a request to list services associated with a gateway.\terrors\tcounter\t\nconsul.runtime.num_goroutines\tTracks the number of running goroutines and is a general load pressure indicator. This may burst from time to time but should return to a steady state value.\tnumber of goroutines\tgauge\t\nconsul.runtime.alloc_bytes\tMeasures the number of bytes allocated by the Consul process. This may burst from time to time but should return to a steady state value.\tbytes\tgauge\t\nconsul.runtime.heap_objects\tMeasures the number of objects allocated on the heap and is a general memory pressure indicator. This may burst from time to time but should return to a steady state value.\tnumber of objects\tgauge\t\nconsul.state.nodes\tMeasures the current number of nodes registered with Consul. It is only emitted by Consul servers. Added in v1.9.0.\tnumber of objects\tgauge\t\nconsul.state.services\tMeasures the current number of unique services registered with Consul, based on service name. It is only emitted by Consul servers. Added in v1.9.0.\tnumber of objects\tgauge\t\nconsul.state.service_instances\tMeasures the current number of unique service instances registered with Consul. It is only emitted by Consul servers. Added in v1.9.0.\tnumber of objects\tgauge\t\nconsul.state.kv_entries\tMeasures the current number of unique KV entries written in Consul. It is only emitted by Consul servers. Added in v1.10.3.\tnumber of objects\tgauge\t\nconsul.state.connect_instances\tMeasures the current number of unique connect service instances registered with Consul labeled by Kind (e.g. connect-proxy, connect-native, etc). Added in v1.10.4\tnumber of objects\tgauge\t\nconsul.state.config_entries\tMeasures the current number of configuration entries registered with Consul labeled by Kind (e.g. service-defaults, proxy-defaults, etc). See Configuration Entries for more information. Added in v1.10.4\tnumber of objects\tgauge\t\nconsul.members.clients\tMeasures the current number of client agents registered with Consul. It is only emitted by Consul servers. Added in v1.9.6.\tnumber of clients\tgauge\t\nconsul.members.servers\tMeasures the current number of server agents registered with Consul. It is only emitted by Consul servers. Added in v1.9.6.\tnumber of servers\tgauge\t\nconsul.dns.stale_queries\tIncrements when an agent serves a query within the allowed stale threshold.\tqueries\tcounter\t\nconsul.dns.ptr_query.\tMeasures the time spent handling a reverse DNS query for the given node.\tms\ttimer\t\nconsul.dns.domain_query.\tMeasures the time spent handling a domain query for the given node.\tms\ttimer\t\nconsul.http...\tDEPRECATED IN 1.9: Tracks how long it takes to service the given HTTP request for the given verb and path. Paths do not include details like service or key names, for these an underscore will be present as a placeholder (eg. consul.http.GET.v1.kv._)\tms\ttimer\t\nconsul.system.licenseExpiration\tEnterprise This measures the number of hours remaining on the agents license.\thours\tgauge\t\nconsul.version\tMeasures the count of running agents.\tagents\tgauge\t\nThese metrics are used to monitor the health of the Consul servers.\nMetricDescriptionUnitType\nconsul.acl.apply\tMeasures the time it takes to complete an update to the ACL store.\tms\ttimer\t\nconsul.acl.resolveTokenLegacy\tMeasures the time it takes to resolve an ACL token using the legacy ACL system.\tms\ttimer\t\nconsul.acl.ResolveToken\tMeasures the time it takes to resolve an ACL token.\tms\ttimer\t\nconsul.acl.ResolveTokenToIdentity\tMeasures the time it takes to resolve an ACL token to an Identity.\tms\ttimer\t\nconsul.acl.token.cache_hit\tIncrements if Consul is able to resolve a token's identity, or a legacy token, from the cache.\tcache read op\tcounter\t\nconsul.acl.token.cache_miss\tIncrements if Consul cannot resolve a token's identity, or a legacy token, from the cache.\tcache read op\tcounter\t\nconsul.cache.bypass\tCounts how many times a request bypassed the cache because no cache-key was provided.\tcounter\tcounter\t\nconsul.cache.fetch_success\tCounts the number of successful fetches by the cache.\tcounter\tcounter\t\nconsul.cache.fetch_error\tCounts the number of failed fetches by the cache.\tcounter\tcounter\t\nconsul.cache.evict_expired\tCounts the number of expired entries that are evicted.\tcounter\tcounter\t\nconsul.raft.applied_index\tRepresents the raft applied index.\tindex\tgauge\t\nconsul.raft.apply\tCounts the number of Raft transactions occurring over the interval, which is a general indicator of the write load on the Consul servers.\traft transactions / interval\tcounter\t\nconsul.raft.barrier\tCounts the number of times the agent has started the barrier i.e the number of times it has issued a blocking call, to ensure that the agent has all the pending operations that were queued, to be applied to the agent's FSM.\tblocks / interval\tcounter\t\nconsul.raft.commitNumLogs\tMeasures the count of logs processed for application to the FSM in a single batch.\tlogs\tgauge\t\nconsul.raft.commitTime\tMeasures the time it takes to commit a new entry to the Raft log on the leader.\tms\ttimer\t\nconsul.raft.fsm.lastRestoreDuration\tMeasures the time taken to restore the FSM from a snapshot on an agent restart or from the leader calling installSnapshot. This is a gauge that holds it's value since most servers only restore during restarts which are typically infrequent.\tms\tgauge\t\nconsul.raft.fsm.snapshot\tMeasures the time taken by the FSM to record the current state for the snapshot.\tms\ttimer\t\nconsul.raft.fsm.apply\tThe number of logs committed since the last interval.\tcommit logs / interval\tcounter\t\nconsul.raft.fsm.enqueue\tMeasures the amount of time to enqueue a batch of logs for the FSM to apply.\tms\ttimer\t\nconsul.raft.fsm.restore\tMeasures the time taken by the FSM to restore its state from a snapshot.\tms\ttimer\t\nconsul.raft.last_index\tRepresents the raft applied index.\tindex\tgauge\t\nconsul.raft.leader.dispatchLog\tMeasures the time it takes for the leader to write log entries to disk.\tms\ttimer\t\nconsul.raft.leader.dispatchNumLogs\tMeasures the number of logs committed to disk in a batch.\tlogs\tgauge\t\nconsul.raft.leader.lastContact\tMeasures the time since the leader was last able to contact the follower nodes when checking its leader lease. It can be used as a measure for how stable the Raft timing is and how close the leader is to timing out its lease.The lease timeout is 500 ms times the raft_multiplier configuration, so this telemetry value should not be getting close to that configured value, otherwise the Raft timing is marginal and might need to be tuned, or more powerful servers might be needed. See the Server Performance guide for more details.\tms\ttimer\t\nconsul.raft.leader.oldestLogAge\tThe number of milliseconds since the oldest log in the leader's log store was written. This can be important for replication health where write rate is high and the snapshot is large as followers may be unable to recover from a restart if restoring takes longer than the minimum value for the current leader. Compare this with consul.raft.fsm.lastRestoreDuration and consul.raft.rpc.installSnapshot to monitor. In normal usage this gauge value will grow linearly over time until a snapshot completes on the leader and the log is truncated. Note: this metric won't be emitted until the leader writes a snapshot. After an upgrade to Consul 1.10.0 it won't be emitted until the oldest log was written after the upgrade.\tms\tgauge\t\nconsul.raft.replication.heartbeat\tMeasures the time taken to invoke appendEntries on a peer, so that it doesnt timeout on a periodic basis.\tms\ttimer\t\nconsul.raft.replication.appendEntries\tMeasures the time it takes to replicate log entries to followers. This is a general indicator of the load pressure on the Consul servers, as well as the performance of the communication between the servers.\tms\ttimer\t\nconsul.raft.replication.appendEntries.rpc\tMeasures the time taken by the append entries RFC, to replicate the log entries of a leader agent onto its follower agent(s)\tms\ttimer\t\nconsul.raft.replication.appendEntries.logs\tMeasures the number of logs replicated to an agent, to bring it up to speed with the leader's logs.\tlogs appended/ interval\tcounter\t\nconsul.raft.restore\tCounts the number of times the restore operation has been performed by the agent. Here, restore refers to the action of raft consuming an external snapshot to restore its state.\toperation invoked / interval\tcounter\t\nconsul.raft.restoreUserSnapshot\tMeasures the time taken by the agent to restore the FSM state from a user's snapshot\tms\ttimer\t\nconsul.raft.rpc.appendEntries\tMeasures the time taken to process an append entries RPC call from an agent.\tms\ttimer\t\nconsul.raft.rpc.appendEntries.storeLogs\tMeasures the time taken to add any outstanding logs for an agent, since the last appendEntries was invoked\tms\ttimer\t\nconsul.raft.rpc.appendEntries.processLogs\tMeasures the time taken to process the outstanding log entries of an agent.\tms\ttimer\t\nconsul.raft.rpc.installSnapshot\tMeasures the time taken to process the installSnapshot RPC call. This metric should only be seen on agents which are currently in the follower state.\tms\ttimer\t\nconsul.raft.rpc.processHeartBeat\tMeasures the time taken to process a heartbeat request.\tms\ttimer\t\nconsul.raft.rpc.requestVote\tMeasures the time taken to process the request vote RPC call.\tms\ttimer\t\nconsul.raft.snapshot.create\tMeasures the time taken to initialize the snapshot process.\tms\ttimer\t\nconsul.raft.snapshot.persist\tMeasures the time taken to dump the current snapshot taken by the Consul agent to the disk.\tms\ttimer\t\nconsul.raft.snapshot.takeSnapshot\tMeasures the total time involved in taking the current snapshot (creating one and persisting it) by the Consul agent.\tms\ttimer\t\nconsul.serf.snapshot.appendLine\tMeasures the time taken by the Consul agent to append an entry into the existing log.\tms\ttimer\t\nconsul.serf.snapshot.compact\tMeasures the time taken by the Consul agent to compact a log. This operation occurs only when the snapshot becomes large enough to justify the compaction .\tms\ttimer\t\nconsul.raft.state.candidate\tIncrements whenever a Consul server starts an election. If this increments without a leadership change occurring it could indicate that a single server is overloaded or is experiencing network connectivity issues.\telection attempts / interval\tcounter\t\nconsul.raft.state.leader\tIncrements whenever a Consul server becomes a leader. If there are frequent leadership changes this may be indication that the servers are overloaded and aren't meeting the soft real-time requirements for Raft, or that there are networking problems between the servers.\tleadership transitions / interval\tcounter\t\nconsul.raft.state.follower\tCounts the number of times an agent has entered the follower mode. This happens when a new agent joins the cluster or after the end of a leader election.\tfollower state entered / interval\tcounter\t\nconsul.raft.transition.heartbeat_timeout\tThe number of times an agent has transitioned to the Candidate state, after receive no heartbeat messages from the last known leader.\ttimeouts / interval\tcounter\t\nconsul.raft.verify_leader\tCounts the number of times an agent checks whether it is still the leader or not\tchecks / interval\tCounter\t\nconsul.rpc.accept_conn\tIncrements when a server accepts an RPC connection.\tconnections\tcounter\t\nconsul.catalog.register\tMeasures the time it takes to complete a catalog register operation.\tms\ttimer\t\nconsul.catalog.deregister\tMeasures the time it takes to complete a catalog deregister operation.\tms\ttimer\t\nconsul.fsm.register\tMeasures the time it takes to apply a catalog register operation to the FSM.\tms\ttimer\t\nconsul.fsm.deregister\tMeasures the time it takes to apply a catalog deregister operation to the FSM.\tms\ttimer\t\nconsul.fsm.acl.\tMeasures the time it takes to apply the given ACL operation to the FSM.\tms\ttimer\t\nconsul.fsm.session.\tMeasures the time it takes to apply the given session operation to the FSM.\tms\ttimer\t\nconsul.fsm.kvs.\tMeasures the time it takes to apply the given KV operation to the FSM.\tms\ttimer\t\nconsul.fsm.tombstone.\tMeasures the time it takes to apply the given tombstone operation to the FSM.\tms\ttimer\t\nconsul.fsm.coordinate.batch-update\tMeasures the time it takes to apply the given batch coordinate update to the FSM.\tms\ttimer\t\nconsul.fsm.prepared-query.\tMeasures the time it takes to apply the given prepared query update operation to the FSM.\tms\ttimer\t\nconsul.fsm.txn\tMeasures the time it takes to apply the given transaction update to the FSM.\tms\ttimer\t\nconsul.fsm.autopilot\tMeasures the time it takes to apply the given autopilot update to the FSM.\tms\ttimer\t\nconsul.fsm.persist\tMeasures the time it takes to persist the FSM to a raft snapshot.\tms\ttimer\t\nconsul.fsm.intention\tMeasures the time it takes to apply an intention operation to the state store.\tms\ttimer\t\nconsul.fsm.ca\tMeasures the time it takes to apply CA configuration operations to the FSM.\tms\ttimer\t\nconsul.fsm.ca.leaf\tMeasures the time it takes to apply an operation while signing a leaf certificate.\tms\ttimer\t\nconsul.fsm.acl.token\tMeasures the time it takes to apply an ACL token operation to the FSM.\tms\ttimer\t\nconsul.fsm.acl.policy\tMeasures the time it takes to apply an ACL policy operation to the FSM.\tms\ttimer\t\nconsul.fsm.acl.bindingrule\tMeasures the time it takes to apply an ACL binding rule operation to the FSM.\tms\ttimer\t\nconsul.fsm.acl.authmethod\tMeasures the time it takes to apply an ACL authmethod operation to the FSM.\tms\ttimer\t\nconsul.fsm.system_metadata\tMeasures the time it takes to apply a system metadata operation to the FSM.\tms\ttimer\t\nconsul.kvs.apply\tMeasures the time it takes to complete an update to the KV store.\tms\ttimer\t\nconsul.leader.barrier\tMeasures the time spent waiting for the raft barrier upon gaining leadership.\tms\ttimer\t\nconsul.leader.reconcile\tMeasures the time spent updating the raft store from the serf member information.\tms\ttimer\t\nconsul.leader.reconcileMember\tMeasures the time spent updating the raft store for a single serf member's information.\tms\ttimer\t\nconsul.leader.reapTombstones\tMeasures the time spent clearing tombstones.\tms\ttimer\t\nconsul.leader.replication.acl-policies.status\tThis will only be emitted by the leader in a secondary datacenter. The value will be a 1 if the last round of ACL policy replication was successful or 0 if there was an error.\thealthy\tgauge\t\nconsul.leader.replication.acl-policies.index\tThis will only be emitted by the leader in a secondary datacenter. Increments to the index of ACL policies in the primary datacenter that have been successfully replicated.\tindex\tgauge\t\nconsul.leader.replication.acl-roles.status\tThis will only be emitted by the leader in a secondary datacenter. The value will be a 1 if the last round of ACL role replication was successful or 0 if there was an error.\thealthy\tgauge\t\nconsul.leader.replication.acl-roles.index\tThis will only be emitted by the leader in a secondary datacenter. Increments to the index of ACL roles in the primary datacenter that have been successfully replicated.\tindex\tgauge\t\nconsul.leader.replication.acl-tokens.status\tThis will only be emitted by the leader in a secondary datacenter. The value will be a 1 if the last round of ACL token replication was successful or 0 if there was an error.\thealthy\tgauge\t\nconsul.leader.replication.acl-tokens.index\tThis will only be emitted by the leader in a secondary datacenter. Increments to the index of ACL tokens in the primary datacenter that have been successfully replicated.\tindex\tgauge\t\nconsul.leader.replication.config-entries.status\tThis will only be emitted by the leader in a secondary datacenter. The value will be a 1 if the last round of config entry replication was successful or 0 if there was an error.\thealthy\tgauge\t\nconsul.leader.replication.config-entries.index\tThis will only be emitted by the leader in a secondary datacenter. Increments to the index of config entries in the primary datacenter that have been successfully replicated.\tindex\tgauge\t\nconsul.leader.replication.federation-state.status\tThis will only be emitted by the leader in a secondary datacenter. The value will be a 1 if the last round of federation state replication was successful or 0 if there was an error.\thealthy\tgauge\t\nconsul.leader.replication.federation-state.index\tThis will only be emitted by the leader in a secondary datacenter. Increments to the index of federation states in the primary datacenter that have been successfully replicated.\tindex\tgauge\t\nconsul.leader.replication.namespaces.status\tEnterprise This will only be emitted by the leader in a secondary datacenter. The value will be a 1 if the last round of namespace replication was successful or 0 if there was an error.\thealthy\tgauge\t\nconsul.leader.replication.namespaces.index\tEnterprise This will only be emitted by the leader in a secondary datacenter. Increments to the index of namespaces in the primary datacenter that have been successfully replicated.\tindex\tgauge\t\nconsul.prepared-query.apply\tMeasures the time it takes to apply a prepared query update.\tms\ttimer\t\nconsul.prepared-query.explain\tMeasures the time it takes to process a prepared query explain request.\tms\ttimer\t\nconsul.prepared-query.execute\tMeasures the time it takes to process a prepared query execute request.\tms\ttimer\t\nconsul.prepared-query.execute_remote\tMeasures the time it takes to process a prepared query execute request that was forwarded to another datacenter.\tms\ttimer\t\nconsul.rpc.raft_handoff\tIncrements when a server accepts a Raft-related RPC connection.\tconnections\tcounter\t\nconsul.rpc.request_error\tIncrements when a server returns an error from an RPC request.\terrors\tcounter\t\nconsul.rpc.request\tIncrements when a server receives a Consul-related RPC request.\trequests\tcounter\t\nconsul.rpc.query\tIncrements when a server receives a read RPC request, indicating the rate of new read queries. See consul.rpc.queries_blocking for the current number of in-flight blocking RPC calls. This metric changed in 1.7.0 to only increment on the the start of a query. The rate of queries will appear lower, but is more accurate.\tqueries\tcounter\t\nconsul.rpc.queries_blocking\tThe current number of in-flight blocking queries the server is handling.\tqueries\tgauge\t\nconsul.rpc.cross-dc\tIncrements when a server sends a (potentially blocking) cross datacenter RPC query.\tqueries\tcounter\t\nconsul.rpc.consistentRead\tMeasures the time spent confirming that a consistent read can be performed.\tms\ttimer\t\nconsul.session.apply\tMeasures the time spent applying a session update.\tms\ttimer\t\nconsul.session.renew\tMeasures the time spent renewing a session.\tms\ttimer\t\nconsul.session_ttl.invalidate\tMeasures the time spent invalidating an expired session.\tms\ttimer\t\nconsul.txn.apply\tMeasures the time spent applying a transaction operation.\tms\ttimer\t\nconsul.txn.read\tMeasures the time spent returning a read transaction.\tms\ttimer\t\nconsul.grpc.client.request.count\tCounts the number of gRPC requests made by the client agent to a Consul server.\trequests\tcounter\t\nconsul.grpc.client.connection.count\tCounts the number of new gRPC connections opened by the client agent to a Consul server.\tconnections\tcounter\t\nconsul.grpc.client.connections\tMeasures the number of active gRPC connections open from the client agent to any Consul servers.\tconnections\tgauge\t\nconsul.grpc.server.request.count\tCounts the number of gRPC requests received by the server.\trequests\tcounter\t\nconsul.grpc.server.connection.count\tCounts the number of new gRPC connections received by the server.\tconnections\tcounter\t\nconsul.grpc.server.connections\tMeasures the number of active gRPC connections open on the server.\tconnections\tgauge\t\nconsul.grpc.server.stream.count\tCounts the number of new gRPC streams received by the server.\tstreams\tcounter\t\nconsul.grpc.server.streams\tMeasures the number of active gRPC streams handled by the server.\tstreams\tgauge\t\nconsul.xds.server.streams\tMeasures the number of active xDS streams handled by the server split by protocol version.\tstreams\tgauge\t\nThese metrics give insight into the health of the cluster as a whole.\nMetricDescriptionUnitType\nconsul.memberlist.degraded.probe\tCounts the number of times the agent has performed failure detection on another agent at a slower probe rate. The agent uses its own health metric as an indicator to perform this action. (If its health score is low, means that the node is healthy, and vice versa.)\tprobes / interval\tcounter\t\nconsul.memberlist.degraded.timeout\tCounts the number of times an agent was marked as a dead node, whilst not getting enough confirmations from a randomly selected list of agent nodes in an agent's membership.\toccurrence / interval\tcounter\t\nconsul.memberlist.msg.dead\tCounts the number of times an agent has marked another agent to be a dead node.\tmessages / interval\tcounter\t\nconsul.memberlist.health.score\tDescribes a node's perception of its own health based on how well it is meeting the soft real-time requirements of the protocol. This metric ranges from 0 to 8, where 0 indicates \"totally healthy\". This health score is used to scale the time between outgoing probes, and higher scores translate into longer probing intervals. For more details see section IV of the Lifeguard paper: https://arxiv.org/pdf/1707.00788.pdf\tscore\tgauge\t\nconsul.memberlist.msg.suspect\tIncrements when an agent suspects another as failed when executing random probes as part of the gossip protocol. These can be an indicator of overloaded agents, network problems, or configuration errors where agents can not connect to each other on the required ports.\tsuspect messages received / interval\tcounter\t\nconsul.memberlist.tcp.accept\tCounts the number of times an agent has accepted an incoming TCP stream connection.\tconnections accepted / interval\tcounter\t\nconsul.memberlist.udp.sent/received\tMeasures the total number of bytes sent/received by an agent through the UDP protocol.\tbytes sent or bytes received / interval\tcounter\t\nconsul.memberlist.tcp.connect\tCounts the number of times an agent has initiated a push/pull sync with an other agent.\tpush/pull initiated / interval\tcounter\t\nconsul.memberlist.tcp.sent\tMeasures the total number of bytes sent by an agent through the TCP protocol\tbytes sent / interval\tcounter\t\nconsul.memberlist.gossip\tMeasures the time taken for gossip messages to be broadcasted to a set of randomly selected nodes.\tms\ttimer\t\nconsul.memberlist.msg_alive\tCounts the number of alive messages, that the agent has processed so far, based on the message information given by the network layer.\tmessages / Interval\tcounter\t\nconsul.memberlist.msg_dead\tThe number of dead messages that the agent has processed so far, based on the message information given by the network layer.\tmessages / Interval\tcounter\t\nconsul.memberlist.msg_suspect\tThe number of suspect messages that the agent has processed so far, based on the message information given by the network layer.\tmessages / Interval\tcounter\t\nconsul.memberlist.probeNode\tMeasures the time taken to perform a single round of failure detection on a select agent.\tnodes / Interval\tcounter\t\nconsul.memberlist.pushPullNode\tMeasures the number of agents that have exchanged state with this agent.\tnodes / Interval\tcounter\t\nconsul.serf.member.failed\tIncrements when an agent is marked dead. This can be an indicator of overloaded agents, network problems, or configuration errors where agents cannot connect to each other on the required ports.\tfailures / interval\tcounter\t\nconsul.serf.member.flap\tAvailable in Consul 0.7 and later, this increments when an agent is marked dead and then recovers within a short time period. This can be an indicator of overloaded agents, network problems, or configuration errors where agents cannot connect to each other on the required ports.\tflaps / interval\tcounter\t\nconsul.serf.member.join\tIncrements when an agent joins the cluster. If an agent flapped or failed this counter also increments when it re-joins.\tjoins / interval\tcounter\t\nconsul.serf.member.left\tIncrements when an agent leaves the cluster.\tleaves / interval\tcounter\t\nconsul.serf.events\tIncrements when an agent processes an event. Consul uses events internally so there may be additional events showing in telemetry. There are also a per-event counters emitted as consul.serf.events..\tevents / interval\tcounter\t\nconsul.serf.msgs.sent\tThis metric is sample of the number of bytes of messages broadcast to the cluster. In a given time interval, the sum of this metric is the total number of bytes sent and the count is the number of messages sent.\tmessage bytes / interval\tcounter\t\nconsul.autopilot.failure_tolerance\tTracks the number of voting servers that the cluster can lose while continuing to function.\tservers\tgauge\t\nconsul.autopilot.healthy\tTracks the overall health of the local server cluster. If all servers are considered healthy by Autopilot, this will be set to 1. If any are unhealthy, this will be 0. All non-leader servers will report NaN.\tboolean\tgauge\t\nconsul.session_ttl.active\tTracks the active number of sessions being tracked.\tsessions\tgauge\t\nconsul.catalog.service.query.\tIncrements for each catalog query for the given service.\tqueries\tcounter\t\nconsul.catalog.service.query-tag..\tIncrements for each catalog query for the given service with the given tag.\tqueries\tcounter\t\nconsul.catalog.service.query-tags..\tIncrements for each catalog query for the given service with the given tags.\tqueries\tcounter\t\nconsul.catalog.service.not-found.\tIncrements for each catalog query where the given service could not be found.\tqueries\tcounter\t\nconsul.catalog.connect.query.\tIncrements for each connect-based catalog query for the given service.\tqueries\tcounter\t\nconsul.catalog.connect.query-tag..\tIncrements for each connect-based catalog query for the given service with the given tag.\tqueries\tcounter\t\nconsul.catalog.connect.query-tags..\tIncrements for each connect-based catalog query for the given service with the given tags.\tqueries\tcounter\t\nconsul.catalog.connect.not-found.\tIncrements for each connect-based catalog query where the given service could not be found.\tqueries\tcounter\t\nConsul Connect's built-in proxy is by default configured to log metrics to the same sink as the agent that starts it.\nWhen running in this mode it emits some basic metrics. These will be expanded upon in the future.\nAll metrics are prefixed with consul.proxy.<proxied-service-id> to distinguish between multiple proxies on a given host. The table below use web as an example service name for brevity.\nLabels\nMost labels have a dst label and some have a src label. When using metrics sinks and timeseries stores that support labels or tags, these allow aggregating the connections by service name.\nAssuming all services are using a managed built-in proxy, you can get a complete overview of both number of open connections and bytes sent and received between all services by aggregating over these metrics.\nFor example aggregating over all upstream (i.e. outbound) connections which have both src and dst labels, you can get a sum of all the bandwidth in and out of a given service or the total number of connections between two services.\nMetrics Reference\nThe standard go runtime metrics are exported by go-metrics as with Consul agent. The table below describes the additional metrics exported by the proxy.\nMetricDescriptionUnitType\nconsul.proxy.web.runtime.*\tThe same go runtime metrics as documented for the agent above.\tmixed\tmixed\t\nconsul.proxy.web.inbound.conns\tShows the current number of connections open from inbound requests to the proxy. Where supported a dst label is added indicating the service name the proxy represents.\tconnections\tgauge\t\nconsul.proxy.web.inbound.rx_bytes\tIncrements by the number of bytes received from an inbound client connection. Where supported a dst label is added indicating the service name the proxy represents.\tbytes\tcounter\t\nconsul.proxy.web.inbound.tx_bytes\tIncrements by the number of bytes transferred to an inbound client connection. Where supported a dst label is added indicating the service name the proxy represents.\tbytes\tcounter\t\nconsul.proxy.web.upstream.conns\tShows the current number of connections open from a proxy instance to an upstream. Where supported a src label is added indicating the service name the proxy represents, and a dst label is added indicating the service name the upstream is connecting to.\tconnections\tgauge\t\nconsul.proxy.web.inbound.rx_bytes\tIncrements by the number of bytes received from an upstream connection. Where supported a src label is added indicating the service name the proxy represents, and a dst label is added indicating the service name the upstream is connecting to.\tbytes\tcounter\t\nconsul.proxy.web.inbound.tx_bytes\tIncrements by the number of bytes transferred to an upstream connection. Where supported a src label is added indicating the service name the proxy represents, and a dst label is added indicating the service name the upstream is connecting to.\tbytes\tcounter"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.12.x/agent/sentinel",
  "text": "Sentinel ACL Policies (Enterprise) | Consul\nSentinel for KV ACL Policy Enforcement\nConsul 1.0 adds integration with Sentinel for policy enforcement. Sentinel policies help extend the ACL system in Consul beyond the static \"read\", \"write\", and \"deny\" policies to support full conditional logic and integration with external systems.\nSentinel policies are applied during writes to the KV Store.\nAn optional sentinel field specifying code and enforcement level can be added to ACL policy definitions for Consul KV. The following policy ensures that the value written during a KV update must end with \"dc1\".\nEnsure values written during KV updates end in 'dc1'\nkey \"datacenter_name\" { policy = \"write\" sentinel { code = <<EOF import \"strings\" main = rule { strings.has_suffix(value, \"dc1\") } EOF enforcementlevel = \"soft-mandatory\" } } \nIf the enforcementlevel property is not set, it defaults to \"hard-mandatory\".\nConsul imports all the standard imports from Sentinel except http. All functions in these imports are available to be used in policies.\nConsul passes some context as variables into Sentinel, which are available to use inside any policies you write.\nVariables injected during KV store writes\nVariable NameTypeDescription\nkey\tstring\tKey being written\t\nvalue\tstring\tValue being written\t\nflags\tuint64\tFlags\t\nThe following are two examples of ACL policies with Sentinel rules.\nRequired Key Suffix\nAny values stored under the key 'dc1' end with 'dev'\nkey \"dc1\" { policy = \"write\" sentinel { code = <<EOF import \"strings\" main = rule { strings.has_suffix(value, \"dev\") } EOF } } \nRestricted Update Time\nThe key 'haproxy_version' can only be updated during business hours\nkey \"haproxy_version\" { policy = \"write\" sentinel { code = <<EOF import \"time\" main = rule { time.now.hour > 8 and time.now.hour < 17 } EOF } }"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.10.x/enterprise/license/overview",
  "text": "Consul Enterprise License | Consul\nAll Consul Enterprise agents must be licensed when they are started. Where that license comes from will depend on which binary is in use, whether the agent is a server, client or snapshot agent and whether ACLs have been enabled for the cluster.\nConsul Enterprise 1.10.0 removed temporary licensing. In previous versions Consul Enterprise agents could start without a license and then have a license applied to them later on via the CLI or API. That functionality has been removed and replaced with the ability to load licenses from the agent's configuration or environment. Also prior to 1.10.0 server agents would automatically propagate the license between themselves. This no longer occurs and the license must be present on each server when they are started.\nBinaries with Built In Licenses\nIf you are downloading Consul from Amazon S3, then the license is included in the binary and you do not need to take further action. This is the most common use case.\nIn the S3 bucket you will find three Enterprise zip packages. The packages with +pro and +prem in the name, are the binaries that include the license. The package with +ent in the name does not include the license.\nWhen using these binaries no further action is necessary to configure the license.\nBinaries Without Built In Licenses\nFor Consul Enterprise 1.10.0 or greater, binaries that do not include built in licenses a license must be available at the time the agent starts. For server agents this means that they must either have the license_path configuration set or have a license configured in the servers environment with the CONSUL_LICENSE or CONSUL_LICENSE_PATH environment variables. Both the configuration item and the CONSUL_LICENSE_PATH environment variable point to a file containing the license whereas the CONSUL_LICENSE environment variable should contain the license as the value. If multiple of these are set the order of precedence is:\nBoth client agents and the snapshot agent may also be licensed in the very same manner. However to prevent the need to configure the license on many client agents and snapshot agents those agents have the capability to retrieve the license automatically under specific circumstances.\nClient Agent License Retrieval\nWhen a client agent starts without a license in its configuration or environment, it will try to retrieve the license from the servers via RPCs. That RPC always requires a valid non-anonymous ACL token to authorize the request but the token doesn't need any particular permissions. As the license is required before the client actually joins the cluster, where to make those RPC requests to is inferred from the start_join or retry_join configurations. If those are both unset or no agent token is set then the client agent will immediately shut itself down.\nIf all preliminary checks pass the client agent will attempt to reach out to any server on its RPC port to request the license. These requests will be retried for up to 5 minutes and if it is unable to retrieve a license within that time frame it will shut itself down.\nIf ACLs are disabled then the license must be provided to the client agent through one of the three methods listed below. Failure in providing the client agent with a licence will prevent the client agent from joining the cluster.\nSnapshot Agent License Retrieval\nThe snapshot agent has similar functionality to the client agent for automatically retrieving the license. However, instead of requiring a server agent to talk to, the snapshot agent can request the license from the server or client agent it would use for all other operations. It still requires an ACL token to authorize the request. Also like client agents, the snapshot agent will shut itself down after being unable to retrieve the license for 5 minutes."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.10.x/agent/sentinel",
  "text": "Sentinel in Consul | Consul\nConsul 1.0 adds integration with Sentinel for policy enforcement. Sentinel policies help extend the ACL system in Consul beyond the static \"read\", \"write\", and \"deny\" policies to support full conditional logic and integration with external systems.\nSentinel policies are applied during writes to the KV Store.\nAn optional sentinel field specifying code and enforcement level can be added to ACL policy definitions for Consul KV. The following policy ensures that the value written during a KV update must end with \"dc1\".\nkey \"datacenter_name\" { policy = \"write\" sentinel { code = <<EOF import \"strings\" main = rule { strings.has_suffix(value, \"dc1\") } EOF enforcementlevel = \"soft-mandatory\" } } \nIf the enforcementlevel property is not set, it defaults to \"hard-mandatory\".\nConsul imports all the standard imports from Sentinel except http. All functions in these imports are available to be used in policies.\nConsul passes some context as variables into Sentinel, which are available to use inside any policies you write.\nVariables injected during KV store writes\nVariable NameTypeDescription\nkey\tstring\tKey being written\t\nvalue\tstring\tValue being written\t\nflags\tuint64\tFlags\t\nThe following are two examples of ACL policies with Sentinel rules.\nRequired Key Suffix\nAny values stored under the key prefix \"dc1\" must end with \"dev\"\nkey \"dc1\" { policy = \"write\" sentinel { code = <<EOF import \"strings\" main = rule { strings.has_suffix(value, \"dev\") } EOF } } \nRestricted Update Time\nThe key \"haproxy_version\" can only be updated during business hours.\nkey \"haproxy_version\" { policy = \"write\" sentinel { code = <<EOF import \"time\" main = rule { time.now.hour > 8 and time.now.hour < 17 } EOF } }"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.10.x/discovery/dns",
  "text": "Find Services - DNS Interface | Consul\nOne of the primary query interfaces for Consul is DNS. The DNS interface allows applications to make use of service discovery without any high-touch integration with Consul.\nFor example, instead of making HTTP API requests to Consul, a host can use the DNS server directly via name lookups like redis.service.us-east-1.consul. This query automatically translates to a lookup of nodes that provide the redis service, are located in the us-east-1 datacenter, and have no failing health checks. It's that simple!\nThere are a number of configuration options that are important for the DNS interface, specifically client_addr, ports.dns, recursors, domain, and dns_config. By default, Consul will listen on 127.0.0.1:8600 for DNS queries in the consul. domain, without support for further DNS recursion. Please consult the documentation on configuration options, specifically the configuration items linked above, for more details.\nThere are a few ways to use the DNS interface. One option is to use a custom DNS resolver library and point it at Consul. Another option is to set Consul as the DNS server for a node and provide a recursors configuration so that non-Consul queries can also be resolved. The last method is to forward all queries for the \"consul.\" domain to a Consul agent from the existing DNS server. Review the DNS Forwarding tutorial for examples.\nYou can experiment with Consul's DNS server on the command line using tools such as dig:\n$ dig @127.0.0.1 -p 8600 redis.service.dc1.consul. ANY \nNote: In DNS, all queries are case-insensitive. A lookup of PostgreSQL.node.dc1.consul will find all nodes named postgresql.\nTo resolve names, Consul relies on a very specific format for queries. There are fundamentally two types of queries: node lookups and service lookups. A node lookup, a simple query for the address of a named node, looks like this:\n<node>.node[.datacenter].<domain> \nFor example, if we have a foo node with default settings, we could look for foo.node.dc1.consul. The datacenter is an optional part of the FQDN: if not provided, it defaults to the datacenter of the agent. If we know foo is running in the same datacenter as our local agent, we can instead use foo.node.consul. This convention allows for terse syntax where appropriate while supporting queries of nodes in remote datacenters as necessary.\nFor a node lookup, the only records returned are A and AAAA records containing the IP address, and TXT records containing the node_meta values of the node.\n$ dig @127.0.0.1 -p 8600 foo.node.consul ANY ; <<>> DiG 9.8.3-P1 <<>> @127.0.0.1 -p 8600 foo.node.consul ANY ; (1 server found) ;; global options: +cmd ;; Got answer: ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 24355 ;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 1, ADDITIONAL: 0 ;; WARNING: recursion requested but not available ;; QUESTION SECTION: ;foo.node.consul. IN ANY ;; ANSWER SECTION: foo.node.consul. 0 IN A 10.1.10.12 foo.node.consul. 0 IN TXT \"meta_key=meta_value\" foo.node.consul. 0 IN TXT \"value only\" ;; AUTHORITY SECTION: consul. 0 IN SOA ns.consul. postmaster.consul. 1392836399 3600 600 86400 0 \nBy default the TXT records value will match the node's metadata key-value pairs according to RFC1464. Alternatively, the TXT record will only include the node's metadata value when the node's metadata key starts with rfc1035-.\nA service lookup is used to query for service providers. Service queries support two lookup methods: standard and strict RFC 2782.\nBy default, SRV weights are all set at 1, but changing weights is supported using the Weights attribute of the service definition.\nNote that DNS is limited in size per request, even when performing DNS TCP queries.\nFor services having many instances (more than 500), it might not be possible to retrieve the complete list of instances for the service.\nWhen DNS SRV response are sent, order is randomized, but weights are not taken into account. In the case of truncation different clients using weighted SRV responses will have partial and inconsistent views of instances weights so the request distribution could be skewed from the intended weights. In that case, it is recommended to use the HTTP API to retrieve the list of nodes.\nStandard Lookup\nThe format of a standard service lookup is:\n[tag.]<service>.service[.datacenter].<domain> \nThe tag is optional, and, as with node lookups, the datacenter is as well. If no tag is provided, no filtering is done on tag. If no datacenter is provided, the datacenter of this Consul agent is assumed.\nIf we want to find any redis service providers in our local datacenter, we could query redis.service.consul. If we want to find the PostgreSQL primary in a particular datacenter, we could query primary.postgresql.service.dc2.consul.\nThe DNS query system makes use of health check information to prevent routing to unhealthy nodes. When a service query is made, any services failing their health check or failing a node system check will be omitted from the results. To allow for simple load balancing, the set of nodes returned is also randomized each time. These mechanisms make it easy to use DNS along with application-level retries as the foundation for an auto-healing service oriented architecture.\nFor standard services queries, both A and SRV records are supported. SRV records provide the port that a service is registered on, enabling clients to avoid relying on well-known ports. SRV records are only served if the client specifically requests them, like so:\n$ dig @127.0.0.1 -p 8600 consul.service.consul SRV ; <<>> DiG 9.8.3-P1 <<>> @127.0.0.1 -p 8600 consul.service.consul ANY ; (1 server found) ;; global options: +cmd ;; Got answer: ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 50483 ;; flags: qr aa rd; QUERY: 1, ANSWER: 3, AUTHORITY: 1, ADDITIONAL: 1 ;; WARNING: recursion requested but not available ;; QUESTION SECTION: ;consul.service.consul. IN SRV ;; ANSWER SECTION: consul.service.consul. 0 IN SRV 1 1 8300 foobar.node.dc1.consul. ;; ADDITIONAL SECTION: foobar.node.dc1.consul. 0 IN A 10.1.10.12 \nRFC 2782 Lookup\nThe format for RFC 2782 SRV lookups is:\n_<service>._<protocol>[.service][.datacenter][.domain] \nPer RFC 2782, SRV queries should use underscores, _, as a prefix to the service and protocol values in a query to prevent DNS collisions. The protocol value can be any of the tags for a service. If the service has no tags, tcp should be used. If tcp is specified as the protocol, the query will not perform any tag filtering.\nOther than the query format and default tcp protocol/tag value, the behavior of the RFC style lookup is the same as the standard style of lookup.\nIf you registered the service rabbitmq on port 5672 and tagged it with amqp, you could make an RFC 2782 query for its SRV record as _rabbitmq._amqp.service.consul:\n$ dig @127.0.0.1 -p 8600 _rabbitmq._amqp.service.consul SRV ; <<>> DiG 9.8.3-P1 <<>> @127.0.0.1 -p 8600 _rabbitmq._amqp.service.consul ANY ; (1 server found) ;; global options: +cmd ;; Got answer: ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 52838 ;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; WARNING: recursion requested but not available ;; QUESTION SECTION: ;_rabbitmq._amqp.service.consul. IN SRV ;; ANSWER SECTION: _rabbitmq._amqp.service.consul. 0 IN SRV 1 1 5672 rabbitmq.node1.dc1.consul. ;; ADDITIONAL SECTION: rabbitmq.node1.dc1.consul. 0 IN A 10.1.11.20 \nAgain, note that the SRV record returns the port of the service as well as its IP.\nSRV response for hosts in the .addr subdomain\nIf a service registered to Consul has an explicit IP address or tagged address(es) defined on the service registration, the hostname returned in the target field of the answer section for the DNS SRV query for the service will be in the format of <hexadecimal-encoded IP>.addr.<datacenter>.consul.\nIn the example below, the rabbitmq service has been registered with an explicit IPv4 address of 192.0.2.10.\nService defined with explicit IPv4 address in agent config\nnode_name = \"node1\" services { name = \"rabbitmq\" address = \"192.0.2.10\" port = 5672 } \nWhen performing an SRV query for this service, the SRV response contains a single record with a hostname in the format of <hexadecimal-encoded IP>.addr.<datacenter>.consul.\n$ dig @127.0.0.1 -p 8600 -t srv _rabbitmq._tcp.service.consul +short 1 1 5672 c000020a.addr.dc1.consul. \nIn this example, the hex-encoded IP from the returned hostname is c000020a. Converting each hex octet to decimal reveals the IP address that was specified in the service registration.\n$ echo -n \"c000020a\" | perl -ne 'printf(\"%vd\\n\", pack(\"H*\", $_))' 192.0.2.10 \nPrepared Query Lookups\nThe format of a prepared query lookup is:\n<query or name>.query[.datacenter].<domain> \nThe datacenter is optional, and if not provided, the datacenter of this Consul agent is assumed.\nThe query or name is the ID or given name of an existing Prepared Query. These behave like standard service queries but provide a much richer set of features, such as filtering by multiple tags and automatically failing over to look for services in remote datacenters if no healthy nodes are available in the local datacenter. Consul 0.6.4 and later also added support for prepared query templates which can match names using a prefix match, allowing one template to apply to potentially many services.\nTo allow for simple load balancing, the set of nodes returned is randomized each time. Both A and SRV records are supported. SRV records provide the port that a service is registered on, enabling clients to avoid relying on well-known ports. SRV records are only served if the client specifically requests them.\nConnect-Capable Service Lookups\nTo find Connect-capable services:\n<service>.connect.<domain> \nThis will find all Connect-capable endpoints for the given service. A Connect-capable endpoint may be both a proxy for a service or a natively integrated Connect application. The DNS interface does not differentiate the two.\nMost services will use a proxy that handles service discovery automatically and therefore won't use this DNS format. This DNS format is primarily useful for Connect-native applications.\nThis endpoint currently only finds services within the same datacenter and doesn't support tags. This DNS interface will be expanded over time. If you need more complex behavior, please use the catalog API.\nIngress Service Lookups\nTo find ingress-enabled services:\n<service>.ingress.<domain> \nThis will find all ingress gateway endpoints for the given service.\nThis endpoint currently only finds services within the same datacenter and doesn't support tags. This DNS interface will be expanded over time. If you need more complex behavior, please use the catalog API.\nUDP Based DNS Queries\nWhen the DNS query is performed using UDP, Consul will truncate the results without setting the truncate bit. This is to prevent a redundant lookup over TCP that generates additional load. If the lookup is done over TCP, the results are not truncated.\nAlternative Domain\nBy default, Consul responds to DNS queries in the consul domain, but you can set a specific domain for responding to DNS queries by configuring the domain parameter.\nIn some instances, Consul may need to respond to queries in more than one domain, such as during a DNS migration or to distinguish between internal and external queries.\nConsul versions 1.5.2+ can be configured to respond to DNS queries on an alternative domain through the alt_domain agent configuration option. As of Consul versions 1.11.0+, Consul's DNS response will use the same domain as was used in the query; in prior versions, the response may use the primary domain no matter which domain was used in the query.\nIn the following example, the alt_domain parameter is set to test-domain:\nalt_domain = \"test-domain\" \n$ dig @127.0.0.1 -p 8600 consul.service.test-domain SRV \nThe following responses are returned:\n;; QUESTION SECTION: ;consul.service.test-domain. IN SRV ;; ANSWER SECTION: consul.service.test-domain. 0 IN SRV 1 1 8300 machine.node.dc1.test-domain. ;; ADDITIONAL SECTION: machine.node.dc1.test-domain. 0 IN A 127.0.0.1 machine.node.dc1.test-domain. 0 IN TXT \"consul-network-segment=\" \nPTR queries: Responses to PTR queries (<ip>.in-addr.arpa.) will always use the primary domain (not the alternative domain), as there is no way for the query to specify a domain.\nBy default, all DNS results served by Consul set a 0 TTL value. This disables caching of DNS results. However, there are many situations in which caching is desirable for performance and scalability. This is discussed more in the tutorial for DNS caching.\nBy default, Consul DNS queries will return a node's local address, even when being queried from a remote datacenter. If you need to use a different address to reach a node from outside its datacenter, you can configure this behavior using the advertise-wan and translate_wan_addrs configuration options.\nConsul Enterprise 1.7.0 added support for namespaces including resolving namespaced services via DNS. To maintain backwards compatibility existing queries can be used and these will resolve services within the default namespace. However, for resolving services from other namespaces the following form can be used:\n[tag.]<service>.service.<namespace>.<datacenter>.<domain> \nThis is the canonical name of a Consul Enterprise service with all parts present. Like Consul OSS some parts may be omitted but which parts depend on the value of the prefer_namespace configuration.\nWith prefer_namespace set to true the datacenter may be omitted and will be defaulted to the local agents datacenter:\n[tag.]<service>.service.<namespace>.<domain> \nWith prefer_namespace set to false the namespace may be omitted and will be defaulted to the default namespace:\n[tag.]<service>.service.<datacenter> \nFinally, both the namespace and datacenter may be omitted and the service will be resolved in the default namespace and in the datacenter of the local agent.\nIn order to use the DNS interface when Access Control Lists (ACLs) are enabled, you must first create ACL tokens with the necessary policies.\nConsul agents resolve DNS requests using one of the preconfigured tokens below, listed in order of precedence:\nThe agent's default token.\nThe built-in anonymous token. Because the anonymous token is used when any request is made to Consul without explicitly specifying a token, production deployments should not apply policies needed for DNS to this token.\nConsul will either accept or deny the request depending on whether the token has the appropriate authorization. The following table describes the available DNS lookups and required policies when ACLs are enabled:\nFor guidance on how to configure an appropriate token for DNS, refer to the securing Consul with ACLs guides for:\nProduction Environments\nDevelopment Environments"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.16.x/k8s/connect/cluster-peering/usage/manage-peering",
  "text": "To confirm that you deleted your peering connection in cluster-01, query the the /health HTTP endpoint:\nQuery the the /health HTTP endpoint. Peered services with deleted connections should no longe appear."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.10.x/connect/registration/service-registration",
  "text": "Connect - Service Registration | Consul\nThis topic describes how to declare a proxy as a connect-proxy in service definitions. The kind must be declared and information about the service they represent must be provided to function as a Consul service mesh proxy.\nConfigure a service mesh proxy using the following syntax:\nBasic syntax for configuring a service mesh proxy\nname = <name of the service> kind = \"connect-proxy\" proxy = { destination_service_name = \"<name of the service that the proxy represents>\" <additional proxy parameters> = \"<additional parameter values>\" } port = <port where services can discover and connect to proxied services> \nThe following table describes the parameters that must be added to the service definition to declare the service as a proxy. \nParameterDescriptionRequiredDefault\nkind\tString value that declares the type for the service. This should always be set to connect-proxy to declare the services as a service mesh proxy.\tRequired\tNone\t\nproxy\tObject that contains the proxies parameters. \nThe destination_service_name parameter must be included in the proxy configuration. The destination_service_name parameter specifies the name of the services that the proxy represents. \nThis parameter replaces proxy_destination used in Consul 1.2.0 to 1.3.0. The proxy_destination parameter was deprecated in 1.5.0.\tRequired\tNone\t\nport\tInteger value that specifies the port where other services in the mesh can discover and connect to proxied services.\tRequired\tNone\t\naddress\tSpecifies the IP address of the proxy.\tOptional \nThe address will be inherited from the node configuration.\taddress specified in the node configuration.\t\nYou can specify several additional parameters to configure the proxy to meet your requirements. See Proxy Parameters for additional information.\nExample\nIn the following example, a proxy named redis-proxy is registered as a service mesh proxy. It proxies to the redis service and is available at port 8181. As a result, any service mesh clients searching for a Connect-capable endpoint for redis will find this proxy.\nMinimal example of a service mesh proxy\nkind = \"connect-proxy\" name = \"redis-proxy\" port = 8181 proxy = { destination_service_name = \"redis\" } \nSidecar Proxy Configuration\nMany service mesh proxies are deployed as sidecars. Sidecar proxies are co-located with the single service instance they represent and proxy all inbound traffic to. \nSpecify the following parameters in the proxy code block to configure a sidecar proxy in its own service registration:\ndestination_service_id: String value that specifies the ID of the service being proxied. Refer to the proxy parameters reference for details.\nlocal_service_port: Integer value that specifes the port that the proxy should use to connect to the local service instance. Refer to the proxy parameters reference for details. \nlocal_service_address: String value that specifies the IP address or hostname that the proxy should use to connect to the local service. Refer to the proxy parameters reference for details. \nSee (Sidecar Service Registration)[/docs/connect/registration/sidecar-service] for additional information about configuring service mesh proxies as sidecars.\nComplete Configuration Example\nThe following example includes values for all availble options when registering a proxy instance.\nExample that includes all configuration options when registering a proxy instance\nkind = \"connect-proxy\" name = \"redis-proxy\" port = 8181 proxy = { config = {} destination_service_id = \"redis1\" destination_service_name = \"redis\" expose = {} local_service_address = \"127.0.0.1\" local_service_port = 9090 local_service_socket_path = \"/tmp/redis.sock\" mesh_gateway = {} mode = \"transparent\" transparent_proxy = {} upstreams = [] \nProxy Parameters\nThe following table describes all parameters that can be defined in the proxy block.\nParameterDescriptionRequiredDefault\ndestination_service_id \tString value that specifies the ID of a single service instance represented by the proxy. \nThis parameter is only applicable for sidecar proxies that run on the same node as the service. \nConsul checks for the proxied service on the same agent. \nThe ID is unique and may differ from its name value. \nSpecifying this parameter helps tools identify which sidecar proxy instances are associated with which application instance, as well as enable fine-grained analysis of the metrics coming from the proxy.\tRequired when registering proxy as a sidecar\tNone\t\nlocal_service_port \tInteger value that specifes the port that a sidecar proxy should use to connect to the local service instance.\tRequired when registering proxy as a sidecar\tPort advertised by the service instance configured in destination_service_id\t\nlocal_service_address \tString value that specifies the IP address or hostname that a sidecar proxy should use to connect to the local service.\tOptional\t127.0.0.1\t\ndestination_service_name\tString value that specifies the name of the service the instance is proxying. The name is used during service discovery to route to the correct proxy instances for a given service name.\tRequired\tNone\t\nlocal_service_socket_path\tString value that specifies the path of a Unix domain socket for connecting to the local application instance. \nThis parameter value is created by the application and conflicts with local_service_address and local_service_port. \nSupported when using Envoy for the proxy.\tOptional\tNone\t\nmode\tString value that specifies the proxy mode. See Proxy Modes for additional information.\tOptional\tdirect\t\ntransparent_proxy\tObject value that specifies the configuration specific to proxies in transparent mode. \nSee Proxy Modes and Transparent Proxy Configuration Reference for additional information. \nThis parameter was added in Consul 1.10.0.\tOptional\tNone\t\nconfig\tObject value that specifies an opaque JSON configuration. The JSON is stored and returned along with the service instance when called from the API.\tOptional\tNone\t\nupstreams\tAn array of objects that specify the upstream services that the proxy should create listeners for. Refer to Upstream Configuration Reference for details.\tOptional\tNone\t\nmesh_gateway\tObject value that specifies the mesh gateway configuration for the proxy. Refer to Mesh Gateway Configuration Reference for details.\tOptional\tNone\t\nexpose\tObject value that specifies a configuration for exposing HTTP paths through the proxy. \nThis parameter is only compatible with Envoy proxies. \nRefer to Expose Paths Configuration Reference for details.\tOptional\tNone\t\nUpstream Configuration Reference\nYou can configure the service mesh proxy to create listeners for upstream services. The listeners enable the upstream service to accept requests. You can specify the following parameters to configure upstream service listeners.\nParameterDescriptionRequiredDefautl\ndestination_name\tString value that specifies the name of the service or prepared query to route the service mesh to. The prepared query should be the name or the ID of the prepared query.\tRequired\tNone\t\ndestination_namespace\tString value that specifies the namespace containing the upstream service. Enterprise\tOptional\tdefault\t\ndestination_partition\tString value that specifies the name of the admin partition containing the upstream service.\tOptional\tdefault\t\nlocal_bind_port\tInteger value that specifies the port to bind a local listener to. The application will make outbound connections to the upstream from the local port.\tRequired\tNone\t\nlocal_bind_address\tString value that specifies the address to bind a local listener to. The application will make outbound connecttions to the upstream service from the local bind address.\tOptional\t127.0.0.1\t\nlocal_bind_socket_path\tString value that specifies the path at which to bind a Unix domain socket listener. The application will make outbound connections to the upstream from the local bind socket path. \nThis parameter conflicts with the local_bind_port or local_bind_address parameters. \nSupported when using Envoy as a proxy.\tOptional\tNone\t\nlocal_bind_socket_mode\tString value that specifies a Unix octal that configures file permissions for the socket.\tOptional\tNone\t\ndestination_type\tString value that specifies the type of discovery query the proxy should use for finding service mesh instances. The following values are supported: \nservice: Queries for upstream service types. \nprepared_query: Queries for upstream prepared queries.\n\tOptional\tservice\t\ndatacenter\tString value that specifies the datacenter to issue the discovery query to.\tOptional\tDefaults to the local datacenter.\t\nconfig\tObject value that specifies opaque configuration options that will be provided to the proxy instance for the upstream. \nValid JSON objects are also suppported. \nThe config parameter can specify timeouts, retries, and other proxy-specific features for the given upstream. \nSee the built-in proxy configuration reference for configuration options when using the built-in proxy. \nIf using Envoy as a proxy, see Envoy configuration reference\tOptional\tNone\t\nmesh_gateway\tObject that defines the mesh gateway configuration for the proxy. Refer to the Mesh Gateway Configuration Reference for configuration details.\tOptional\tNone\t\nUpstream Configuration Examples\nUpstreams support multiple destination types. The following examples include information about each implmentation. \nSnake case: The examples in this topic use snake_case because the syntax is supported in configuration files and API registrations. See Service Definition Parameter Case for additional information.\nExample service destination upstream\ndestination_type = \"service\" destination_name = \"redis\" datacenter = \"dc1\" local_bind_address = \"127.0.0.1\" local_bind_port = 1234 local_bind_socket_path = \"/tmp/redis_5678.sock\" local_bind_socket_mode = \"0700\" mesh_gateway = { mode = \"local\" } \nExample prepared query upstream\ndestination_type = \"prepared_query\" destination_name = \"database\" local_bind_address = \"127.0.0.1\" local_bind_port = 1234 config = {} \nExample of dialing remote upstreams across admin partitions\ndestination_partition = \"finance\" destination_namespace = \"default\" destination_type = \"service\" destination_name = \"billing\" local_bind_port = 9090 \nYou can configure which mode a proxy operates in by specifying \"direct\" or \"transparent\" in the mode parameter. The proxy mode determines the how proxies direct traffic. This feature was added in Consul 1.10.0.\ntransparent: In this mode, inbound and outbound application traffic is captured and redirected through the proxy. This mode does not enable the traffic redirection. It directs Consul to configure Envoy as if traffic is already being redirected.\ndirect: In this mode, the proxy's listeners must be dialed directly by the local application and other proxies.\nYou can also specify an empty string (\"\"), which configures the proxy to operate in the default mode. The default mode is inherited from parent parameters in the following order of precedence: \nProxy service's Proxy configuration\nThe global proxy-defaults.\nThe proxy will default to direct mode if a mode cannot be determined from the parent parameters.\nTransparent Proxy Configuration Reference\nThe following examples show additional configuration for transparent proxies.\nAdded in v1.10.0.\nNote that snake_case is used here as it works in both config file and API registrations.\nConfigure a proxy listener for outbound traffic on port 22500\n{ \"outbound_listener_port\": 22500, \"dialed_directly\": true } \noutbound_listener_port (int: 15001) - The port the proxy should listen on for outbound traffic. This must be the port where outbound application traffic is captured and redirected to.\ndialed_directly (bool: false) - Determines whether this proxy instance's IP address can be dialed directly by transparent proxies. Typically transparent proxies dial upstreams using the \"virtual\" tagged address, which load balances across instances. Dialing individual instances can be helpful in cases like stateful services such as a database cluster with a leader. \nNote: Dynamic routing rules such as failovers and redirects do not apply to services dialed directly. Additionally, the connection is proxied using a TCP proxy with a connection timeout of 5 seconds. \nMesh Gateway Configuration Reference\nThe following examples show all possible mesh gateway configurations.\nNote that snake_case is used here as it works in both config file and API registrations.\nUsing a Local/Egress Gateway in the Local Datacenter\nDirect to a Remote/Ingress in a Remote Datacenter\nPrevent Using a Mesh Gateway\nDefault Mesh Gateway Mode\nmode (string: \"\") - This defines the mode of operation for how upstreams with a remote destination datacenter get resolved.\n\"local\" - Mesh gateway services in the local datacenter will be used as the next-hop destination for the upstream connection.\n\"remote\" - Mesh gateway services in the remote/target datacenter will be used as the next-hop destination for the upstream connection.\n\"none\" - No mesh gateway services will be used and the next-hop destination for the connection will be directly to the final service(s).\n\"\" - Default mode. The default mode will be \"none\" if no other configuration enables them. The order of precedence for setting the mode is\nUpstream\nProxy Service's Proxy configuration\nThe global proxy-defaults.\nExpose Paths Configuration Reference\nThe following examples show possible configurations to expose HTTP paths through Envoy.\nExposing paths through Envoy enables a service to protect itself by only listening on localhost, while still allowing non-Connect-enabled applications to contact an HTTP endpoint. Some examples include: exposing a /metrics path for Prometheus or /healthz for kubelet liveness checks.\nNote that snake_case is used here as it works in both config file and API registrations.\nExpose listeners in Envoy for HTTP and GRPC checks registered with the local Consul agent\n{ \"expose\": { \"checks\": true } } \nExpose an HTTP listener in Envoy at port 21500 that routes to an HTTP server listening at port 8080\n{ \"expose\": { \"paths\": [ { \"path\": \"/healthz\", \"local_path_port\": 8080, \"listener_port\": 21500 } ] } } \nExpose an HTTP2 listener in Envoy at port 21501 that routes to a gRPC server listening at port 9090\n{ \"expose\": { \"paths\": [ { \"path\": \"/grpc.health.v1.Health/Check\", \"protocol\": \"http2\", \"local_path_port\": 9090, \"listener_port\": 21501 } ] } } \nchecks (bool: false) - If enabled, all HTTP and gRPC checks registered with the agent are exposed through Envoy. Envoy will expose listeners for these checks and will only accept connections originating from localhost or Consul's advertise address. The port for these listeners are dynamically allocated from expose_min_port to expose_max_port. This flag is useful when a Consul client cannot reach registered services over localhost. One example is when running Consul on Kubernetes, and Consul agents run in their own pods.\npaths array<Path>: [] - A list of paths to expose through Envoy.\npath (string: \"\") - The HTTP path to expose. The path must be prefixed by a slash. ie: /metrics.\nlocal_path_port (int: 0) - The port where the local service is listening for connections to the path.\nlistener_port (int: 0) - The port where the proxy will listen for connections. This port must be available for the listener to be set up. If the port is not free then Envoy will not expose a listener for the path, but the proxy registration will not fail.\nprotocol (string: \"http\") - Sets the protocol of the listener. One of http or http2. For gRPC use http2.\nUnix Domain Sockets\nThe following examples show additional configuration for Unix domain sockets.\nAdded in v1.10.0.\nTo connect to a service via local Unix Domain Socket instead of a port, add local_bind_socket_path and optionally local_bind_socket_mode to the upstream config for a service:\nupstreams = [ { destination_name = \"service-1\" local_bind_socket_path = \"/tmp/socket_service_1\" local_bind_socket_mode = \"0700\" } ] \nThis will cause Envoy to create a socket with the path and mode provided, and connect that to service-1.\nThe mode field is optional, and if omitted will use the default mode for Envoy. This is not applicable for abstract sockets. See the Envoy documentation for details.\nThese options conflict with the local_bind_socket_port and local_bind_socket_address options. For a given upstream the proxy can bind either to an IP port or a Unix socket, but not both.\nSimilarly to expose a service listening on a Unix Domain socket to the service mesh, use either the socket_path field in the service definition or the local_service_socket_path field in the proxy definition. These fields are analogous to the port and service_port fields in their respective locations.\nservices { name = \"service-2\" socket_path = \"/tmp/socket_service_2\" } \nOr in the proxy definition:\nservices { name = \"socket_service_2\" connect { sidecar_service { proxy { name = \"service-2\" local_service_socket_path = \"/tmp/socket_service_2\" ... } } } } \nThere is no mode field since the service is expected to create the socket it is listening on, not the Envoy proxy. Again, the socket_path and local_service_socket_path fields conflict with address/port and local_service_address/local_service_port configuration options."
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.16.x/peering",
  "text": "Usage: consul peering <subcommand> [options] # ... Subcommands: delete Close and delete a peering connection establish Consume a peering token and establish a connection with the accepting cluster generate-token Generate a peering token for use by a dialing cluster list List the local cluster's peering connections read Read detailed information on a peering connection "
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.16.x/k8s/connect/cluster-peering/usage/create-sameness-groups",
  "text": "Create sameness groups | Consul\nCreate sameness groups on Kubernetes\nThis topic describes how to designate groups of services as functionally identical in your network on Consul deployments running Kubernetes. Sameness groups are the preferred failover strategy for deployments with cluster peering connections. For more information about sameness groups and failover, refer to Failover overview.\nWarning\nSameness groups are a beta feature in this version of Consul. Functionality is subject to change. You should never use the beta release in secure environments or production scenarios. Features in beta may experience performance issues, scaling issues, and limited support.\nSameness groups are a user-defined set of partitions that Consul uses to identify services in different administrative partitions with the same name as being the same service. One of the use cases for sameness groups is to create a blanket failover policy for deployments with cluster peering connections.\nTo create and use sameness groups in your network, complete the following steps:\nCreate sameness group configuration entries for each member of the group. For each partition that you want to include in the sameness group, you must write and apply a sameness group configuration entry that defines the groups members from that partitions perspective. Refer to the sameness group configuration entry reference for details on configuration hierarchy, default values, and specifications.\nExport services to members of the sameness group. You must write and apply an exported services configuration entry that makes the partitions services available to other members of the group. Refer to exported services configuration entry reference for additional specification information.\nCreate service intentions for each member of the sameness group. For each partition that you want to include in the sameness group, you must write and apply service intentions configuration entries to authorize traffic to your services from all members of the group. Refer to the service intentions configuration entry reference for additional specification information.\nAll datacenters where you want to create sameness groups must run Consul v1.16 or later. Refer to upgrade instructions for more information about how to upgrade your deployment.\nA Consul Enterprise license is required.\nBefore you begin\nBefore creating a sameness group, take the following actions to prepare your network:\nCheck namespace and service naming conventions\nSameness groups are defined at the partition level. As a result, Consul assumes that namespaces and services in a sameness group that have the same name are identical. This behavior occurs even when two different partitions in the group contain functionally different services that share a name and namespace. This situation would lead to errors, as requests will be sent to the incorrect service.\nTo prevent errors, check the names of the services deployed to your network and the namespaces they are deployed in. Pay particular attention to the default namespace to confirm that services have unique names. If different services share a name, you should either change one of the services name or deploy one of the services to a different namespace. After adding the partition to the sameness group, differentiating the names of services or namespaces enables Consul to correctly assume which services are identical.\nDeploy mesh gateways for each partition\nMesh gateways are required for cluster peering connections and recommended to secure cross-partition traffic in a single datacenter. Therefore, we recommend securing your network, and especially your production environment, by deploying mesh gateways to each datacenter. Refer to mesh gateways specifications for more information about configuring mesh gateways.\nEstablish cluster peering relationships between remote partitions\nYou must establish connections with cluster peers before you can create a sameness group that includes them. A cluster peering connection exists between two admin partitions in different datacenters, and each connection between two partitions must be established separately with each peer. Refer to establish cluster peering connections for step-by-step instructions.\nTo establish cluster peering connections and define a define group as part of the same workflow, follow instructions up to Export services between clusters. You can use the same exported services and service intention configuration entries to establish the cluster peering connection and create the sameness group.\nTo create a sameness group, you must write and apply sets of three configuration entries for each partition that is a member of the group:\nSameness group configuration entries: Define the sameness group from each partitions perspective.\nExported services configuration entries: Make services available to other partitions in the group.\nService intentions configuration entries: Authorize traffic between services across partitions.\nDefine the sameness group from each partitions perspective\nTo define a sameness group for a partition, create a sameness group configuration entry that describes the partitions and cluster peers that are part of the group. Typically, this order follows this pattern:\nThe local partition\nOther partitions in the same datacenter\nPartitions with established cluster peering relationships\nIf you want all services to failover to other instances in the sameness group by default, set spec.defaultForFailover=true and list the group members in the order you want to use in a failover scenario.\nAfter you create the configuration entry, apply it to the Consul server with the following kubectl CLI command:\n$ kubectl apply --filename sameness-group-a.yaml \nThen, repeat the process to create and apply a configuration entry for every partition that is a member of the sameness group.\nBe aware that the sameness group configuration entries are different for each partition. The following example demonstrates how to format three different configuration entries for three partitions that are part of the sameness group sameness-group-a when Partition 1 and Partition 2 are in DC1, and the third partition is Partition 1 in DC2:\nsameness-group-a.yaml\napiVersion: consul.hashicorp.com/v1alpha1 kind: SamenessGroup metadata: name: sameness-group-a spec: defaultForFailover: true members: - partition: partition-1 - partition: partition-2 - peer: dc2-partition-1 \nExport services to other partitions in the sameness group\nTo make services available to other members of the sameness group, you must write and apply an exported services configuration entry to each partition in the group. This configuration entry exports the local partition's services to the rest of the group members. In each configuration entry, set the sameness group as the Consumer for the exported services. You can export multiple services in a single exported services configuration entry.\nBecause you are configuring the consumer to reference the sameness group instead of listing out each partition and cluster peer, you do not need to edit this configuration again when you add a partition or peer to the group.\nThe following example demonstrates how to format three different exported-service configuration entries to make a service named api deployed to the store namespace of each partition available to all other group members:\napiVersion: consul.hashicorp.com/v1alpha1 Kind: ExportedServices metadata: name: partition-1 spec: services: - name: api namespace: store consumers: - samenessGroup: sameness-group-a \nFor more information about exporting services, including examples of configuration entries that export multiple services at the same time, refer to the exported services configuration entry reference.\nAfter you create each exported services configuration entry, apply it to the Consul server with the following CLI command:\n$ kubectl apply --filename group-a-export.yaml \nExport services for cluster peers and sameness groups at the same time\nIt is possible to combine the exported services configuration entry for creating sameness groups with the entry required to export services when establishing cluster peering connections.\nIn this workflow, configure the services[].consumers block with the samenessGroup field instead of the peer field. You should write the sameness-group configuration entry to Consul before referencing it in the exported-services configuration entry.\nWhile exporting the service to each member of the sameness group makes the services visible to remote partitions, you must also create service intentions so that local services are authorized to send and receive traffic from a member of the sameness group.\nFor each partition that is member of the group, write and apply a service intentions configuration entry that defines intentions for the services that are part of the group. In the sources block of the configuration entry, include the service name, its namespace, the sameness group and grant allow permissions.\nBecause you are using the sameness group in the sources block rather than listing out each partition and cluster peer, you do not have to make further edits to the service intentions configuration entries when members are added to or removed from the group.\nThe following example demonstrates how to format three different service-intentions configuration entries to make a service named api available to all instances of payments deployed in all members of the sameness group including the local partition. In this example, api is deployed to the store namespace in all three partitions.\napiVersion: consul.hashicorp.com/v1alpha1 kind: ServiceIntentions metadata: name: api spec: sources: - name: api action: allow namespace: store samenessGroup: sameness-group-a \nRefer to create and manage intentions for more information about how to create and apply service intentions in Consul.\nAfter you create each service intentions configuration entry, apply it to the Consul server with the following CLI command:\n$ kubectl apply --filename api.yaml \nCreate service intentions for cluster peers and sameness groups at the same time\nIt is possible to combine the service intentions configuration entry for creating sameness groups with the entry required to authorize services for peers.\nIn this workflow, configure the sources block with the samenessGroup field instead of the peer field. You should write the sameness-group configuration entry to Consul before referencing it in the service-intentions configuration entry."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.16.x/k8s/connect/cluster-peering/usage/l7-traffic",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.8.x/upgrading/compatibility",
  "text": "Consul Protocol Compatibility Promise | Consul\nWe expect Consul to run in large clusters of long-running agents. Because safely upgrading agents in this sort of environment relies heavily on backwards compatibility, we have a strong commitment to keeping different Consul versions protocol-compatible with each other.\nWe promise that every subsequent release of Consul will remain backwards compatible with at least one prior version. Concretely: version 0.5 can speak to 0.4 (and vice versa) but may not be able to speak to 0.1.\nBackwards compatibility is automatic unless otherwise noted. Consul agents by default will speak the latest protocol but can understand earlier ones.\nNote: If speaking an earlier protocol, new features may not be available.\nThe ability for an agent to speak an earlier protocol is to ensure that any agent can be upgraded without cluster disruption. Consul agents can be updated one at a time, one version at a time.\nFor more details on the specifics of upgrading, see the upgrading page.\nConsul VersionProtocol Compatibility\n0.1 - 0.3\t1\t\n0.4\t1, 2\t\n0.5\t1, 2. 0.5.X servers cannot be mixed with older servers.\t\n0.6\t1, 2, 3\t\n>= 0.7\t2, 3. Will automatically use protocol > 2 when speaking to compatible agents\t\nNote: Raft Protocol is versioned separately, but maintains compatibility with at least one prior version. See here for details."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.12.x/agent/config-entries",
  "text": "How to Use Configuration Entries | Consul\nConfiguration entries can be created to provide cluster-wide defaults for various aspects of Consul.\nOutside of Kubernetes, configuration entries can be specified in HCL or JSON using either snake_case or CamelCase for key names. On Kubernetes, configuration entries can be managed by custom resources in YAML.\nOutside of Kubernetes, every configuration entry specified in HCL or JSON has at least two fields: Kind and Name. Those two fields are used to uniquely identify a configuration entry. Configuration entries specified as HCL or JSON objects use either snake_case or CamelCase for key names.\nExample config specified outside of Kubernetes\nKind = \"<supported kind>\" Name = \"<name of entry>\" \nOn Kubernetes, Kind is set as the custom resource kind and Name is set as metadata.name:\nExample config specified on Kubernetes\napiVersion: consul.hashicorp.com/v1alpha1 kind: <supported kind> metadata: name: <name of entry> \nSee Service Mesh - Config Entries for the list of supported config entries.\nSee Kubernetes Custom Resource Definitions.\nConfiguration entries outside of Kubernetes should be managed with the Consul CLI or API. Additionally, as a convenience for initial cluster bootstrapping, configuration entries can be specified in all of the Consul servers's configuration files\nManaging Configuration Entries with the CLI\nCreating or Updating a Configuration Entry\nThe consul config write command is used to create and update configuration entries. This command will load either a JSON or HCL file holding the configuration entry definition and then will push this configuration to Consul.\nExample HCL Configuration File:\nproxy-defaults.hcl\nKind = \"proxy-defaults\" Name = \"global\" Config { local_connect_timeout_ms = 1000 handshake_timeout_ms = 10000 } \nThen to apply this configuration, run:\n$ consul config write proxy-defaults.hcl \nIf you need to make changes to a configuration entry, simple edit that file and then rerun the command. This command will not output anything unless there is an error in applying the configuration entry. The write command also supports a -cas option to enable performing a compare-and-swap operation to prevent overwriting other unknown modifications.\nReading a Configuration Entry\nThe consul config read command is used to read the current value of a configuration entry. The configuration entry will be displayed in JSON form which is how its transmitted between the CLI client and Consul's HTTP API.\n$ consul config read -kind service-defaults -name web { \"Kind\": \"service-defaults\", \"Name\": \"web\", \"Protocol\": \"http\" } \nListing Configuration Entries\nThe consul config list command is used to list out all the configuration entries for a given kind.\n$ consul config list -kind service-defaults web api db \nDeleting Configuration Entries\nThe consul config delete command is used to delete an entry by specifying both its kind and name.\n$ consul config delete -kind service-defaults -name web \nThis command will not output anything when the deletion is successful.\nConfiguration Entry Management with Namespaces Enterprise\nConfiguration entry operations support passing a namespace in order to isolate the entry to affect only operations within that namespace. This was added in Consul 1.7.0.\n$ consul config write service-defaults.hcl -namespace foo \n$ consul config list -kind service-defaults -namespace foo web api \nBootstrapping From A Configuration File\nConfiguration entries can be bootstrapped by adding them inline to each Consul server's configuration file. When a server gains leadership, it will attempt to initialize the configuration entries. If a configuration entry does not already exist outside of the servers configuration, then it will create it. If a configuration entry does exist, that matches both kind and name, then the server will do nothing."
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.8.x/operator/area",
  "text": "Commands: Operator Area | Consul\nCommand: consul operator area\nConsul Enterprise supports network areas, which are operator-defined relationships between servers in two different Consul datacenters. The operator area command is used to interact with Consul's network area subsystem.\nUnlike Consul's WAN feature, network areas use just the server RPC port for communication, and relationships can be made between independent pairs of datacenters, so not all servers need to be fully connected. This allows for complex topologies among Consul datacenters like hub/spoke and more general trees.\nSee the Network Areas Guide for more details.\nUsage: consul operator area <subcommand> [options] The operator area command is used to interact with Consul's network area subsystem. Network areas are used to link together Consul servers in different Consul datacenters. With network areas, Consul datacenters can be linked together in ways other than a fully-connected mesh, as is required for Consul's WAN. Subcommands: create Create a new network area delete Remove a network area join Join Consul servers into an existing network area list List network areas members Display Consul server members present in network areas update Update the configuration of a network area \nIf ACLs are enabled, the client will need to supply an ACL Token with operator read or write privileges to use these commands.\nThis command creates a new network area.\nUsage: consul operator area create [options]\n-retry-join=<value> Specifies the address of a Consul server to join to, such as an IP or hostname with an optional port number. This is optional and can be specified multiple times.\nThe output looks like this, displaying the ID of the newly-created network area:\nCreated area \"d2872ec5-68ea-b862-b75d-0bee99aca100\" with peer datacenter \"other\"! \nThis command deletes an existing network area.\nUsage: consul operator area delete [options]\nDeleted area \"154941b0-80e2-9d69-c560-ab2c02807332\"! \nThis command joins Consul servers into an existing network area by address, such as an IP or hostname with an optional port. Multiple addresses may be given.\nUsage: consul operator area join [options] ADDRESSES\nAddress Joined Error 10.1.2.3 false failed to connect to \"10.1.2.3:8300\": dial tcp 10.1.2.3:8300: i/o timeout 10.1.2.4 true (none) 10.1.2.5 true (none) \nThe Error field will have a human-readable error message if Consul was unable to join the given address.\nThis command lists all network areas.\nUsage: consul operator area list [options]\nArea PeerDC RetryJoin 6a52a0af-62e2-dad4-da60-e66acc37096c dc2 10.1.2.3,10.1.2.4,10.1.2.5 96e33424-f5ce-9fcd-ecab-27974e36678f other (none) \nPeerDC is the peer datacenter for the area.\nRetryJoin is the list of servers to join, defined when the area was created.\nThis command displays Consul server nodes present in a network area, or all areas if no area is specified.\nUsage: consul operator area members [options]\nArea Node Address Status Build Protocol DC RTT 6a52a0af-62e2-dad4-da60-e66acc37096c node-1.dc1 127.0.0.1:8300 alive 0.8.0 2 dc1 0s 6a52a0af-62e2-dad4-da60-e66acc37096c node-2.dc1 127.0.0.2:8300 alive 0.8.0 2 dc1 594.191s 96e33424-f5ce-9fcd-ecab-27974e36678f node-1.dc1 127.0.0.1:8300 alive 0.8.0 2 dc1 0s 96e33424-f5ce-9fcd-ecab-27974e36678f node-2.dc1 127.0.0.2:8300 alive 0.8.0 2 dc1 634.109s \nNode is the name of the node.\nAddress is the IP and server RPC port for the node.\nStatus is the current health status of the node, as determined by the network area distributed failure detector. This will be \"alive\", \"leaving\", \"left\", or \"failed\". A \"failed\" status means that other servers are not able to probe this server over its server RPC interface.\nBuild has the Consul version running on the node.\nProtocol is the protocol version being spoken by the node.\nDC is the node's Consul datacenter.\nRTT is an estimated network round trip time from the server answering the query to the given server, in a human-readable format. This is computed using network coordinates.\nThis command updates the configuration of network area.\nUsage: consul operator area update [options]\nUpdated area \"d2872ec5-68ea-b862-b75d-0bee99aca100\" "
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.8.x/connect/registration",
  "text": "Connect - Proxy Registration | Consul\nTo make Connect aware of proxies you will need to register them in a service definition, just like you would register any other service with Consul. This section outlines your options for registering Connect proxies, either using independent registrations, or in nested sidecar registrations.\nTo register proxies with independent proxy service registrations, you can define them in either in config files or via the API just like any other service. Learn more about all of the options you can define when registering your proxy service in the proxy registration documentation.\nTo reduce the amount of boilerplate needed for a sidecar proxy, application service definitions may define an inline sidecar service block. This is an opinionated shorthand for a separate full proxy registration as described above. For a description of how to configure the sidecar proxy as well as the opinionated defaults, see the sidecar service registrations documentation."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.12.x/troubleshoot/faq",
  "text": "Common Error Messages | Troubleshoot | Consul\nFrequently Asked Questions\nQ: Can I upgrade directly to a specific Helm chart version or should I upgrade one patch release at a time?\nIt is safe to upgrade directly to a specific version. Be sure to read the release notes for all versions you're upgrading through and look for any breaking changes.\nQ: Can I upgrade in place or should I spin up a new Kubernetes cluster?\nIt is always safer to spin up a new Kubernetes cluster but that is not an option for most teams. Consul supports upgrading in place.\nNon-production environments should be upgraded first. If upgrading a Consul version, Consul data should be backed up.\nQ: How can I run tcpdump on Consul servers?\nFirst, add the following to your values.yaml file so you can kubectl exec into the Consul server containers as root:\nserver: securityContext: runAsNonRoot: false runAsGroup: 0 runAsUser: 0 fsGroup: 0 \nRun a helm upgrade (see Upgrade Consul on Kubernetes for full upgrade instructions).\nNow, kubectl exec into a server pod:\n$ kubectl exec -it consul-server-0 -- sh \nInstall tcpdump:\n$ apk add --no-cache tcpdump $ which tcpdump /usr/bin/tcpdump \nQ: What is Checkpoint? / Does Consul call home?\nConsul makes use of a HashiCorp service called Checkpoint which is used to check for updates and critical security bulletins. Only anonymous information, which cannot be used to identify the user or host, is sent to Checkpoint. An anonymous ID is sent which helps de-duplicate warning messages. This anonymous ID can be disabled. In fact, using the Checkpoint service is optional and can be disabled.\nSee disable_anonymous_signature and disable_update_check.\nQ: Does Consul rely on UDP Broadcast or Multicast?\nConsul uses the Serf gossip protocol which relies on TCP and UDP unicast. Broadcast and Multicast are rarely available in a multi-tenant or cloud network environment. For that reason, Consul and Serf were both designed to avoid any dependence on those capabilities.\nQ: Is Consul eventually or strongly consistent?\nConsul has two important subsystems, the service catalog and the gossip protocol. The service catalog stores all the nodes, service instances, health check data, ACLs, and KV information. It is strongly consistent, and replicated using the consensus protocol.\nThe gossip protocol is used to track which nodes are part of the cluster and to detect a node or agent failure. This information is eventually consistent by nature. When the servers detects a change in membership, or receive a health update, they update the service catalog appropriately.\nBecause of this split, the answer to the question is subtle. Almost all client APIs interact with the service catalog and are strongly consistent. Updates to the catalog may come via the gossip protocol which is eventually consistent meaning the current state of the catalog can lag behind until the state is reconciled.\nQ: Are failed or left nodes ever removed?\nTo prevent an accumulation of dead nodes (nodes in either failed or left states), Consul will automatically remove dead nodes out of the catalog. This process is called reaping. This is currently done on a configurable interval of 72 hours. Reaping is similar to leaving, causing all associated services to be deregistered. Changing the reap interval for aesthetic reasons to trim the number of failed or left nodes is not advised (nodes in the failed or left state do not cause any additional burden on Consul).\nQ: Does Consul support delta updates for watchers or blocking queries?\nConsul does not currently support sending a delta or a change only response to a watcher or a blocking query. The API simply allows for an edge-trigger return with the full result. A client should keep the results of their last read and compute the delta client side.\nBy design, Consul offloads this to clients instead of attempting to support the delta calculation. This avoids expensive state maintenance on the servers as well as race conditions between data updates and watch registrations.\nQ: What network ports does Consul use?\nThe Ports Used section of the Configuration documentation lists all ports that Consul uses.\nQ: Does Consul require certain user process resource limits?\nThere should be only a small number of open file descriptors required for a Consul client agent. The gossip layers perform transient connections with other nodes, each connection to the client agent (such as for a blocking query) will open a connection, and there will typically be connections to one of the Consul servers. A small number of file descriptors are also required for watch handlers, health checks, log files, and so on.\nFor a Consul server agent, you should plan on the above requirements and an additional incoming connection from each of the nodes in the cluster. This should not be the common case, but in the worst case if there is a problem with the other servers you would expect the other client agents to all connect to a single server and so preparation for this possibility is helpful.\nThe default ulimits are usually sufficient for Consul, but you should closely scrutinize your own environment's specific needs and identify the root cause of any excessive resource utilization before arbitrarily increasing the limits.\nQ: What is the per-key value size limitation for Consul's key/value store?\nThe default recommended limit on a key's value size is 512KB. This is strictly enforced and an HTTP 413 status will be returned to any client that attempts to store more than that limit in a value. The limit can be increased by using the kv_max_value_size configuration option.\nIt should be noted that the Consul key/value store is not designed to be used as a general purpose database. See Server Performance for more details.\nQ: What data is replicated between Consul datacenters?\nIn general, data is not replicated between different Consul datacenters. When a request is made for a resource in another datacenter, the local Consul servers forward an RPC request to the remote Consul servers for that resource and return the results. If the remote datacenter is not available, then those resources will also not be available from that datacenter. That will not affect the requests to the local datacenter. There are some special situations where a limited subset of data can be replicated, such as with Consul's built-in ACL replication capability, or external tools like consul-replicate.\nQ: Can Consul natively handle protecting against other processes accessing Consul's memory state?\nConsul does not provide built-in memory access protections, and doesn't interact with the host system to change or manipulate viewing and doesn't interact with the host system to change or manipulate application security.\nWe recommend taking any precautions or remediation steps that you would normally do for individual processes, based on your operating system.\nPlease see our Security Model for more information.\nQ: Are the Consul Docker Images OCI Compliant?\nThe official Consul Docker image uses Docker image schema V2, which is OCI Compliant. To check the docker images on Docker Hub, use the command docker manifest inspect consul to inspect the manifest payload. The docker manifest inspect may require you to enable experimental features to use.\nQ: What browsers are supported by the Consul UI?\nConsul currently supports all 'evergreen' browsers, as they are generally on up-to-date versions. This means we support:\nChrome\nFirefox\nSafari\nMicrosoft Edge\nWe do not support Internet Explorer 11 (IE 11). Consul follows a similar alignment with Microsoft's own stance on IE 11, found on their support website."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.12.x/connect/cluster-peering/create-manage-peering",
  "text": "Cluster Peering - Create and Manage Connections | Consul\nCluster peering is currently in technical preview: Functionality associated with cluster peering is subject to change. You should never use the technical preview release in secure environments or production scenarios. Features in technical preview may have performance issues, scaling issues, and limited support.\nA peering token enables cluster peering between different datacenters. Once you generate a peering token, you can use it to establish a connection between clusters. Then you can export services and authorize other clusters to call those services.\nTo peer clusters, you must complete the following steps in order:\nCreate a peering token\nEstablish a connection between clusters\nExport service endpoints\nAuthorize connections between peers\nYou can generate peering tokens and initiate connections using the Consul API on any available agent. However, we recommend performing these operations through a client agent in the partition you want to connect.\nTo begin the cluster peering process, generate a peering token in one of your clusters. The other cluster uses this token to establish the peering connection.\nIn cluster-01, issue a request for a peering token using the HTTP API.\n$ curl --request POST --data '{\"PeerName\":\"cluster-02\"}' --url http://localhost:8500/v1/peering/token \nThe CLI outputs the peering token, which is a base64-encoded string containing the token details.\nCreate a JSON file that contains the first cluster's name and the peering token.\npeering_token.json\n{ \"PeerName\": \"cluster-01\", \"PeeringToken\": \"eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImF1ZCI6IlNvbHIifQ.5T7L_L1MPfQ_5FjKGa1fTPqrzwK4bNSM812nW6oyjb8\" } \nNext, use peering_token.json to establish a secure connection between the clusters. In the client agents of \"cluster-02,\" establish the peering connection using the HTTP API. This endpoint does not generate an output unless there is an error.\n$ curl --request POST --data @peering_token.json http://127.0.0.1:8500/v1/peering/establish \nWhen you connect server agents through cluster peering, they peer their default partitions. To establish peering connections for other partitions through server agents, you must add the Partition field to peering_token.json and specify the partitions you want to peer. For additional configuration information, refer to Cluster Peering - HTTP API.\nAfter you establish a connection between the clusters, you need to create a configuration entry that defines the services that are available for other clusters. Consul uses this configuration entry to advertise service information and support service mesh connections across clusters.\nFirst, create a configuration entry and specify the Kind as \"exported-services\".\npeering-config.hcl\nKind = \"exported-services\" Services = [ { ## The name and namespace of the service to export. Name = \"service-name\" Namespace = \"default\" ## The list of peer clusters to export the service to. Consumers = [ { ## The peer name to reference in config is the one set ## during the peering process. Peer = \"cluster-02\" } ] \nThen, add the configuration entry to your cluster.\n$ consul config write peering-config.hcl \nBefore you proceed, wait for the clusters to sync and make services available to their peers. You can issue an endpoint query to check the peered cluster status.\nBefore you can call services from peered clusters, you must set service intentions that authorize those clusters to use specific services. Consul prevents services from being exported to unauthorized clusters.\nFirst, create a configuration entry and specify the Kind as \"service-intentions\". Declare the service on \"cluster-02\" that can access the service in \"cluster-01.\" The following example sets service intentions so that \"frontend-service\" can access \"backend-service.\"\npeering-intentions.hcl\nKind = \"service-intentions\" Name = \"backend-service\" Sources = [ { Name = \"frontend-service\" Peer = \"cluster-02\" Action = \"allow\" } ] \nIf the peers name is not specified in Peer, then Consul assumes that the service is in the local cluster.\nThen, add the configuration entry to your cluster.\n$ consul config write peering-intentions.hcl \nTo confirm that you peered your clusters, you can query the /health/service endpoint of one cluster from the other cluster. For example, in \"cluster-02,\" query the endpoint and add the peer=cluster-01 query parameter to the end of the URL.\n$ curl \\ \"http://127.0.0.1:8500/v1/health/service/<service-name>?peer=cluster-01\" \nA successful query will include service information in the output.\nAfter you create a peering connection between clusters in different datacenters, you can disconnect the peered clusters. Deleting a peering connection stops data replication to the peer and deletes imported data, including services and CA certificates.\nIn \"cluster-01,\" request the deletion via the HTTP API.\n$ curl --request DELETE http://127.0.0.1:8500/v1/peering/cluster-02"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.8.x/agent/checks",
  "text": "This page does not exist for version v1.8.x."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.13.x/connect/cluster-peering/create-manage-peering",
  "text": "Cluster Peering - Create and Manage Connections | Consul\nCluster peering is currently in beta: Functionality associated with cluster peering is subject to change. You should never use the beta release in secure environments or production scenarios. Features in beta may have performance issues, scaling issues, and limited support.\nCluster peering is not currently available in the HCP Consul offering.\nA peering token enables cluster peering between different datacenters. Once you generate a peering token, you can use it to establish a connection between clusters. Then you can export services and create intentions so that peered clusters can call those services.\nCluster peering is not enabled by default on Consul servers. To peer clusters, you must first configure all Consul servers so that peering is enabled and the gRPC port(8502) accepts traffic from the peering cluster (e.g., client_addr=\"0.0.0.0\"). For additional information, refer to Configuration Files.\nAfter enabling peering for all Consul servers, complete the following steps in order:\nCreate a peering token\nEstablish a connection between clusters\nExport services between clusters\nAuthorize services for peers\nYou can generate peering tokens and initiate connections on any available agent using either the API or the Consul UI. If you use the API, we recommend performing these operations through a client agent in the partition you want to connect.\nThe UI does not currently support exporting services between clusters or authorizing services for peers.\nCreate a peering token\nTo begin the cluster peering process, generate a peering token in one of your clusters. The other cluster uses this token to establish the peering connection.\nEvery time you generate a peering token, a single-use establishment secret is embedded in the token. Because regenerating a peering token invalidates the previously generated secret, you must use the most recently created token to establish peering connections.\nIn cluster-01, issue a request for a peering token.\n$ curl --request POST --data '{\"PeerName\":\"cluster-02\"}' --url http://localhost:8500/v1/peering/token \nThe CLI outputs the peering token, which is a base64-encoded string containing the token details.\nCreate a JSON file that contains the first cluster's name and the peering token.\npeering_token.json\n{ \"PeerName\": \"cluster-01\", \"PeeringToken\": \"eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImF1ZCI6IlNvbHIifQ.5T7L_L1MPfQ_5FjKGa1fTPqrzwK4bNSM812nW6oyjb8\" } \nEstablish a connection between clusters\nNext, use the peering token to establish a secure connection between the clusters.\nIn one of the client agents in \"cluster-02,\" use peering_token.json to establish the peering connection. This endpoint does not generate an output unless there is an error.\n$ curl --request POST --data @peering_token.json http://127.0.0.1:8500/v1/peering/establish \nWhen you connect server agents through cluster peering, they peer their default partitions. To establish peering connections for other partitions through server agents, you must add the Partition field to peering_token.json and specify the partitions you want to peer. For additional configuration information, refer to Cluster Peering - HTTP API.\nExport services between clusters\nAfter you establish a connection between the clusters, you need to create a configuration entry that defines the services that are available for other clusters. Consul uses this configuration entry to advertise service information and support service mesh connections across clusters.\nFirst, create a configuration entry and specify the Kind as \"exported-services\".\npeering-config.hcl\nKind = \"exported-services\" Name = \"default\" Services = [ { ## The name and namespace of the service to export. Name = \"service-name\" Namespace = \"default\" ## The list of peer clusters to export the service to. Consumers = [ { ## The peer name to reference in config is the one set ## during the peering process. PeerName = \"cluster-02\" } ] } ] \nThen, add the configuration entry to your cluster.\n$ consul config write peering-config.hcl \nBefore you proceed, wait for the clusters to sync and make services available to their peers. You can issue an endpoint query to check the peered cluster status.\nBefore you can call services from peered clusters, you must set service intentions that authorize those clusters to use specific services. Consul prevents services from being exported to unauthorized clusters.\nFirst, create a configuration entry and specify the Kind as \"service-intentions\". Declare the service on \"cluster-02\" that can access the service in \"cluster-01.\" The following example sets service intentions so that \"frontend-service\" can access \"backend-service.\"\npeering-intentions.hcl\nKind = \"service-intentions\" Name = \"backend-service\" Sources = [ { Name = \"frontend-service\" Peer = \"cluster-02\" Action = \"allow\" } ] \nIf the peer's name is not specified in Peer, then Consul assumes that the service is in the local cluster.\nThen, add the configuration entry to your cluster.\n$ consul config write peering-intentions.hcl \nAfter you establish a peering connection, you can get a list of all active peering connections, read a specific peering connection's information, check peering connection health, and delete peering connections.\nList all peering connections\nYou can list all active peering connections in a cluster.\nAfter you establish a peering connection, query the /peering/ endpoint to get a list of all peering connections. For example, the following command requests a list of all peering connections on localhost and returns the information as a series of JSON objects:\n$ curl http://127.0.0.1:8500/v1/peerings [ { \"ID\": \"462c45e8-018e-f19d-85eb-1fc1bcc2ef12\", \"Name\": \"cluster-02\", \"State\": \"ACTIVE\", \"Partition\": \"default\", \"PeerID\": \"e83a315c-027e-bcb1-7c0c-a46650904a05\", \"PeerServerName\": \"server.dc1.consul\", \"PeerServerAddresses\": [ \"10.0.0.1:8300\" ], \"CreateIndex\": 89, \"ModifyIndex\": 89 }, { \"ID\": \"1460ada9-26d2-f30d-3359-2968aa7dc47d\", \"Name\": \"cluster-03\", \"State\": \"INITIAL\", \"Partition\": \"default\", \"Meta\": { \"env\": \"production\" }, \"CreateIndex\": 109, \"ModifyIndex\": 119 }, ] \nRead a peering connection\nYou can get information about individual peering connections between clusters.\nAfter you establish a peering connection, query the /peering/:name endpoint to get peering information about for a specific cluster. For example, the following command requests peering connection information for \"cluster-02\" and returns the info as a JSON object:\n$ curl http://127.0.0.1:8500/v1/peering/cluster-02 { \"ID\": \"462c45e8-018e-f19d-85eb-1fc1bcc2ef12\", \"Name\": \"cluster-02\", \"State\": \"INITIAL\", \"PeerID\": \"e83a315c-027e-bcb1-7c0c-a46650904a05\", \"PeerServerName\": \"server.dc1.consul\", \"PeerServerAddresses\": [ \"10.0.0.1:8300\" ], \"CreateIndex\": 89, \"ModifyIndex\": 89 } \nCheck peering connection health\nYou can check the status of your peering connection to perform health checks.\nTo confirm that the peering connection between your clusters remains healthy, query the health/service endpoint of one cluster from the other cluster. For example, in \"cluster-02,\" query the endpoint and add the peer=cluster-01 query parameter to the end of the URL.\n$ curl \\ \"http://127.0.0.1:8500/v1/health/service/<service-name>?peer=cluster-01\" \nA successful query includes service information in the output.\nDelete peering connections\nYou can disconnect the peered clusters by deleting their connection. Deleting a peering connection stops data replication to the peer and deletes imported data, including services and CA certificates.\nIn \"cluster-01,\" request the deletion through the /peering/ endpoint.\n$ curl --request DELETE http://127.0.0.1:8500/v1/peering/cluster-02"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.13.x/connect/gateways/mesh-gateway/service-to-service-traffic-peers",
  "text": "Mesh Gateways between Peered Clusters | Consul\nCluster peering is currently in beta: Functionality associated with cluster peering is subject to change. You should never use the beta release in secure environments or production scenarios. Features in beta may have performance issues, scaling issues, and limited support.\nMesh gateways are required for you to route service mesh traffic between different Consul clusters. Clusters can reside in different clouds or runtime environments where general interconnectivity between all services in all clusters is not feasible.\nUnlike mesh gateways for datacenters and partitions, mesh gateways for cluster peering decrypt data to HTTP services within the mTLS session. Data must be decrypted in order to evaluate and apply dynamic routing rules at the destination cluster, which reduces coupling between peers.\nTo configure mesh gateways for cluster peering, make sure your Consul environment meets the following requirements:\nConsul version 1.13.0 or newer.\nA local Consul agent is required to manage mesh gateway configuration.\nEnable Consul service mesh in all clusters.\nEnable peering on all Consul servers.\nUse Envoy proxies. Envoy is the only proxy with mesh gateway capabilities in Consul.\nConfigure the following settings to register and use the mesh gateway as a service in Consul.\nGateway registration\nSpecify mesh-gateway in the kind field to register the gateway with Consul.\nDefine the Proxy.Config settings using opaque parameters compatible with your proxy. For Envoy, refer to the Gateway Options and Escape-hatch Overrides documentation for additional configuration information.\nAlternatively, you can also use the CLI to spin up and register a gateway in Consul. For additional information, refer to the consul connect envoy command.\nSidecar registration\nConfigure the proxy.upstreams parameters to route traffic to the correct service, namespace, and peer. Refer to the upstreams documentation for details.\nThe service proxy.upstreams.destination_name is always required.\nThe proxy.upstreams.destination_peer must be configured to enable cross-cluster traffic.\nThe proxy.upstream/destination_namespace configuration is only necessary if the destination service is in a non-default namespace.\nService exports\nInclude the exported-services configuration entry to enable Consul to export services contained in a cluster to one or more additional clusters. For additional information, refer to the Exported Services documentation.\nACL configuration\nIf ACLs are enabled, you must add a token granting service:write for the gateway's service name and service:read for all services in the Enterprise admin partition or OSS datacenter to the gateway's service definition. These permissions authorize the token to route communications for other Consul service mesh services.\nModes\nModes are not configurable for mesh gateways that connect peered clusters. By default, all proxies connecting to peered clusters use mesh gateways in remote mode."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.18.x/ecs/reference/architecture",
  "text": "This page does not exist for version v1.18.x."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.8.x/connect/config-entries",
  "text": "This page does not exist for version v1.8.x."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.18.x/ecs/deploy/migrate-existing-tasks",
  "text": "Migrate existing tasks to Consul service mesh | Consul\nMigrate existing tasks to Consul on ECS with Terraform\nTo migrate existing tasks to Consul, rewrite the existing Terraform code for your tasks so that the container definitions include the mesh-task Terraform module.\nYour tasks must already be defined in Terraform using the ecs_task_definition resource so that they can then be converted to use the mesh-task module.\nThe following example shows an existing task definition configured in Terraform:\nresource \"aws_ecs_task_definition\" \"my_task\" { family = \"my_task\" requires_compatibilities = [\"FARGATE\"] network_mode = \"awsvpc\" cpu = 256 memory = 512 execution_role_arn = \"arn:aws:iam::111111111111:role/execution-role\" task_role_arn = \"arn:aws:iam::111111111111:role/task-role\" container_definitions = jsonencode( [{ name = \"example-client-app\" image = \"docker.io/org/my_task:v0.0.1\" essential = true portMappings = [ { containerPort = 9090 hostPort = 9090 protocol = \"tcp\" } ] cpu = 0 mountPoints = [] volumesFrom = [] }] ) } resource \"aws_ecs_service\" \"my_task\" { name = \"my_task\" cluster = \"arn:aws:ecs:us-east-1:111111111111:cluster/my-cluster\" task_definition = aws_ecs_task_definition.my_task.arn desired_count = 1 network_configuration { subnets = [\"subnet-abc123\"] } launch_type = \"FARGATE\" } \nReplace the aws_ecs_task_definition resource with the mesh-task module so that Consul adds the necessary dataplane containers that enable your task to join the mesh. The mesh-task module uses inputs similar to your old ECS task definition but creates a new version of the task definition with additional containers.\nThe following Terraform configuration uses the mesh-task module to replace the previous example's task definition:\nmodule \"my_task\" { source = \"hashicorp/consul-ecs/aws//modules/mesh-task\" version = \"<latest version>\" family = \"my_task\" container_definitions = [ { name = \"example-client-app\" image = \"docker.io/org/my_task:v0.0.1\" essential = true portMappings = [ { containerPort = 9090 hostPort = 9090 protocol = \"tcp\" } ] cpu = 0 mountPoints = [] volumesFrom = [] } ] port = 9090 consul_server_hosts = \"<address of the Consul server>\" } \nNote the following differences:\nThe execution_role_arn and task_role_arn fields are removed. The mesh-task module creates the task and execution roles by default. If you need to use existing IAM roles, set the task_role and execution_role fields to pass in existing roles.\nThe port field specifies the port that your application listens on. If your application has no listening port, set outbound_only = true and remove the port field.\nThe jsonencode() function is removed from the container_definitions field.\nThe mesh-task module creates a new version of your task definition with the necessary dataplane containers so you can delete your existing aws_ecs_task_definition resource."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.18.x/release-notes/consul-ecs/v0_7_x",
  "text": "0.7.x | Consul | HashiCorp Developer\nConsul Dataplane: Consul on ECS 0.7.x adopts the Dataplane architecture to simplify connecting your ECS workloads to Consul. Refer to the documentation to learn more about the updated ECS components and how to deploy Consul to ECS using the Terraform module.\nNew control-plane command: The new, unified control-plane command combines the capabilities for the deprecated mesh-init and health-sync commands. The control-plane command starts a long running process with the following responsibilities:\nAutomatically (re)discover and (re)connect to Consul servers using connection manager.\nMake an ACL Login request to obtain an ACL token when using the Consul AWS IAM auth method.\nRegister the service and sidecar proxy with the central catalog on the Consul servers.\nWrite the configuration for Consul Dataplane to a file on a shared volume.\nSync ECS health check statuses for the ECS task into the central catalog on the Consul servers on a periodic basis.\nGracefully shutdown when an ECS task is stopped. Upon receiving a SIGTERM, mark synced health checks critical and wait for Consul Dataplane to stop. Then remove health checks, services, and perform an ACL Logout if necessary.\nNew controller command: The new controller command replaces the acl-controller command with the following changes:\nRemove all CLI flags. Configuration is read from the ECS_CONFIG_JSON environment variable.\nAutomatically (re)discover and (re)connect to Consul servers, similar to the control-plane command.\nBecause Consul client agents are no longer used, the controller no longer configures the \"client\" auth method, policy, role, and binding rule which previously enabled Consul client agents to login.\nRegister the ECS cluster as a synthetic node in the central catalog on the Consul servers. The synthetic node is used to register services running in the ECS cluster.\nEnsure leftover tokens and services are removed for ECS tasks that have stopped.\nLocality aware routing (Enterprise): Consul on ECS 0.7.x supports locality-aware routing. In your ECS task meta JSON, set the AWS_REGION container environment variable and AvailabilityZone attributes to set the locality parameters in Consul service and proxy registrations. Consul uses these parameters to perform locality aware routing in Consul Enterprise installations.\nThe new Dataplane architecture comes with the following breaking changes to configuring Consul on ECS. Refer to the Upgrade to Consul dataplane architecture documentation for a step-by-step upgrade guide.\nConsul client agents are no longer used.\nConsul Dataplane must be run in place of Envoy in each ECS task. Consul Dataplane manages the Envoy process and proxies xDS requests from Envoy to Consul servers.\nThe consul-ecs binary now communicates with Consul servers using HTTP(S) and GRPC.\nServices are registered directly with the central catalog on the Consul servers. Services in the same ECS cluster are registered to the same Consul node name.\nReplaced the mesh-init and health-sync commands with a unified control-plane.\nReplaced the acl-controller command with controller.\nAdd the go-discover binary to the Consul ECS image to better support cloud auto-join.\nChanges to ECS_CONFIG_JSON schema.\nRemove the consulHTTPAddr and consulCACertFile fields.\nAdd the consulLogin.datacenter field.\nAdd the controller field to support configuring the new controller command.\nAdd the consulServers field to specify the Consul server location and protocol-specific settings.\nThe consulServers.hosts field is required. This specifies the Consul server location as an IP address, DNS name, or exec= string specifying a command that returns a list of IP addresses. To use cloud auto-join, use an exec= string to run the discover CLI. For example, the following string invokes the discover CLI with a cloud auto-join string: \nexec=discover -q addrs provider=aws region=us-west-2 tag_key=consul-server tag_value=true \nBy default, Consul ECS and Consul Dataplane images include the discover CLI.\nAdd the proxy.healthCheckPort field which can be hit to determine Envoy's readiness.\nAdd the proxy.upstreams.destinationPeer field to enable the proxy to hit upstreams present in peer Consul clusters.\nAdd the meshGateway.healthCheckPort field which can be hit to determine Envoy's readiness.\nAdd the proxy.localServiceAddress field to configure Envoy to use a different address for the local service.\nRemove the service.checks field. Consul agent health checks are no longer supported because Consul client agents are not used. Instead, set the healthSyncContainers field to have consul-ecs sync ECS health checks into Consul.\nConsul: 1.17.x\nThe changelogs for this major release version and any maintenance versions are listed below.\nNote: These links will take you to the changelogs on the GitHub website.\n0.7.0"
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.18.x/acl/policy/delete",
  "text": "Commands: ACL Policy Delete | Consul\nCommand: consul acl policy delete\nCorresponding HTTP API Endpoint: [DELETE] /v1/acl/policy/:id\nThe acl policy delete command deletes a policy. Policies may be deleted by their ID or by name.\nThe table below shows this command's required ACLs. Configuration of blocking queries and agent caching are not supported from commands, but may be from the corresponding HTTP endpoint.\nUsage: consul acl policy delete [options]\n-id=<string> - The ID of the policy to delete. It may be specified as a unique ID prefix but will error if the prefix matches multiple policy IDs.\n-name=<string> - The Name of the policy to delete.\nEnterprise Options\n-partition=<string> - Specifies the partition to query. If not provided, the partition will be inferred from the request's ACL token, or will default to the default partition. Partitions are a Consul Enterprise feature added in v1.11.0.\n-namespace=<string> - Specifies the namespace to query. If not provided, the namespace will be inferred from the request's ACL token, or will default to the default namespace. Namespaces are a Consul Enterprise feature added in v1.7.0.\nDelete a policy:\n$ consul acl policy delete -id 35b8 Policy \"35b8ecb0-707c-ee18-2002-81b238b54b38\" deleted successfully \nDelete a policy by name:\n$ consul acl policy delete -name acl-replication Policy \"35b8ecb0-707c-ee18-2002-81b238b54b38\" deleted successfully"
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.18.x/acl/role/delete",
  "text": "Commands: ACL Role Delete | Consul\nCommand: consul acl role delete\nCorresponding HTTP API Endpoint: [DELETE] /v1/acl/role/:id\nThe acl role delete command deletes a role. Roles may be deleted by their ID or by name.\nThe table below shows this command's required ACLs. Configuration of blocking queries and agent caching are not supported from commands, but may be from the corresponding HTTP endpoint.\nUsage: consul acl role delete [options]\n-id=<string> - The ID of the role to delete. It may be specified as a unique ID prefix but will error if the prefix matches multiple role IDs.\n-name=<string> - The Name of the role to delete.\nEnterprise Options\n-partition=<string> - Specifies the partition to query. If not provided, the partition will be inferred from the request's ACL token, or will default to the default partition. Partitions are a Consul Enterprise feature added in v1.11.0.\n-namespace=<string> - Specifies the namespace to query. If not provided, the namespace will be inferred from the request's ACL token, or will default to the default namespace. Namespaces are a Consul Enterprise feature added in v1.7.0.\nDelete a role by prefix:\n$ consul acl role delete -id 57147 Role \"57147d87-6bf7-f794-1a6e-7d038c4e4ae9\" deleted successfully \nDelete a role by name:\n$ consul acl role delete -name crawler Role \"a365fdc9-ac71-e754-0645-7ab6bd747301\" deleted successfully"
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.13.x/peering",
  "text": "Usage: consul peering <subcommand> [options] # ... Subcommands: delete Close and delete a peering connection establish Consume a peering token and establish a connection with the accepting cluster generate-token Generate a peering token for use by a dialing cluster list List the local cluster's peering connections read Read detailed information on a peering connection "
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.18.x/ecs/reference/config-json-schema",
  "text": "This page does not exist for version v1.18.x."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/agent/dns",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.18.x/acl/auth-method/delete",
  "text": "Commands: ACL Auth Method Delete | Consul\nCommand: consul acl auth-method delete\nCorresponding HTTP API Endpoint: [DELETE] /v1/acl/auth-method/:name\nThe acl auth-method delete command deletes an auth method.\nThe table below shows this command's required ACLs. Configuration of blocking queries and agent caching are not supported from commands, but may be from the corresponding HTTP endpoint.\nUsage: consul acl auth-method delete [options]\n-name=<string> - The Name of the auth method to delete.\nEnterprise Options\n-partition=<string> - Specifies the partition to query. If not provided, the partition will be inferred from the request's ACL token, or will default to the default partition. Partitions are a Consul Enterprise feature added in v1.11.0.\n-namespace=<string> - Specifies the namespace to query. If not provided, the namespace will be inferred from the request's ACL token, or will default to the default namespace. Namespaces are a Consul Enterprise feature added in v1.7.0.\nDelete an auth method:\n$ consul acl auth-method delete -name minikube Auth-method \"minikube\" deleted successfully"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.17.x/ecs/deploy/migrate-existing-tasks",
  "text": "Migrate existing tasks to Consul service mesh | Consul\nMigrate existing tasks to Consul on ECS with Terraform\nTo migrate existing tasks to Consul, rewrite the existing Terraform code for your tasks so that the container definitions include the mesh-task Terraform module. \nYour tasks must already be defined in Terraform using the ecs_task_definition resource so that they can then be converted to use the mesh-task module.\nThe following example shows an existing task definition configured in Terraform:\nresource \"aws_ecs_task_definition\" \"my_task\" { family = \"my_task\" requires_compatibilities = [\"FARGATE\"] network_mode = \"awsvpc\" cpu = 256 memory = 512 execution_role_arn = \"arn:aws:iam::111111111111:role/execution-role\" task_role_arn = \"arn:aws:iam::111111111111:role/task-role\" container_definitions = jsonencode( [{ name = \"example-client-app\" image = \"docker.io/org/my_task:v0.0.1\" essential = true portMappings = [ { containerPort = 9090 hostPort = 9090 protocol = \"tcp\" } ] cpu = 0 mountPoints = [] volumesFrom = [] }] ) } resource \"aws_ecs_service\" \"my_task\" { name = \"my_task\" cluster = \"arn:aws:ecs:us-east-1:111111111111:cluster/my-cluster\" task_definition = aws_ecs_task_definition.my_task.arn desired_count = 1 network_configuration { subnets = [\"subnet-abc123\"] } launch_type = \"FARGATE\" } \nReplace the aws_ecs_task_definition resource with the mesh-task module so that Consul adds the necessary dataplane containers that enable your task to join the mesh. The mesh-task module uses inputs similar to your old ECS task definition but creates a new version of the task definition with additional containers.\nThe following Terraform configuration uses the mesh-task module to replace the previous example's task definition:\nmodule \"my_task\" { source = \"hashicorp/consul-ecs/aws//modules/mesh-task\" version = \"<latest version>\" family = \"my_task\" container_definitions = [ { name = \"example-client-app\" image = \"docker.io/org/my_task:v0.0.1\" essential = true portMappings = [ { containerPort = 9090 hostPort = 9090 protocol = \"tcp\" } ] cpu = 0 mountPoints = [] volumesFrom = [] } ] port = 9090 consul_server_hosts = \"<address of the Consul server>\" } \nNote the following differences:\nThe execution_role_arn and task_role_arn fields are removed. The mesh-task module creates the task and execution roles by default. If you need to use existing IAM roles, set the task_role and execution_role fields to pass in existing roles.\nThe port field specifes the port that your application listens on. If your application has no listening port, set outbound_only = true and remove the port field.\nThe jsonencode() function is removed from the container_definitions field.\nThe mesh-task module creates a new version of your task definition with the necessary dataplane containers so you can delete your existing aws_ecs_task_definition resource."
},
{
  "url": "https://developer.hashicorp.com/consul/commands/peering/exported-services",
  "text": "Commands: Peering Exported Services | Consul\nCommand: consul peering exported-services\nCorresponding HTTP API Endpoint: [GET] /v1/peering/:name\nThe peering exported-services command displays all of the services that were exported to the cluster peer using an exported-services configuration entry.\nThe table below shows this command's required ACLs.\nUsage: consul peering exported-services [options] -name <peer name>\n-name=<string> - (Required) The name of the peer associated with a connection.\n-format={pretty|json} - Command output format. The default value is pretty.\nEnterprise Options\n-partition=<string> - Enterprise Specifies the partition to query. If not provided, the partition is inferred from the request's ACL token, or defaults to the default partition. \nThe following example outputs the exported services to a peering connection locally referred to as \"cluster-02\":\n$ consul peering exported-services -name cluster-02 backend frontend web"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.11.x/connect/gateways/mesh-gateway/service-to-service-traffic-peers",
  "text": "Mesh Gateways between Peered Clusters | Consul\nCluster peering is currently in beta: Functionality associated with cluster peering is subject to change. You should never use the beta release in secure environments or production scenarios. Features in beta may have performance issues, scaling issues, and limited support.\nMesh gateways are required for you to route service mesh traffic between different Consul clusters. Clusters can reside in different clouds or runtime environments where general interconnectivity between all services in all clusters is not feasible.\nUnlike mesh gateways for datacenters and partitions, mesh gateways for cluster peering decrypt data to HTTP services within the mTLS session. Data must be decrypted in order to evaluate and apply dynamic routing rules at the destination cluster, which reduces coupling between peers.\nTo configure mesh gateways for cluster peering, make sure your Consul environment meets the following requirements:\nConsul version 1.13.0 or newer.\nA local Consul agent is required to manage mesh gateway configuration.\nEnable Consul service mesh in all clusters.\nEnable peering on all Consul servers.\nUse Envoy proxies. Envoy is the only proxy with mesh gateway capabilities in Consul.\nConfigure the following settings to register and use the mesh gateway as a service in Consul.\nGateway registration\nSpecify mesh-gateway in the kind field to register the gateway with Consul.\nDefine the Proxy.Config settings using opaque parameters compatible with your proxy. For Envoy, refer to the Gateway Options and Escape-hatch Overrides documentation for additional configuration information.\nAlternatively, you can also use the CLI to spin up and register a gateway in Consul. For additional information, refer to the consul connect envoy command.\nSidecar registration\nConfigure the proxy.upstreams parameters to route traffic to the correct service, namespace, and peer. Refer to the upstreams documentation for details.\nThe service proxy.upstreams.destination_name is always required.\nThe proxy.upstreams.destination_peer must be configured to enable cross-cluster traffic.\nThe proxy.upstream/destination_namespace configuration is only necessary if the destination service is in a non-default namespace.\nService exports\nInclude the exported-services configuration entry to enable Consul to export services contained in a cluster to one or more additional clusters. For additional information, refer to the Exported Services documentation.\nACL configuration\nIf ACLs are enabled, you must add a token granting service:write for the gateway's service name and service:read for all services in the Enterprise admin partition or OSS datacenter to the gateway's service definition. These permissions authorize the token to route communications for other Consul service mesh services.\nModes\nModes are not configurable for mesh gateways that connect peered clusters. By default, all proxies connecting to peered clusters use mesh gateways in remote mode."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.11.x/connect/cluster-peering/create-manage-peering",
  "text": "Cluster Peering - Create and Manage Connections | Consul\nCluster peering is currently in beta: Functionality associated with cluster peering is subject to change. You should never use the beta release in secure environments or production scenarios. Features in beta may have performance issues, scaling issues, and limited support.\nCluster peering is not currently available in the HCP Consul offering.\nA peering token enables cluster peering between different datacenters. Once you generate a peering token, you can use it to establish a connection between clusters. Then you can export services and create intentions so that peered clusters can call those services.\nCluster peering is not enabled by default on Consul servers. To peer clusters, you must first configure all Consul servers so that peering is enabled. For additional information, refer to Configuration Files.\nAfter enabling peering for all Consul servers, complete the following steps in order:\nCreate a peering token\nEstablish a connection between clusters\nExport services between clusters\nAuthorize services for peers\nYou can generate peering tokens and initiate connections on any available agent using either the API or the Consul UI. If you use the API, we recommend performing these operations through a client agent in the partition you want to connect.\nThe UI does not currently support exporting services between clusters or authorizing services for peers.\nTo begin the cluster peering process, generate a peering token in one of your clusters. The other cluster uses this token to establish the peering connection.\nEvery time you generate a peering token, a single-use establishment secret is embedded in the token. Because regenerating a peering token invalidates the previously generated secret, you must use the most recently created token to establish peering connections.\nIn cluster-01, use the /peering/token endpoint to issue a request for a peering token.\n$ curl --request POST --data '{\"PeerName\":\"cluster-02\"}' --url http://localhost:8500/v1/peering/token \nThe CLI outputs the peering token, which is a base64-encoded string containing the token details.\nCreate a JSON file that contains the first cluster's name and the peering token.\npeering_token.json\n{ \"PeerName\": \"cluster-01\", \"PeeringToken\": \"eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImF1ZCI6IlNvbHIifQ.5T7L_L1MPfQ_5FjKGa1fTPqrzwK4bNSM812nW6oyjb8\" } \nNext, use the peering token to establish a secure connection between the clusters.\nIn one of the client agents in \"cluster-02,\" use peering_token.json and the /peering/establish endpoint to establish the peering connection. This endpoint does not generate an output unless there is an error.\n$ curl --request POST --data @peering_token.json http://127.0.0.1:8500/v1/peering/establish \nWhen you connect server agents through cluster peering, they peer their default partitions. To establish peering connections for other partitions through server agents, you must add the Partition field to peering_token.json and specify the partitions you want to peer. For additional configuration information, refer to Cluster Peering - HTTP API.\nYou can dial the peering/establish endpoint once per peering token. Peering tokens cannot be reused after being used to establish a connection. If you need to re-establish a connection, you must generate a new peering token.\nExport services between clusters\nAfter you establish a connection between the clusters, you need to create a configuration entry that defines the services that are available for other clusters. Consul uses this configuration entry to advertise service information and support service mesh connections across clusters.\nFirst, create a configuration entry and specify the Kind as \"exported-services\".\npeering-config.hcl\nKind = \"exported-services\" Name = \"default\" Services = [ { ## The name and namespace of the service to export. Name = \"service-name\" Namespace = \"default\" ## The list of peer clusters to export the service to. Consumers = [ { ## The peer name to reference in config is the one set ## during the peering process. PeerName = \"cluster-02\" } ] } ] \n$ consul config write peering-config.hcl \nBefore you proceed, wait for the clusters to sync and make services available to their peers. You can issue an endpoint query to check the peered cluster status.\nBefore you can call services from peered clusters, you must set service intentions that authorize those clusters to use specific services. Consul prevents services from being exported to unauthorized clusters.\nFirst, create a configuration entry and specify the Kind as \"service-intentions\". Declare the service on \"cluster-02\" that can access the service in \"cluster-01.\" The following example sets service intentions so that \"frontend-service\" can access \"backend-service.\"\npeering-intentions.hcl\nKind = \"service-intentions\" Name = \"backend-service\" Sources = [ { Name = \"frontend-service\" Peer = \"cluster-02\" Action = \"allow\" } ] \nIf the peer's name is not specified in Peer, then Consul assumes that the service is in the local cluster.\n$ consul config write peering-intentions.hcl \nAfter you establish a peering connection, you can get a list of all active peering connections, read a specific peering connection's information, check peering connection health, and delete peering connections.\nList all peering connections\nYou can list all active peering connections in a cluster.\nAfter you establish a peering connection, query the /peerings/ endpoint to get a list of all peering connections. For example, the following command requests a list of all peering connections on localhost and returns the information as a series of JSON objects:\n$ curl http://127.0.0.1:8500/v1/peerings [ { \"ID\": \"462c45e8-018e-f19d-85eb-1fc1bcc2ef12\", \"Name\": \"cluster-02\", \"State\": \"ACTIVE\", \"Partition\": \"default\", \"PeerID\": \"e83a315c-027e-bcb1-7c0c-a46650904a05\", \"PeerServerName\": \"server.dc1.consul\", \"PeerServerAddresses\": [ \"10.0.0.1:8300\" ], \"CreateIndex\": 89, \"ModifyIndex\": 89 }, { \"ID\": \"1460ada9-26d2-f30d-3359-2968aa7dc47d\", \"Name\": \"cluster-03\", \"State\": \"INITIAL\", \"Partition\": \"default\", \"Meta\": { \"env\": \"production\" }, \"CreateIndex\": 109, \"ModifyIndex\": 119 }, ] \nRead a peering connection\nYou can get information about individual peering connections between clusters.\nAfter you establish a peering connection, query the /peering/ endpoint to get peering information about for a specific cluster. For example, the following command requests peering connection information for \"cluster-02\" and returns the info as a JSON object:\n$ curl http://127.0.0.1:8500/v1/peering/cluster-02 { \"ID\": \"462c45e8-018e-f19d-85eb-1fc1bcc2ef12\", \"Name\": \"cluster-02\", \"State\": \"INITIAL\", \"PeerID\": \"e83a315c-027e-bcb1-7c0c-a46650904a05\", \"PeerServerName\": \"server.dc1.consul\", \"PeerServerAddresses\": [ \"10.0.0.1:8300\" ], \"CreateIndex\": 89, \"ModifyIndex\": 89 } \nCheck peering connection health\nYou can check the status of your peering connection to perform health checks.\nTo confirm that the peering connection between your clusters remains healthy, query the health/service endpoint of one cluster from the other cluster. For example, in \"cluster-02,\" query the endpoint and add the peer=cluster-01 query parameter to the end of the URL.\n$ curl \\ \"http://127.0.0.1:8500/v1/health/service/<service-name>?peer=cluster-01\" \nA successful query includes service information in the output.\nDelete peering connections\nYou can disconnect the peered clusters by deleting their connection. Deleting a peering connection stops data replication to the peer and deletes imported data, including services and CA certificates.\nIn \"cluster-01,\" request the deletion through the /peering/ endpoint.\n$ curl --request DELETE http://127.0.0.1:8500/v1/peering/cluster-02"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/release-notes/consul-ecs/v0_5_x",
  "text": "0.5.x | Consul | HashiCorp Developer\nAudit Logging (Enterprise) : Consul on ECS now captures authentication events and processes them with the HTTP API. Audit logging provides insight into access and usage patterns. Refer to Audit Logging for usage information.\nAWS IAM Auth Method : This feature provides support for Consul's AWS IAM auth method. This allows AWS IAM roles and users to authenticate with Consul to obtain ACL tokens. Refer to ECS Configuration Reference for configuration information.\nMesh Gateways : This feature introduces support for running mesh gateways as ECS tasks. Mesh gateways enable service mesh communication across datacenter and admin partition boundaries. Refer to ECS Installation with Terraform for usage information.\nConsul: 1.12.x\nThe changelogs for this major release version and any maintenance versions are listed below.\nNote: These links will take you to the changelogs on the GitHub website.\n0.5.1\n0.5.0"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/release-notes/consul-ecs/v0_3_x",
  "text": "0.3.x | Consul | HashiCorp Developer\nSupport file-based configuration : All CLI flags and options can be passed via a JSON file. This feature also enables manual install of Consul on ECS.\nConsul: 1.10.x, 1.11.x\nThe changelogs for this Major release version and any Maintenance versions, are listed below.\nNote: These links will take you to the changelogs on the GitHub website.\n0.3.0"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/release-notes/consul-ecs/v0_2_x",
  "text": "0.2.x | Consul | HashiCorp Developer\nSupport Secure Deployment on ECS : Enable gossip encryption and TLS for the Consul service mesh control plane. Add new ACL controller module and enable ACLs for other components.\nEnable Consul Native Health Checks : Add support for both Consul native and ECS health checks. Added health-sync container to default to either of the health checks based on if checks are defined and containers are essential.\nConsul 1.10.x\nThe changelogs for this major release version and any maintenance versions are listed below.\nNote: These links will take you to the changelogs on the GitHub website.\n0.2.0"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/proxies/deploy-sidecar-services",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.17.x/ecs/reference/architecture",
  "text": "This page does not exist for version v1.17.x."
},
{
  "url": "https://developer.hashicorp.com/consul/docs/release-notes/consul-ecs/v0_4_x",
  "text": "0.4.x | Consul | HashiCorp Developer\nAdmin Partitions (Enterprise) : This feature supports various deployment patterns that enable multiple ECS clusters from different tenants to share a single Consul control plane.\nNamespaces (Enterprise) : This feature enables autonomy across teams by allowing creation of isolated environments in shared clusters.\nConsul: 1.10.x, 1.11.x, 1.12.x\nThe changelogs for this major release version and any maintenance versions are listed below.\n0.4.1\n0.4.0"
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.17.x/acl/policy/delete",
  "text": "Commands: ACL Policy Delete | Consul\nCommand: consul acl policy delete\nCorresponding HTTP API Endpoint: [DELETE] /v1/acl/policy/:id\nThe acl policy delete command deletes a policy. Policies may be deleted by their ID or by name.\nThe table below shows this command's required ACLs. Configuration of blocking queries and agent caching are not supported from commands, but may be from the corresponding HTTP endpoint.\nUsage: consul acl policy delete [options]\n-id=<string> - The ID of the policy to delete. It may be specified as a unique ID prefix but will error if the prefix matches multiple policy IDs.\n-name=<string> - The Name of the policy to delete.\n-partition=<string> - Specifies the partition to query. If not provided, the partition will be inferred from the request's ACL token, or will default to the default partition. Partitions are a Consul Enterprise feature added in v1.11.0.\n-namespace=<string> - Specifies the namespace to query. If not provided, the namespace will be inferred from the request's ACL token, or will default to the default namespace. Namespaces are a Consul Enterprise feature added in v1.7.0.\nDelete a policy:\n$ consul acl policy delete -id 35b8 Policy \"35b8ecb0-707c-ee18-2002-81b238b54b38\" deleted successfully \nDelete a policy by name:\n$ consul acl policy delete -name acl-replication Policy \"35b8ecb0-707c-ee18-2002-81b238b54b38\" deleted successfully"
},
{
  "url": "https://developer.hashicorp.com/consul/docs/v1.17.x/release-notes/consul-ecs/v0_7_x",
  "text": "0.7.x | Consul | HashiCorp Developer\nConsul Dataplane: Consul on ECS 0.7.x adopts the Dataplane architecture to simplify connecting your ECS workloads to Consul. Refer to the documentation to learn more about the updated ECS components and how to deploy Consul to ECS using the Terraform module.\nNew control-plane command: The new, unified control-plane command combines the capabilities for the deprecated mesh-init and health-sync commands. The control-plane command starts a long running process with the following responsibilities:\nAutomatically (re)discover and (re)connect to Consul servers using connection manager.\nMake an ACL Login request to obtain an ACL token when using the Consul AWS IAM auth method.\nRegister the service and sidecar proxy with the central catalog on the Consul servers.\nWrite the configuration for Consul Dataplane to a file on a shared volume.\nSync ECS health check statuses for the ECS task into the central catalog on the Consul servers on a periodic basis.\nGracefully shutdown when an ECS task is stopped. Upon receiving a SIGTERM, mark synced health checks critical and wait for Consul Dataplane to stop. Then remove health checks, services, and perform an ACL Logout if necessary.\nNew controller command: The new controller command replaces the acl-controller command with the following changes:\nRemove all CLI flags. Configuration is read from the ECS_CONFIG_JSON environment variable.\nAutomatically (re)discover and (re)connect to Consul servers, similar to the control-plane command.\nBecause Consul client agents are no longer used, the controller no longer configures the \"client\" auth method, policy, role, and binding rule which previously enabled Consul client agents to login.\nRegister the ECS cluster as a synthetic node in the central catalog on the Consul servers. The synthetic node is used to register services running in the ECS cluster.\nEnsure leftover tokens and services are removed for ECS tasks that have stopped.\nLocality aware routing (Enterprise): Consul on ECS 0.7.x supports locality-aware routing. In your ECS task meta JSON, set the AWS_REGION container environment variable and AvailabilityZone attributes to set the locality parameters in Consul service and proxy registrations. Consul uses these parameters to perform locality aware routing in Consul Enterprise installations.\nThe new Dataplane architecture comes with the following breaking changes to configuring Consul on ECS. Refer to the Upgrade to Consul dataplane architecture documentation for a step-by-step upgrade guide.\nConsul client agents are no longer used.\nConsul Dataplane must be run in place of Envoy in each ECS task. Consul Dataplane manages the Envoy process and proxies xDS requests from Envoy to Consul servers.\nThe consul-ecs binary now communicates with Consul servers using HTTP(S) and GRPC.\nServices are registered directly with the central catalog on the Consul servers. Services in the same ECS cluster are registered to the same Consul node name.\nReplaced the mesh-init and health-sync commands with a unified control-plane.\nReplaced the acl-controller command with controller.\nAdd the go-discover binary to the Consul ECS image to better support cloud auto-join.\nChanges to ECS_CONFIG_JSON schema.\nRemove the consulHTTPAddr and consulCACertFile fields.\nAdd the consulLogin.datacenter field.\nAdd the controller field to support configuring the new controller command.\nAdd the consulServers field to specify the Consul server location and protocol-specific settings.\nThe consulServers.hosts field is required. This specifies the Consul server location as an IP address, DNS name, or exec= string specifying a command that returns a list of IP addresses. To use cloud auto-join, use an exec= string to run the discover CLI. For example, the following string invokes the discover CLI with a cloud auto-join string: \nexec=discover -q addrs provider=aws region=us-west-2 tag_key=consul-server tag_value=true \nBy default, Consul ECS and Consul Dataplane images include the discover CLI.\nAdd the proxy.healthCheckPort field which can be hit to determine Envoy's readiness.\nAdd the proxy.upstreams.destinationPeer field to enable the proxy to hit upstreams present in peer Consul clusters.\nAdd the meshGateway.healthCheckPort field which can be hit to determine Envoy's readiness.\nAdd the proxy.localServiceAddress field to configure Envoy to use a different address for the local service.\nRemove the service.checks field. Consul agent health checks are no longer supported because Consul client agents are not used. Instead, set the healthSyncContainers field to have consul-ecs sync ECS health checks into Consul.\nConsul: 1.17.x\n0.7.0"
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.17.x/acl/auth-method/delete",
  "text": "Commands: ACL Auth Method Delete | Consul\nCommand: consul acl auth-method delete\nCorresponding HTTP API Endpoint: [DELETE] /v1/acl/auth-method/:name\nThe acl auth-method delete command deletes an auth method.\nUsage: consul acl auth-method delete [options]\n-name=<string> - The Name of the auth method to delete.\nDelete an auth method:\n$ consul acl auth-method delete -name minikube Auth-method \"minikube\" deleted successfully"
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.17.x/acl/role/delete",
  "text": "Commands: ACL Role Delete | Consul\nCommand: consul acl role delete\nCorresponding HTTP API Endpoint: [DELETE] /v1/acl/role/:id\nThe acl role delete command deletes a role. Roles may be deleted by their ID or by name.\nUsage: consul acl role delete [options]\n-id=<string> - The ID of the role to delete. It may be specified as a unique ID prefix but will error if the prefix matches multiple role IDs.\n-name=<string> - The Name of the role to delete.\nDelete a role by prefix:\n$ consul acl role delete -id 57147 Role \"57147d87-6bf7-f794-1a6e-7d038c4e4ae9\" deleted successfully \nDelete a role by name:\n$ consul acl role delete -name crawler Role \"a365fdc9-ac71-e754-0645-7ab6bd747301\" deleted successfully"
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.16.x/acl/policy/delete",
  "text": "Commands: ACL Policy Delete | Consul\nCommand: consul acl policy delete\nCorresponding HTTP API Endpoint: [DELETE] /v1/acl/policy/:id\nThe acl policy delete command deletes a policy. Policies may be deleted by their ID or by name.\nUsage: consul acl policy delete [options]\n-id=<string> - The ID of the policy to delete. It may be specified as a unique ID prefix but will error if the prefix matches multiple policy IDs.\n-name=<string> - The Name of the policy to delete.\nDelete a policy:\n$ consul acl policy delete -id 35b8 Policy \"35b8ecb0-707c-ee18-2002-81b238b54b38\" deleted successfully \nDelete a policy by name:\n$ consul acl policy delete -name acl-replication Policy \"35b8ecb0-707c-ee18-2002-81b238b54b38\" deleted successfully"
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.14.x/acl/policy/delete",
  "text": "Commands: ACL Policy Delete | Consul\nCommand: consul acl policy delete\nCorresponding HTTP API Endpoint: [DELETE] /v1/acl/policy/:id\nThe acl policy delete command deletes a policy. Policies may be deleted by their ID or by name.\nUsage: consul acl policy delete [options]\n-id=<string> - The ID of the policy to delete. It may be specified as a unique ID prefix but will error if the prefix matches multiple policy IDs.\n-name=<string> - The Name of the policy to delete.\nDelete a policy:\n$ consul acl policy delete -id 35b8 Policy \"35b8ecb0-707c-ee18-2002-81b238b54b38\" deleted successfully \nDelete a policy by name:\n$ consul acl policy delete -name acl-replication Policy \"35b8ecb0-707c-ee18-2002-81b238b54b38\" deleted successfully"
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.15.x/acl/policy/delete",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.13.x/acl/policy/delete",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.12.x/acl/policy/delete",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.11.x/acl/policy/delete",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.10.x/acl/policy/delete",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.9.x/acl/policy/delete",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.8.x/acl/policy/delete",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.16.x/acl/role/delete",
  "text": "Commands: ACL Role Delete | Consul\nCommand: consul acl role delete\nCorresponding HTTP API Endpoint: [DELETE] /v1/acl/role/:id\nThe acl role delete command deletes a role. Roles may be deleted by their ID or by name.\nUsage: consul acl role delete [options]\n-id=<string> - The ID of the role to delete. It may be specified as a unique ID prefix but will error if the prefix matches multiple role IDs.\n-name=<string> - The Name of the role to delete.\nDelete a role by prefix:\n$ consul acl role delete -id 57147 Role \"57147d87-6bf7-f794-1a6e-7d038c4e4ae9\" deleted successfully \nDelete a role by name:\n$ consul acl role delete -name crawler Role \"a365fdc9-ac71-e754-0645-7ab6bd747301\" deleted successfully"
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.15.x/acl/role/delete",
  "text": "Commands: ACL Role Delete | Consul\nCommand: consul acl role delete\nCorresponding HTTP API Endpoint: [DELETE] /v1/acl/role/:id\nThe acl role delete command deletes a role. Roles may be deleted by their ID or by name.\nUsage: consul acl role delete [options]\n-id=<string> - The ID of the role to delete. It may be specified as a unique ID prefix but will error if the prefix matches multiple role IDs.\n-name=<string> - The Name of the role to delete.\nDelete a role by prefix:\n$ consul acl role delete -id 57147 Role \"57147d87-6bf7-f794-1a6e-7d038c4e4ae9\" deleted successfully \nDelete a role by name:\n$ consul acl role delete -name crawler Role \"a365fdc9-ac71-e754-0645-7ab6bd747301\" deleted successfully"
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.14.x/acl/role/delete",
  "text": ""
},
{
  "url": "https://developer.hashicorp.com/consul/commands/v1.13.x/acl/role/delete",
  "text": ""
}]